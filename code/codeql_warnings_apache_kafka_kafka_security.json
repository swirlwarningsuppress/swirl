[
    {
        "url": "apache/kafka/blob/main/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/Crypto.java#L66",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.connect.runtime.distributed;\n\nimport java.security.NoSuchAlgorithmException;\n\nimport javax.crypto.KeyGenerator;\nimport javax.crypto.Mac;\n\n/**\n * An interface to allow the dependency injection of {@link Mac} and {@link KeyGenerator} instances for testing.\n * Implementations of this class should be thread-safe.\n */\npublic interface Crypto {\n\n    /**\n     * Use the system implementation for cryptography calls.\n     */\n    Crypto SYSTEM = new SystemCrypto();\n\n    /**\n     * Returns a {@code Mac} object that implements the\n     * specified MAC algorithm. See {@link Mac#getInstance(String)}.\n     *\n     * @param algorithm the standard name of the requested MAC algorithm.\n     * @return the new {@code Mac} object\n     * @throws NoSuchAlgorithmException if no {@code Provider} supports a\n     *         {@code MacSpi} implementation for the specified algorithm\n     */\n    Mac mac(String algorithm) throws NoSuchAlgorithmException;\n\n    /**\n     * Returns a {@code KeyGenerator} object that generates secret keys\n     * for the specified algorithm. See {@link KeyGenerator#getInstance(String)}.\n     *\n     * @param algorithm the standard name of the requested key algorithm.\n     * @return the new {@code KeyGenerator} object\n     * @throws NoSuchAlgorithmException if no {@code Provider} supports a\n     *         {@code KeyGeneratorSpi} implementation for the\n     *         specified algorithm\n     */\n    KeyGenerator keyGenerator(String algorithm) throws NoSuchAlgorithmException;\n\n    class SystemCrypto implements Crypto {\n        @Override\n        public Mac mac(String algorithm) throws NoSuchAlgorithmException {\n            return Mac.getInstance(algorithm);\n        }\n\n        @Override\n        public KeyGenerator keyGenerator(String algorithm) throws NoSuchAlgorithmException {\n            return KeyGenerator.getInstance(algorithm);\n        }\n    }\n\n}\n",
        "methodName": null,
        "exampleID": 2,
        "dataset": "codeql",
        "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/Crypto.java",
        "line": 66,
        "sink": "used.",
        "source": "-",
        "sourceLine": 66,
        "qualifier": "Cryptographic algorithm [AES/CBC/PKCS5Padding](1) is weak and should not be used.",
        "line_number": 66,
        "steps": [
            {
                "line": 44,
                "source": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "filepath": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "methodName": null,
                "exampleID": 3
            },
            {
                "line": 44,
                "source": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "filepath": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "methodName": null,
                "exampleID": 3
            },
            {
                "line": 44,
                "source": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "filepath": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "methodName": null,
                "exampleID": 3
            },
            {
                "line": 44,
                "source": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "filepath": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "methodName": null,
                "exampleID": 3
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/server-common/src/main/java/org/apache/kafka/security/EncryptingPasswordEncoder.java#L133",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.security;\n\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.config.types.Password;\nimport org.apache.kafka.server.util.Csv;\n\nimport java.nio.charset.StandardCharsets;\nimport java.security.GeneralSecurityException;\nimport java.security.NoSuchAlgorithmException;\nimport java.security.SecureRandom;\nimport java.security.spec.InvalidKeySpecException;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.stream.Collectors;\n\nimport javax.crypto.Cipher;\nimport javax.crypto.SecretKeyFactory;\nimport javax.crypto.spec.PBEKeySpec;\nimport javax.crypto.spec.SecretKeySpec;\n\n/**\n * Password encoder and decoder implementation. Encoded passwords are persisted as a CSV map\n * containing the encoded password in base64 and along with the properties used for encryption.\n */\npublic class EncryptingPasswordEncoder implements PasswordEncoder {\n\n    private final SecureRandom secureRandom = new SecureRandom();\n\n    private final Password secret;\n    private final String keyFactoryAlgorithm;\n    private final String cipherAlgorithm;\n    private final int keyLength;\n    private final int iterations;\n    private final CipherParamsEncoder cipherParamsEncoder;\n\n\n    /**\n     * @param secret The secret used for encoding and decoding\n     * @param keyFactoryAlgorithm  Key factory algorithm if configured. By default, PBKDF2WithHmacSHA512 is\n     *                             used if available, PBKDF2WithHmacSHA1 otherwise.\n     * @param cipherAlgorithm Cipher algorithm used for encoding.\n     * @param keyLength Key length used for encoding. This should be valid for the specified algorithms.\n     * @param iterations Iteration count used for encoding.\n     * The provided `keyFactoryAlgorithm`, `cipherAlgorithm`, `keyLength` and `iterations` are used for encoding passwords.\n     * The values used for encoding are stored along with the encoded password and the stored values are used for decoding.\n     */\n    public EncryptingPasswordEncoder(\n            Password secret,\n            String keyFactoryAlgorithm,\n            String cipherAlgorithm,\n            int keyLength,\n            int iterations) {\n        this.secret = secret;\n        this.keyFactoryAlgorithm = keyFactoryAlgorithm;\n        this.cipherAlgorithm = cipherAlgorithm;\n        this.keyLength = keyLength;\n        this.iterations = iterations;\n        this.cipherParamsEncoder = cipherParamsInstance(cipherAlgorithm);\n    }\n\n    @Override\n    public String encode(Password password) throws GeneralSecurityException {\n        byte[] salt = new byte[256];\n        secureRandom.nextBytes(salt);\n        Cipher cipher = Cipher.getInstance(cipherAlgorithm);\n        SecretKeyFactory keyFactory = secretKeyFactory(keyFactoryAlgorithm);\n        SecretKeySpec keySpec = secretKeySpec(keyFactory, cipherAlgorithm, keyLength, salt, iterations);\n        cipher.init(Cipher.ENCRYPT_MODE, keySpec);\n        byte[] encryptedPassword = cipher.doFinal(password.value().getBytes(StandardCharsets.UTF_8));\n        Map<String, String> encryptedMap = new HashMap<>();\n        encryptedMap.put(PasswordEncoder.KEY_FACTORY_ALGORITHM, keyFactory.getAlgorithm());\n        encryptedMap.put(PasswordEncoder.CIPHER_ALGORITHM, cipherAlgorithm);\n        encryptedMap.put(PasswordEncoder.KEY_LENGTH, String.valueOf(keyLength));\n        encryptedMap.put(PasswordEncoder.SALT, PasswordEncoder.base64Encode(salt));\n        encryptedMap.put(PasswordEncoder.ITERATIONS, String.valueOf(iterations));\n        encryptedMap.put(PasswordEncoder.ENCRYPTED_PASSWORD, PasswordEncoder.base64Encode(encryptedPassword));\n        encryptedMap.put(PasswordEncoder.PASSWORD_LENGTH, String.valueOf(password.value().length()));\n        encryptedMap.putAll(cipherParamsEncoder.toMap(cipher.getParameters()));\n\n        return encryptedMap.entrySet().stream()\n                .map(entry -> entry.getKey() + \":\" + entry.getValue())\n                .collect(Collectors.joining(\",\"));\n    }\n\n    @Override\n    public Password decode(String encodedPassword) throws GeneralSecurityException {\n        Map<String, String> params = Csv.parseCsvMap(encodedPassword);\n        String keyFactoryAlg = params.get(PasswordEncoder.KEY_FACTORY_ALGORITHM);\n        String cipherAlg = params.get(PasswordEncoder.CIPHER_ALGORITHM);\n        int keyLength = Integer.parseInt(params.get(PasswordEncoder.KEY_LENGTH));\n        byte[] salt = PasswordEncoder.base64Decode(params.get(PasswordEncoder.SALT));\n        int iterations = Integer.parseInt(params.get(PasswordEncoder.ITERATIONS));\n        byte[] encryptedPassword = PasswordEncoder.base64Decode(params.get(PasswordEncoder.ENCRYPTED_PASSWORD));\n        int passwordLengthProp = Integer.parseInt(params.get(PasswordEncoder.PASSWORD_LENGTH));\n        Cipher cipher = Cipher.getInstance(cipherAlg);\n        SecretKeyFactory keyFactory = secretKeyFactory(keyFactoryAlg);\n        SecretKeySpec keySpec = secretKeySpec(keyFactory, cipherAlg, keyLength, salt, iterations);\n        cipher.init(Cipher.DECRYPT_MODE, keySpec, cipherParamsEncoder.toParameterSpec(params));\n        try {\n            byte[] decrypted = cipher.doFinal(encryptedPassword);\n            String password = new String(decrypted, StandardCharsets.UTF_8);\n            if (password.length() != passwordLengthProp) // Sanity check\n                throw new ConfigException(\"Password could not be decoded, sanity check of length failed\");\n            return new Password(password);\n        } catch (Exception e) {\n            throw new ConfigException(\"Password could not be decoded\", e);\n        }\n    }\n\n    private SecretKeyFactory secretKeyFactory(String keyFactoryAlg) throws NoSuchAlgorithmException {\n        if (keyFactoryAlg != null) {\n            return SecretKeyFactory.getInstance(keyFactoryAlg);\n        } else {\n            try {\n                return SecretKeyFactory.getInstance(\"PBKDF2WithHmacSHA512\");\n            } catch (NoSuchAlgorithmException nsae) {\n                return SecretKeyFactory.getInstance(\"PBKDF2WithHmacSHA1\");\n            }\n        }\n    }\n\n    private SecretKeySpec secretKeySpec(SecretKeyFactory keyFactory,\n                                        String cipherAlg,\n                                        int keyLength,\n                                        byte[] salt,\n                                        int iterations) throws InvalidKeySpecException {\n        PBEKeySpec keySpec = new PBEKeySpec(secret.value().toCharArray(), salt, iterations, keyLength);\n        String algorithm = (cipherAlg.indexOf('/') > 0) ? cipherAlg.substring(0, cipherAlg.indexOf('/')) : cipherAlg;\n        return new SecretKeySpec(keyFactory.generateSecret(keySpec).getEncoded(), algorithm);\n    }\n\n    private CipherParamsEncoder cipherParamsInstance(String cipherAlgorithm) {\n        if (cipherAlgorithm.startsWith(\"AES/GCM/\")) {\n            return new GcmParamsEncoder();\n        } else {\n            return new IvParamsEncoder();\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 4,
        "dataset": "codeql",
        "filepath": "server-common/src/main/java/org/apache/kafka/security/EncryptingPasswordEncoder.java",
        "line": 133,
        "sink": "used.",
        "source": "-",
        "sourceLine": 133,
        "qualifier": "Cryptographic algorithm [PBKDF2WithHmacSHA1](1) is weak and should not be used.",
        "line_number": 133,
        "steps": []
    },
    {
        "url": "apache/kafka/blob/main/storage/src/main/java/org/apache/kafka/storage/internals/log/SkimpyOffsetMap.java#L86",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.storage.internals.log;\n\nimport org.apache.kafka.common.utils.ByteUtils;\nimport org.apache.kafka.common.utils.Utils;\n\nimport java.nio.ByteBuffer;\nimport java.security.DigestException;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.Arrays;\n\n/**\n * A hash table used for de-duplicating the log. This hash table uses a cryptographically secure hash of the key as a proxy for the key\n * for comparisons and to save space on object overhead. Collisions are resolved by probing. This hash table does not support deletes.\n */\npublic class SkimpyOffsetMap implements OffsetMap {\n\n    /**\n     * The number of bytes of space each entry uses (the number of bytes in the hash plus an 8 byte offset)\n     */\n    public final int bytesPerEntry;\n\n    private final ByteBuffer bytes;\n\n    /* the hash algorithm instance to use */\n    private final MessageDigest digest;\n\n    /* the number of bytes for this hash algorithm */\n    private final int hashSize;\n\n    /**\n     * The maximum number of entries this map can contain\n     */\n    private final int slots;\n\n    /* cache some hash buffers to avoid reallocating each time */\n    private final byte[] hash1;\n    private final byte[] hash2;\n\n    /* number of entries put into the map */\n    private int entries = 0;\n\n    /* number of lookups on the map */\n    private long lookups = 0L;\n\n    /* the number of probes for all lookups */\n    private long probes = 0L;\n\n    /* the latest offset written into the map */\n    private long lastOffset = -1L;\n\n    /**\n     * Create an instance of SkimplyOffsetMap with the default hash algorithm (MD5).\n     *\n     * @param memory The amount of memory this map can use\n     */\n    public SkimpyOffsetMap(int memory) throws NoSuchAlgorithmException {\n        this(memory, \"MD5\");\n    }\n\n    /**\n     * Create an instance of SkimpyOffsetMap.\n     *\n     * @param memory The amount of memory this map can use\n     * @param hashAlgorithm The hash algorithm instance to use: MD2, MD5, SHA-1, SHA-256, SHA-384, SHA-512\n     */\n    public SkimpyOffsetMap(int memory, String hashAlgorithm) throws NoSuchAlgorithmException {\n        this.bytes = ByteBuffer.allocate(memory);\n\n        this.digest = MessageDigest.getInstance(hashAlgorithm);\n\n        this.hashSize = digest.getDigestLength();\n        this.bytesPerEntry = hashSize + 8;\n        this.slots = memory / bytesPerEntry;\n\n        this.hash1 = new byte[hashSize];\n        this.hash2 = new byte[hashSize];\n    }\n\n    @Override\n    public int slots() {\n        return slots;\n    }\n\n    /**\n     * Get the offset associated with this key.\n     * @param key The key\n     * @return The offset associated with this key or -1 if the key is not found\n     */\n    @Override\n    public long get(ByteBuffer key) throws DigestException {\n        ++lookups;\n        hashInto(key, hash1);\n        // search for the hash of this key by repeated probing until we find the hash we are looking for or we find an empty slot\n        int attempt = 0;\n        //we need to guard against attempt integer overflow if the map is full\n        //limit attempt to number of slots once positionOf(..) enters linear search mode\n        int maxAttempts = slots + hashSize - 4;\n        do {\n            if (attempt >= maxAttempts)\n                return -1L;\n            int pos = positionOf(hash1, attempt);\n            bytes.position(pos);\n            if (isEmpty(pos))\n                return -1L;\n            bytes.get(hash2);\n            ++attempt;\n        } while (!Arrays.equals(hash1, hash2));\n        return bytes.getLong();\n    }\n\n    /**\n     * Associate this offset to the given key.\n     * @param key The key\n     * @param offset The offset\n     */\n    @Override\n    public void put(ByteBuffer key, long offset) throws DigestException {\n        if (entries >= slots)\n            throw new IllegalArgumentException(\"Attempted to add a new entry to a full offset map, \"\n                + \"entries: \" + entries + \", slots: \" + slots);\n\n        ++lookups;\n        hashInto(key, hash1);\n\n        // probe until we find the first empty slot\n        int attempt = 0;\n        int pos = positionOf(hash1, attempt);\n        while (!isEmpty(pos)) {\n            bytes.position(pos);\n            bytes.get(hash2);\n            if (Arrays.equals(hash1, hash2)) {\n                // we found an existing entry, overwrite it and return (size does not change)\n                bytes.putLong(offset);\n                lastOffset = offset;\n                return;\n            }\n            ++attempt;\n            pos = positionOf(hash1, attempt);\n        }\n\n        // found an empty slot, update it - size grows by 1\n        bytes.position(pos);\n        bytes.put(hash1);\n        bytes.putLong(offset);\n        lastOffset = offset;\n        ++entries;\n    }\n\n    @Override\n    public void updateLatestOffset(long offset) {\n        this.lastOffset = offset;\n    }\n\n    /**\n     * Change the salt used for key hashing making all existing keys unfindable.\n     */\n    @Override\n    public void clear() {\n        this.entries = 0;\n        this.lookups = 0L;\n        this.probes = 0L;\n        this.lastOffset = -1L;\n        Arrays.fill(bytes.array(), bytes.arrayOffset(), bytes.arrayOffset() + bytes.limit(), (byte) 0);\n    }\n\n    /**\n     * The number of entries put into the map (note that not all may remain)\n     */\n    @Override\n    public int size() {\n        return entries;\n    }\n\n    /**\n     * The latest offset put into the map\n     */\n    @Override\n    public long latestOffset() {\n        return lastOffset;\n    }\n\n    /**\n     * The rate of collisions in the lookups\n     */\n    // Visible for testing\n    public double collisionRate() {\n        return (this.probes - this.lookups) / (double) this.lookups;\n    }\n\n    /**\n     * Check that there is no entry at the given position\n     */\n    private boolean isEmpty(int position) {\n        return bytes.getLong(position) == 0\n            && bytes.getLong(position + 8) == 0\n            && bytes.getLong(position + 16) == 0;\n    }\n\n    /**\n     * Calculate the ith probe position. We first try reading successive integers from the hash itself\n     * then if all of those fail we degrade to linear probing.\n     * @param hash The hash of the key to find the position for\n     * @param attempt The ith probe\n     * @return The byte offset in the buffer at which the ith probing for the given hash would reside\n     */\n    private int positionOf(byte[] hash, int attempt) {\n        int probe = ByteUtils.readIntBE(hash, Math.min(attempt, hashSize - 4)) + Math.max(0, attempt - hashSize + 4);\n        int slot = Utils.abs(probe) % slots;\n        ++this.probes;\n        return slot * bytesPerEntry;\n    }\n\n    /**\n     * The offset at which we have stored the given key\n     * @param key The key to hash\n     * @param buffer The buffer to store the hash into\n     */\n    private void hashInto(ByteBuffer key, byte[] buffer) throws DigestException {\n        key.mark();\n        digest.update(key);\n        key.reset();\n        digest.digest(buffer, 0, hashSize);\n    }\n}\n",
        "methodName": null,
        "exampleID": 6,
        "dataset": "codeql",
        "filepath": "storage/src/main/java/org/apache/kafka/storage/internals/log/SkimpyOffsetMap.java",
        "line": 86,
        "sink": "used.",
        "source": "-",
        "sourceLine": 86,
        "qualifier": "Cryptographic algorithm [MD5](1) is weak and should not be used.",
        "line_number": 86,
        "steps": [
            {
                "line": 74,
                "source": "storage/src/main/java/org/apache/kafka/storage/internals/log/SkimpyOffsetMap.java",
                "filepath": "storage/src/main/java/org/apache/kafka/storage/internals/log/SkimpyOffsetMap.java",
                "methodName": null,
                "exampleID": 7
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/Crypto.java#L66",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.connect.runtime.distributed;\n\nimport java.security.NoSuchAlgorithmException;\n\nimport javax.crypto.KeyGenerator;\nimport javax.crypto.Mac;\n\n/**\n * An interface to allow the dependency injection of {@link Mac} and {@link KeyGenerator} instances for testing.\n * Implementations of this class should be thread-safe.\n */\npublic interface Crypto {\n\n    /**\n     * Use the system implementation for cryptography calls.\n     */\n    Crypto SYSTEM = new SystemCrypto();\n\n    /**\n     * Returns a {@code Mac} object that implements the\n     * specified MAC algorithm. See {@link Mac#getInstance(String)}.\n     *\n     * @param algorithm the standard name of the requested MAC algorithm.\n     * @return the new {@code Mac} object\n     * @throws NoSuchAlgorithmException if no {@code Provider} supports a\n     *         {@code MacSpi} implementation for the specified algorithm\n     */\n    Mac mac(String algorithm) throws NoSuchAlgorithmException;\n\n    /**\n     * Returns a {@code KeyGenerator} object that generates secret keys\n     * for the specified algorithm. See {@link KeyGenerator#getInstance(String)}.\n     *\n     * @param algorithm the standard name of the requested key algorithm.\n     * @return the new {@code KeyGenerator} object\n     * @throws NoSuchAlgorithmException if no {@code Provider} supports a\n     *         {@code KeyGeneratorSpi} implementation for the\n     *         specified algorithm\n     */\n    KeyGenerator keyGenerator(String algorithm) throws NoSuchAlgorithmException;\n\n    class SystemCrypto implements Crypto {\n        @Override\n        public Mac mac(String algorithm) throws NoSuchAlgorithmException {\n            return Mac.getInstance(algorithm);\n        }\n\n        @Override\n        public KeyGenerator keyGenerator(String algorithm) throws NoSuchAlgorithmException {\n            return KeyGenerator.getInstance(algorithm);\n        }\n    }\n\n}\n",
        "methodName": null,
        "exampleID": 8,
        "dataset": "codeql",
        "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/Crypto.java",
        "line": 66,
        "sink": "used.",
        "source": "-",
        "sourceLine": 66,
        "qualifier": "Cryptographic algorithm [AES/CBC/PKCS5Padding](1) is weak and should not be used.",
        "line_number": 66,
        "steps": [
            {
                "line": 44,
                "source": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "filepath": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "methodName": null,
                "exampleID": 9
            },
            {
                "line": 44,
                "source": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "filepath": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "methodName": null,
                "exampleID": 9
            },
            {
                "line": 44,
                "source": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "filepath": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "methodName": null,
                "exampleID": 9
            },
            {
                "line": 44,
                "source": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "filepath": "server-common/src/main/java/org/apache/kafka/security/PasswordEncoderConfigs.java",
                "methodName": null,
                "exampleID": 9
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/server-common/src/main/java/org/apache/kafka/security/EncryptingPasswordEncoder.java#L133",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.security;\n\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.config.types.Password;\nimport org.apache.kafka.server.util.Csv;\n\nimport java.nio.charset.StandardCharsets;\nimport java.security.GeneralSecurityException;\nimport java.security.NoSuchAlgorithmException;\nimport java.security.SecureRandom;\nimport java.security.spec.InvalidKeySpecException;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.stream.Collectors;\n\nimport javax.crypto.Cipher;\nimport javax.crypto.SecretKeyFactory;\nimport javax.crypto.spec.PBEKeySpec;\nimport javax.crypto.spec.SecretKeySpec;\n\n/**\n * Password encoder and decoder implementation. Encoded passwords are persisted as a CSV map\n * containing the encoded password in base64 and along with the properties used for encryption.\n */\npublic class EncryptingPasswordEncoder implements PasswordEncoder {\n\n    private final SecureRandom secureRandom = new SecureRandom();\n\n    private final Password secret;\n    private final String keyFactoryAlgorithm;\n    private final String cipherAlgorithm;\n    private final int keyLength;\n    private final int iterations;\n    private final CipherParamsEncoder cipherParamsEncoder;\n\n\n    /**\n     * @param secret The secret used for encoding and decoding\n     * @param keyFactoryAlgorithm  Key factory algorithm if configured. By default, PBKDF2WithHmacSHA512 is\n     *                             used if available, PBKDF2WithHmacSHA1 otherwise.\n     * @param cipherAlgorithm Cipher algorithm used for encoding.\n     * @param keyLength Key length used for encoding. This should be valid for the specified algorithms.\n     * @param iterations Iteration count used for encoding.\n     * The provided `keyFactoryAlgorithm`, `cipherAlgorithm`, `keyLength` and `iterations` are used for encoding passwords.\n     * The values used for encoding are stored along with the encoded password and the stored values are used for decoding.\n     */\n    public EncryptingPasswordEncoder(\n            Password secret,\n            String keyFactoryAlgorithm,\n            String cipherAlgorithm,\n            int keyLength,\n            int iterations) {\n        this.secret = secret;\n        this.keyFactoryAlgorithm = keyFactoryAlgorithm;\n        this.cipherAlgorithm = cipherAlgorithm;\n        this.keyLength = keyLength;\n        this.iterations = iterations;\n        this.cipherParamsEncoder = cipherParamsInstance(cipherAlgorithm);\n    }\n\n    @Override\n    public String encode(Password password) throws GeneralSecurityException {\n        byte[] salt = new byte[256];\n        secureRandom.nextBytes(salt);\n        Cipher cipher = Cipher.getInstance(cipherAlgorithm);\n        SecretKeyFactory keyFactory = secretKeyFactory(keyFactoryAlgorithm);\n        SecretKeySpec keySpec = secretKeySpec(keyFactory, cipherAlgorithm, keyLength, salt, iterations);\n        cipher.init(Cipher.ENCRYPT_MODE, keySpec);\n        byte[] encryptedPassword = cipher.doFinal(password.value().getBytes(StandardCharsets.UTF_8));\n        Map<String, String> encryptedMap = new HashMap<>();\n        encryptedMap.put(PasswordEncoder.KEY_FACTORY_ALGORITHM, keyFactory.getAlgorithm());\n        encryptedMap.put(PasswordEncoder.CIPHER_ALGORITHM, cipherAlgorithm);\n        encryptedMap.put(PasswordEncoder.KEY_LENGTH, String.valueOf(keyLength));\n        encryptedMap.put(PasswordEncoder.SALT, PasswordEncoder.base64Encode(salt));\n        encryptedMap.put(PasswordEncoder.ITERATIONS, String.valueOf(iterations));\n        encryptedMap.put(PasswordEncoder.ENCRYPTED_PASSWORD, PasswordEncoder.base64Encode(encryptedPassword));\n        encryptedMap.put(PasswordEncoder.PASSWORD_LENGTH, String.valueOf(password.value().length()));\n        encryptedMap.putAll(cipherParamsEncoder.toMap(cipher.getParameters()));\n\n        return encryptedMap.entrySet().stream()\n                .map(entry -> entry.getKey() + \":\" + entry.getValue())\n                .collect(Collectors.joining(\",\"));\n    }\n\n    @Override\n    public Password decode(String encodedPassword) throws GeneralSecurityException {\n        Map<String, String> params = Csv.parseCsvMap(encodedPassword);\n        String keyFactoryAlg = params.get(PasswordEncoder.KEY_FACTORY_ALGORITHM);\n        String cipherAlg = params.get(PasswordEncoder.CIPHER_ALGORITHM);\n        int keyLength = Integer.parseInt(params.get(PasswordEncoder.KEY_LENGTH));\n        byte[] salt = PasswordEncoder.base64Decode(params.get(PasswordEncoder.SALT));\n        int iterations = Integer.parseInt(params.get(PasswordEncoder.ITERATIONS));\n        byte[] encryptedPassword = PasswordEncoder.base64Decode(params.get(PasswordEncoder.ENCRYPTED_PASSWORD));\n        int passwordLengthProp = Integer.parseInt(params.get(PasswordEncoder.PASSWORD_LENGTH));\n        Cipher cipher = Cipher.getInstance(cipherAlg);\n        SecretKeyFactory keyFactory = secretKeyFactory(keyFactoryAlg);\n        SecretKeySpec keySpec = secretKeySpec(keyFactory, cipherAlg, keyLength, salt, iterations);\n        cipher.init(Cipher.DECRYPT_MODE, keySpec, cipherParamsEncoder.toParameterSpec(params));\n        try {\n            byte[] decrypted = cipher.doFinal(encryptedPassword);\n            String password = new String(decrypted, StandardCharsets.UTF_8);\n            if (password.length() != passwordLengthProp) // Sanity check\n                throw new ConfigException(\"Password could not be decoded, sanity check of length failed\");\n            return new Password(password);\n        } catch (Exception e) {\n            throw new ConfigException(\"Password could not be decoded\", e);\n        }\n    }\n\n    private SecretKeyFactory secretKeyFactory(String keyFactoryAlg) throws NoSuchAlgorithmException {\n        if (keyFactoryAlg != null) {\n            return SecretKeyFactory.getInstance(keyFactoryAlg);\n        } else {\n            try {\n                return SecretKeyFactory.getInstance(\"PBKDF2WithHmacSHA512\");\n            } catch (NoSuchAlgorithmException nsae) {\n                return SecretKeyFactory.getInstance(\"PBKDF2WithHmacSHA1\");\n            }\n        }\n    }\n\n    private SecretKeySpec secretKeySpec(SecretKeyFactory keyFactory,\n                                        String cipherAlg,\n                                        int keyLength,\n                                        byte[] salt,\n                                        int iterations) throws InvalidKeySpecException {\n        PBEKeySpec keySpec = new PBEKeySpec(secret.value().toCharArray(), salt, iterations, keyLength);\n        String algorithm = (cipherAlg.indexOf('/') > 0) ? cipherAlg.substring(0, cipherAlg.indexOf('/')) : cipherAlg;\n        return new SecretKeySpec(keyFactory.generateSecret(keySpec).getEncoded(), algorithm);\n    }\n\n    private CipherParamsEncoder cipherParamsInstance(String cipherAlgorithm) {\n        if (cipherAlgorithm.startsWith(\"AES/GCM/\")) {\n            return new GcmParamsEncoder();\n        } else {\n            return new IvParamsEncoder();\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 10,
        "dataset": "codeql",
        "filepath": "server-common/src/main/java/org/apache/kafka/security/EncryptingPasswordEncoder.java",
        "line": 133,
        "sink": "used.",
        "source": "-",
        "sourceLine": 133,
        "qualifier": "Cryptographic algorithm [PBKDF2WithHmacSHA1](1) is weak and should not be used.",
        "line_number": 133,
        "steps": []
    },
    {
        "url": "apache/kafka/blob/main/storage/src/main/java/org/apache/kafka/storage/internals/log/SkimpyOffsetMap.java#L86",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.storage.internals.log;\n\nimport org.apache.kafka.common.utils.ByteUtils;\nimport org.apache.kafka.common.utils.Utils;\n\nimport java.nio.ByteBuffer;\nimport java.security.DigestException;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.Arrays;\n\n/**\n * A hash table used for de-duplicating the log. This hash table uses a cryptographically secure hash of the key as a proxy for the key\n * for comparisons and to save space on object overhead. Collisions are resolved by probing. This hash table does not support deletes.\n */\npublic class SkimpyOffsetMap implements OffsetMap {\n\n    /**\n     * The number of bytes of space each entry uses (the number of bytes in the hash plus an 8 byte offset)\n     */\n    public final int bytesPerEntry;\n\n    private final ByteBuffer bytes;\n\n    /* the hash algorithm instance to use */\n    private final MessageDigest digest;\n\n    /* the number of bytes for this hash algorithm */\n    private final int hashSize;\n\n    /**\n     * The maximum number of entries this map can contain\n     */\n    private final int slots;\n\n    /* cache some hash buffers to avoid reallocating each time */\n    private final byte[] hash1;\n    private final byte[] hash2;\n\n    /* number of entries put into the map */\n    private int entries = 0;\n\n    /* number of lookups on the map */\n    private long lookups = 0L;\n\n    /* the number of probes for all lookups */\n    private long probes = 0L;\n\n    /* the latest offset written into the map */\n    private long lastOffset = -1L;\n\n    /**\n     * Create an instance of SkimplyOffsetMap with the default hash algorithm (MD5).\n     *\n     * @param memory The amount of memory this map can use\n     */\n    public SkimpyOffsetMap(int memory) throws NoSuchAlgorithmException {\n        this(memory, \"MD5\");\n    }\n\n    /**\n     * Create an instance of SkimpyOffsetMap.\n     *\n     * @param memory The amount of memory this map can use\n     * @param hashAlgorithm The hash algorithm instance to use: MD2, MD5, SHA-1, SHA-256, SHA-384, SHA-512\n     */\n    public SkimpyOffsetMap(int memory, String hashAlgorithm) throws NoSuchAlgorithmException {\n        this.bytes = ByteBuffer.allocate(memory);\n\n        this.digest = MessageDigest.getInstance(hashAlgorithm);\n\n        this.hashSize = digest.getDigestLength();\n        this.bytesPerEntry = hashSize + 8;\n        this.slots = memory / bytesPerEntry;\n\n        this.hash1 = new byte[hashSize];\n        this.hash2 = new byte[hashSize];\n    }\n\n    @Override\n    public int slots() {\n        return slots;\n    }\n\n    /**\n     * Get the offset associated with this key.\n     * @param key The key\n     * @return The offset associated with this key or -1 if the key is not found\n     */\n    @Override\n    public long get(ByteBuffer key) throws DigestException {\n        ++lookups;\n        hashInto(key, hash1);\n        // search for the hash of this key by repeated probing until we find the hash we are looking for or we find an empty slot\n        int attempt = 0;\n        //we need to guard against attempt integer overflow if the map is full\n        //limit attempt to number of slots once positionOf(..) enters linear search mode\n        int maxAttempts = slots + hashSize - 4;\n        do {\n            if (attempt >= maxAttempts)\n                return -1L;\n            int pos = positionOf(hash1, attempt);\n            bytes.position(pos);\n            if (isEmpty(pos))\n                return -1L;\n            bytes.get(hash2);\n            ++attempt;\n        } while (!Arrays.equals(hash1, hash2));\n        return bytes.getLong();\n    }\n\n    /**\n     * Associate this offset to the given key.\n     * @param key The key\n     * @param offset The offset\n     */\n    @Override\n    public void put(ByteBuffer key, long offset) throws DigestException {\n        if (entries >= slots)\n            throw new IllegalArgumentException(\"Attempted to add a new entry to a full offset map, \"\n                + \"entries: \" + entries + \", slots: \" + slots);\n\n        ++lookups;\n        hashInto(key, hash1);\n\n        // probe until we find the first empty slot\n        int attempt = 0;\n        int pos = positionOf(hash1, attempt);\n        while (!isEmpty(pos)) {\n            bytes.position(pos);\n            bytes.get(hash2);\n            if (Arrays.equals(hash1, hash2)) {\n                // we found an existing entry, overwrite it and return (size does not change)\n                bytes.putLong(offset);\n                lastOffset = offset;\n                return;\n            }\n            ++attempt;\n            pos = positionOf(hash1, attempt);\n        }\n\n        // found an empty slot, update it - size grows by 1\n        bytes.position(pos);\n        bytes.put(hash1);\n        bytes.putLong(offset);\n        lastOffset = offset;\n        ++entries;\n    }\n\n    @Override\n    public void updateLatestOffset(long offset) {\n        this.lastOffset = offset;\n    }\n\n    /**\n     * Change the salt used for key hashing making all existing keys unfindable.\n     */\n    @Override\n    public void clear() {\n        this.entries = 0;\n        this.lookups = 0L;\n        this.probes = 0L;\n        this.lastOffset = -1L;\n        Arrays.fill(bytes.array(), bytes.arrayOffset(), bytes.arrayOffset() + bytes.limit(), (byte) 0);\n    }\n\n    /**\n     * The number of entries put into the map (note that not all may remain)\n     */\n    @Override\n    public int size() {\n        return entries;\n    }\n\n    /**\n     * The latest offset put into the map\n     */\n    @Override\n    public long latestOffset() {\n        return lastOffset;\n    }\n\n    /**\n     * The rate of collisions in the lookups\n     */\n    // Visible for testing\n    public double collisionRate() {\n        return (this.probes - this.lookups) / (double) this.lookups;\n    }\n\n    /**\n     * Check that there is no entry at the given position\n     */\n    private boolean isEmpty(int position) {\n        return bytes.getLong(position) == 0\n            && bytes.getLong(position + 8) == 0\n            && bytes.getLong(position + 16) == 0;\n    }\n\n    /**\n     * Calculate the ith probe position. We first try reading successive integers from the hash itself\n     * then if all of those fail we degrade to linear probing.\n     * @param hash The hash of the key to find the position for\n     * @param attempt The ith probe\n     * @return The byte offset in the buffer at which the ith probing for the given hash would reside\n     */\n    private int positionOf(byte[] hash, int attempt) {\n        int probe = ByteUtils.readIntBE(hash, Math.min(attempt, hashSize - 4)) + Math.max(0, attempt - hashSize + 4);\n        int slot = Utils.abs(probe) % slots;\n        ++this.probes;\n        return slot * bytesPerEntry;\n    }\n\n    /**\n     * The offset at which we have stored the given key\n     * @param key The key to hash\n     * @param buffer The buffer to store the hash into\n     */\n    private void hashInto(ByteBuffer key, byte[] buffer) throws DigestException {\n        key.mark();\n        digest.update(key);\n        key.reset();\n        digest.digest(buffer, 0, hashSize);\n    }\n}\n",
        "methodName": null,
        "exampleID": 12,
        "dataset": "codeql",
        "filepath": "storage/src/main/java/org/apache/kafka/storage/internals/log/SkimpyOffsetMap.java",
        "line": 86,
        "sink": "used.",
        "source": "-",
        "sourceLine": 86,
        "qualifier": "Cryptographic algorithm [MD5](1) is weak and should not be used.",
        "line_number": 86,
        "steps": [
            {
                "line": 74,
                "source": "storage/src/main/java/org/apache/kafka/storage/internals/log/SkimpyOffsetMap.java",
                "filepath": "storage/src/main/java/org/apache/kafka/storage/internals/log/SkimpyOffsetMap.java",
                "methodName": null,
                "exampleID": 13
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/test/java/org/apache/kafka/common/network/SslSender.java#L50",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.network;\n\nimport java.io.OutputStream;\nimport java.net.InetSocketAddress;\nimport java.security.cert.X509Certificate;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.TimeUnit;\n\nimport javax.net.ssl.SSLContext;\nimport javax.net.ssl.SSLSocket;\nimport javax.net.ssl.TrustManager;\nimport javax.net.ssl.X509TrustManager;\n\npublic class SslSender extends Thread {\n\n    private final String tlsProtocol;\n    private final InetSocketAddress serverAddress;\n    private final byte[] payload;\n    private final CountDownLatch handshaked = new CountDownLatch(1);\n\n    @SuppressWarnings(\"this-escape\")\n    public SslSender(String tlsProtocol, InetSocketAddress serverAddress, byte[] payload) {\n        this.tlsProtocol = tlsProtocol;\n        this.serverAddress = serverAddress;\n        this.payload = payload;\n        setDaemon(true);\n        setName(\"SslSender - \" + payload.length + \" bytes @ \" + serverAddress);\n    }\n\n    @Override\n    public void run() {\n        try {\n            SSLContext sc = SSLContext.getInstance(tlsProtocol);\n            sc.init(null, new TrustManager[]{new NaiveTrustManager()}, new java.security.SecureRandom());\n            try (SSLSocket connection = (SSLSocket) sc.getSocketFactory().createSocket(serverAddress.getAddress(), serverAddress.getPort())) {\n                OutputStream os = connection.getOutputStream();\n                connection.startHandshake();\n                handshaked.countDown();\n                os.write(payload);\n                os.flush();\n            }\n        } catch (Exception e) {\n            e.printStackTrace(System.err);\n        }\n    }\n\n    public boolean waitForHandshake(long timeoutMillis) throws InterruptedException {\n        return handshaked.await(timeoutMillis, TimeUnit.MILLISECONDS);\n    }\n\n    /**\n     * blindly trust any certificate presented to it\n     */\n    private static class NaiveTrustManager implements X509TrustManager {\n        @Override\n        public void checkClientTrusted(X509Certificate[] x509Certificates, String s) {\n            //nop\n        }\n\n        @Override\n        public void checkServerTrusted(X509Certificate[] x509Certificates, String s) {\n            //nop\n        }\n\n        @Override\n        public X509Certificate[] getAcceptedIssuers() {\n            return new X509Certificate[0];\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 14,
        "dataset": "codeql",
        "filepath": "clients/src/test/java/org/apache/kafka/common/network/SslSender.java",
        "line": 50,
        "sink": "certificate.",
        "source": "-",
        "sourceLine": 50,
        "qualifier": "This uses [TrustManager](1), which is defined in [SslSender$NaiveTrustManager](2) and trusts any certificate.",
        "line_number": 50,
        "steps": [
            {
                "line": 50,
                "source": "clients/src/test/java/org/apache/kafka/common/network/SslSender.java",
                "filepath": "clients/src/test/java/org/apache/kafka/common/network/SslSender.java",
                "methodName": null,
                "exampleID": 15
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactory.java#L478",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.security.ssl;\n\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.config.SslClientAuth;\nimport org.apache.kafka.common.config.SslConfigs;\nimport org.apache.kafka.common.config.internals.BrokerSecurityConfigs;\nimport org.apache.kafka.common.config.types.Password;\nimport org.apache.kafka.common.errors.InvalidConfigurationException;\nimport org.apache.kafka.common.network.ConnectionMode;\nimport org.apache.kafka.common.security.auth.SslEngineFactory;\nimport org.apache.kafka.common.utils.SecurityUtils;\nimport org.apache.kafka.common.utils.Utils;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.security.GeneralSecurityException;\nimport java.security.Key;\nimport java.security.KeyFactory;\nimport java.security.KeyStore;\nimport java.security.KeyStoreException;\nimport java.security.NoSuchAlgorithmException;\nimport java.security.PrivateKey;\nimport java.security.SecureRandom;\nimport java.security.cert.Certificate;\nimport java.security.cert.CertificateFactory;\nimport java.security.spec.InvalidKeySpecException;\nimport java.security.spec.PKCS8EncodedKeySpec;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Base64;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Set;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\n\nimport javax.crypto.Cipher;\nimport javax.crypto.EncryptedPrivateKeyInfo;\nimport javax.crypto.SecretKey;\nimport javax.crypto.SecretKeyFactory;\nimport javax.crypto.spec.PBEKeySpec;\nimport javax.net.ssl.KeyManager;\nimport javax.net.ssl.KeyManagerFactory;\nimport javax.net.ssl.SSLContext;\nimport javax.net.ssl.SSLEngine;\nimport javax.net.ssl.SSLParameters;\nimport javax.net.ssl.TrustManager;\nimport javax.net.ssl.TrustManagerFactory;\n\npublic class DefaultSslEngineFactory implements SslEngineFactory {\n\n    private static final Logger log = LoggerFactory.getLogger(DefaultSslEngineFactory.class);\n    public static final String PEM_TYPE = \"PEM\";\n\n    private Map<String, ?> configs;\n    private String protocol;\n    private String provider;\n    private String kmfAlgorithm;\n    private String tmfAlgorithm;\n    private SecurityStore keystore;\n    private SecurityStore truststore;\n    private String[] cipherSuites;\n    private String[] enabledProtocols;\n    private SecureRandom secureRandomImplementation;\n    private SSLContext sslContext;\n    private SslClientAuth sslClientAuth;\n\n\n    @Override\n    public SSLEngine createClientSslEngine(String peerHost, int peerPort, String endpointIdentification) {\n        return createSslEngine(ConnectionMode.CLIENT, peerHost, peerPort, endpointIdentification);\n    }\n\n    @Override\n    public SSLEngine createServerSslEngine(String peerHost, int peerPort) {\n        return createSslEngine(ConnectionMode.SERVER, peerHost, peerPort, null);\n    }\n\n    @Override\n    public boolean shouldBeRebuilt(Map<String, Object> nextConfigs) {\n        if (!nextConfigs.equals(configs)) {\n            return true;\n        }\n        if (truststore != null && truststore.modified()) {\n            return true;\n        }\n        return keystore != null && keystore.modified();\n    }\n\n    @Override\n    public Set<String> reconfigurableConfigs() {\n        return SslConfigs.RECONFIGURABLE_CONFIGS;\n    }\n\n    @Override\n    public KeyStore keystore() {\n        return this.keystore != null ? this.keystore.get() : null;\n    }\n\n    @Override\n    public KeyStore truststore() {\n        return this.truststore != null ? this.truststore.get() : null;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @Override\n    public void configure(Map<String, ?> configs) {\n        this.configs = Collections.unmodifiableMap(configs);\n        this.protocol = (String) configs.get(SslConfigs.SSL_PROTOCOL_CONFIG);\n        this.provider = (String) configs.get(SslConfigs.SSL_PROVIDER_CONFIG);\n        SecurityUtils.addConfiguredSecurityProviders(this.configs);\n\n        List<String> cipherSuitesList = (List<String>) configs.get(SslConfigs.SSL_CIPHER_SUITES_CONFIG);\n        if (cipherSuitesList != null && !cipherSuitesList.isEmpty()) {\n            this.cipherSuites = cipherSuitesList.toArray(new String[0]);\n        } else {\n            this.cipherSuites = null;\n        }\n\n        List<String> enabledProtocolsList = (List<String>) configs.get(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG);\n        if (enabledProtocolsList != null && !enabledProtocolsList.isEmpty()) {\n            this.enabledProtocols = enabledProtocolsList.toArray(new String[0]);\n        } else {\n            this.enabledProtocols = null;\n        }\n\n        this.secureRandomImplementation = createSecureRandom((String)\n                configs.get(SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG));\n\n        this.sslClientAuth = createSslClientAuth((String) configs.get(\n                BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG));\n\n        this.kmfAlgorithm = (String) configs.get(SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG);\n        this.tmfAlgorithm = (String) configs.get(SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG);\n\n        this.keystore = createKeystore((String) configs.get(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG),\n                (String) configs.get(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEY_PASSWORD_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEYSTORE_KEY_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEYSTORE_CERTIFICATE_CHAIN_CONFIG));\n\n        this.truststore = createTruststore((String) configs.get(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG),\n                (String) configs.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_TRUSTSTORE_CERTIFICATES_CONFIG));\n\n        this.sslContext = createSSLContext(keystore, truststore);\n    }\n\n    @Override\n    public void close() {\n        this.sslContext = null;\n    }\n\n    //For Test only\n    public SSLContext sslContext() {\n        return this.sslContext;\n    }\n\n    private SSLEngine createSslEngine(ConnectionMode connectionMode, String peerHost, int peerPort, String endpointIdentification) {\n        SSLEngine sslEngine = sslContext.createSSLEngine(peerHost, peerPort);\n        if (cipherSuites != null) sslEngine.setEnabledCipherSuites(cipherSuites);\n        if (enabledProtocols != null) sslEngine.setEnabledProtocols(enabledProtocols);\n\n        if (connectionMode == ConnectionMode.SERVER) {\n            sslEngine.setUseClientMode(false);\n            switch (sslClientAuth) {\n                case REQUIRED:\n                    sslEngine.setNeedClientAuth(true);\n                    break;\n                case REQUESTED:\n                    sslEngine.setWantClientAuth(true);\n                    break;\n                case NONE:\n                    break;\n            }\n        } else {\n            sslEngine.setUseClientMode(true);\n            SSLParameters sslParams = sslEngine.getSSLParameters();\n            // SSLParameters#setEndpointIdentificationAlgorithm enables endpoint validation\n            // only in client mode. Hence, validation is enabled only for clients.\n            sslParams.setEndpointIdentificationAlgorithm(endpointIdentification);\n            sslEngine.setSSLParameters(sslParams);\n        }\n        return sslEngine;\n    }\n    private static SslClientAuth createSslClientAuth(String key) {\n        SslClientAuth auth = SslClientAuth.forConfig(key);\n        if (auth != null) {\n            return auth;\n        }\n        log.warn(\"Unrecognized client authentication configuration {}.  Falling \" +\n                \"back to NONE.  Recognized client authentication configurations are {}.\",\n                key, SslClientAuth.VALUES.stream().\n                        map(Enum::name).collect(Collectors.joining(\", \")));\n        return SslClientAuth.NONE;\n    }\n\n    private static SecureRandom createSecureRandom(String key) {\n        if (key == null) {\n            return null;\n        }\n        try {\n            return SecureRandom.getInstance(key);\n        } catch (GeneralSecurityException e) {\n            throw new KafkaException(e);\n        }\n    }\n\n    private SSLContext createSSLContext(SecurityStore keystore, SecurityStore truststore) {\n        try {\n            SSLContext sslContext;\n            if (provider != null)\n                sslContext = SSLContext.getInstance(protocol, provider);\n            else\n                sslContext = SSLContext.getInstance(protocol);\n\n            KeyManager[] keyManagers = null;\n            if (keystore != null || kmfAlgorithm != null) {\n                String kmfAlgorithm = this.kmfAlgorithm != null ?\n                        this.kmfAlgorithm : KeyManagerFactory.getDefaultAlgorithm();\n                KeyManagerFactory kmf = KeyManagerFactory.getInstance(kmfAlgorithm);\n                if (keystore != null) {\n                    kmf.init(keystore.get(), keystore.keyPassword());\n                } else {\n                    kmf.init(null, null);\n                }\n                keyManagers = kmf.getKeyManagers();\n            }\n\n            String tmfAlgorithm = this.tmfAlgorithm != null ? this.tmfAlgorithm : TrustManagerFactory.getDefaultAlgorithm();\n            TrustManager[] trustManagers = getTrustManagers(truststore, tmfAlgorithm);\n\n            sslContext.init(keyManagers, trustManagers, this.secureRandomImplementation);\n            log.debug(\"Created SSL context with keystore {}, truststore {}, provider {}.\",\n                    keystore, truststore, sslContext.getProvider().getName());\n            return sslContext;\n        } catch (Exception e) {\n            throw new KafkaException(e);\n        }\n    }\n\n    protected TrustManager[] getTrustManagers(SecurityStore truststore, String tmfAlgorithm) throws NoSuchAlgorithmException, KeyStoreException {\n        TrustManagerFactory tmf = TrustManagerFactory.getInstance(tmfAlgorithm);\n        KeyStore ts = truststore == null ? null : truststore.get();\n        tmf.init(ts);\n        return tmf.getTrustManagers();\n    }\n\n    // Visibility to override for testing\n    protected SecurityStore createKeystore(String type, String path, Password password, Password keyPassword, Password privateKey, Password certificateChain) {\n        if (privateKey != null) {\n            if (!PEM_TYPE.equals(type))\n                throw new InvalidConfigurationException(\"SSL private key can be specified only for PEM, but key store type is \" + type + \".\");\n            else if (certificateChain == null)\n                throw new InvalidConfigurationException(\"SSL private key is specified, but certificate chain is not specified.\");\n            else if (path != null)\n                throw new InvalidConfigurationException(\"Both SSL key store location and separate private key are specified.\");\n            else if (password != null)\n                throw new InvalidConfigurationException(\"SSL key store password cannot be specified with PEM format, only key password may be specified.\");\n            else\n                return new PemStore(certificateChain, privateKey, keyPassword);\n        } else if (certificateChain != null) {\n            throw new InvalidConfigurationException(\"SSL certificate chain is specified, but private key is not specified\");\n        } else if (PEM_TYPE.equals(type) && path != null) {\n            if (password != null)\n                throw new InvalidConfigurationException(\"SSL key store password cannot be specified with PEM format, only key password may be specified\");\n            else\n                return new FileBasedPemStore(path, keyPassword, true);\n        } else if (path == null && password != null) {\n            throw new InvalidConfigurationException(\"SSL key store is not specified, but key store password is specified.\");\n        } else if (path != null && password == null) {\n            throw new InvalidConfigurationException(\"SSL key store is specified, but key store password is not specified.\");\n        } else if (path != null && password != null) {\n            return new FileBasedStore(type, path, password, keyPassword, true);\n        } else\n            return null; // path == null, clients may use this path with brokers that don't require client auth\n    }\n\n    private static SecurityStore createTruststore(String type, String path, Password password, Password trustStoreCerts) {\n        if (trustStoreCerts != null) {\n            if (!PEM_TYPE.equals(type))\n                throw new InvalidConfigurationException(\"SSL trust store certs can be specified only for PEM, but trust store type is \" + type + \".\");\n            else if (path != null)\n                throw new InvalidConfigurationException(\"Both SSL trust store location and separate trust certificates are specified.\");\n            else if (password != null)\n                throw new InvalidConfigurationException(\"SSL trust store password cannot be specified for PEM format.\");\n            else\n                return new PemStore(trustStoreCerts);\n        } else if (PEM_TYPE.equals(type) && path != null) {\n            if (password != null)\n                throw new InvalidConfigurationException(\"SSL trust store password cannot be specified for PEM format.\");\n            else\n                return new FileBasedPemStore(path, null, false);\n        } else if (path == null && password != null) {\n            throw new InvalidConfigurationException(\"SSL trust store is not specified, but trust store password is specified.\");\n        } else if (path != null) {\n            return new FileBasedStore(type, path, password, null, false);\n        } else\n            return null;\n    }\n\n    interface SecurityStore {\n        KeyStore get();\n        char[] keyPassword();\n        boolean modified();\n    }\n\n    // package access for testing\n    static class FileBasedStore implements SecurityStore {\n        private final String type;\n        protected final String path;\n        private final Password password;\n        protected final Password keyPassword;\n        private final Long fileLastModifiedMs;\n        private final KeyStore keyStore;\n\n        FileBasedStore(String type, String path, Password password, Password keyPassword, boolean isKeyStore) {\n            Objects.requireNonNull(type, \"type must not be null\");\n            this.type = type;\n            this.path = path;\n            this.password = password;\n            this.keyPassword = keyPassword;\n            fileLastModifiedMs = lastModifiedMs(path);\n            this.keyStore = load(isKeyStore);\n        }\n\n        @Override\n        public KeyStore get() {\n            return keyStore;\n        }\n\n        @Override\n        public char[] keyPassword() {\n            Password passwd = keyPassword != null ? keyPassword : password;\n            return passwd == null ? null : passwd.value().toCharArray();\n        }\n\n        /**\n         * Loads this keystore\n         * @return the keystore\n         * @throws KafkaException if the file could not be read or if the keystore could not be loaded\n         *   using the specified configs (e.g. if the password or keystore type is invalid)\n         */\n        protected KeyStore load(boolean isKeyStore) {\n            try (InputStream in = Files.newInputStream(Paths.get(path))) {\n                KeyStore ks = KeyStore.getInstance(type);\n                // If a password is not set access to the truststore is still available, but integrity checking is disabled.\n                char[] passwordChars = password != null ? password.value().toCharArray() : null;\n                ks.load(in, passwordChars);\n                return ks;\n            } catch (GeneralSecurityException | IOException e) {\n                throw new KafkaException(\"Failed to load SSL keystore \" + path + \" of type \" + type, e);\n            }\n        }\n\n        private Long lastModifiedMs(String path) {\n            try {\n                return Files.getLastModifiedTime(Paths.get(path)).toMillis();\n            } catch (IOException e) {\n                log.error(\"Modification time of key store could not be obtained: \" + path, e);\n                return null;\n            }\n        }\n\n        public boolean modified() {\n            Long modifiedMs = lastModifiedMs(path);\n            return modifiedMs != null && !Objects.equals(modifiedMs, this.fileLastModifiedMs);\n        }\n\n        @Override\n        public String toString() {\n            return \"SecurityStore(\" +\n                    \"path=\" + path +\n                    \", modificationTime=\" + (fileLastModifiedMs == null ? null : new Date(fileLastModifiedMs)) + \")\";\n        }\n    }\n\n    static class FileBasedPemStore extends FileBasedStore {\n        FileBasedPemStore(String path, Password keyPassword, boolean isKeyStore) {\n            super(PEM_TYPE, path, null, keyPassword, isKeyStore);\n        }\n\n        @Override\n        protected KeyStore load(boolean isKeyStore) {\n            try {\n                Password storeContents = new Password(Utils.readFileAsString(path));\n                PemStore pemStore = isKeyStore ? new PemStore(storeContents, storeContents, keyPassword) :\n                    new PemStore(storeContents);\n                return pemStore.keyStore;\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Failed to load PEM SSL keystore \" + path, e);\n            }\n        }\n    }\n\n    static class PemStore implements SecurityStore {\n        private static final PemParser CERTIFICATE_PARSER = new PemParser(\"CERTIFICATE\");\n        private static final PemParser PRIVATE_KEY_PARSER = new PemParser(\"PRIVATE KEY\");\n        private static final List<KeyFactory> KEY_FACTORIES = Arrays.asList(\n                keyFactory(\"RSA\"),\n                keyFactory(\"DSA\"),\n                keyFactory(\"EC\")\n        );\n\n        private final char[] keyPassword;\n        private final KeyStore keyStore;\n\n        PemStore(Password certificateChain, Password privateKey, Password keyPassword) {\n            this.keyPassword = keyPassword == null ? null : keyPassword.value().toCharArray();\n            keyStore = createKeyStoreFromPem(privateKey.value(), certificateChain.value(), this.keyPassword);\n        }\n\n        PemStore(Password trustStoreCerts) {\n            this.keyPassword = null;\n            keyStore = createTrustStoreFromPem(trustStoreCerts.value());\n        }\n\n        @Override\n        public KeyStore get() {\n            return keyStore;\n        }\n\n        @Override\n        public char[] keyPassword() {\n            return keyPassword;\n        }\n\n        @Override\n        public boolean modified() {\n            return false;\n        }\n\n        private KeyStore createKeyStoreFromPem(String privateKeyPem, String certChainPem, char[] keyPassword) {\n            try {\n                KeyStore ks = KeyStore.getInstance(\"PKCS12\");\n                ks.load(null, null);\n                Key key = privateKey(privateKeyPem, keyPassword);\n                Certificate[] certChain = certs(certChainPem);\n                ks.setKeyEntry(\"kafka\", key, keyPassword, certChain);\n                return ks;\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Invalid PEM keystore configs\", e);\n            }\n        }\n\n        private KeyStore createTrustStoreFromPem(String trustedCertsPem) {\n            try {\n                KeyStore ts = KeyStore.getInstance(\"PKCS12\");\n                ts.load(null, null);\n                Certificate[] certs = certs(trustedCertsPem);\n                for (int i = 0; i < certs.length; i++) {\n                    ts.setCertificateEntry(\"kafka\" + i, certs[i]);\n                }\n                return ts;\n            } catch (InvalidConfigurationException e) {\n                throw e;\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Invalid PEM truststore configs\", e);\n            }\n        }\n\n        private Certificate[] certs(String pem) throws GeneralSecurityException {\n            List<byte[]> certEntries = CERTIFICATE_PARSER.pemEntries(pem);\n            if (certEntries.isEmpty())\n                throw new InvalidConfigurationException(\"At least one certificate expected, but none found\");\n\n            Certificate[] certs = new Certificate[certEntries.size()];\n            for (int i = 0; i < certs.length; i++) {\n                certs[i] = CertificateFactory.getInstance(\"X.509\")\n                    .generateCertificate(new ByteArrayInputStream(certEntries.get(i)));\n            }\n            return certs;\n        }\n\n        private PrivateKey privateKey(String pem, char[] keyPassword) throws Exception {\n            List<byte[]> keyEntries = PRIVATE_KEY_PARSER.pemEntries(pem);\n            if (keyEntries.isEmpty())\n                throw new InvalidConfigurationException(\"Private key not provided\");\n            if (keyEntries.size() != 1)\n                throw new InvalidConfigurationException(\"Expected one private key, but found \" + keyEntries.size());\n\n            byte[] keyBytes = keyEntries.get(0);\n            PKCS8EncodedKeySpec keySpec;\n            if (keyPassword == null) {\n                keySpec = new PKCS8EncodedKeySpec(keyBytes);\n            } else {\n                EncryptedPrivateKeyInfo keyInfo = new EncryptedPrivateKeyInfo(keyBytes);\n                String algorithm = keyInfo.getAlgName();\n                SecretKeyFactory keyFactory = SecretKeyFactory.getInstance(algorithm);\n                SecretKey pbeKey = keyFactory.generateSecret(new PBEKeySpec(keyPassword));\n                Cipher cipher = Cipher.getInstance(algorithm);\n                cipher.init(Cipher.DECRYPT_MODE, pbeKey, keyInfo.getAlgParameters());\n                keySpec = keyInfo.getKeySpec(cipher);\n            }\n\n            InvalidKeySpecException firstException = null;\n            for (KeyFactory factory : KEY_FACTORIES) {\n                try {\n                    return factory.generatePrivate(keySpec);\n                } catch (InvalidKeySpecException e) {\n                    if (firstException == null)\n                        firstException = e;\n                }\n            }\n            throw new InvalidConfigurationException(\"Private key could not be loaded\", firstException);\n        }\n\n        private static KeyFactory keyFactory(String algorithm) {\n            try {\n                return KeyFactory.getInstance(algorithm);\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Could not create key factory for algorithm \" + algorithm, e);\n            }\n        }\n    }\n\n    /**\n     * Parser to process certificate/private key entries from PEM files\n     * Examples:\n     *   -----BEGIN CERTIFICATE-----\n     *   Base64 cert\n     *   -----END CERTIFICATE-----\n     *\n     *   -----BEGIN ENCRYPTED PRIVATE KEY-----\n     *   Base64 private key\n     *   -----END ENCRYPTED PRIVATE KEY-----\n     *   Additional data may be included before headers, so we match all entries within the PEM.\n     */\n    static class PemParser {\n        private final String name;\n        private final Pattern pattern;\n\n        PemParser(String name) {\n            this.name = name;\n            String beginOrEndFormat = \"-+%s\\\\s*.*%s[^-]*-+\\\\s+\";\n            String nameIgnoreSpace = name.replace(\" \", \"\\\\s+\");\n\n            String encodingParams = \"\\\\s*[^\\\\r\\\\n]*:[^\\\\r\\\\n]*[\\\\r\\\\n]+\";\n            String base64Pattern = \"([a-zA-Z0-9/+=\\\\s]*)\";\n            String patternStr =  String.format(beginOrEndFormat, \"BEGIN\", nameIgnoreSpace) +\n                String.format(\"(?:%s)*\", encodingParams) +\n                base64Pattern +\n                String.format(beginOrEndFormat, \"END\", nameIgnoreSpace);\n            pattern = Pattern.compile(patternStr);\n        }\n\n        private List<byte[]> pemEntries(String pem) {\n            Matcher matcher = pattern.matcher(pem + \"\\n\"); // allow last newline to be omitted in value\n            List<byte[]>  entries = new ArrayList<>();\n            while (matcher.find()) {\n                String base64Str = matcher.group(1).replaceAll(\"\\\\s\", \"\");\n                entries.add(Base64.getDecoder().decode(base64Str));\n            }\n            if (entries.isEmpty())\n                throw new InvalidConfigurationException(\"No matching \" + name + \" entries in PEM file\");\n            return entries;\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 16,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactory.java",
        "line": 478,
        "sink": "source.](1).",
        "source": "-",
        "sourceLine": 478,
        "qualifier": "Potential Insecure randomness due to a [Insecure randomness source.](1).",
        "line_number": 478,
        "steps": [
            {
                "line": 254,
                "source": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "filepath": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "methodName": null,
                "exampleID": 17
            },
            {
                "line": 254,
                "source": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "filepath": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "methodName": null,
                "exampleID": 17
            },
            {
                "line": 254,
                "source": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "filepath": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "methodName": null,
                "exampleID": 17
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactory.java#L513",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.security.ssl;\n\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.config.SslClientAuth;\nimport org.apache.kafka.common.config.SslConfigs;\nimport org.apache.kafka.common.config.internals.BrokerSecurityConfigs;\nimport org.apache.kafka.common.config.types.Password;\nimport org.apache.kafka.common.errors.InvalidConfigurationException;\nimport org.apache.kafka.common.network.ConnectionMode;\nimport org.apache.kafka.common.security.auth.SslEngineFactory;\nimport org.apache.kafka.common.utils.SecurityUtils;\nimport org.apache.kafka.common.utils.Utils;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.security.GeneralSecurityException;\nimport java.security.Key;\nimport java.security.KeyFactory;\nimport java.security.KeyStore;\nimport java.security.KeyStoreException;\nimport java.security.NoSuchAlgorithmException;\nimport java.security.PrivateKey;\nimport java.security.SecureRandom;\nimport java.security.cert.Certificate;\nimport java.security.cert.CertificateFactory;\nimport java.security.spec.InvalidKeySpecException;\nimport java.security.spec.PKCS8EncodedKeySpec;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Base64;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Set;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\n\nimport javax.crypto.Cipher;\nimport javax.crypto.EncryptedPrivateKeyInfo;\nimport javax.crypto.SecretKey;\nimport javax.crypto.SecretKeyFactory;\nimport javax.crypto.spec.PBEKeySpec;\nimport javax.net.ssl.KeyManager;\nimport javax.net.ssl.KeyManagerFactory;\nimport javax.net.ssl.SSLContext;\nimport javax.net.ssl.SSLEngine;\nimport javax.net.ssl.SSLParameters;\nimport javax.net.ssl.TrustManager;\nimport javax.net.ssl.TrustManagerFactory;\n\npublic class DefaultSslEngineFactory implements SslEngineFactory {\n\n    private static final Logger log = LoggerFactory.getLogger(DefaultSslEngineFactory.class);\n    public static final String PEM_TYPE = \"PEM\";\n\n    private Map<String, ?> configs;\n    private String protocol;\n    private String provider;\n    private String kmfAlgorithm;\n    private String tmfAlgorithm;\n    private SecurityStore keystore;\n    private SecurityStore truststore;\n    private String[] cipherSuites;\n    private String[] enabledProtocols;\n    private SecureRandom secureRandomImplementation;\n    private SSLContext sslContext;\n    private SslClientAuth sslClientAuth;\n\n\n    @Override\n    public SSLEngine createClientSslEngine(String peerHost, int peerPort, String endpointIdentification) {\n        return createSslEngine(ConnectionMode.CLIENT, peerHost, peerPort, endpointIdentification);\n    }\n\n    @Override\n    public SSLEngine createServerSslEngine(String peerHost, int peerPort) {\n        return createSslEngine(ConnectionMode.SERVER, peerHost, peerPort, null);\n    }\n\n    @Override\n    public boolean shouldBeRebuilt(Map<String, Object> nextConfigs) {\n        if (!nextConfigs.equals(configs)) {\n            return true;\n        }\n        if (truststore != null && truststore.modified()) {\n            return true;\n        }\n        return keystore != null && keystore.modified();\n    }\n\n    @Override\n    public Set<String> reconfigurableConfigs() {\n        return SslConfigs.RECONFIGURABLE_CONFIGS;\n    }\n\n    @Override\n    public KeyStore keystore() {\n        return this.keystore != null ? this.keystore.get() : null;\n    }\n\n    @Override\n    public KeyStore truststore() {\n        return this.truststore != null ? this.truststore.get() : null;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @Override\n    public void configure(Map<String, ?> configs) {\n        this.configs = Collections.unmodifiableMap(configs);\n        this.protocol = (String) configs.get(SslConfigs.SSL_PROTOCOL_CONFIG);\n        this.provider = (String) configs.get(SslConfigs.SSL_PROVIDER_CONFIG);\n        SecurityUtils.addConfiguredSecurityProviders(this.configs);\n\n        List<String> cipherSuitesList = (List<String>) configs.get(SslConfigs.SSL_CIPHER_SUITES_CONFIG);\n        if (cipherSuitesList != null && !cipherSuitesList.isEmpty()) {\n            this.cipherSuites = cipherSuitesList.toArray(new String[0]);\n        } else {\n            this.cipherSuites = null;\n        }\n\n        List<String> enabledProtocolsList = (List<String>) configs.get(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG);\n        if (enabledProtocolsList != null && !enabledProtocolsList.isEmpty()) {\n            this.enabledProtocols = enabledProtocolsList.toArray(new String[0]);\n        } else {\n            this.enabledProtocols = null;\n        }\n\n        this.secureRandomImplementation = createSecureRandom((String)\n                configs.get(SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG));\n\n        this.sslClientAuth = createSslClientAuth((String) configs.get(\n                BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG));\n\n        this.kmfAlgorithm = (String) configs.get(SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG);\n        this.tmfAlgorithm = (String) configs.get(SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG);\n\n        this.keystore = createKeystore((String) configs.get(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG),\n                (String) configs.get(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEY_PASSWORD_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEYSTORE_KEY_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEYSTORE_CERTIFICATE_CHAIN_CONFIG));\n\n        this.truststore = createTruststore((String) configs.get(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG),\n                (String) configs.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_TRUSTSTORE_CERTIFICATES_CONFIG));\n\n        this.sslContext = createSSLContext(keystore, truststore);\n    }\n\n    @Override\n    public void close() {\n        this.sslContext = null;\n    }\n\n    //For Test only\n    public SSLContext sslContext() {\n        return this.sslContext;\n    }\n\n    private SSLEngine createSslEngine(ConnectionMode connectionMode, String peerHost, int peerPort, String endpointIdentification) {\n        SSLEngine sslEngine = sslContext.createSSLEngine(peerHost, peerPort);\n        if (cipherSuites != null) sslEngine.setEnabledCipherSuites(cipherSuites);\n        if (enabledProtocols != null) sslEngine.setEnabledProtocols(enabledProtocols);\n\n        if (connectionMode == ConnectionMode.SERVER) {\n            sslEngine.setUseClientMode(false);\n            switch (sslClientAuth) {\n                case REQUIRED:\n                    sslEngine.setNeedClientAuth(true);\n                    break;\n                case REQUESTED:\n                    sslEngine.setWantClientAuth(true);\n                    break;\n                case NONE:\n                    break;\n            }\n        } else {\n            sslEngine.setUseClientMode(true);\n            SSLParameters sslParams = sslEngine.getSSLParameters();\n            // SSLParameters#setEndpointIdentificationAlgorithm enables endpoint validation\n            // only in client mode. Hence, validation is enabled only for clients.\n            sslParams.setEndpointIdentificationAlgorithm(endpointIdentification);\n            sslEngine.setSSLParameters(sslParams);\n        }\n        return sslEngine;\n    }\n    private static SslClientAuth createSslClientAuth(String key) {\n        SslClientAuth auth = SslClientAuth.forConfig(key);\n        if (auth != null) {\n            return auth;\n        }\n        log.warn(\"Unrecognized client authentication configuration {}.  Falling \" +\n                \"back to NONE.  Recognized client authentication configurations are {}.\",\n                key, SslClientAuth.VALUES.stream().\n                        map(Enum::name).collect(Collectors.joining(\", \")));\n        return SslClientAuth.NONE;\n    }\n\n    private static SecureRandom createSecureRandom(String key) {\n        if (key == null) {\n            return null;\n        }\n        try {\n            return SecureRandom.getInstance(key);\n        } catch (GeneralSecurityException e) {\n            throw new KafkaException(e);\n        }\n    }\n\n    private SSLContext createSSLContext(SecurityStore keystore, SecurityStore truststore) {\n        try {\n            SSLContext sslContext;\n            if (provider != null)\n                sslContext = SSLContext.getInstance(protocol, provider);\n            else\n                sslContext = SSLContext.getInstance(protocol);\n\n            KeyManager[] keyManagers = null;\n            if (keystore != null || kmfAlgorithm != null) {\n                String kmfAlgorithm = this.kmfAlgorithm != null ?\n                        this.kmfAlgorithm : KeyManagerFactory.getDefaultAlgorithm();\n                KeyManagerFactory kmf = KeyManagerFactory.getInstance(kmfAlgorithm);\n                if (keystore != null) {\n                    kmf.init(keystore.get(), keystore.keyPassword());\n                } else {\n                    kmf.init(null, null);\n                }\n                keyManagers = kmf.getKeyManagers();\n            }\n\n            String tmfAlgorithm = this.tmfAlgorithm != null ? this.tmfAlgorithm : TrustManagerFactory.getDefaultAlgorithm();\n            TrustManager[] trustManagers = getTrustManagers(truststore, tmfAlgorithm);\n\n            sslContext.init(keyManagers, trustManagers, this.secureRandomImplementation);\n            log.debug(\"Created SSL context with keystore {}, truststore {}, provider {}.\",\n                    keystore, truststore, sslContext.getProvider().getName());\n            return sslContext;\n        } catch (Exception e) {\n            throw new KafkaException(e);\n        }\n    }\n\n    protected TrustManager[] getTrustManagers(SecurityStore truststore, String tmfAlgorithm) throws NoSuchAlgorithmException, KeyStoreException {\n        TrustManagerFactory tmf = TrustManagerFactory.getInstance(tmfAlgorithm);\n        KeyStore ts = truststore == null ? null : truststore.get();\n        tmf.init(ts);\n        return tmf.getTrustManagers();\n    }\n\n    // Visibility to override for testing\n    protected SecurityStore createKeystore(String type, String path, Password password, Password keyPassword, Password privateKey, Password certificateChain) {\n        if (privateKey != null) {\n            if (!PEM_TYPE.equals(type))\n                throw new InvalidConfigurationException(\"SSL private key can be specified only for PEM, but key store type is \" + type + \".\");\n            else if (certificateChain == null)\n                throw new InvalidConfigurationException(\"SSL private key is specified, but certificate chain is not specified.\");\n            else if (path != null)\n                throw new InvalidConfigurationException(\"Both SSL key store location and separate private key are specified.\");\n            else if (password != null)\n                throw new InvalidConfigurationException(\"SSL key store password cannot be specified with PEM format, only key password may be specified.\");\n            else\n                return new PemStore(certificateChain, privateKey, keyPassword);\n        } else if (certificateChain != null) {\n            throw new InvalidConfigurationException(\"SSL certificate chain is specified, but private key is not specified\");\n        } else if (PEM_TYPE.equals(type) && path != null) {\n            if (password != null)\n                throw new InvalidConfigurationException(\"SSL key store password cannot be specified with PEM format, only key password may be specified\");\n            else\n                return new FileBasedPemStore(path, keyPassword, true);\n        } else if (path == null && password != null) {\n            throw new InvalidConfigurationException(\"SSL key store is not specified, but key store password is specified.\");\n        } else if (path != null && password == null) {\n            throw new InvalidConfigurationException(\"SSL key store is specified, but key store password is not specified.\");\n        } else if (path != null && password != null) {\n            return new FileBasedStore(type, path, password, keyPassword, true);\n        } else\n            return null; // path == null, clients may use this path with brokers that don't require client auth\n    }\n\n    private static SecurityStore createTruststore(String type, String path, Password password, Password trustStoreCerts) {\n        if (trustStoreCerts != null) {\n            if (!PEM_TYPE.equals(type))\n                throw new InvalidConfigurationException(\"SSL trust store certs can be specified only for PEM, but trust store type is \" + type + \".\");\n            else if (path != null)\n                throw new InvalidConfigurationException(\"Both SSL trust store location and separate trust certificates are specified.\");\n            else if (password != null)\n                throw new InvalidConfigurationException(\"SSL trust store password cannot be specified for PEM format.\");\n            else\n                return new PemStore(trustStoreCerts);\n        } else if (PEM_TYPE.equals(type) && path != null) {\n            if (password != null)\n                throw new InvalidConfigurationException(\"SSL trust store password cannot be specified for PEM format.\");\n            else\n                return new FileBasedPemStore(path, null, false);\n        } else if (path == null && password != null) {\n            throw new InvalidConfigurationException(\"SSL trust store is not specified, but trust store password is specified.\");\n        } else if (path != null) {\n            return new FileBasedStore(type, path, password, null, false);\n        } else\n            return null;\n    }\n\n    interface SecurityStore {\n        KeyStore get();\n        char[] keyPassword();\n        boolean modified();\n    }\n\n    // package access for testing\n    static class FileBasedStore implements SecurityStore {\n        private final String type;\n        protected final String path;\n        private final Password password;\n        protected final Password keyPassword;\n        private final Long fileLastModifiedMs;\n        private final KeyStore keyStore;\n\n        FileBasedStore(String type, String path, Password password, Password keyPassword, boolean isKeyStore) {\n            Objects.requireNonNull(type, \"type must not be null\");\n            this.type = type;\n            this.path = path;\n            this.password = password;\n            this.keyPassword = keyPassword;\n            fileLastModifiedMs = lastModifiedMs(path);\n            this.keyStore = load(isKeyStore);\n        }\n\n        @Override\n        public KeyStore get() {\n            return keyStore;\n        }\n\n        @Override\n        public char[] keyPassword() {\n            Password passwd = keyPassword != null ? keyPassword : password;\n            return passwd == null ? null : passwd.value().toCharArray();\n        }\n\n        /**\n         * Loads this keystore\n         * @return the keystore\n         * @throws KafkaException if the file could not be read or if the keystore could not be loaded\n         *   using the specified configs (e.g. if the password or keystore type is invalid)\n         */\n        protected KeyStore load(boolean isKeyStore) {\n            try (InputStream in = Files.newInputStream(Paths.get(path))) {\n                KeyStore ks = KeyStore.getInstance(type);\n                // If a password is not set access to the truststore is still available, but integrity checking is disabled.\n                char[] passwordChars = password != null ? password.value().toCharArray() : null;\n                ks.load(in, passwordChars);\n                return ks;\n            } catch (GeneralSecurityException | IOException e) {\n                throw new KafkaException(\"Failed to load SSL keystore \" + path + \" of type \" + type, e);\n            }\n        }\n\n        private Long lastModifiedMs(String path) {\n            try {\n                return Files.getLastModifiedTime(Paths.get(path)).toMillis();\n            } catch (IOException e) {\n                log.error(\"Modification time of key store could not be obtained: \" + path, e);\n                return null;\n            }\n        }\n\n        public boolean modified() {\n            Long modifiedMs = lastModifiedMs(path);\n            return modifiedMs != null && !Objects.equals(modifiedMs, this.fileLastModifiedMs);\n        }\n\n        @Override\n        public String toString() {\n            return \"SecurityStore(\" +\n                    \"path=\" + path +\n                    \", modificationTime=\" + (fileLastModifiedMs == null ? null : new Date(fileLastModifiedMs)) + \")\";\n        }\n    }\n\n    static class FileBasedPemStore extends FileBasedStore {\n        FileBasedPemStore(String path, Password keyPassword, boolean isKeyStore) {\n            super(PEM_TYPE, path, null, keyPassword, isKeyStore);\n        }\n\n        @Override\n        protected KeyStore load(boolean isKeyStore) {\n            try {\n                Password storeContents = new Password(Utils.readFileAsString(path));\n                PemStore pemStore = isKeyStore ? new PemStore(storeContents, storeContents, keyPassword) :\n                    new PemStore(storeContents);\n                return pemStore.keyStore;\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Failed to load PEM SSL keystore \" + path, e);\n            }\n        }\n    }\n\n    static class PemStore implements SecurityStore {\n        private static final PemParser CERTIFICATE_PARSER = new PemParser(\"CERTIFICATE\");\n        private static final PemParser PRIVATE_KEY_PARSER = new PemParser(\"PRIVATE KEY\");\n        private static final List<KeyFactory> KEY_FACTORIES = Arrays.asList(\n                keyFactory(\"RSA\"),\n                keyFactory(\"DSA\"),\n                keyFactory(\"EC\")\n        );\n\n        private final char[] keyPassword;\n        private final KeyStore keyStore;\n\n        PemStore(Password certificateChain, Password privateKey, Password keyPassword) {\n            this.keyPassword = keyPassword == null ? null : keyPassword.value().toCharArray();\n            keyStore = createKeyStoreFromPem(privateKey.value(), certificateChain.value(), this.keyPassword);\n        }\n\n        PemStore(Password trustStoreCerts) {\n            this.keyPassword = null;\n            keyStore = createTrustStoreFromPem(trustStoreCerts.value());\n        }\n\n        @Override\n        public KeyStore get() {\n            return keyStore;\n        }\n\n        @Override\n        public char[] keyPassword() {\n            return keyPassword;\n        }\n\n        @Override\n        public boolean modified() {\n            return false;\n        }\n\n        private KeyStore createKeyStoreFromPem(String privateKeyPem, String certChainPem, char[] keyPassword) {\n            try {\n                KeyStore ks = KeyStore.getInstance(\"PKCS12\");\n                ks.load(null, null);\n                Key key = privateKey(privateKeyPem, keyPassword);\n                Certificate[] certChain = certs(certChainPem);\n                ks.setKeyEntry(\"kafka\", key, keyPassword, certChain);\n                return ks;\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Invalid PEM keystore configs\", e);\n            }\n        }\n\n        private KeyStore createTrustStoreFromPem(String trustedCertsPem) {\n            try {\n                KeyStore ts = KeyStore.getInstance(\"PKCS12\");\n                ts.load(null, null);\n                Certificate[] certs = certs(trustedCertsPem);\n                for (int i = 0; i < certs.length; i++) {\n                    ts.setCertificateEntry(\"kafka\" + i, certs[i]);\n                }\n                return ts;\n            } catch (InvalidConfigurationException e) {\n                throw e;\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Invalid PEM truststore configs\", e);\n            }\n        }\n\n        private Certificate[] certs(String pem) throws GeneralSecurityException {\n            List<byte[]> certEntries = CERTIFICATE_PARSER.pemEntries(pem);\n            if (certEntries.isEmpty())\n                throw new InvalidConfigurationException(\"At least one certificate expected, but none found\");\n\n            Certificate[] certs = new Certificate[certEntries.size()];\n            for (int i = 0; i < certs.length; i++) {\n                certs[i] = CertificateFactory.getInstance(\"X.509\")\n                    .generateCertificate(new ByteArrayInputStream(certEntries.get(i)));\n            }\n            return certs;\n        }\n\n        private PrivateKey privateKey(String pem, char[] keyPassword) throws Exception {\n            List<byte[]> keyEntries = PRIVATE_KEY_PARSER.pemEntries(pem);\n            if (keyEntries.isEmpty())\n                throw new InvalidConfigurationException(\"Private key not provided\");\n            if (keyEntries.size() != 1)\n                throw new InvalidConfigurationException(\"Expected one private key, but found \" + keyEntries.size());\n\n            byte[] keyBytes = keyEntries.get(0);\n            PKCS8EncodedKeySpec keySpec;\n            if (keyPassword == null) {\n                keySpec = new PKCS8EncodedKeySpec(keyBytes);\n            } else {\n                EncryptedPrivateKeyInfo keyInfo = new EncryptedPrivateKeyInfo(keyBytes);\n                String algorithm = keyInfo.getAlgName();\n                SecretKeyFactory keyFactory = SecretKeyFactory.getInstance(algorithm);\n                SecretKey pbeKey = keyFactory.generateSecret(new PBEKeySpec(keyPassword));\n                Cipher cipher = Cipher.getInstance(algorithm);\n                cipher.init(Cipher.DECRYPT_MODE, pbeKey, keyInfo.getAlgParameters());\n                keySpec = keyInfo.getKeySpec(cipher);\n            }\n\n            InvalidKeySpecException firstException = null;\n            for (KeyFactory factory : KEY_FACTORIES) {\n                try {\n                    return factory.generatePrivate(keySpec);\n                } catch (InvalidKeySpecException e) {\n                    if (firstException == null)\n                        firstException = e;\n                }\n            }\n            throw new InvalidConfigurationException(\"Private key could not be loaded\", firstException);\n        }\n\n        private static KeyFactory keyFactory(String algorithm) {\n            try {\n                return KeyFactory.getInstance(algorithm);\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Could not create key factory for algorithm \" + algorithm, e);\n            }\n        }\n    }\n\n    /**\n     * Parser to process certificate/private key entries from PEM files\n     * Examples:\n     *   -----BEGIN CERTIFICATE-----\n     *   Base64 cert\n     *   -----END CERTIFICATE-----\n     *\n     *   -----BEGIN ENCRYPTED PRIVATE KEY-----\n     *   Base64 private key\n     *   -----END ENCRYPTED PRIVATE KEY-----\n     *   Additional data may be included before headers, so we match all entries within the PEM.\n     */\n    static class PemParser {\n        private final String name;\n        private final Pattern pattern;\n\n        PemParser(String name) {\n            this.name = name;\n            String beginOrEndFormat = \"-+%s\\\\s*.*%s[^-]*-+\\\\s+\";\n            String nameIgnoreSpace = name.replace(\" \", \"\\\\s+\");\n\n            String encodingParams = \"\\\\s*[^\\\\r\\\\n]*:[^\\\\r\\\\n]*[\\\\r\\\\n]+\";\n            String base64Pattern = \"([a-zA-Z0-9/+=\\\\s]*)\";\n            String patternStr =  String.format(beginOrEndFormat, \"BEGIN\", nameIgnoreSpace) +\n                String.format(\"(?:%s)*\", encodingParams) +\n                base64Pattern +\n                String.format(beginOrEndFormat, \"END\", nameIgnoreSpace);\n            pattern = Pattern.compile(patternStr);\n        }\n\n        private List<byte[]> pemEntries(String pem) {\n            Matcher matcher = pattern.matcher(pem + \"\\n\"); // allow last newline to be omitted in value\n            List<byte[]>  entries = new ArrayList<>();\n            while (matcher.find()) {\n                String base64Str = matcher.group(1).replaceAll(\"\\\\s\", \"\");\n                entries.add(Base64.getDecoder().decode(base64Str));\n            }\n            if (entries.isEmpty())\n                throw new InvalidConfigurationException(\"No matching \" + name + \" entries in PEM file\");\n            return entries;\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 18,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactory.java",
        "line": 513,
        "sink": "source.](1).",
        "source": "-",
        "sourceLine": 513,
        "qualifier": "Potential Insecure randomness due to a [Insecure randomness source.](1).",
        "line_number": 513,
        "steps": [
            {
                "line": 254,
                "source": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "filepath": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "methodName": null,
                "exampleID": 19
            },
            {
                "line": 254,
                "source": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "filepath": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "methodName": null,
                "exampleID": 19
            },
            {
                "line": 254,
                "source": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "filepath": "tools/src/main/java/org/apache/kafka/tools/consumer/ConsoleConsumerOptions.java",
                "methodName": null,
                "exampleID": 19
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/metrics/JmxReporter.java#L328",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.metrics;\n\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.MetricName;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.utils.ConfigUtils;\nimport org.apache.kafka.common.utils.Sanitizer;\nimport org.apache.kafka.common.utils.Utils;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.lang.management.ManagementFactory;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Set;\nimport java.util.function.Predicate;\nimport java.util.regex.Pattern;\nimport java.util.regex.PatternSyntaxException;\n\nimport javax.management.Attribute;\nimport javax.management.AttributeList;\nimport javax.management.AttributeNotFoundException;\nimport javax.management.DynamicMBean;\nimport javax.management.JMException;\nimport javax.management.MBeanAttributeInfo;\nimport javax.management.MBeanInfo;\nimport javax.management.MBeanServer;\nimport javax.management.MalformedObjectNameException;\nimport javax.management.ObjectName;\n\n/**\n * Register metrics in JMX as dynamic mbeans based on the metric names\n */\npublic class JmxReporter implements MetricsReporter {\n\n    public static final String METRICS_CONFIG_PREFIX = \"metrics.jmx.\";\n\n    public static final String EXCLUDE_CONFIG = METRICS_CONFIG_PREFIX + \"exclude\";\n    public static final String EXCLUDE_CONFIG_ALIAS = METRICS_CONFIG_PREFIX + \"blacklist\";\n\n    public static final String INCLUDE_CONFIG = METRICS_CONFIG_PREFIX + \"include\";\n    public static final String INCLUDE_CONFIG_ALIAS = METRICS_CONFIG_PREFIX + \"whitelist\";\n\n\n    public static final Set<String> RECONFIGURABLE_CONFIGS = Utils.mkSet(INCLUDE_CONFIG,\n                                                                         INCLUDE_CONFIG_ALIAS,\n                                                                         EXCLUDE_CONFIG,\n                                                                         EXCLUDE_CONFIG_ALIAS);\n\n    public static final String DEFAULT_INCLUDE = \".*\";\n    public static final String DEFAULT_EXCLUDE = \"\";\n\n    private static final Logger log = LoggerFactory.getLogger(JmxReporter.class);\n    private static final Object LOCK = new Object();\n    private String prefix;\n    private final Map<String, KafkaMbean> mbeans = new HashMap<>();\n    private Predicate<String> mbeanPredicate = s -> true;\n\n    public JmxReporter() {\n        this(\"\");\n    }\n\n    /**\n     * Create a JMX reporter that prefixes all metrics with the given string.\n     *  @deprecated Since 2.6.0. Use {@link JmxReporter#JmxReporter()}\n     *  Initialize JmxReporter with {@link JmxReporter#contextChange(MetricsContext)}\n     *  Populate prefix by adding _namespace/prefix key value pair to {@link MetricsContext}\n     */\n    @Deprecated\n    public JmxReporter(String prefix) {\n        this.prefix = prefix != null ? prefix : \"\";\n    }\n\n    @Override\n    public void configure(Map<String, ?> configs) {\n        reconfigure(configs);\n    }\n\n    @Override\n    public Set<String> reconfigurableConfigs() {\n        return RECONFIGURABLE_CONFIGS;\n    }\n\n    @Override\n    public void validateReconfiguration(Map<String, ?> configs) throws ConfigException {\n        compilePredicate(configs);\n    }\n\n    @Override\n    public void reconfigure(Map<String, ?> configs) {\n        synchronized (LOCK) {\n            this.mbeanPredicate = JmxReporter.compilePredicate(configs);\n\n            mbeans.forEach((name, mbean) -> {\n                if (mbeanPredicate.test(name)) {\n                    reregister(mbean);\n                } else {\n                    unregister(mbean);\n                }\n            });\n        }\n    }\n\n    @Override\n    public void init(List<KafkaMetric> metrics) {\n        synchronized (LOCK) {\n            for (KafkaMetric metric : metrics)\n                addAttribute(metric);\n\n            mbeans.forEach((name, mbean) -> {\n                if (mbeanPredicate.test(name)) {\n                    reregister(mbean);\n                }\n            });\n        }\n    }\n\n    public boolean containsMbean(String mbeanName) {\n        return mbeans.containsKey(mbeanName);\n    }\n\n    @Override\n    public void metricChange(KafkaMetric metric) {\n        synchronized (LOCK) {\n            String mbeanName = addAttribute(metric);\n            if (mbeanName != null && mbeanPredicate.test(mbeanName)) {\n                reregister(mbeans.get(mbeanName));\n            }\n        }\n    }\n\n    @Override\n    public void metricRemoval(KafkaMetric metric) {\n        synchronized (LOCK) {\n            MetricName metricName = metric.metricName();\n            String mBeanName = getMBeanName(prefix, metricName);\n            KafkaMbean mbean = removeAttribute(metric, mBeanName);\n            if (mbean != null) {\n                if (mbean.metrics.isEmpty()) {\n                    unregister(mbean);\n                    mbeans.remove(mBeanName);\n                } else if (mbeanPredicate.test(mBeanName))\n                    reregister(mbean);\n            }\n        }\n    }\n\n    private KafkaMbean removeAttribute(KafkaMetric metric, String mBeanName) {\n        MetricName metricName = metric.metricName();\n        KafkaMbean mbean = this.mbeans.get(mBeanName);\n        if (mbean != null)\n            mbean.removeAttribute(metricName.name());\n        return mbean;\n    }\n\n    private String addAttribute(KafkaMetric metric) {\n        try {\n            MetricName metricName = metric.metricName();\n            String mBeanName = getMBeanName(prefix, metricName);\n            if (!this.mbeans.containsKey(mBeanName))\n                mbeans.put(mBeanName, new KafkaMbean(mBeanName));\n            KafkaMbean mbean = this.mbeans.get(mBeanName);\n            mbean.setAttribute(metricName.name(), metric);\n            return mBeanName;\n        } catch (JMException e) {\n            throw new KafkaException(\"Error creating mbean attribute for metricName :\" + metric.metricName(), e);\n        }\n    }\n\n    /**\n     * @param metricName\n     * @return standard JMX MBean name in the following format domainName:type=metricType,key1=val1,key2=val2\n     */\n    static String getMBeanName(String prefix, MetricName metricName) {\n        StringBuilder mBeanName = new StringBuilder();\n        mBeanName.append(prefix);\n        mBeanName.append(\":type=\");\n        mBeanName.append(metricName.group());\n        for (Map.Entry<String, String> entry : metricName.tags().entrySet()) {\n            if (entry.getKey().isEmpty() || entry.getValue().isEmpty())\n                continue;\n            mBeanName.append(\",\");\n            mBeanName.append(entry.getKey());\n            mBeanName.append(\"=\");\n            mBeanName.append(Sanitizer.jmxSanitize(entry.getValue()));\n        }\n        return mBeanName.toString();\n    }\n\n    public void close() {\n        synchronized (LOCK) {\n            for (KafkaMbean mbean : this.mbeans.values())\n                unregister(mbean);\n        }\n    }\n\n    private void unregister(KafkaMbean mbean) {\n        MBeanServer server = ManagementFactory.getPlatformMBeanServer();\n        try {\n            if (server.isRegistered(mbean.name()))\n                server.unregisterMBean(mbean.name());\n        } catch (JMException e) {\n            throw new KafkaException(\"Error unregistering mbean\", e);\n        }\n    }\n\n    private void reregister(KafkaMbean mbean) {\n        unregister(mbean);\n        try {\n            ManagementFactory.getPlatformMBeanServer().registerMBean(mbean, mbean.name());\n        } catch (JMException e) {\n            throw new KafkaException(\"Error registering mbean \" + mbean.name(), e);\n        }\n    }\n\n    private static class KafkaMbean implements DynamicMBean {\n        private final ObjectName objectName;\n        private final Map<String, KafkaMetric> metrics;\n\n        KafkaMbean(String mbeanName) throws MalformedObjectNameException {\n            this.metrics = new HashMap<>();\n            this.objectName = new ObjectName(mbeanName);\n        }\n\n        public ObjectName name() {\n            return objectName;\n        }\n\n        void setAttribute(String name, KafkaMetric metric) {\n            this.metrics.put(name, metric);\n        }\n\n        @Override\n        public Object getAttribute(String name) throws AttributeNotFoundException {\n            if (this.metrics.containsKey(name))\n                return this.metrics.get(name).metricValue();\n            else\n                throw new AttributeNotFoundException(\"Could not find attribute \" + name);\n        }\n\n        @Override\n        public AttributeList getAttributes(String[] names) {\n            AttributeList list = new AttributeList();\n            for (String name : names) {\n                try {\n                    list.add(new Attribute(name, getAttribute(name)));\n                } catch (Exception e) {\n                    log.warn(\"Error getting JMX attribute '{}'\", name, e);\n                }\n            }\n            return list;\n        }\n\n        KafkaMetric removeAttribute(String name) {\n            return this.metrics.remove(name);\n        }\n\n        @Override\n        public MBeanInfo getMBeanInfo() {\n            MBeanAttributeInfo[] attrs = new MBeanAttributeInfo[metrics.size()];\n            int i = 0;\n            for (Map.Entry<String, KafkaMetric> entry : this.metrics.entrySet()) {\n                String attribute = entry.getKey();\n                KafkaMetric metric = entry.getValue();\n                attrs[i] = new MBeanAttributeInfo(attribute,\n                                                  double.class.getName(),\n                                                  metric.metricName().description(),\n                                                  true,\n                                                  false,\n                                                  false);\n                i += 1;\n            }\n            return new MBeanInfo(this.getClass().getName(), \"\", attrs, null, null, null);\n        }\n\n        @Override\n        public Object invoke(String name, Object[] params, String[] sig) {\n            throw new UnsupportedOperationException(\"Set not allowed.\");\n        }\n\n        @Override\n        public void setAttribute(Attribute attribute) {\n            throw new UnsupportedOperationException(\"Set not allowed.\");\n        }\n\n        @Override\n        public AttributeList setAttributes(AttributeList list) {\n            throw new UnsupportedOperationException(\"Set not allowed.\");\n        }\n\n    }\n\n    public static Predicate<String> compilePredicate(Map<String, ?> originalConfig) {\n        Map<String, ?> configs = ConfigUtils.translateDeprecatedConfigs(\n            originalConfig, new String[][]{{INCLUDE_CONFIG, INCLUDE_CONFIG_ALIAS},\n                                           {EXCLUDE_CONFIG, EXCLUDE_CONFIG_ALIAS}});\n        String include = (String) configs.get(INCLUDE_CONFIG);\n        String exclude = (String) configs.get(EXCLUDE_CONFIG);\n\n        if (include == null) {\n            include = DEFAULT_INCLUDE;\n        }\n\n        if (exclude == null) {\n            exclude = DEFAULT_EXCLUDE;\n        }\n\n        try {\n            Pattern includePattern = Pattern.compile(include);\n            Pattern excludePattern = Pattern.compile(exclude);\n\n            return s -> includePattern.matcher(s).matches()\n                        && !excludePattern.matcher(s).matches();\n        } catch (PatternSyntaxException e) {\n            throw new ConfigException(\"JMX filter for configuration\" + METRICS_CONFIG_PREFIX\n                                      + \".(include/exclude) is not a valid regular expression\");\n        }\n    }\n\n    @Override\n    public void contextChange(MetricsContext metricsContext) {\n        String namespace = metricsContext.contextLabels().get(MetricsContext.NAMESPACE);\n        Objects.requireNonNull(namespace);\n        synchronized (LOCK) {\n            if (!mbeans.isEmpty()) {\n                throw new IllegalStateException(\"JMX MetricsContext can only be updated before JMX metrics are created\");\n            }\n\n            // prevent prefix from getting reset back to empty for backwards compatibility\n            // with the deprecated JmxReporter(String prefix) constructor, in case contextChange gets called\n            // via one of the Metrics() constructor with a default empty MetricsContext()\n            if (namespace.isEmpty()) {\n                return;\n            }\n\n            prefix = namespace;\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 20,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/metrics/JmxReporter.java",
        "line": 328,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 328,
        "qualifier": "This regular expression is constructed from a [user-provided value](1).\nThis regular expression is constructed from a [user-provided value](2).\nThis regular expression is constructed from a [user-provided value](3).\nThis regular expression is constructed from a [user-provided value](4).\nThis regular expression is constructed from a [user-provided value](5).\nThis regular expression is constructed from a [user-provided value](6).\nThis regular expression is constructed from a [user-provided value](7).\nThis regular expression is constructed from a [user-provided value](8).\nThis regular expression is constructed from a [user-provided value](9).",
        "line_number": 328,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 21
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 21
            },
            {
                "line": 251,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 21
            },
            {
                "line": 265,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 21
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/metrics/JmxReporter.java#L329",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.metrics;\n\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.MetricName;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.utils.ConfigUtils;\nimport org.apache.kafka.common.utils.Sanitizer;\nimport org.apache.kafka.common.utils.Utils;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.lang.management.ManagementFactory;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Set;\nimport java.util.function.Predicate;\nimport java.util.regex.Pattern;\nimport java.util.regex.PatternSyntaxException;\n\nimport javax.management.Attribute;\nimport javax.management.AttributeList;\nimport javax.management.AttributeNotFoundException;\nimport javax.management.DynamicMBean;\nimport javax.management.JMException;\nimport javax.management.MBeanAttributeInfo;\nimport javax.management.MBeanInfo;\nimport javax.management.MBeanServer;\nimport javax.management.MalformedObjectNameException;\nimport javax.management.ObjectName;\n\n/**\n * Register metrics in JMX as dynamic mbeans based on the metric names\n */\npublic class JmxReporter implements MetricsReporter {\n\n    public static final String METRICS_CONFIG_PREFIX = \"metrics.jmx.\";\n\n    public static final String EXCLUDE_CONFIG = METRICS_CONFIG_PREFIX + \"exclude\";\n    public static final String EXCLUDE_CONFIG_ALIAS = METRICS_CONFIG_PREFIX + \"blacklist\";\n\n    public static final String INCLUDE_CONFIG = METRICS_CONFIG_PREFIX + \"include\";\n    public static final String INCLUDE_CONFIG_ALIAS = METRICS_CONFIG_PREFIX + \"whitelist\";\n\n\n    public static final Set<String> RECONFIGURABLE_CONFIGS = Utils.mkSet(INCLUDE_CONFIG,\n                                                                         INCLUDE_CONFIG_ALIAS,\n                                                                         EXCLUDE_CONFIG,\n                                                                         EXCLUDE_CONFIG_ALIAS);\n\n    public static final String DEFAULT_INCLUDE = \".*\";\n    public static final String DEFAULT_EXCLUDE = \"\";\n\n    private static final Logger log = LoggerFactory.getLogger(JmxReporter.class);\n    private static final Object LOCK = new Object();\n    private String prefix;\n    private final Map<String, KafkaMbean> mbeans = new HashMap<>();\n    private Predicate<String> mbeanPredicate = s -> true;\n\n    public JmxReporter() {\n        this(\"\");\n    }\n\n    /**\n     * Create a JMX reporter that prefixes all metrics with the given string.\n     *  @deprecated Since 2.6.0. Use {@link JmxReporter#JmxReporter()}\n     *  Initialize JmxReporter with {@link JmxReporter#contextChange(MetricsContext)}\n     *  Populate prefix by adding _namespace/prefix key value pair to {@link MetricsContext}\n     */\n    @Deprecated\n    public JmxReporter(String prefix) {\n        this.prefix = prefix != null ? prefix : \"\";\n    }\n\n    @Override\n    public void configure(Map<String, ?> configs) {\n        reconfigure(configs);\n    }\n\n    @Override\n    public Set<String> reconfigurableConfigs() {\n        return RECONFIGURABLE_CONFIGS;\n    }\n\n    @Override\n    public void validateReconfiguration(Map<String, ?> configs) throws ConfigException {\n        compilePredicate(configs);\n    }\n\n    @Override\n    public void reconfigure(Map<String, ?> configs) {\n        synchronized (LOCK) {\n            this.mbeanPredicate = JmxReporter.compilePredicate(configs);\n\n            mbeans.forEach((name, mbean) -> {\n                if (mbeanPredicate.test(name)) {\n                    reregister(mbean);\n                } else {\n                    unregister(mbean);\n                }\n            });\n        }\n    }\n\n    @Override\n    public void init(List<KafkaMetric> metrics) {\n        synchronized (LOCK) {\n            for (KafkaMetric metric : metrics)\n                addAttribute(metric);\n\n            mbeans.forEach((name, mbean) -> {\n                if (mbeanPredicate.test(name)) {\n                    reregister(mbean);\n                }\n            });\n        }\n    }\n\n    public boolean containsMbean(String mbeanName) {\n        return mbeans.containsKey(mbeanName);\n    }\n\n    @Override\n    public void metricChange(KafkaMetric metric) {\n        synchronized (LOCK) {\n            String mbeanName = addAttribute(metric);\n            if (mbeanName != null && mbeanPredicate.test(mbeanName)) {\n                reregister(mbeans.get(mbeanName));\n            }\n        }\n    }\n\n    @Override\n    public void metricRemoval(KafkaMetric metric) {\n        synchronized (LOCK) {\n            MetricName metricName = metric.metricName();\n            String mBeanName = getMBeanName(prefix, metricName);\n            KafkaMbean mbean = removeAttribute(metric, mBeanName);\n            if (mbean != null) {\n                if (mbean.metrics.isEmpty()) {\n                    unregister(mbean);\n                    mbeans.remove(mBeanName);\n                } else if (mbeanPredicate.test(mBeanName))\n                    reregister(mbean);\n            }\n        }\n    }\n\n    private KafkaMbean removeAttribute(KafkaMetric metric, String mBeanName) {\n        MetricName metricName = metric.metricName();\n        KafkaMbean mbean = this.mbeans.get(mBeanName);\n        if (mbean != null)\n            mbean.removeAttribute(metricName.name());\n        return mbean;\n    }\n\n    private String addAttribute(KafkaMetric metric) {\n        try {\n            MetricName metricName = metric.metricName();\n            String mBeanName = getMBeanName(prefix, metricName);\n            if (!this.mbeans.containsKey(mBeanName))\n                mbeans.put(mBeanName, new KafkaMbean(mBeanName));\n            KafkaMbean mbean = this.mbeans.get(mBeanName);\n            mbean.setAttribute(metricName.name(), metric);\n            return mBeanName;\n        } catch (JMException e) {\n            throw new KafkaException(\"Error creating mbean attribute for metricName :\" + metric.metricName(), e);\n        }\n    }\n\n    /**\n     * @param metricName\n     * @return standard JMX MBean name in the following format domainName:type=metricType,key1=val1,key2=val2\n     */\n    static String getMBeanName(String prefix, MetricName metricName) {\n        StringBuilder mBeanName = new StringBuilder();\n        mBeanName.append(prefix);\n        mBeanName.append(\":type=\");\n        mBeanName.append(metricName.group());\n        for (Map.Entry<String, String> entry : metricName.tags().entrySet()) {\n            if (entry.getKey().isEmpty() || entry.getValue().isEmpty())\n                continue;\n            mBeanName.append(\",\");\n            mBeanName.append(entry.getKey());\n            mBeanName.append(\"=\");\n            mBeanName.append(Sanitizer.jmxSanitize(entry.getValue()));\n        }\n        return mBeanName.toString();\n    }\n\n    public void close() {\n        synchronized (LOCK) {\n            for (KafkaMbean mbean : this.mbeans.values())\n                unregister(mbean);\n        }\n    }\n\n    private void unregister(KafkaMbean mbean) {\n        MBeanServer server = ManagementFactory.getPlatformMBeanServer();\n        try {\n            if (server.isRegistered(mbean.name()))\n                server.unregisterMBean(mbean.name());\n        } catch (JMException e) {\n            throw new KafkaException(\"Error unregistering mbean\", e);\n        }\n    }\n\n    private void reregister(KafkaMbean mbean) {\n        unregister(mbean);\n        try {\n            ManagementFactory.getPlatformMBeanServer().registerMBean(mbean, mbean.name());\n        } catch (JMException e) {\n            throw new KafkaException(\"Error registering mbean \" + mbean.name(), e);\n        }\n    }\n\n    private static class KafkaMbean implements DynamicMBean {\n        private final ObjectName objectName;\n        private final Map<String, KafkaMetric> metrics;\n\n        KafkaMbean(String mbeanName) throws MalformedObjectNameException {\n            this.metrics = new HashMap<>();\n            this.objectName = new ObjectName(mbeanName);\n        }\n\n        public ObjectName name() {\n            return objectName;\n        }\n\n        void setAttribute(String name, KafkaMetric metric) {\n            this.metrics.put(name, metric);\n        }\n\n        @Override\n        public Object getAttribute(String name) throws AttributeNotFoundException {\n            if (this.metrics.containsKey(name))\n                return this.metrics.get(name).metricValue();\n            else\n                throw new AttributeNotFoundException(\"Could not find attribute \" + name);\n        }\n\n        @Override\n        public AttributeList getAttributes(String[] names) {\n            AttributeList list = new AttributeList();\n            for (String name : names) {\n                try {\n                    list.add(new Attribute(name, getAttribute(name)));\n                } catch (Exception e) {\n                    log.warn(\"Error getting JMX attribute '{}'\", name, e);\n                }\n            }\n            return list;\n        }\n\n        KafkaMetric removeAttribute(String name) {\n            return this.metrics.remove(name);\n        }\n\n        @Override\n        public MBeanInfo getMBeanInfo() {\n            MBeanAttributeInfo[] attrs = new MBeanAttributeInfo[metrics.size()];\n            int i = 0;\n            for (Map.Entry<String, KafkaMetric> entry : this.metrics.entrySet()) {\n                String attribute = entry.getKey();\n                KafkaMetric metric = entry.getValue();\n                attrs[i] = new MBeanAttributeInfo(attribute,\n                                                  double.class.getName(),\n                                                  metric.metricName().description(),\n                                                  true,\n                                                  false,\n                                                  false);\n                i += 1;\n            }\n            return new MBeanInfo(this.getClass().getName(), \"\", attrs, null, null, null);\n        }\n\n        @Override\n        public Object invoke(String name, Object[] params, String[] sig) {\n            throw new UnsupportedOperationException(\"Set not allowed.\");\n        }\n\n        @Override\n        public void setAttribute(Attribute attribute) {\n            throw new UnsupportedOperationException(\"Set not allowed.\");\n        }\n\n        @Override\n        public AttributeList setAttributes(AttributeList list) {\n            throw new UnsupportedOperationException(\"Set not allowed.\");\n        }\n\n    }\n\n    public static Predicate<String> compilePredicate(Map<String, ?> originalConfig) {\n        Map<String, ?> configs = ConfigUtils.translateDeprecatedConfigs(\n            originalConfig, new String[][]{{INCLUDE_CONFIG, INCLUDE_CONFIG_ALIAS},\n                                           {EXCLUDE_CONFIG, EXCLUDE_CONFIG_ALIAS}});\n        String include = (String) configs.get(INCLUDE_CONFIG);\n        String exclude = (String) configs.get(EXCLUDE_CONFIG);\n\n        if (include == null) {\n            include = DEFAULT_INCLUDE;\n        }\n\n        if (exclude == null) {\n            exclude = DEFAULT_EXCLUDE;\n        }\n\n        try {\n            Pattern includePattern = Pattern.compile(include);\n            Pattern excludePattern = Pattern.compile(exclude);\n\n            return s -> includePattern.matcher(s).matches()\n                        && !excludePattern.matcher(s).matches();\n        } catch (PatternSyntaxException e) {\n            throw new ConfigException(\"JMX filter for configuration\" + METRICS_CONFIG_PREFIX\n                                      + \".(include/exclude) is not a valid regular expression\");\n        }\n    }\n\n    @Override\n    public void contextChange(MetricsContext metricsContext) {\n        String namespace = metricsContext.contextLabels().get(MetricsContext.NAMESPACE);\n        Objects.requireNonNull(namespace);\n        synchronized (LOCK) {\n            if (!mbeans.isEmpty()) {\n                throw new IllegalStateException(\"JMX MetricsContext can only be updated before JMX metrics are created\");\n            }\n\n            // prevent prefix from getting reset back to empty for backwards compatibility\n            // with the deprecated JmxReporter(String prefix) constructor, in case contextChange gets called\n            // via one of the Metrics() constructor with a default empty MetricsContext()\n            if (namespace.isEmpty()) {\n                return;\n            }\n\n            prefix = namespace;\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 22,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/metrics/JmxReporter.java",
        "line": 329,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 329,
        "qualifier": "This regular expression is constructed from a [user-provided value](1).\nThis regular expression is constructed from a [user-provided value](2).\nThis regular expression is constructed from a [user-provided value](3).\nThis regular expression is constructed from a [user-provided value](4).\nThis regular expression is constructed from a [user-provided value](5).\nThis regular expression is constructed from a [user-provided value](6).\nThis regular expression is constructed from a [user-provided value](7).\nThis regular expression is constructed from a [user-provided value](8).\nThis regular expression is constructed from a [user-provided value](9).",
        "line_number": 329,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 23
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 23
            },
            {
                "line": 251,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 23
            },
            {
                "line": 265,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 23
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosRule.java#L70",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.security.kerberos;\n\nimport java.io.IOException;\nimport java.util.Locale;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n/**\n * An encoding of a rule for translating kerberos names.\n */\nclass KerberosRule {\n\n    /**\n     * A pattern that matches a string without '$' and then a single\n     * parameter with $n.\n     */\n    private static final Pattern PARAMETER_PATTERN = Pattern.compile(\"([^$]*)(\\\\$(\\\\d*))?\");\n\n    /**\n     * A pattern that recognizes simple/non-simple names.\n     */\n    private static final Pattern NON_SIMPLE_PATTERN = Pattern.compile(\"[/@]\");\n\n    private final String defaultRealm;\n    private final boolean isDefault;\n    private final int numOfComponents;\n    private final String format;\n    private final Pattern match;\n    private final Pattern fromPattern;\n    private final String toPattern;\n    private final boolean repeat;\n    private final boolean toLowerCase;\n    private final boolean toUpperCase;\n\n    KerberosRule(String defaultRealm) {\n        this.defaultRealm = defaultRealm;\n        isDefault = true;\n        numOfComponents = 0;\n        format = null;\n        match = null;\n        fromPattern = null;\n        toPattern = null;\n        repeat = false;\n        toLowerCase = false;\n        toUpperCase = false;\n    }\n\n    KerberosRule(String defaultRealm, int numOfComponents, String format, String match, String fromPattern,\n                 String toPattern, boolean repeat, boolean toLowerCase, boolean toUpperCase) {\n        this.defaultRealm = defaultRealm;\n        isDefault = false;\n        this.numOfComponents = numOfComponents;\n        this.format = format;\n        this.match = match == null ? null : Pattern.compile(match);\n        this.fromPattern =\n                fromPattern == null ? null : Pattern.compile(fromPattern);\n        this.toPattern = toPattern;\n        this.repeat = repeat;\n        this.toLowerCase = toLowerCase;\n        this.toUpperCase = toUpperCase;\n    }\n\n    @Override\n    public String toString() {\n        StringBuilder buf = new StringBuilder();\n        if (isDefault) {\n            buf.append(\"DEFAULT\");\n        } else {\n            buf.append(\"RULE:[\");\n            buf.append(numOfComponents);\n            buf.append(':');\n            buf.append(format);\n            buf.append(']');\n            if (match != null) {\n                buf.append('(');\n                buf.append(match);\n                buf.append(')');\n            }\n            if (fromPattern != null) {\n                buf.append(\"s/\");\n                buf.append(fromPattern);\n                buf.append('/');\n                buf.append(toPattern);\n                buf.append('/');\n                if (repeat) {\n                    buf.append('g');\n                }\n            }\n            if (toLowerCase) {\n                buf.append(\"/L\");\n            }\n            if (toUpperCase) {\n                buf.append(\"/U\");\n            }\n        }\n        return buf.toString();\n    }\n\n    /**\n     * Replace the numbered parameters of the form $n where n is from 0 to\n     * the length of params - 1. Normal text is copied directly and $n is replaced\n     * by the corresponding parameter.\n     * @param format the string to replace parameters again\n     * @param params the list of parameters\n     * @return the generated string with the parameter references replaced.\n     * @throws BadFormatString\n     */\n    static String replaceParameters(String format,\n                                    String[] params) throws BadFormatString {\n        Matcher match = PARAMETER_PATTERN.matcher(format);\n        int start = 0;\n        StringBuilder result = new StringBuilder();\n        while (start < format.length() && match.find(start)) {\n            result.append(match.group(1));\n            String paramNum = match.group(3);\n            if (paramNum != null) {\n                try {\n                    int num = Integer.parseInt(paramNum);\n                    if (num < 0 || num >= params.length) {\n                        throw new BadFormatString(\"index \" + num + \" from \" + format +\n                                \" is outside of the valid range 0 to \" +\n                                (params.length - 1));\n                    }\n                    result.append(params[num]);\n                } catch (NumberFormatException nfe) {\n                    throw new BadFormatString(\"bad format in username mapping in \" +\n                            paramNum, nfe);\n                }\n\n            }\n            start = match.end();\n        }\n        return result.toString();\n    }\n\n    /**\n     * Replace the matches of the from pattern in the base string with the value\n     * of the to string.\n     * @param base the string to transform\n     * @param from the pattern to look for in the base string\n     * @param to the string to replace matches of the pattern with\n     * @param repeat whether the substitution should be repeated\n     * @return\n     */\n    static String replaceSubstitution(String base, Pattern from, String to,\n                                      boolean repeat) {\n        Matcher match = from.matcher(base);\n        if (repeat) {\n            return match.replaceAll(to);\n        } else {\n            return match.replaceFirst(to);\n        }\n    }\n\n    /**\n     * Try to apply this rule to the given name represented as a parameter\n     * array.\n     * @param params first element is the realm, second and later elements are\n     *        are the components of the name \"a/b@FOO\" -> {\"FOO\", \"a\", \"b\"}\n     * @return the short name if this rule applies or null\n     * @throws IOException throws if something is wrong with the rules\n     */\n    String apply(String[] params) throws IOException {\n        String result = null;\n        if (isDefault) {\n            if (defaultRealm.equals(params[0])) {\n                result = params[1];\n            }\n        } else if (params.length - 1 == numOfComponents) {\n            String base = replaceParameters(format, params);\n            if (match == null || match.matcher(base).matches()) {\n                if (fromPattern == null) {\n                    result = base;\n                } else {\n                    result = replaceSubstitution(base, fromPattern, toPattern,  repeat);\n                }\n            }\n        }\n        if (result != null && NON_SIMPLE_PATTERN.matcher(result).find()) {\n            throw new NoMatchingRule(\"Non-simple name \" + result + \" after auth_to_local rule \" + this);\n        }\n        if (toLowerCase && result != null) {\n            result = result.toLowerCase(Locale.ENGLISH);\n        } else if (toUpperCase && result != null) {\n            result = result.toUpperCase(Locale.ENGLISH);\n        }\n\n        return result;\n    }\n}\n",
        "methodName": null,
        "exampleID": 24,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosRule.java",
        "line": 70,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 70,
        "qualifier": "This regular expression is constructed from a [user-provided value](1).\nThis regular expression is constructed from a [user-provided value](2).\nThis regular expression is constructed from a [user-provided value](3).\nThis regular expression is constructed from a [user-provided value](4).\nThis regular expression is constructed from a [user-provided value](5).\nThis regular expression is constructed from a [user-provided value](6).\nThis regular expression is constructed from a [user-provided value](7).\nThis regular expression is constructed from a [user-provided value](8).\nThis regular expression is constructed from a [user-provided value](9).",
        "line_number": 70,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 25
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 25
            },
            {
                "line": 251,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 25
            },
            {
                "line": 265,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 25
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosRule.java#L72",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.security.kerberos;\n\nimport java.io.IOException;\nimport java.util.Locale;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n/**\n * An encoding of a rule for translating kerberos names.\n */\nclass KerberosRule {\n\n    /**\n     * A pattern that matches a string without '$' and then a single\n     * parameter with $n.\n     */\n    private static final Pattern PARAMETER_PATTERN = Pattern.compile(\"([^$]*)(\\\\$(\\\\d*))?\");\n\n    /**\n     * A pattern that recognizes simple/non-simple names.\n     */\n    private static final Pattern NON_SIMPLE_PATTERN = Pattern.compile(\"[/@]\");\n\n    private final String defaultRealm;\n    private final boolean isDefault;\n    private final int numOfComponents;\n    private final String format;\n    private final Pattern match;\n    private final Pattern fromPattern;\n    private final String toPattern;\n    private final boolean repeat;\n    private final boolean toLowerCase;\n    private final boolean toUpperCase;\n\n    KerberosRule(String defaultRealm) {\n        this.defaultRealm = defaultRealm;\n        isDefault = true;\n        numOfComponents = 0;\n        format = null;\n        match = null;\n        fromPattern = null;\n        toPattern = null;\n        repeat = false;\n        toLowerCase = false;\n        toUpperCase = false;\n    }\n\n    KerberosRule(String defaultRealm, int numOfComponents, String format, String match, String fromPattern,\n                 String toPattern, boolean repeat, boolean toLowerCase, boolean toUpperCase) {\n        this.defaultRealm = defaultRealm;\n        isDefault = false;\n        this.numOfComponents = numOfComponents;\n        this.format = format;\n        this.match = match == null ? null : Pattern.compile(match);\n        this.fromPattern =\n                fromPattern == null ? null : Pattern.compile(fromPattern);\n        this.toPattern = toPattern;\n        this.repeat = repeat;\n        this.toLowerCase = toLowerCase;\n        this.toUpperCase = toUpperCase;\n    }\n\n    @Override\n    public String toString() {\n        StringBuilder buf = new StringBuilder();\n        if (isDefault) {\n            buf.append(\"DEFAULT\");\n        } else {\n            buf.append(\"RULE:[\");\n            buf.append(numOfComponents);\n            buf.append(':');\n            buf.append(format);\n            buf.append(']');\n            if (match != null) {\n                buf.append('(');\n                buf.append(match);\n                buf.append(')');\n            }\n            if (fromPattern != null) {\n                buf.append(\"s/\");\n                buf.append(fromPattern);\n                buf.append('/');\n                buf.append(toPattern);\n                buf.append('/');\n                if (repeat) {\n                    buf.append('g');\n                }\n            }\n            if (toLowerCase) {\n                buf.append(\"/L\");\n            }\n            if (toUpperCase) {\n                buf.append(\"/U\");\n            }\n        }\n        return buf.toString();\n    }\n\n    /**\n     * Replace the numbered parameters of the form $n where n is from 0 to\n     * the length of params - 1. Normal text is copied directly and $n is replaced\n     * by the corresponding parameter.\n     * @param format the string to replace parameters again\n     * @param params the list of parameters\n     * @return the generated string with the parameter references replaced.\n     * @throws BadFormatString\n     */\n    static String replaceParameters(String format,\n                                    String[] params) throws BadFormatString {\n        Matcher match = PARAMETER_PATTERN.matcher(format);\n        int start = 0;\n        StringBuilder result = new StringBuilder();\n        while (start < format.length() && match.find(start)) {\n            result.append(match.group(1));\n            String paramNum = match.group(3);\n            if (paramNum != null) {\n                try {\n                    int num = Integer.parseInt(paramNum);\n                    if (num < 0 || num >= params.length) {\n                        throw new BadFormatString(\"index \" + num + \" from \" + format +\n                                \" is outside of the valid range 0 to \" +\n                                (params.length - 1));\n                    }\n                    result.append(params[num]);\n                } catch (NumberFormatException nfe) {\n                    throw new BadFormatString(\"bad format in username mapping in \" +\n                            paramNum, nfe);\n                }\n\n            }\n            start = match.end();\n        }\n        return result.toString();\n    }\n\n    /**\n     * Replace the matches of the from pattern in the base string with the value\n     * of the to string.\n     * @param base the string to transform\n     * @param from the pattern to look for in the base string\n     * @param to the string to replace matches of the pattern with\n     * @param repeat whether the substitution should be repeated\n     * @return\n     */\n    static String replaceSubstitution(String base, Pattern from, String to,\n                                      boolean repeat) {\n        Matcher match = from.matcher(base);\n        if (repeat) {\n            return match.replaceAll(to);\n        } else {\n            return match.replaceFirst(to);\n        }\n    }\n\n    /**\n     * Try to apply this rule to the given name represented as a parameter\n     * array.\n     * @param params first element is the realm, second and later elements are\n     *        are the components of the name \"a/b@FOO\" -> {\"FOO\", \"a\", \"b\"}\n     * @return the short name if this rule applies or null\n     * @throws IOException throws if something is wrong with the rules\n     */\n    String apply(String[] params) throws IOException {\n        String result = null;\n        if (isDefault) {\n            if (defaultRealm.equals(params[0])) {\n                result = params[1];\n            }\n        } else if (params.length - 1 == numOfComponents) {\n            String base = replaceParameters(format, params);\n            if (match == null || match.matcher(base).matches()) {\n                if (fromPattern == null) {\n                    result = base;\n                } else {\n                    result = replaceSubstitution(base, fromPattern, toPattern,  repeat);\n                }\n            }\n        }\n        if (result != null && NON_SIMPLE_PATTERN.matcher(result).find()) {\n            throw new NoMatchingRule(\"Non-simple name \" + result + \" after auth_to_local rule \" + this);\n        }\n        if (toLowerCase && result != null) {\n            result = result.toLowerCase(Locale.ENGLISH);\n        } else if (toUpperCase && result != null) {\n            result = result.toUpperCase(Locale.ENGLISH);\n        }\n\n        return result;\n    }\n}\n",
        "methodName": null,
        "exampleID": 26,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosRule.java",
        "line": 72,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 72,
        "qualifier": "This regular expression is constructed from a [user-provided value](1).\nThis regular expression is constructed from a [user-provided value](2).\nThis regular expression is constructed from a [user-provided value](3).\nThis regular expression is constructed from a [user-provided value](4).\nThis regular expression is constructed from a [user-provided value](5).\nThis regular expression is constructed from a [user-provided value](6).\nThis regular expression is constructed from a [user-provided value](7).\nThis regular expression is constructed from a [user-provided value](8).\nThis regular expression is constructed from a [user-provided value](9).",
        "line_number": 72,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 27
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 27
            },
            {
                "line": 251,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 27
            },
            {
                "line": 265,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 27
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/security/ssl/SslPrincipalMapper.java#L123",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.security.ssl;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport static org.apache.kafka.common.config.internals.BrokerSecurityConfigs.DEFAULT_SSL_PRINCIPAL_MAPPING_RULES;\n\npublic class SslPrincipalMapper {\n\n    private static final String RULE_PATTERN = \"(DEFAULT)|RULE:((\\\\\\\\.|[^\\\\\\\\/])*)/((\\\\\\\\.|[^\\\\\\\\/])*)/([LU]?).*?|(.*?)\";\n    private static final Pattern RULE_SPLITTER = Pattern.compile(\"\\\\s*(\" + RULE_PATTERN + \")\\\\s*(,\\\\s*|$)\");\n    private static final Pattern RULE_PARSER = Pattern.compile(RULE_PATTERN);\n\n    private final List<Rule> rules;\n\n    public SslPrincipalMapper(String sslPrincipalMappingRules) {\n        this.rules = parseRules(splitRules(sslPrincipalMappingRules));\n    }\n\n    public static SslPrincipalMapper fromRules(String sslPrincipalMappingRules) {\n        return new SslPrincipalMapper(sslPrincipalMappingRules);\n    }\n\n    private static List<String> splitRules(String sslPrincipalMappingRules) {\n        if (sslPrincipalMappingRules == null) {\n            sslPrincipalMappingRules = DEFAULT_SSL_PRINCIPAL_MAPPING_RULES;\n        }\n\n        List<String> result = new ArrayList<>();\n        Matcher matcher = RULE_SPLITTER.matcher(sslPrincipalMappingRules.trim());\n        while (matcher.find()) {\n            result.add(matcher.group(1));\n        }\n\n        return result;\n    }\n\n    private static List<Rule> parseRules(List<String> rules) {\n        List<Rule> result = new ArrayList<>();\n        for (String rule : rules) {\n            Matcher matcher = RULE_PARSER.matcher(rule);\n            if (!matcher.lookingAt()) {\n                throw new IllegalArgumentException(\"Invalid rule: \" + rule);\n            }\n            if (rule.length() != matcher.end()) {\n                throw new IllegalArgumentException(\"Invalid rule: `\" + rule + \"`, unmatched substring: `\" + rule.substring(matcher.end()) + \"`\");\n            }\n\n            // empty rules are ignored\n            if (matcher.group(1) != null) {\n                result.add(new Rule());\n            } else if (matcher.group(2) != null) {\n                result.add(new Rule(matcher.group(2),\n                                    matcher.group(4),\n                                    \"L\".equals(matcher.group(6)),\n                                    \"U\".equals(matcher.group(6))));\n            }\n        }\n\n        return result;\n    }\n\n    public String getName(String distinguishedName) throws IOException {\n        for (Rule r : rules) {\n            String principalName = r.apply(distinguishedName);\n            if (principalName != null) {\n                return principalName;\n            }\n        }\n        throw new NoMatchingRule(\"No rules apply to \" + distinguishedName + \", rules \" + rules);\n    }\n\n    @Override\n    public String toString() {\n        return \"SslPrincipalMapper(rules = \" + rules + \")\";\n    }\n\n    public static class NoMatchingRule extends IOException {\n        NoMatchingRule(String msg) {\n            super(msg);\n        }\n    }\n\n    private static class Rule {\n        private static final Pattern BACK_REFERENCE_PATTERN = Pattern.compile(\"\\\\$(\\\\d+)\");\n\n        private final boolean isDefault;\n        private final Pattern pattern;\n        private final String replacement;\n        private final boolean toLowerCase;\n        private final boolean toUpperCase;\n\n        Rule() {\n            isDefault = true;\n            pattern = null;\n            replacement = null;\n            toLowerCase = false;\n            toUpperCase = false;\n        }\n\n        Rule(String pattern, String replacement, boolean toLowerCase, boolean toUpperCase) {\n            isDefault = false;\n            this.pattern = pattern == null ? null : Pattern.compile(pattern);\n            this.replacement = replacement;\n            this.toLowerCase = toLowerCase;\n            this.toUpperCase = toUpperCase;\n        }\n\n        String apply(String distinguishedName) {\n            if (isDefault) {\n                return distinguishedName;\n            }\n\n            String result = null;\n            final Matcher m = pattern.matcher(distinguishedName);\n\n            if (m.matches()) {\n                result = distinguishedName.replaceAll(pattern.pattern(), escapeLiteralBackReferences(replacement, m.groupCount()));\n            }\n\n            if (toLowerCase && result != null) {\n                result = result.toLowerCase(Locale.ENGLISH);\n            } else if (toUpperCase && result != null) {\n                result = result.toUpperCase(Locale.ENGLISH);\n            }\n\n            return result;\n        }\n\n        //If we find a back reference that is not valid, then we will treat it as a literal string. For example, if we have 3 capturing\n        //groups and the Replacement Value has the value is \"$1@$4\", then we want to treat the $4 as a literal \"$4\", rather\n        //than attempting to use it as a back reference.\n        //This method was taken from Apache Nifi project : org.apache.nifi.authorization.util.IdentityMappingUtil\n        private String escapeLiteralBackReferences(final String unescaped, final int numCapturingGroups) {\n            if (numCapturingGroups == 0) {\n                return unescaped;\n            }\n\n            String value = unescaped;\n            final Matcher backRefMatcher = BACK_REFERENCE_PATTERN.matcher(value);\n            while (backRefMatcher.find()) {\n                final String backRefNum = backRefMatcher.group(1);\n                if (backRefNum.startsWith(\"0\")) {\n                    continue;\n                }\n                int backRefIndex = Integer.parseInt(backRefNum);\n\n\n                // if we have a replacement value like $123, and we have less than 123 capturing groups, then\n                // we want to truncate the 3 and use capturing group 12; if we have less than 12 capturing groups,\n                // then we want to truncate the 2 and use capturing group 1; if we don't have a capturing group then\n                // we want to truncate the 1 and get 0.\n                while (backRefIndex > numCapturingGroups && backRefIndex >= 10) {\n                    backRefIndex /= 10;\n                }\n\n                if (backRefIndex > numCapturingGroups) {\n                    final StringBuilder sb = new StringBuilder(value.length() + 1);\n                    final int groupStart = backRefMatcher.start(1);\n\n                    sb.append(value, 0, groupStart - 1);\n                    sb.append(\"\\\\\");\n                    sb.append(value.substring(groupStart - 1));\n                    value = sb.toString();\n                }\n            }\n\n            return value;\n        }\n\n        @Override\n        public String toString() {\n            StringBuilder buf = new StringBuilder();\n            if (isDefault) {\n                buf.append(\"DEFAULT\");\n            } else {\n                buf.append(\"RULE:\");\n                if (pattern != null) {\n                    buf.append(pattern);\n                }\n                if (replacement != null) {\n                    buf.append(\"/\");\n                    buf.append(replacement);\n                }\n                if (toLowerCase) {\n                    buf.append(\"/L\");\n                } else if (toUpperCase) {\n                    buf.append(\"/U\");\n                }\n            }\n            return buf.toString();\n        }\n\n    }\n}\n",
        "methodName": null,
        "exampleID": 28,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/security/ssl/SslPrincipalMapper.java",
        "line": 123,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 123,
        "qualifier": "This regular expression is constructed from a [user-provided value](1).\nThis regular expression is constructed from a [user-provided value](2).\nThis regular expression is constructed from a [user-provided value](3).\nThis regular expression is constructed from a [user-provided value](4).\nThis regular expression is constructed from a [user-provided value](5).\nThis regular expression is constructed from a [user-provided value](6).\nThis regular expression is constructed from a [user-provided value](7).\nThis regular expression is constructed from a [user-provided value](8).\nThis regular expression is constructed from a [user-provided value](9).",
        "line_number": 123,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 29
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 29
            },
            {
                "line": 251,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 29
            },
            {
                "line": 265,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 29
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java#L147",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.connect.runtime;\n\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigDef.Importance;\nimport org.apache.kafka.common.config.ConfigDef.Type;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.config.ConfigValue;\nimport org.apache.kafka.common.utils.Utils;\nimport org.apache.kafka.connect.runtime.isolation.Plugins;\nimport org.apache.kafka.connect.sink.SinkTask;\nimport org.apache.kafka.connect.transforms.util.RegexValidator;\n\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\n\n/**\n * Configuration needed for all {@link org.apache.kafka.connect.sink.SinkConnector sink connectors}\n */\n\npublic class SinkConnectorConfig extends ConnectorConfig {\n\n    public static final String TOPICS_CONFIG = SinkTask.TOPICS_CONFIG;\n    private static final String TOPICS_DOC = \"List of topics to consume, separated by commas\";\n    public static final String TOPICS_DEFAULT = \"\";\n    private static final String TOPICS_DISPLAY = \"Topics\";\n\n    public static final String TOPICS_REGEX_CONFIG = SinkTask.TOPICS_REGEX_CONFIG;\n    private static final String TOPICS_REGEX_DOC = \"Regular expression giving topics to consume. \" +\n        \"Under the hood, the regex is compiled to a <code>java.util.regex.Pattern</code>. \" +\n        \"Only one of \" + TOPICS_CONFIG + \" or \" + TOPICS_REGEX_CONFIG + \" should be specified.\";\n    public static final String TOPICS_REGEX_DEFAULT = \"\";\n    private static final String TOPICS_REGEX_DISPLAY = \"Topics regex\";\n\n    public static final String DLQ_PREFIX = \"errors.deadletterqueue.\";\n\n    public static final String DLQ_TOPIC_NAME_CONFIG = DLQ_PREFIX + \"topic.name\";\n    public static final String DLQ_TOPIC_NAME_DOC = \"The name of the topic to be used as the dead letter queue (DLQ) for messages that \" +\n        \"result in an error when processed by this sink connector, or its transformations or converters. The topic name is blank by default, \" +\n        \"which means that no messages are to be recorded in the DLQ.\";\n    public static final String DLQ_TOPIC_DEFAULT = \"\";\n    private static final String DLQ_TOPIC_DISPLAY = \"Dead Letter Queue Topic Name\";\n\n    public static final String DLQ_TOPIC_REPLICATION_FACTOR_CONFIG = DLQ_PREFIX + \"topic.replication.factor\";\n    private static final String DLQ_TOPIC_REPLICATION_FACTOR_CONFIG_DOC = \"Replication factor used to create the dead letter queue topic when it doesn't already exist.\";\n    public static final short DLQ_TOPIC_REPLICATION_FACTOR_CONFIG_DEFAULT = 3;\n    private static final String DLQ_TOPIC_REPLICATION_FACTOR_CONFIG_DISPLAY = \"Dead Letter Queue Topic Replication Factor\";\n\n    public static final String DLQ_CONTEXT_HEADERS_ENABLE_CONFIG = DLQ_PREFIX + \"context.headers.enable\";\n    public static final boolean DLQ_CONTEXT_HEADERS_ENABLE_DEFAULT = false;\n    public static final String DLQ_CONTEXT_HEADERS_ENABLE_DOC = \"If true, add headers containing error context to the messages \" +\n            \"written to the dead letter queue. To avoid clashing with headers from the original record, all error context header \" +\n            \"keys, all error context header keys will start with <code>__connect.errors.</code>\";\n    private static final String DLQ_CONTEXT_HEADERS_ENABLE_DISPLAY = \"Enable Error Context Headers\";\n\n    static final ConfigDef CONFIG = ConnectorConfig.configDef()\n        .define(TOPICS_CONFIG, ConfigDef.Type.LIST, TOPICS_DEFAULT, ConfigDef.Importance.HIGH, TOPICS_DOC, COMMON_GROUP, 4, ConfigDef.Width.LONG, TOPICS_DISPLAY)\n        .define(TOPICS_REGEX_CONFIG, ConfigDef.Type.STRING, TOPICS_REGEX_DEFAULT, new RegexValidator(), ConfigDef.Importance.HIGH, TOPICS_REGEX_DOC, COMMON_GROUP, 4, ConfigDef.Width.LONG, TOPICS_REGEX_DISPLAY)\n        .define(DLQ_TOPIC_NAME_CONFIG, ConfigDef.Type.STRING, DLQ_TOPIC_DEFAULT, Importance.MEDIUM, DLQ_TOPIC_NAME_DOC, ERROR_GROUP, 6, ConfigDef.Width.MEDIUM, DLQ_TOPIC_DISPLAY)\n        .define(DLQ_TOPIC_REPLICATION_FACTOR_CONFIG, ConfigDef.Type.SHORT, DLQ_TOPIC_REPLICATION_FACTOR_CONFIG_DEFAULT, Importance.MEDIUM, DLQ_TOPIC_REPLICATION_FACTOR_CONFIG_DOC, ERROR_GROUP, 7, ConfigDef.Width.MEDIUM, DLQ_TOPIC_REPLICATION_FACTOR_CONFIG_DISPLAY)\n        .define(DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, ConfigDef.Type.BOOLEAN, DLQ_CONTEXT_HEADERS_ENABLE_DEFAULT, Importance.MEDIUM, DLQ_CONTEXT_HEADERS_ENABLE_DOC, ERROR_GROUP, 8, ConfigDef.Width.MEDIUM, DLQ_CONTEXT_HEADERS_ENABLE_DISPLAY);\n\n    public static ConfigDef configDef() {\n        return CONFIG;\n    }\n\n    public SinkConnectorConfig(Plugins plugins, Map<String, String> props) {\n        super(plugins, CONFIG, props);\n    }\n\n    /**\n     * Throw an exception if the passed-in properties do not constitute a valid sink.\n     * @param props sink configuration properties\n     */\n    public static void validate(Map<String, String> props) {\n        Map<String, ConfigValue> validatedConfig = new LinkedHashMap<>();\n        validate(props, validatedConfig);\n        validatedConfig.values().stream()\n                .filter(configValue -> !configValue.errorMessages().isEmpty())\n                .findFirst()\n                .ifPresent(configValue -> {\n                    throw new ConfigException(configValue.name(), configValue.value(), configValue.errorMessages().get(0));\n                });\n    }\n\n    /**\n     * Perform preflight validation for the sink-specific properties for a connector.\n     *\n     * @param props           the configuration for the sink connector\n     * @param validatedConfig any already-known {@link ConfigValue validation results} for the configuration.\n     *                        May be empty, but may not be null. Any configuration errors discovered by this method will\n     *                        be {@link ConfigValue#addErrorMessage(String) added} to a value in this map, adding a new\n     *                        entry if one for the problematic property does not already exist.\n     */\n    public static void validate(Map<String, String> props, Map<String, ConfigValue> validatedConfig) {\n        final String topicsList = props.get(TOPICS_CONFIG);\n        final String topicsRegex = props.get(TOPICS_REGEX_CONFIG);\n        final String dlqTopic = props.getOrDefault(DLQ_TOPIC_NAME_CONFIG, \"\").trim();\n        final boolean hasTopicsConfig = !Utils.isBlank(topicsList);\n        final boolean hasTopicsRegexConfig = !Utils.isBlank(topicsRegex);\n        final boolean hasDlqTopicConfig = !Utils.isBlank(dlqTopic);\n\n        if (hasTopicsConfig && hasTopicsRegexConfig) {\n            String errorMessage = TOPICS_CONFIG + \" and \" + TOPICS_REGEX_CONFIG + \" are mutually exclusive options, but both are set.\";\n            addErrorMessage(validatedConfig, TOPICS_CONFIG, topicsList, errorMessage);\n            addErrorMessage(validatedConfig, TOPICS_REGEX_CONFIG, topicsRegex, errorMessage);\n        }\n\n        if (!hasTopicsConfig && !hasTopicsRegexConfig) {\n            String errorMessage = \"Must configure one of \" + TOPICS_CONFIG + \" or \" + TOPICS_REGEX_CONFIG;\n            addErrorMessage(validatedConfig, TOPICS_CONFIG, topicsList, errorMessage);\n            addErrorMessage(validatedConfig, TOPICS_REGEX_CONFIG, topicsRegex, errorMessage);\n        }\n\n        if (hasDlqTopicConfig) {\n            if (hasTopicsConfig) {\n                List<String> topics = parseTopicsList(props);\n                if (topics.contains(dlqTopic)) {\n                    String errorMessage = String.format(\n                            \"The DLQ topic '%s' may not be included in the list of topics ('%s=%s') consumed by the connector\",\n                            dlqTopic, TOPICS_CONFIG, topics\n                    );\n                    addErrorMessage(validatedConfig, TOPICS_CONFIG, topicsList, errorMessage);\n                }\n            }\n            if (hasTopicsRegexConfig) {\n                Pattern pattern = Pattern.compile(topicsRegex);\n                if (pattern.matcher(dlqTopic).matches()) {\n                    String errorMessage = String.format(\n                            \"The DLQ topic '%s' may not be matched by the regex for the topics ('%s=%s') consumed by the connector\",\n                            dlqTopic, TOPICS_REGEX_CONFIG, topicsRegex\n                    );\n                    addErrorMessage(validatedConfig, TOPICS_REGEX_CONFIG, topicsRegex, errorMessage);\n                }\n            }\n        }\n    }\n\n    private static void addErrorMessage(Map<String, ConfigValue> validatedConfig, String name, String value, String errorMessage) {\n        validatedConfig.computeIfAbsent(\n                name,\n                p -> new ConfigValue(name, value, Collections.emptyList(), new ArrayList<>())\n        ).addErrorMessage(\n                errorMessage\n        );\n    }\n\n    public static boolean hasTopicsConfig(Map<String, String> props) {\n        String topicsStr = props.get(TOPICS_CONFIG);\n        return !Utils.isBlank(topicsStr);\n    }\n\n    public static boolean hasDlqTopicConfig(Map<String, String> props) {\n        String dqlTopicStr = props.get(DLQ_TOPIC_NAME_CONFIG);\n        return !Utils.isBlank(dqlTopicStr);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    public static List<String> parseTopicsList(Map<String, String> props) {\n        List<String> topics = (List<String>) ConfigDef.parseType(TOPICS_CONFIG, props.get(TOPICS_CONFIG), Type.LIST);\n        if (topics == null) {\n            return Collections.emptyList();\n        }\n        return topics\n                .stream()\n                .filter(topic -> !topic.isEmpty())\n                .distinct()\n                .collect(Collectors.toList());\n    }\n\n    public String dlqTopicName() {\n        return getString(DLQ_TOPIC_NAME_CONFIG);\n    }\n\n    public short dlqTopicReplicationFactor() {\n        return getShort(DLQ_TOPIC_REPLICATION_FACTOR_CONFIG);\n    }\n\n    public boolean isDlqContextHeadersEnabled() {\n        return getBoolean(DLQ_CONTEXT_HEADERS_ENABLE_CONFIG);\n    }\n\n    public boolean enableErrantRecordReporter() {\n        String dqlTopic = dlqTopicName();\n        return !dqlTopic.isEmpty() || enableErrorLog();\n    }\n\n    public static void main(String[] args) {\n        System.out.println(CONFIG.toHtml(4, config -> \"sinkconnectorconfigs_\" + config));\n    }\n}\n",
        "methodName": null,
        "exampleID": 30,
        "dataset": "codeql",
        "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java",
        "line": 147,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 147,
        "qualifier": "This regular expression is constructed from a [user-provided value](1).\nThis regular expression is constructed from a [user-provided value](2).\nThis regular expression is constructed from a [user-provided value](3).\nThis regular expression is constructed from a [user-provided value](4).\nThis regular expression is constructed from a [user-provided value](5).",
        "line_number": 147,
        "steps": [
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 31
            },
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 31
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 31
            },
            {
                "line": 232,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 31
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java#L78",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.connect.runtime;\n\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.connect.util.TopicAdmin;\n\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.regex.Pattern;\nimport java.util.regex.PatternSyntaxException;\n\n/**\n * Configuration options for source connector automatic topic creation.\n *\n * @see <a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-158%3A+Kafka+Connect+should+allow+source+connectors+to+set+topic-specific+settings+for+new+topics\">KIP-158</a>\n */\npublic class TopicCreationConfig {\n\n    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n\n    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n    private static final String INCLUDE_REGEX_DOC = \"A list of regular expression literals \"\n            + \"used to match the topic names used by the source connector. This list is used \"\n            + \"to include topics that should be created using the topic settings defined by this group.\";\n\n    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n    private static final String EXCLUDE_REGEX_DOC = \"A list of regular expression literals \"\n            + \"used to match the topic names used by the source connector. This list is used \"\n            + \"to exclude topics from being created with the topic settings defined by this group. \"\n            + \"Note that exclusion rules have precedent and override any inclusion rules for the topics.\";\n\n    public static final String REPLICATION_FACTOR_CONFIG = \"replication.factor\";\n    private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n            + \"created for this connector using this group. This value may be -1 to use the broker's\"\n            + \"default replication factor, or may be a positive number not larger than the number of \"\n            + \"brokers in the Kafka cluster. A value larger than the number of brokers in the Kafka cluster \"\n            + \"will result in an error when the new topic is created. For the default group this configuration \"\n            + \"is required. For any other group defined in topic.creation.groups this config is \"\n            + \"optional and if it's missing it gets the value of the default group\";\n\n    public static final String PARTITIONS_CONFIG = \"partitions\";\n    private static final String PARTITIONS_DOC = \"The number of partitions new topics created for \"\n            + \"this connector. This value may be -1 to use the broker's default number of partitions, \"\n            + \"or a positive number representing the desired number of partitions. \"\n            + \"For the default group this configuration is required. For any \"\n            + \"other group defined in topic.creation.groups this config is optional and if it's \"\n            + \"missing it gets the value of the default group\";\n\n    public static final ConfigDef.Validator REPLICATION_FACTOR_VALIDATOR = ConfigDef.LambdaValidator.with(\n        (name, value) -> validateReplicationFactor(name, (short) value),\n        () -> \"Positive number not larger than the number of brokers in the Kafka cluster, or -1 to use the broker's default\"\n    );\n    public static final ConfigDef.Validator PARTITIONS_VALIDATOR = ConfigDef.LambdaValidator.with(\n        (name, value) -> validatePartitions(name, (int) value),\n        () -> \"Positive number, or -1 to use the broker's default\"\n    );\n    @SuppressWarnings(\"unchecked\")\n    public static final ConfigDef.Validator REGEX_VALIDATOR = ConfigDef.LambdaValidator.with(\n        (name, value) -> {\n            try {\n                ((List<String>) value).forEach(Pattern::compile);\n            } catch (PatternSyntaxException e) {\n                throw new ConfigException(name, value,\n                        \"Syntax error in regular expression: \" + e.getMessage());\n            }\n        },\n        () -> \"Positive number, or -1 to use the broker's default\"\n    );\n\n    private static void validatePartitions(String configName, int factor) {\n        if (factor != TopicAdmin.NO_PARTITIONS && factor < 1) {\n            throw new ConfigException(configName, factor,\n                    \"Number of partitions must be positive, or -1 to use the broker's default\");\n        }\n    }\n\n    private static void validateReplicationFactor(String configName, short factor) {\n        if (factor != TopicAdmin.NO_REPLICATION_FACTOR && factor < 1) {\n            throw new ConfigException(configName, factor,\n                    \"Replication factor must be positive and not larger than the number of brokers in the Kafka cluster, or -1 to use the broker's default\");\n        }\n    }\n\n    public static ConfigDef configDef(String group, short defaultReplicationFactor, int defaultPartitionCount) {\n        int orderInGroup = 0;\n        ConfigDef configDef = new ConfigDef();\n        configDef\n                .define(INCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n                        REGEX_VALIDATOR, ConfigDef.Importance.LOW,\n                        INCLUDE_REGEX_DOC, group, ++orderInGroup, ConfigDef.Width.LONG,\n                        \"Inclusion Topic Pattern for \" + group)\n                .define(EXCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n                        REGEX_VALIDATOR, ConfigDef.Importance.LOW,\n                        EXCLUDE_REGEX_DOC, group, ++orderInGroup, ConfigDef.Width.LONG,\n                        \"Exclusion Topic Pattern for \" + group)\n                .define(REPLICATION_FACTOR_CONFIG, ConfigDef.Type.SHORT,\n                        defaultReplicationFactor, REPLICATION_FACTOR_VALIDATOR,\n                        ConfigDef.Importance.LOW, REPLICATION_FACTOR_DOC, group, ++orderInGroup,\n                        ConfigDef.Width.LONG, \"Replication Factor for Topics in \" + group)\n                .define(PARTITIONS_CONFIG, ConfigDef.Type.INT,\n                        defaultPartitionCount, PARTITIONS_VALIDATOR,\n                        ConfigDef.Importance.LOW, PARTITIONS_DOC, group, ++orderInGroup,\n                        ConfigDef.Width.LONG, \"Partition Count for Topics in \" + group);\n        return configDef;\n    }\n\n    public static ConfigDef defaultGroupConfigDef() {\n        int orderInGroup = 0;\n        ConfigDef configDef = new ConfigDef();\n        configDef\n                .define(INCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, \".*\",\n                        new ConfigDef.NonNullValidator(), ConfigDef.Importance.LOW,\n                        INCLUDE_REGEX_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup, ConfigDef.Width.LONG,\n                        \"Inclusion Topic Pattern for \" + DEFAULT_TOPIC_CREATION_GROUP)\n                .define(EXCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n                        new ConfigDef.NonNullValidator(), ConfigDef.Importance.LOW,\n                        EXCLUDE_REGEX_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup, ConfigDef.Width.LONG,\n                        \"Exclusion Topic Pattern for \" + DEFAULT_TOPIC_CREATION_GROUP)\n                .define(REPLICATION_FACTOR_CONFIG, ConfigDef.Type.SHORT,\n                        ConfigDef.NO_DEFAULT_VALUE, REPLICATION_FACTOR_VALIDATOR,\n                        ConfigDef.Importance.LOW, REPLICATION_FACTOR_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup,\n                        ConfigDef.Width.LONG, \"Replication Factor for Topics in \" + DEFAULT_TOPIC_CREATION_GROUP)\n                .define(PARTITIONS_CONFIG, ConfigDef.Type.INT,\n                        ConfigDef.NO_DEFAULT_VALUE, PARTITIONS_VALIDATOR,\n                        ConfigDef.Importance.LOW, PARTITIONS_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup,\n                        ConfigDef.Width.LONG, \"Partition Count for Topics in \" + DEFAULT_TOPIC_CREATION_GROUP);\n        return configDef;\n    }\n\n}\n",
        "methodName": null,
        "exampleID": 32,
        "dataset": "codeql",
        "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java",
        "line": 78,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 78,
        "qualifier": "This regular expression is constructed from a [user-provided value](1).\nThis regular expression is constructed from a [user-provided value](2).\nThis regular expression is constructed from a [user-provided value](3).",
        "line_number": 78,
        "steps": [
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 33
            },
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 33
            },
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 33
            },
            {
                "line": 232,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 33
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/RegexValidator.java#L29",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.connect.transforms.util;\n\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigException;\n\nimport java.util.regex.Pattern;\n\npublic class RegexValidator implements ConfigDef.Validator {\n\n    @Override\n    public void ensureValid(String name, Object value) {\n        try {\n            Pattern.compile((String) value);\n        } catch (Exception e) {\n            throw new ConfigException(name, value, \"Invalid regex: \" + e.getMessage());\n        }\n    }\n\n    @Override\n    public String toString() {\n        return \"valid regex\";\n    }\n\n}\n",
        "methodName": null,
        "exampleID": 34,
        "dataset": "codeql",
        "filepath": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/RegexValidator.java",
        "line": 29,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 29,
        "qualifier": "This regular expression is constructed from a [user-provided value](1).\nThis regular expression is constructed from a [user-provided value](2).\nThis regular expression is constructed from a [user-provided value](3).\nThis regular expression is constructed from a [user-provided value](4).\nThis regular expression is constructed from a [user-provided value](5).\nThis regular expression is constructed from a [user-provided value](6).\nThis regular expression is constructed from a [user-provided value](7).\nThis regular expression is constructed from a [user-provided value](8).\nThis regular expression is constructed from a [user-provided value](9).\nThis regular expression is constructed from a [user-provided value](10).\nThis regular expression is constructed from a [user-provided value](11).\nThis regular expression is constructed from a [user-provided value](12).",
        "line_number": 29,
        "steps": [
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 35
            },
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 35
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 35
            },
            {
                "line": 232,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 35
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java#L126",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.config;\n\nimport org.apache.kafka.common.config.provider.ConfigProvider;\nimport org.apache.kafka.common.config.provider.FileConfigProvider;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n/**\n * This class wraps a set of {@link ConfigProvider} instances and uses them to perform\n * transformations.\n *\n * <p>The default variable pattern is of the form <code>${provider:[path:]key}</code>,\n * where the <code>provider</code> corresponds to a {@link ConfigProvider} instance, as passed to\n * {@link ConfigTransformer#ConfigTransformer(Map)}.  The pattern will extract a set\n * of paths (which are optional) and keys and then pass them to {@link ConfigProvider#get(String, Set)} to obtain the\n * values with which to replace the variables.\n *\n * <p>For example, if a Map consisting of an entry with a provider name \"file\" and provider instance\n * {@link FileConfigProvider} is passed to the {@link ConfigTransformer#ConfigTransformer(Map)}, and a Properties\n * file with contents\n * <pre>\n * fileKey=someValue\n * </pre>\n * resides at the path \"/tmp/properties.txt\", then when a configuration Map which has an entry with a key \"someKey\" and\n * a value \"${file:/tmp/properties.txt:fileKey}\" is passed to the {@link #transform(Map)} method, then the transformed\n * Map will have an entry with key \"someKey\" and a value \"someValue\".\n *\n * <p>This class only depends on {@link ConfigProvider#get(String, Set)} and does not depend on subscription support\n * in a {@link ConfigProvider}, such as the {@link ConfigProvider#subscribe(String, Set, ConfigChangeCallback)} and\n * {@link ConfigProvider#unsubscribe(String, Set, ConfigChangeCallback)} methods.\n */\npublic class ConfigTransformer {\n    public static final Pattern DEFAULT_PATTERN = Pattern.compile(\"\\\\$\\\\{([^}]*?):(([^}]*?):)?([^}]*?)\\\\}\");\n    private static final String EMPTY_PATH = \"\";\n\n    private final Map<String, ConfigProvider> configProviders;\n\n    /**\n     * Creates a ConfigTransformer with the default pattern, of the form <code>${provider:[path:]key}</code>.\n     *\n     * @param configProviders a Map of provider names and {@link ConfigProvider} instances.\n     */\n    public ConfigTransformer(Map<String, ConfigProvider> configProviders) {\n        this.configProviders = configProviders;\n    }\n\n    /**\n     * Transforms the given configuration data by using the {@link ConfigProvider} instances to\n     * look up values to replace the variables in the pattern.\n     *\n     * @param configs the configuration values to be transformed\n     * @return an instance of {@link ConfigTransformerResult}\n     */\n    public ConfigTransformerResult transform(Map<String, String> configs) {\n        Map<String, Map<String, Set<String>>> keysByProvider = new HashMap<>();\n        Map<String, Map<String, Map<String, String>>> lookupsByProvider = new HashMap<>();\n\n        // Collect the variables from the given configs that need transformation\n        for (Map.Entry<String, String> config : configs.entrySet()) {\n            if (config.getValue() != null) {\n                List<ConfigVariable> configVars = getVars(config.getValue(), DEFAULT_PATTERN);\n                for (ConfigVariable configVar : configVars) {\n                    Map<String, Set<String>> keysByPath = keysByProvider.computeIfAbsent(configVar.providerName, k -> new HashMap<>());\n                    Set<String> keys = keysByPath.computeIfAbsent(configVar.path, k -> new HashSet<>());\n                    keys.add(configVar.variable);\n                }\n            }\n        }\n\n        // Retrieve requested variables from the ConfigProviders\n        Map<String, Long> ttls = new HashMap<>();\n        for (Map.Entry<String, Map<String, Set<String>>> entry : keysByProvider.entrySet()) {\n            String providerName = entry.getKey();\n            ConfigProvider provider = configProviders.get(providerName);\n            Map<String, Set<String>> keysByPath = entry.getValue();\n            if (provider != null && keysByPath != null) {\n                for (Map.Entry<String, Set<String>> pathWithKeys : keysByPath.entrySet()) {\n                    String path = pathWithKeys.getKey();\n                    Set<String> keys = new HashSet<>(pathWithKeys.getValue());\n                    ConfigData configData = provider.get(path, keys);\n                    Map<String, String> data = configData.data();\n                    Long ttl = configData.ttl();\n                    if (ttl != null && ttl >= 0) {\n                        ttls.put(path, ttl);\n                    }\n                    Map<String, Map<String, String>> keyValuesByPath =\n                            lookupsByProvider.computeIfAbsent(providerName, k -> new HashMap<>());\n                    keyValuesByPath.put(path, data);\n                }\n            }\n        }\n\n        // Perform the transformations by performing variable replacements\n        Map<String, String> data = new HashMap<>(configs);\n        for (Map.Entry<String, String> config : configs.entrySet()) {\n            data.put(config.getKey(), replace(lookupsByProvider, config.getValue(), DEFAULT_PATTERN));\n        }\n        return new ConfigTransformerResult(data, ttls);\n    }\n\n    private static List<ConfigVariable> getVars(String value, Pattern pattern) {\n        List<ConfigVariable> configVars = new ArrayList<>();\n        Matcher matcher = pattern.matcher(value);\n        while (matcher.find()) {\n            configVars.add(new ConfigVariable(matcher));\n        }\n        return configVars;\n    }\n\n    private static String replace(Map<String, Map<String, Map<String, String>>> lookupsByProvider,\n                                  String value,\n                                  Pattern pattern) {\n        if (value == null) {\n            return null;\n        }\n        Matcher matcher = pattern.matcher(value);\n        StringBuilder builder = new StringBuilder();\n        int i = 0;\n        while (matcher.find()) {\n            ConfigVariable configVar = new ConfigVariable(matcher);\n            Map<String, Map<String, String>> lookupsByPath = lookupsByProvider.get(configVar.providerName);\n            if (lookupsByPath != null) {\n                Map<String, String> keyValues = lookupsByPath.get(configVar.path);\n                String replacement = keyValues.get(configVar.variable);\n                builder.append(value, i, matcher.start());\n                if (replacement == null) {\n                    // No replacements will be performed; just return the original value\n                    builder.append(matcher.group(0));\n                } else {\n                    builder.append(replacement);\n                }\n                i = matcher.end();\n            }\n        }\n        builder.append(value, i, value.length());\n        return builder.toString();\n    }\n\n    private static class ConfigVariable {\n        final String providerName;\n        final String path;\n        final String variable;\n\n        ConfigVariable(Matcher matcher) {\n            this.providerName = matcher.group(1);\n            this.path = matcher.group(3) != null ? matcher.group(3) : EMPTY_PATH;\n            this.variable = matcher.group(4);\n        }\n\n        public String toString() {\n            return \"(\" + providerName + \":\" + (path != null ? path + \":\" : \"\") + variable + \")\";\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 36,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java",
        "line": 126,
        "sink": "'${{|'.",
        "source": "-",
        "sourceLine": 126,
        "qualifier": "This [regular expression](1) that depends on a [user-provided value](2) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](2) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](2) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](5) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](5) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](5) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](6) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](6) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](6) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](7) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](7) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](7) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](8) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](8) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](8) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](9) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](9) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](9) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](10) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](10) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](10) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](11) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](11) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](11) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](12) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](12) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](12) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](13) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](13) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](13) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](14) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](14) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](14) may run slow on strings starting with '${{:' and with many repetitions of ':|'.",
        "line_number": 126,
        "steps": [
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 37
            },
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 37
            },
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 37
            },
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 37
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java#L139",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.config;\n\nimport org.apache.kafka.common.config.provider.ConfigProvider;\nimport org.apache.kafka.common.config.provider.FileConfigProvider;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n/**\n * This class wraps a set of {@link ConfigProvider} instances and uses them to perform\n * transformations.\n *\n * <p>The default variable pattern is of the form <code>${provider:[path:]key}</code>,\n * where the <code>provider</code> corresponds to a {@link ConfigProvider} instance, as passed to\n * {@link ConfigTransformer#ConfigTransformer(Map)}.  The pattern will extract a set\n * of paths (which are optional) and keys and then pass them to {@link ConfigProvider#get(String, Set)} to obtain the\n * values with which to replace the variables.\n *\n * <p>For example, if a Map consisting of an entry with a provider name \"file\" and provider instance\n * {@link FileConfigProvider} is passed to the {@link ConfigTransformer#ConfigTransformer(Map)}, and a Properties\n * file with contents\n * <pre>\n * fileKey=someValue\n * </pre>\n * resides at the path \"/tmp/properties.txt\", then when a configuration Map which has an entry with a key \"someKey\" and\n * a value \"${file:/tmp/properties.txt:fileKey}\" is passed to the {@link #transform(Map)} method, then the transformed\n * Map will have an entry with key \"someKey\" and a value \"someValue\".\n *\n * <p>This class only depends on {@link ConfigProvider#get(String, Set)} and does not depend on subscription support\n * in a {@link ConfigProvider}, such as the {@link ConfigProvider#subscribe(String, Set, ConfigChangeCallback)} and\n * {@link ConfigProvider#unsubscribe(String, Set, ConfigChangeCallback)} methods.\n */\npublic class ConfigTransformer {\n    public static final Pattern DEFAULT_PATTERN = Pattern.compile(\"\\\\$\\\\{([^}]*?):(([^}]*?):)?([^}]*?)\\\\}\");\n    private static final String EMPTY_PATH = \"\";\n\n    private final Map<String, ConfigProvider> configProviders;\n\n    /**\n     * Creates a ConfigTransformer with the default pattern, of the form <code>${provider:[path:]key}</code>.\n     *\n     * @param configProviders a Map of provider names and {@link ConfigProvider} instances.\n     */\n    public ConfigTransformer(Map<String, ConfigProvider> configProviders) {\n        this.configProviders = configProviders;\n    }\n\n    /**\n     * Transforms the given configuration data by using the {@link ConfigProvider} instances to\n     * look up values to replace the variables in the pattern.\n     *\n     * @param configs the configuration values to be transformed\n     * @return an instance of {@link ConfigTransformerResult}\n     */\n    public ConfigTransformerResult transform(Map<String, String> configs) {\n        Map<String, Map<String, Set<String>>> keysByProvider = new HashMap<>();\n        Map<String, Map<String, Map<String, String>>> lookupsByProvider = new HashMap<>();\n\n        // Collect the variables from the given configs that need transformation\n        for (Map.Entry<String, String> config : configs.entrySet()) {\n            if (config.getValue() != null) {\n                List<ConfigVariable> configVars = getVars(config.getValue(), DEFAULT_PATTERN);\n                for (ConfigVariable configVar : configVars) {\n                    Map<String, Set<String>> keysByPath = keysByProvider.computeIfAbsent(configVar.providerName, k -> new HashMap<>());\n                    Set<String> keys = keysByPath.computeIfAbsent(configVar.path, k -> new HashSet<>());\n                    keys.add(configVar.variable);\n                }\n            }\n        }\n\n        // Retrieve requested variables from the ConfigProviders\n        Map<String, Long> ttls = new HashMap<>();\n        for (Map.Entry<String, Map<String, Set<String>>> entry : keysByProvider.entrySet()) {\n            String providerName = entry.getKey();\n            ConfigProvider provider = configProviders.get(providerName);\n            Map<String, Set<String>> keysByPath = entry.getValue();\n            if (provider != null && keysByPath != null) {\n                for (Map.Entry<String, Set<String>> pathWithKeys : keysByPath.entrySet()) {\n                    String path = pathWithKeys.getKey();\n                    Set<String> keys = new HashSet<>(pathWithKeys.getValue());\n                    ConfigData configData = provider.get(path, keys);\n                    Map<String, String> data = configData.data();\n                    Long ttl = configData.ttl();\n                    if (ttl != null && ttl >= 0) {\n                        ttls.put(path, ttl);\n                    }\n                    Map<String, Map<String, String>> keyValuesByPath =\n                            lookupsByProvider.computeIfAbsent(providerName, k -> new HashMap<>());\n                    keyValuesByPath.put(path, data);\n                }\n            }\n        }\n\n        // Perform the transformations by performing variable replacements\n        Map<String, String> data = new HashMap<>(configs);\n        for (Map.Entry<String, String> config : configs.entrySet()) {\n            data.put(config.getKey(), replace(lookupsByProvider, config.getValue(), DEFAULT_PATTERN));\n        }\n        return new ConfigTransformerResult(data, ttls);\n    }\n\n    private static List<ConfigVariable> getVars(String value, Pattern pattern) {\n        List<ConfigVariable> configVars = new ArrayList<>();\n        Matcher matcher = pattern.matcher(value);\n        while (matcher.find()) {\n            configVars.add(new ConfigVariable(matcher));\n        }\n        return configVars;\n    }\n\n    private static String replace(Map<String, Map<String, Map<String, String>>> lookupsByProvider,\n                                  String value,\n                                  Pattern pattern) {\n        if (value == null) {\n            return null;\n        }\n        Matcher matcher = pattern.matcher(value);\n        StringBuilder builder = new StringBuilder();\n        int i = 0;\n        while (matcher.find()) {\n            ConfigVariable configVar = new ConfigVariable(matcher);\n            Map<String, Map<String, String>> lookupsByPath = lookupsByProvider.get(configVar.providerName);\n            if (lookupsByPath != null) {\n                Map<String, String> keyValues = lookupsByPath.get(configVar.path);\n                String replacement = keyValues.get(configVar.variable);\n                builder.append(value, i, matcher.start());\n                if (replacement == null) {\n                    // No replacements will be performed; just return the original value\n                    builder.append(matcher.group(0));\n                } else {\n                    builder.append(replacement);\n                }\n                i = matcher.end();\n            }\n        }\n        builder.append(value, i, value.length());\n        return builder.toString();\n    }\n\n    private static class ConfigVariable {\n        final String providerName;\n        final String path;\n        final String variable;\n\n        ConfigVariable(Matcher matcher) {\n            this.providerName = matcher.group(1);\n            this.path = matcher.group(3) != null ? matcher.group(3) : EMPTY_PATH;\n            this.variable = matcher.group(4);\n        }\n\n        public String toString() {\n            return \"(\" + providerName + \":\" + (path != null ? path + \":\" : \"\") + variable + \")\";\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 38,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java",
        "line": 139,
        "sink": "'${{|'.",
        "source": "-",
        "sourceLine": 139,
        "qualifier": "This [regular expression](1) that depends on a [user-provided value](2) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](2) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](2) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](5) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](5) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](5) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](6) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](6) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](6) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](7) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](7) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](7) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](8) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](8) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](8) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](9) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](9) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](9) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](10) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](10) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](10) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](11) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](11) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](11) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](12) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](12) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](12) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](13) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](13) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](13) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](1) that depends on a [user-provided value](14) may run slow on strings starting with '${{' and with many repetitions of '${{|'.\nThis [regular expression](3) that depends on a [user-provided value](14) may run slow on strings starting with '${{:' and with many repetitions of ':|'.\nThis [regular expression](4) that depends on a [user-provided value](14) may run slow on strings starting with '${{:' and with many repetitions of ':|'.",
        "line_number": 139,
        "steps": [
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 39
            },
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 39
            },
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 39
            },
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 39
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java#L762",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.config;\n\nimport org.apache.kafka.common.config.types.Password;\nimport org.apache.kafka.common.utils.Utils;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.LinkedHashMap;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.function.BiConsumer;\nimport java.util.function.Function;\nimport java.util.function.Supplier;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\n\n/**\n * This class is used for specifying the set of expected configurations. For each configuration, you can specify\n * the name, the type, the default value, the documentation, the group information, the order in the group,\n * the width of the configuration value and the name suitable for display in the UI.\n *\n * You can provide special validation logic used for single configuration validation by overriding {@link Validator}.\n *\n * Moreover, you can specify the dependents of a configuration. The valid values and visibility of a configuration\n * may change according to the values of other configurations. You can override {@link Recommender} to get valid\n * values and set visibility of a configuration given the current configuration values.\n *\n * <p/>\n * To use the class:\n * <p/>\n * <pre>\n * ConfigDef defs = new ConfigDef();\n *\n * // check {@link #define(String, Type, Object, Importance, String)} for more details.\n * defs.define(&quot;config_with_default&quot;, Type.STRING, &quot;default string value&quot;, Importance.High, &quot;Configuration with default value.&quot;);\n * // check {@link #define(String, Type, Object, Validator, Importance, String)} for more details.\n * defs.define(&quot;config_with_validator&quot;, Type.INT, 42, Range.atLeast(0), Importance.High, &quot;Configuration with user provided validator.&quot;);\n * // check {@link #define(String, Type, Importance, String, String, int, Width, String, List) define(String, Type, Importance, String, String, int, Width, String, List&lt;String&gt;)} for more details.\n * defs.define(&quot;config_with_dependents&quot;, Type.INT, Importance.LOW, &quot;Configuration with dependents.&quot;, &quot;group&quot;, 1, Width.SHORT, &quot;Config With Dependents&quot;, Arrays.asList(&quot;config_with_default&quot;,&quot;config_with_validator&quot;));\n *\n * Map&lt;String, String&gt; props = new HashMap&lt;&gt;();\n * props.put(&quot;config_with_default&quot;, &quot;some value&quot;);\n * props.put(&quot;config_with_dependents&quot;, &quot;some other value&quot;);\n *\n * Map&lt;String, Object&gt; configs = defs.parse(props);\n * // will return &quot;some value&quot;\n * String someConfig = (String) configs.get(&quot;config_with_default&quot;);\n * // will return default value of 42\n * int anotherConfig = (Integer) configs.get(&quot;config_with_validator&quot;);\n *\n * // To validate the full configuration, use:\n * List&lt;ConfigValue&gt; configValues = defs.validate(props);\n * // The {@link ConfigValue} contains updated configuration information given the current configuration values.\n * </pre>\n * <p/>\n * This class can be used standalone or in combination with {@link AbstractConfig} which provides some additional\n * functionality for accessing configs.\n */\npublic class ConfigDef {\n\n    private static final Pattern COMMA_WITH_WHITESPACE = Pattern.compile(\"\\\\s*,\\\\s*\");\n\n    /**\n     * A unique Java object which represents the lack of a default value.\n     */\n    public static final Object NO_DEFAULT_VALUE = new Object();\n\n    private final Map<String, ConfigKey> configKeys;\n    private final List<String> groups;\n    private Set<String> configsWithNoParent;\n\n    public ConfigDef() {\n        configKeys = new LinkedHashMap<>();\n        groups = new LinkedList<>();\n        configsWithNoParent = null;\n    }\n\n    public ConfigDef(ConfigDef base) {\n        configKeys = new LinkedHashMap<>(base.configKeys);\n        groups = new LinkedList<>(base.groups);\n        // It is not safe to copy this from the parent because we may subsequently add to the set of configs and\n        // invalidate this\n        configsWithNoParent = null;\n    }\n\n    /**\n     * Returns unmodifiable set of properties names defined in this {@linkplain ConfigDef}\n     *\n     * @return new unmodifiable {@link Set} instance containing the keys\n     */\n    public Set<String> names() {\n        return Collections.unmodifiableSet(configKeys.keySet());\n    }\n\n    public Map<String, Object> defaultValues() {\n        Map<String, Object> defaultValues = new HashMap<>();\n        for (ConfigKey key : configKeys.values()) {\n            if (key.defaultValue != NO_DEFAULT_VALUE)\n                defaultValues.put(key.name, key.defaultValue);\n        }\n        return defaultValues;\n    }\n\n    public ConfigDef define(ConfigKey key) {\n        if (configKeys.containsKey(key.name)) {\n            throw new ConfigException(\"Configuration \" + key.name + \" is defined twice.\");\n        }\n        if (key.group != null && !groups.contains(key.group)) {\n            groups.add(key.group);\n        }\n        configKeys.put(key.name, key);\n        return this;\n    }\n\n    /**\n     * Define a new configuration\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param defaultValue  the default value to use if this config isn't present\n     * @param validator     the validator to use in checking the correctness of the config\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @param dependents    the configurations that are dependents of this configuration\n     * @param recommender   the recommender provides valid values given the parent configuration values\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation,\n                            String group, int orderInGroup, Width width, String displayName, List<String> dependents, Recommender recommender) {\n        return define(new ConfigKey(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName, dependents, recommender, false, null));\n    }\n\n    /**\n     * Define a new configuration\n     * @param name               the name of the config parameter\n     * @param type               the type of the config\n     * @param defaultValue       the default value to use if this config isn't present\n     * @param validator          the validator to use in checking the correctness of the config\n     * @param importance         the importance of this config\n     * @param documentation      the documentation string for the config\n     * @param group              the group this config belongs to\n     * @param orderInGroup       the order of this config in the group\n     * @param width              the width of the config\n     * @param displayName        the name suitable for display\n     * @param dependents         the configurations that are dependents of this configuration\n     * @param recommender        the recommender provides valid values given the parent configuration values\n     * @param alternativeString  the string which will be used to override the string of defaultValue\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation,\n                            String group, int orderInGroup, Width width, String displayName, List<String> dependents, Recommender recommender,\n                            String alternativeString) {\n        return define(new ConfigKey(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName, dependents, recommender, false, alternativeString));\n    }\n\n    /**\n     * Define a new configuration with no custom recommender\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param defaultValue  the default value to use if this config isn't present\n     * @param validator     the validator to use in checking the correctness of the config\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @param dependents    the configurations that are dependents of this configuration\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation,\n                            String group, int orderInGroup, Width width, String displayName, List<String> dependents) {\n        return define(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName, dependents, null);\n    }\n\n    /**\n     * Define a new configuration with no dependents\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param defaultValue  the default value to use if this config isn't present\n     * @param validator     the validator to use in checking the correctness of the config\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @param recommender   the recommender provides valid values given the parent configuration values\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation,\n                            String group, int orderInGroup, Width width, String displayName, Recommender recommender) {\n        return define(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName, Collections.emptyList(), recommender);\n    }\n\n    /**\n     * Define a new configuration with no dependents and no custom recommender\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param defaultValue  the default value to use if this config isn't present\n     * @param validator     the validator to use in checking the correctness of the config\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation,\n                            String group, int orderInGroup, Width width, String displayName) {\n        return define(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName, Collections.emptyList());\n    }\n\n    /**\n     * Define a new configuration with no special validation logic\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param defaultValue  the default value to use if this config isn't present\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @param dependents    the configurations that are dependents of this configuration\n     * @param recommender   the recommender provides valid values given the parent configuration values\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Importance importance, String documentation,\n                            String group, int orderInGroup, Width width, String displayName, List<String> dependents, Recommender recommender) {\n        return define(name, type, defaultValue, null, importance, documentation, group, orderInGroup, width, displayName, dependents, recommender);\n    }\n\n    /**\n     * Define a new configuration with no special validation logic and no custom recommender\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param defaultValue  the default value to use if this config isn't present\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @param dependents    the configurations that are dependents of this configuration\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Importance importance, String documentation,\n                            String group, int orderInGroup, Width width, String displayName, List<String> dependents) {\n        return define(name, type, defaultValue, null, importance, documentation, group, orderInGroup, width, displayName, dependents, null);\n    }\n\n    /**\n     * Define a new configuration with no special validation logic and no custom recommender\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param defaultValue  the default value to use if this config isn't present\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @param recommender   the recommender provides valid values given the parent configuration values\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Importance importance, String documentation,\n                            String group, int orderInGroup, Width width, String displayName, Recommender recommender) {\n        return define(name, type, defaultValue, null, importance, documentation, group, orderInGroup, width, displayName, Collections.emptyList(), recommender);\n    }\n\n    /**\n     * Define a new configuration with no special validation logic, not dependents and no custom recommender\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param defaultValue  the default value to use if this config isn't present\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Importance importance, String documentation,\n                            String group, int orderInGroup, Width width, String displayName) {\n        return define(name, type, defaultValue, null, importance, documentation, group, orderInGroup, width, displayName, Collections.emptyList());\n    }\n\n    /**\n     * Define a new configuration with no default value and no special validation logic\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @param dependents    the configurations that are dependents of this configuration\n     * @param recommender   the recommender provides valid values given the parent configuration value\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Importance importance, String documentation, String group, int orderInGroup,\n                            Width width, String displayName, List<String> dependents, Recommender recommender) {\n        return define(name, type, NO_DEFAULT_VALUE, null, importance, documentation, group, orderInGroup, width, displayName, dependents, recommender);\n    }\n\n    /**\n     * Define a new configuration with no default value, no special validation logic and no custom recommender\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @param dependents    the configurations that are dependents of this configuration\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Importance importance, String documentation, String group, int orderInGroup,\n                            Width width, String displayName, List<String> dependents) {\n        return define(name, type, NO_DEFAULT_VALUE, null, importance, documentation, group, orderInGroup, width, displayName, dependents, null);\n    }\n\n    /**\n     * Define a new configuration with no default value, no special validation logic and no custom recommender\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @param recommender   the recommender provides valid values given the parent configuration value\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Importance importance, String documentation, String group, int orderInGroup,\n                            Width width, String displayName, Recommender recommender) {\n        return define(name, type, NO_DEFAULT_VALUE, null, importance, documentation, group, orderInGroup, width, displayName, Collections.emptyList(), recommender);\n    }\n\n    /**\n     * Define a new configuration with no default value, no special validation logic, no dependents and no custom recommender\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @param group         the group this config belongs to\n     * @param orderInGroup  the order of this config in the group\n     * @param width         the width of the config\n     * @param displayName   the name suitable for display\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Importance importance, String documentation, String group, int orderInGroup,\n                            Width width, String displayName) {\n        return define(name, type, NO_DEFAULT_VALUE, null, importance, documentation, group, orderInGroup, width, displayName, Collections.emptyList());\n    }\n\n    /**\n     * Define a new configuration with no group, no order in group, no width, no display name, no dependents and no custom recommender\n     * @param name          the name of the config parameter\n     * @param type          the type of the config\n     * @param defaultValue  the default value to use if this config isn't present\n     * @param validator     the validator to use in checking the correctness of the config\n     * @param importance    the importance of this config\n     * @param documentation the documentation string for the config\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation) {\n        return define(name, type, defaultValue, validator, importance, documentation, null, -1, Width.NONE, name);\n    }\n\n    /**\n     * Define a new configuration with no special validation logic\n     * @param name          The name of the config parameter\n     * @param type          The type of the config\n     * @param defaultValue  The default value to use if this config isn't present\n     * @param importance    The importance of this config: is this something you will likely need to change.\n     * @param documentation The documentation string for the config\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Importance importance, String documentation) {\n        return define(name, type, defaultValue, null, importance, documentation);\n    }\n\n    /**\n     * Define a new configuration with no special validation logic\n     * @param name              The name of the config parameter\n     * @param type              The type of the config\n     * @param defaultValue      The default value to use if this config isn't present\n     * @param importance        The importance of this config: is this something you will likely need to change.\n     * @param documentation     The documentation string for the config\n     * @param alternativeString The string which will be used to override the string of defaultValue\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Object defaultValue, Importance importance, String documentation, String alternativeString) {\n        return define(name, type, defaultValue, null, importance, documentation, null, -1, Width.NONE,\n                name, Collections.emptyList(), null, alternativeString);\n    }\n\n    /**\n     * Define a new configuration with no default value and no special validation logic\n     * @param name          The name of the config parameter\n     * @param type          The type of the config\n     * @param importance    The importance of this config: is this something you will likely need to change.\n     * @param documentation The documentation string for the config\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef define(String name, Type type, Importance importance, String documentation) {\n        return define(name, type, NO_DEFAULT_VALUE, null, importance, documentation);\n    }\n\n    /**\n     * Define a new internal configuration. Internal configuration won't show up in the docs and aren't\n     * intended for general use.\n     * @param name              The name of the config parameter\n     * @param type              The type of the config\n     * @param defaultValue      The default value to use if this config isn't present\n     * @param importance        The importance of this config (i.e. is this something you will likely need to change?)\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef defineInternal(final String name, final Type type, final Object defaultValue, final Importance importance) {\n        return define(new ConfigKey(name, type, defaultValue, null, importance, \"\", \"\", -1, Width.NONE, name, Collections.emptyList(), null, true, null));\n    }\n\n    /**\n     * Define a new internal configuration. Internal configuration won't show up in the docs and aren't\n     * intended for general use.\n     * @param name              The name of the config parameter\n     * @param type              The type of the config\n     * @param defaultValue      The default value to use if this config isn't present\n     * @param validator         The validator to use in checking the correctness of the config\n     * @param importance        The importance of this config (i.e. is this something you will likely need to change?)\n     * @param documentation     The documentation string for the config\n     * @return This ConfigDef so you can chain calls\n     */\n    public ConfigDef defineInternal(final String name, final Type type, final Object defaultValue, final Validator validator, final Importance importance, final String documentation) {\n        return define(new ConfigKey(name, type, defaultValue, validator, importance, documentation, \"\", -1, Width.NONE, name, Collections.emptyList(), null, true, null));\n    }\n\n    /**\n     * Get the configuration keys\n     * @return a map containing all configuration keys\n     */\n    public Map<String, ConfigKey> configKeys() {\n        return configKeys;\n    }\n\n    /**\n     * Get the groups for the configuration\n     * @return a list of group names\n     */\n    public List<String> groups() {\n        return groups;\n    }\n\n    /**\n     * Add standard SSL client configuration options.\n     * @return this\n     */\n    public ConfigDef withClientSslSupport() {\n        SslConfigs.addClientSslSupport(this);\n        return this;\n    }\n\n    /**\n     * Add standard SASL client configuration options.\n     * @return this\n     */\n    public ConfigDef withClientSaslSupport() {\n        SaslConfigs.addClientSaslSupport(this);\n        return this;\n    }\n\n    /**\n     * Parse and validate configs against this configuration definition. The input is a map of configs. It is expected\n     * that the keys of the map are strings, but the values can either be strings or they may already be of the\n     * appropriate type (int, string, etc). This will work equally well with either java.util.Properties instances or a\n     * programmatically constructed map.\n     *\n     * @param props The configs to parse and validate.\n     * @return Parsed and validated configs. The key will be the config name and the value will be the value parsed into\n     * the appropriate type (int, string, etc).\n     */\n    public Map<String, Object> parse(Map<?, ?> props) {\n        // Check all configurations are defined\n        List<String> undefinedConfigKeys = undefinedDependentConfigs();\n        if (!undefinedConfigKeys.isEmpty()) {\n            String joined = undefinedConfigKeys.stream().map(String::toString).collect(Collectors.joining(\",\"));\n            throw new ConfigException(\"Some configurations in are referred in the dependents, but not defined: \" + joined);\n        }\n        // parse all known keys\n        Map<String, Object> values = new HashMap<>();\n        for (ConfigKey key : configKeys.values())\n            values.put(key.name, parseValue(key, props.get(key.name), props.containsKey(key.name)));\n        return values;\n    }\n\n    Object parseValue(ConfigKey key, Object value, boolean isSet) {\n        Object parsedValue;\n        if (isSet) {\n            parsedValue = parseType(key.name, value, key.type);\n        // props map doesn't contain setting, the key is required because no default value specified - its an error\n        } else if (NO_DEFAULT_VALUE.equals(key.defaultValue)) {\n            throw new ConfigException(\"Missing required configuration \\\"\" + key.name + \"\\\" which has no default value.\");\n        } else {\n            // otherwise assign setting its default value\n            parsedValue = key.defaultValue;\n        }\n        if (key.validator != null) {\n            key.validator.ensureValid(key.name, parsedValue);\n        }\n        return parsedValue;\n    }\n\n    /**\n     * Validate the current configuration values with the configuration definition.\n     * @param props the current configuration values\n     * @return List of Config, each Config contains the updated configuration information given\n     * the current configuration values.\n     */\n    public List<ConfigValue> validate(Map<String, String> props) {\n        return new ArrayList<>(validateAll(props).values());\n    }\n\n    public Map<String, ConfigValue> validateAll(Map<String, String> props) {\n        Map<String, ConfigValue> configValues = new HashMap<>();\n        for (String name: configKeys.keySet()) {\n            configValues.put(name, new ConfigValue(name));\n        }\n\n        List<String> undefinedConfigKeys = undefinedDependentConfigs();\n        for (String undefinedConfigKey: undefinedConfigKeys) {\n            ConfigValue undefinedConfigValue = new ConfigValue(undefinedConfigKey);\n            undefinedConfigValue.addErrorMessage(undefinedConfigKey + \" is referred in the dependents, but not defined.\");\n            undefinedConfigValue.visible(false);\n            configValues.put(undefinedConfigKey, undefinedConfigValue);\n        }\n\n        Map<String, Object> parsed = parseForValidate(props, configValues);\n        return validate(parsed, configValues);\n    }\n\n    // package accessible for testing\n    Map<String, Object> parseForValidate(Map<String, String> props, Map<String, ConfigValue> configValues) {\n        Map<String, Object> parsed = new HashMap<>();\n        Set<String> configsWithNoParent = getConfigsWithNoParent();\n        for (String name: configsWithNoParent) {\n            parseForValidate(name, props, parsed, configValues);\n        }\n        return parsed;\n    }\n\n\n    private Map<String, ConfigValue> validate(Map<String, Object> parsed, Map<String, ConfigValue> configValues) {\n        Set<String> configsWithNoParent = getConfigsWithNoParent();\n        for (String name: configsWithNoParent) {\n            validate(name, parsed, configValues);\n        }\n        return configValues;\n    }\n\n    private List<String> undefinedDependentConfigs() {\n        Set<String> undefinedConfigKeys = new HashSet<>();\n        for (ConfigKey configKey : configKeys.values()) {\n            for (String dependent: configKey.dependents) {\n                if (!configKeys.containsKey(dependent)) {\n                    undefinedConfigKeys.add(dependent);\n                }\n            }\n        }\n        return new ArrayList<>(undefinedConfigKeys);\n    }\n\n    // package accessible for testing\n    Set<String> getConfigsWithNoParent() {\n        if (this.configsWithNoParent != null) {\n            return this.configsWithNoParent;\n        }\n        Set<String> configsWithParent = new HashSet<>();\n\n        for (ConfigKey configKey: configKeys.values()) {\n            List<String> dependents = configKey.dependents;\n            configsWithParent.addAll(dependents);\n        }\n\n        Set<String> configs = new HashSet<>(configKeys.keySet());\n        configs.removeAll(configsWithParent);\n        this.configsWithNoParent = configs;\n        return configs;\n    }\n\n    private void parseForValidate(String name, Map<String, String> props, Map<String, Object> parsed, Map<String, ConfigValue> configs) {\n        if (!configKeys.containsKey(name)) {\n            return;\n        }\n        ConfigKey key = configKeys.get(name);\n        ConfigValue config = configs.get(name);\n\n        Object value = null;\n        if (props.containsKey(key.name)) {\n            try {\n                value = parseType(key.name, props.get(key.name), key.type);\n            } catch (ConfigException e) {\n                config.addErrorMessage(e.getMessage());\n            }\n        } else if (NO_DEFAULT_VALUE.equals(key.defaultValue)) {\n            config.addErrorMessage(\"Missing required configuration \\\"\" + key.name + \"\\\" which has no default value.\");\n        } else {\n            value = key.defaultValue;\n        }\n\n        if (key.validator != null) {\n            try {\n                key.validator.ensureValid(key.name, value);\n            } catch (ConfigException e) {\n                config.addErrorMessage(e.getMessage());\n            }\n        }\n        config.value(value);\n        parsed.put(name, value);\n        for (String dependent: key.dependents) {\n            parseForValidate(dependent, props, parsed, configs);\n        }\n    }\n\n    private void validate(String name, Map<String, Object> parsed, Map<String, ConfigValue> configs) {\n        if (!configKeys.containsKey(name)) {\n            return;\n        }\n        ConfigKey key = configKeys.get(name);\n        ConfigValue value = configs.get(name);\n        if (key.recommender != null) {\n            try {\n                List<Object> recommendedValues = key.recommender.validValues(name, parsed);\n                List<Object> originalRecommendedValues = value.recommendedValues();\n                if (!originalRecommendedValues.isEmpty()) {\n                    Set<Object> originalRecommendedValueSet = new HashSet<>(originalRecommendedValues);\n                    recommendedValues.removeIf(o -> !originalRecommendedValueSet.contains(o));\n                }\n                value.recommendedValues(recommendedValues);\n                value.visible(key.recommender.visible(name, parsed));\n            } catch (ConfigException e) {\n                value.addErrorMessage(e.getMessage());\n            }\n        }\n\n        configs.put(name, value);\n        for (String dependent: key.dependents) {\n            validate(dependent, parsed, configs);\n        }\n    }\n\n    /**\n     * Parse a value according to its expected type.\n     * @param name  The config name\n     * @param value The config value\n     * @param type  The expected type\n     * @return The parsed object\n     */\n    public static Object parseType(String name, Object value, Type type) {\n        try {\n            if (value == null) return null;\n\n            String trimmed = null;\n            if (value instanceof String)\n                trimmed = ((String) value).trim();\n\n            switch (type) {\n                case BOOLEAN:\n                    if (value instanceof String) {\n                        if (trimmed.equalsIgnoreCase(\"true\"))\n                            return true;\n                        else if (trimmed.equalsIgnoreCase(\"false\"))\n                            return false;\n                        else\n                            throw new ConfigException(name, value, \"Expected value to be either true or false\");\n                    } else if (value instanceof Boolean)\n                        return value;\n                    else\n                        throw new ConfigException(name, value, \"Expected value to be either true or false\");\n                case PASSWORD:\n                    if (value instanceof Password)\n                        return value;\n                    else if (value instanceof String)\n                        return new Password(trimmed);\n                    else\n                        throw new ConfigException(name, value, \"Expected value to be a string, but it was a \" + value.getClass().getName());\n                case STRING:\n                    if (value instanceof String)\n                        return trimmed;\n                    else\n                        throw new ConfigException(name, value, \"Expected value to be a string, but it was a \" + value.getClass().getName());\n                case INT:\n                    if (value instanceof Integer) {\n                        return value;\n                    } else if (value instanceof String) {\n                        return Integer.parseInt(trimmed);\n                    } else {\n                        throw new ConfigException(name, value, \"Expected value to be a 32-bit integer, but it was a \" + value.getClass().getName());\n                    }\n                case SHORT:\n                    if (value instanceof Short) {\n                        return value;\n                    } else if (value instanceof String) {\n                        return Short.parseShort(trimmed);\n                    } else {\n                        throw new ConfigException(name, value, \"Expected value to be a 16-bit integer (short), but it was a \" + value.getClass().getName());\n                    }\n                case LONG:\n                    if (value instanceof Integer)\n                        return ((Integer) value).longValue();\n                    if (value instanceof Long)\n                        return value;\n                    else if (value instanceof String)\n                        return Long.parseLong(trimmed);\n                    else\n                        throw new ConfigException(name, value, \"Expected value to be a 64-bit integer (long), but it was a \" + value.getClass().getName());\n                case DOUBLE:\n                    if (value instanceof Number)\n                        return ((Number) value).doubleValue();\n                    else if (value instanceof String)\n                        return Double.parseDouble(trimmed);\n                    else\n                        throw new ConfigException(name, value, \"Expected value to be a double, but it was a \" + value.getClass().getName());\n                case LIST:\n                    if (value instanceof List)\n                        return value;\n                    else if (value instanceof String)\n                        if (trimmed.isEmpty())\n                            return Collections.emptyList();\n                        else\n                            return Arrays.asList(COMMA_WITH_WHITESPACE.split(trimmed, -1));\n                    else\n                        throw new ConfigException(name, value, \"Expected a comma separated list.\");\n                case CLASS:\n                    if (value instanceof Class)\n                        return value;\n                    else if (value instanceof String) {\n                        return Utils.loadClass(trimmed, Object.class);\n                    } else\n                        throw new ConfigException(name, value, \"Expected a Class instance or class name.\");\n                default:\n                    throw new IllegalStateException(\"Unknown type.\");\n            }\n        } catch (NumberFormatException e) {\n            throw new ConfigException(name, value, \"Not a number of type \" + type);\n        } catch (ClassNotFoundException e) {\n            throw new ConfigException(name, value, \"Class \" + value + \" could not be found.\");\n        }\n    }\n\n    public static String convertToString(Object parsedValue, Type type) {\n        if (parsedValue == null) {\n            return null;\n        }\n\n        if (type == null) {\n            return parsedValue.toString();\n        }\n\n        switch (type) {\n            case BOOLEAN:\n            case SHORT:\n            case INT:\n            case LONG:\n            case DOUBLE:\n            case STRING:\n            case PASSWORD:\n                return parsedValue.toString();\n            case LIST:\n                List<?> valueList = (List<?>) parsedValue;\n                return valueList.stream().map(Object::toString).collect(Collectors.joining(\",\"));\n            case CLASS:\n                Class<?> clazz = (Class<?>) parsedValue;\n                return clazz.getName();\n            default:\n                throw new IllegalStateException(\"Unknown type.\");\n        }\n    }\n\n    /**\n     * Converts a map of config (key, value) pairs to a map of strings where each value\n     * is converted to a string. This method should be used with care since it stores\n     * actual password values to String. Values from this map should never be used in log entries.\n     */\n    public static  Map<String, String> convertToStringMapWithPasswordValues(Map<String, ?> configs) {\n        Map<String, String> result = new HashMap<>();\n        for (Map.Entry<String, ?> entry : configs.entrySet()) {\n            Object value = entry.getValue();\n            String strValue;\n            if (value instanceof Password)\n                strValue = ((Password) value).value();\n            else if (value instanceof List)\n                strValue = convertToString(value, Type.LIST);\n            else if (value instanceof Class)\n                strValue = convertToString(value, Type.CLASS);\n            else\n                strValue = convertToString(value, null);\n            if (strValue != null)\n                result.put(entry.getKey(), strValue);\n        }\n        return result;\n    }\n\n    /**\n     * The type for a configuration value\n     */\n    public enum Type {\n        /**\n         * Used for boolean values. Values can be provided as a Boolean object or as a String with values\n         * <code>true</code> or <code>false</code> (this is not case-sensitive), otherwise a {@link ConfigException} is\n         * thrown.\n         */\n        BOOLEAN,\n        /**\n         * Used for string values. Values must be provided as a String object, otherwise a {@link ConfigException} is\n         * thrown.\n         */\n        STRING,\n        /**\n         * Used for numerical values within the Java Integer range. Values must be provided as a Integer object or as\n         * a String being a valid Integer value, otherwise a {@link ConfigException} is thrown.\n         */\n        INT,\n        /**\n         * Used for numerical values within the Java Short range. Values must be provided as a Short object or as\n         * a String being a valid Short value, otherwise a {@link ConfigException} is thrown.\n         */\n        SHORT,\n        /**\n         * Used for numerical values within the Java Long range. Values must be provided as a Long object, as an Integer\n         * object or as a String being a valid Long value, otherwise a {@link ConfigException} is thrown.\n         */\n        LONG,\n        /**\n         * Used for numerical values within the Java Double range. Values must be provided as a Number object, as a\n         * Double object or as a String being a valid Double value, otherwise a {@link ConfigException} is thrown.\n         */\n        DOUBLE,\n        /**\n         * Used for list values. Values must be provided as a List object, as a String object, otherwise a\n         * {@link ConfigException} is thrown. When the value is provided as a String it must use commas to separate the\n         * different entries (for example: <code>first-entry, second-entry</code>) and an empty String maps to an empty List.\n         */\n        LIST,\n        /**\n         * Used for values that implement a Kafka interface. Values must be provided as a Class object or as a\n         * String object, otherwise a {@link ConfigException} is thrown. When the value is provided as a String it must\n         * be the binary name of the Class.\n         */\n        CLASS,\n        /**\n         * Used for string values containing sensitive data such as a password or key. The values of configurations with\n         * of this type are not included in logs and instead replaced with \"[hidden]\". Values must be provided as a\n         * String object, otherwise a {@link ConfigException} is thrown.\n         */\n        PASSWORD;\n\n        /**\n         * Whether this type contains sensitive data such as a password or key.\n         * @return true if the type is {@link #PASSWORD}\n         */\n        public boolean isSensitive() {\n            return this == PASSWORD;\n        }\n    }\n\n    /**\n     * The importance level for a configuration\n     */\n    public enum Importance {\n        HIGH, MEDIUM, LOW\n    }\n\n    /**\n     * The width of a configuration value\n     */\n    public enum Width {\n        NONE, SHORT, MEDIUM, LONG\n    }\n\n    /**\n     * This is used by the {@link #validate(Map)} to get valid values for a configuration given the current\n     * configuration values in order to perform full configuration validation and visibility modification.\n     * In case that there are dependencies between configurations, the valid values and visibility\n     * for a configuration may change given the values of other configurations.\n     */\n    public interface Recommender {\n\n        /**\n         * The valid values for the configuration given the current configuration values.\n         * @param name The name of the configuration\n         * @param parsedConfig The parsed configuration values\n         * @return The list of valid values. To function properly, the returned objects should have the type\n         * defined for the configuration using the recommender.\n         */\n        List<Object> validValues(String name, Map<String, Object> parsedConfig);\n\n        /**\n         * Set the visibility of the configuration given the current configuration values.\n         * @param name The name of the configuration\n         * @param parsedConfig The parsed configuration values\n         * @return The visibility of the configuration\n         */\n        boolean visible(String name, Map<String, Object> parsedConfig);\n    }\n\n    /**\n     * Validation logic the user may provide to perform single configuration validation.\n     */\n    public interface Validator {\n        /**\n         * Perform single configuration validation.\n         * @param name The name of the configuration\n         * @param value The value of the configuration\n         * @throws ConfigException if the value is invalid.\n         */\n        void ensureValid(String name, Object value);\n    }\n\n    /**\n     * Validation logic for numeric ranges\n     */\n    public static class Range implements Validator {\n        private final Number min;\n        private final Number max;\n\n        /**\n         *  A numeric range with inclusive upper bound and inclusive lower bound\n         * @param min  the lower bound\n         * @param max  the upper bound\n         */\n        private Range(Number min, Number max) {\n            this.min = min;\n            this.max = max;\n        }\n\n        /**\n         * A numeric range that checks only the lower bound\n         *\n         * @param min The minimum acceptable value\n         */\n        public static Range atLeast(Number min) {\n            return new Range(min, null);\n        }\n\n        /**\n         * A numeric range that checks both the upper (inclusive) and lower bound\n         */\n        public static Range between(Number min, Number max) {\n            return new Range(min, max);\n        }\n\n        public void ensureValid(String name, Object o) {\n            if (o == null)\n                throw new ConfigException(name, null, \"Value must be non-null\");\n            Number n = (Number) o;\n            if (min != null && n.doubleValue() < min.doubleValue())\n                throw new ConfigException(name, o, \"Value must be at least \" + min);\n            if (max != null && n.doubleValue() > max.doubleValue())\n                throw new ConfigException(name, o, \"Value must be no more than \" + max);\n        }\n\n        public String toString() {\n            if (min == null && max == null)\n                return \"[...]\";\n            else if (min == null)\n                return \"[...,\" + max + \"]\";\n            else if (max == null)\n                return \"[\" + min + \",...]\";\n            else\n                return \"[\" + min + \",...,\" + max + \"]\";\n        }\n    }\n\n    public static class ValidList implements Validator {\n\n        final ValidString validString;\n\n        private ValidList(List<String> validStrings) {\n            this.validString = new ValidString(validStrings);\n        }\n\n        public static ValidList in(String... validStrings) {\n            return new ValidList(Arrays.asList(validStrings));\n        }\n\n        @Override\n        public void ensureValid(final String name, final Object value) {\n            @SuppressWarnings(\"unchecked\")\n            List<String> values = (List<String>) value;\n            for (String string : values) {\n                validString.ensureValid(name, string);\n            }\n        }\n\n        public String toString() {\n            return validString.toString();\n        }\n    }\n\n    public static class ValidString implements Validator {\n        final List<String> validStrings;\n\n        private ValidString(List<String> validStrings) {\n            this.validStrings = validStrings;\n        }\n\n        public static ValidString in(String... validStrings) {\n            return new ValidString(Arrays.asList(validStrings));\n        }\n\n        @Override\n        public void ensureValid(String name, Object o) {\n            String s = (String) o;\n            if (!validStrings.contains(s)) {\n                throw new ConfigException(name, o, \"String must be one of: \" + String.join(\", \", validStrings));\n            }\n\n        }\n\n        public String toString() {\n            return \"[\" + String.join(\", \", validStrings) + \"]\";\n        }\n    }\n\n    public static class CaseInsensitiveValidString implements Validator {\n\n        final Set<String> validStrings;\n\n        private CaseInsensitiveValidString(List<String> validStrings) {\n            this.validStrings = validStrings.stream()\n                .map(s -> s.toUpperCase(Locale.ROOT))\n                .collect(Collectors.toSet());\n        }\n\n        public static CaseInsensitiveValidString in(String... validStrings) {\n            return new CaseInsensitiveValidString(Arrays.asList(validStrings));\n        }\n\n        @Override\n        public void ensureValid(String name, Object o) {\n            String s = (String) o;\n            if (s == null || !validStrings.contains(s.toUpperCase(Locale.ROOT))) {\n                throw new ConfigException(name, o, \"String must be one of (case insensitive): \" + String.join(\", \", validStrings));\n            }\n        }\n\n        public String toString() {\n            return \"(case insensitive) [\" + String.join(\", \", validStrings) + \"]\";\n        }\n    }\n\n    public static class NonNullValidator implements Validator {\n        @Override\n        public void ensureValid(String name, Object value) {\n            if (value == null) {\n                // Pass in the string null to avoid the spotbugs warning\n                throw new ConfigException(name, \"null\", \"entry must be non null\");\n            }\n        }\n\n        public String toString() {\n            return \"non-null string\";\n        }\n    }\n\n    public static class LambdaValidator implements Validator {\n        BiConsumer<String, Object> ensureValid;\n        Supplier<String> toStringFunction;\n\n        private LambdaValidator(BiConsumer<String, Object> ensureValid,\n                                Supplier<String> toStringFunction) {\n            this.ensureValid = ensureValid;\n            this.toStringFunction = toStringFunction;\n        }\n\n        public static LambdaValidator with(BiConsumer<String, Object> ensureValid,\n                                           Supplier<String> toStringFunction) {\n            return new LambdaValidator(ensureValid, toStringFunction);\n        }\n\n        @Override\n        public void ensureValid(String name, Object value) {\n            ensureValid.accept(name, value);\n        }\n\n        @Override\n        public String toString() {\n            return toStringFunction.get();\n        }\n    }\n\n    public static class CompositeValidator implements Validator {\n        private final List<Validator> validators;\n\n        private CompositeValidator(List<Validator> validators) {\n            this.validators = Collections.unmodifiableList(validators);\n        }\n\n        public static CompositeValidator of(Validator... validators) {\n            return new CompositeValidator(Arrays.asList(validators));\n        }\n\n        @Override\n        public void ensureValid(String name, Object value) {\n            for (Validator validator: validators) {\n                validator.ensureValid(name, value);\n            }\n        }\n\n        @Override\n        public String toString() {\n            if (validators == null) return \"\";\n            StringBuilder desc = new StringBuilder();\n            for (Validator v: validators) {\n                if (desc.length() > 0) {\n                    desc.append(',').append(' ');\n                }\n                desc.append(v);\n            }\n            return desc.toString();\n        }\n    }\n\n    public static class NonEmptyString implements Validator {\n\n        @Override\n        public void ensureValid(String name, Object o) {\n            String s = (String) o;\n            if (s != null && s.isEmpty()) {\n                throw new ConfigException(name, o, \"String must be non-empty\");\n            }\n        }\n\n        @Override\n        public String toString() {\n            return \"non-empty string\";\n        }\n    }\n\n    public static class NonEmptyStringWithoutControlChars implements Validator {\n\n        public static NonEmptyStringWithoutControlChars nonEmptyStringWithoutControlChars() {\n            return new NonEmptyStringWithoutControlChars();\n        }\n\n        @Override\n        public void ensureValid(String name, Object value) {\n            String s = (String) value;\n\n            if (s == null) {\n                // This can happen during creation of the config object due to no default value being defined for the\n                // name configuration - a missing name parameter is caught when checking for mandatory parameters,\n                // thus we can ok a null value here\n                return;\n            } else if (s.isEmpty()) {\n                throw new ConfigException(name, value, \"String may not be empty\");\n            }\n\n            // Check name string for illegal characters\n            ArrayList<Integer> foundIllegalCharacters = new ArrayList<>();\n\n            for (int i = 0; i < s.length(); i++) {\n                if (Character.isISOControl(s.codePointAt(i))) {\n                    foundIllegalCharacters.add(s.codePointAt(i));\n                }\n            }\n\n            if (!foundIllegalCharacters.isEmpty()) {\n                throw new ConfigException(name, value, \"String may not contain control sequences but had the following ASCII chars: \" +\n                        foundIllegalCharacters.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n            }\n        }\n\n        public String toString() {\n            return \"non-empty string without ISO control characters\";\n        }\n    }\n\n    public static class ListSize implements Validator {\n        final int maxSize;\n\n        private ListSize(final int maxSize) {\n            this.maxSize = maxSize;\n        }\n\n        public static ListSize atMostOfSize(final int maxSize) {\n            return new ListSize(maxSize);\n        }\n\n        @Override\n        public void ensureValid(final String name, final Object value) {\n            @SuppressWarnings(\"unchecked\")\n            List<String> values = (List<String>) value;\n            if (values.size() > maxSize) {\n                throw new ConfigException(name, value, \"exceeds maximum list size of [\" + maxSize + \"].\");\n            }\n        }\n\n        @Override\n        public String toString() {\n            return \"List containing maximum of \" + maxSize + \" elements\";\n        }\n    }\n\n    public static class ConfigKey {\n        public final String name;\n        public final Type type;\n        public final String documentation;\n        public final Object defaultValue;\n        public final Validator validator;\n        public final Importance importance;\n        public final String group;\n        public final int orderInGroup;\n        public final Width width;\n        public final String displayName;\n        public final List<String> dependents;\n        public final Recommender recommender;\n        public final boolean internalConfig;\n        public final String alternativeString;\n\n        // This constructor is present for backward compatibility reasons.\n        public ConfigKey(String name, Type type, Object defaultValue, Validator validator,\n                         Importance importance, String documentation, String group,\n                         int orderInGroup, Width width, String displayName,\n                         List<String> dependents, Recommender recommender,\n                         boolean internalConfig) {\n            this(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName,\n                dependents, recommender, internalConfig, null);\n        }\n\n        private ConfigKey(String name, Type type, Object defaultValue, Validator validator,\n                         Importance importance, String documentation, String group,\n                         int orderInGroup, Width width, String displayName,\n                         List<String> dependents, Recommender recommender,\n                         boolean internalConfig, String alternativeString) {\n            this.name = name;\n            this.type = type;\n            boolean hasDefault = !NO_DEFAULT_VALUE.equals(defaultValue);\n            this.defaultValue = hasDefault ? parseType(name, defaultValue, type) : NO_DEFAULT_VALUE;\n            this.validator = validator;\n            this.importance = importance;\n            if (this.validator != null && hasDefault)\n                this.validator.ensureValid(name, this.defaultValue);\n            this.documentation = documentation;\n            this.dependents = dependents;\n            this.group = group;\n            this.orderInGroup = orderInGroup;\n            this.width = width;\n            this.displayName = displayName;\n            this.recommender = recommender;\n            this.internalConfig = internalConfig;\n            this.alternativeString = alternativeString;\n        }\n\n        public boolean hasDefault() {\n            return !NO_DEFAULT_VALUE.equals(this.defaultValue);\n        }\n\n        public Type type() {\n            return type;\n        }\n    }\n\n    protected List<String> headers() {\n        return Arrays.asList(\"Name\", \"Description\", \"Type\", \"Default\", \"Valid Values\", \"Importance\");\n    }\n\n    protected String getConfigValue(ConfigKey key, String headerName) {\n        switch (headerName) {\n            case \"Name\":\n                return key.name;\n            case \"Description\":\n                return key.documentation;\n            case \"Type\":\n                return key.type.toString().toLowerCase(Locale.ROOT);\n            case \"Default\":\n                if (key.hasDefault()) {\n                    if (key.defaultValue == null)\n                        return \"null\";\n                    String defaultValueStr = convertToString(key.defaultValue, key.type);\n                    if (defaultValueStr.isEmpty())\n                        return \"\\\"\\\"\";\n                    else {\n                        String suffix = \"\";\n                        if (key.name.endsWith(\".bytes\")) {\n                            suffix = niceMemoryUnits(((Number) key.defaultValue).longValue());\n                        } else if (key.name.endsWith(\".ms\")) {\n                            suffix = niceTimeUnits(((Number) key.defaultValue).longValue());\n                        }\n                        return defaultValueStr + suffix;\n                    }\n                } else\n                    return \"\";\n            case \"Valid Values\":\n                return key.validator != null ? key.validator.toString() : \"\";\n            case \"Importance\":\n                return key.importance.toString().toLowerCase(Locale.ROOT);\n            default:\n                throw new RuntimeException(\"Can't find value for header '\" + headerName + \"' in \" + key.name);\n        }\n    }\n\n    static String niceMemoryUnits(long bytes) {\n        long value = bytes;\n        int i = 0;\n        while (value != 0 && i < 4) {\n            if (value % 1024L == 0) {\n                value /= 1024L;\n                i++;\n            } else {\n                break;\n            }\n        }\n        String resultFormat = \" (\" + value + \" %s\" + (value == 1 ? \")\" : \"s)\");\n        switch (i) {\n            case 1:\n                return String.format(resultFormat, \"kibibyte\");\n            case 2:\n                return String.format(resultFormat, \"mebibyte\");\n            case 3:\n                return String.format(resultFormat, \"gibibyte\");\n            case 4:\n                return String.format(resultFormat, \"tebibyte\");\n            default:\n                return \"\";\n        }\n    }\n\n    static String niceTimeUnits(long millis) {\n        long value = millis;\n        long[] divisors = {1000, 60, 60, 24};\n        String[] units = {\"second\", \"minute\", \"hour\", \"day\"};\n        int i = 0;\n        while (value != 0 && i < 4) {\n            if (value % divisors[i] == 0) {\n                value /= divisors[i];\n                i++;\n            } else {\n                break;\n            }\n        }\n        if (i > 0) {\n            return \" (\" + value + \" \" + units[i - 1] + (value > 1 ? \"s)\" : \")\");\n        }\n        return \"\";\n    }\n\n    public String toHtmlTable() {\n        return toHtmlTable(Collections.emptyMap());\n    }\n\n    private void addHeader(StringBuilder builder, String headerName) {\n        builder.append(\"<th>\");\n        builder.append(headerName);\n        builder.append(\"</th>\\n\");\n    }\n\n    private void addColumnValue(StringBuilder builder, String value) {\n        builder.append(\"<td>\");\n        builder.append(value);\n        builder.append(\"</td>\");\n    }\n\n    /**\n     * Converts this config into an HTML table that can be embedded into docs.\n     * If <code>dynamicUpdateModes</code> is non-empty, a \"Dynamic Update Mode\" column\n     * will be included n the table with the value of the update mode. Default\n     * mode is \"read-only\".\n     * @param dynamicUpdateModes Config name -&gt; update mode mapping\n     */\n    public String toHtmlTable(Map<String, String> dynamicUpdateModes) {\n        boolean hasUpdateModes = !dynamicUpdateModes.isEmpty();\n        List<ConfigKey> configs = sortedConfigs();\n        StringBuilder b = new StringBuilder();\n        b.append(\"<table class=\\\"data-table\\\"><tbody>\\n\");\n        b.append(\"<tr>\\n\");\n        // print column headers\n        for (String headerName : headers()) {\n            addHeader(b, headerName);\n        }\n        if (hasUpdateModes)\n            addHeader(b, \"Dynamic Update Mode\");\n        b.append(\"</tr>\\n\");\n        for (ConfigKey key : configs) {\n            if (key.internalConfig) {\n                continue;\n            }\n            b.append(\"<tr>\\n\");\n            // print column values\n            for (String headerName : headers()) {\n                addColumnValue(b, getConfigValue(key, headerName));\n                b.append(\"</td>\");\n            }\n            if (hasUpdateModes) {\n                String updateMode = dynamicUpdateModes.get(key.name);\n                if (updateMode == null)\n                    updateMode = \"read-only\";\n                addColumnValue(b, updateMode);\n            }\n            b.append(\"</tr>\\n\");\n        }\n        b.append(\"</tbody></table>\");\n        return b.toString();\n    }\n\n    /**\n     * Get the configs formatted with reStructuredText, suitable for embedding in Sphinx\n     * documentation.\n     */\n    public String toRst() {\n        StringBuilder b = new StringBuilder();\n        for (ConfigKey key : sortedConfigs()) {\n            if (key.internalConfig) {\n                continue;\n            }\n            getConfigKeyRst(key, b);\n            b.append(\"\\n\");\n        }\n        return b.toString();\n    }\n\n    /**\n     * Configs with new metadata (group, orderInGroup, dependents) formatted with reStructuredText, suitable for embedding in Sphinx\n     * documentation.\n     */\n    public String toEnrichedRst() {\n        StringBuilder b = new StringBuilder();\n\n        String lastKeyGroupName = \"\";\n        for (ConfigKey key : sortedConfigs()) {\n            if (key.internalConfig) {\n                continue;\n            }\n            if (key.group != null) {\n                if (!lastKeyGroupName.equalsIgnoreCase(key.group)) {\n                    b.append(key.group).append(\"\\n\");\n\n                    char[] underLine = new char[key.group.length()];\n                    Arrays.fill(underLine, '^');\n                    b.append(new String(underLine)).append(\"\\n\\n\");\n                }\n                lastKeyGroupName = key.group;\n            }\n\n            getConfigKeyRst(key, b);\n\n            if (key.dependents != null && key.dependents.size() > 0) {\n                int j = 0;\n                b.append(\"  * Dependents: \");\n                for (String dependent : key.dependents) {\n                    b.append(\"``\");\n                    b.append(dependent);\n                    if (++j == key.dependents.size())\n                        b.append(\"``\");\n                    else\n                        b.append(\"``, \");\n                }\n                b.append(\"\\n\");\n            }\n            b.append(\"\\n\");\n        }\n        return b.toString();\n    }\n\n    /**\n     * Shared content on Rst and Enriched Rst.\n     */\n    private void getConfigKeyRst(ConfigKey key, StringBuilder b) {\n        b.append(\"``\").append(key.name).append(\"``\").append(\"\\n\");\n        if (key.documentation != null) {\n            for (String docLine : key.documentation.split(\"\\n\")) {\n                if (docLine.length() == 0) {\n                    continue;\n                }\n                b.append(\"  \").append(docLine).append(\"\\n\\n\");\n            }\n        } else {\n            b.append(\"\\n\");\n        }\n        b.append(\"  * Type: \").append(getConfigValue(key, \"Type\")).append(\"\\n\");\n        if (key.hasDefault()) {\n            b.append(\"  * Default: \").append(getConfigValue(key, \"Default\")).append(\"\\n\");\n        }\n        if (key.validator != null) {\n            b.append(\"  * Valid Values: \").append(getConfigValue(key, \"Valid Values\")).append(\"\\n\");\n        }\n        b.append(\"  * Importance: \").append(getConfigValue(key, \"Importance\")).append(\"\\n\");\n    }\n\n    /**\n     * Get a list of configs sorted taking the 'group' and 'orderInGroup' into account.\n     *\n     * If grouping is not specified, the result will reflect \"natural\" order: listing required fields first, then ordering by importance, and finally by name.\n     */\n    private List<ConfigKey> sortedConfigs() {\n        final Map<String, Integer> groupOrd = new HashMap<>(groups.size());\n        int ord = 0;\n        for (String group: groups) {\n            groupOrd.put(group, ord++);\n        }\n\n        List<ConfigKey> configs = new ArrayList<>(configKeys.values());\n        Collections.sort(configs, (k1, k2) -> compare(k1, k2, groupOrd));\n        return configs;\n    }\n\n    private int compare(ConfigKey k1, ConfigKey k2, Map<String, Integer> groupOrd) {\n        int cmp = k1.group == null\n            ? (k2.group == null ? 0 : -1)\n            : (k2.group == null ? 1 : Integer.compare(groupOrd.get(k1.group), groupOrd.get(k2.group)));\n        if (cmp == 0) {\n            cmp = Integer.compare(k1.orderInGroup, k2.orderInGroup);\n            if (cmp == 0) {\n                // first take anything with no default value\n                if (!k1.hasDefault() && k2.hasDefault())\n                    cmp = -1;\n                else if (!k2.hasDefault() && k1.hasDefault())\n                    cmp = 1;\n                else {\n                    cmp = k1.importance.compareTo(k2.importance);\n                    if (cmp == 0)\n                        return k1.name.compareTo(k2.name);\n                }\n            }\n        }\n        return cmp;\n    }\n\n    public void embed(final String keyPrefix, final String groupPrefix, final int startingOrd, final ConfigDef child) {\n        int orderInGroup = startingOrd;\n        for (ConfigKey key : child.sortedConfigs()) {\n            define(new ConfigKey(\n                    keyPrefix + key.name,\n                    key.type,\n                    key.defaultValue,\n                    embeddedValidator(keyPrefix, key.validator),\n                    key.importance,\n                    key.documentation,\n                    groupPrefix + (key.group == null ? \"\" : \": \" + key.group),\n                    orderInGroup++,\n                    key.width,\n                    key.displayName,\n                    embeddedDependents(keyPrefix, key.dependents),\n                    embeddedRecommender(keyPrefix, key.recommender),\n                    key.internalConfig,\n                    key.alternativeString));\n        }\n    }\n\n    /**\n     * Returns a new validator instance that delegates to the base validator but unprefixes the config name along the way.\n     */\n    private static Validator embeddedValidator(final String keyPrefix, final Validator base) {\n        if (base == null) return null;\n        return new Validator() {\n            public void ensureValid(String name, Object value) {\n                base.ensureValid(name.substring(keyPrefix.length()), value);\n            }\n\n            @Override\n            public String toString() {\n                return base.toString();\n            }\n        };\n    }\n\n    /**\n     * Updated list of dependent configs with the specified {@code prefix} added.\n     */\n    private static List<String> embeddedDependents(final String keyPrefix, final List<String> dependents) {\n        if (dependents == null) return null;\n        final List<String> updatedDependents = new ArrayList<>(dependents.size());\n        for (String dependent : dependents) {\n            updatedDependents.add(keyPrefix + dependent);\n        }\n        return updatedDependents;\n    }\n\n    /**\n     * Returns a new recommender instance that delegates to the base recommender but unprefixes the input parameters along the way.\n     */\n    private static Recommender embeddedRecommender(final String keyPrefix, final Recommender base) {\n        if (base == null) return null;\n        return new Recommender() {\n            private String unprefixed(String k) {\n                return k.substring(keyPrefix.length());\n            }\n\n            private Map<String, Object> unprefixed(Map<String, Object> parsedConfig) {\n                final Map<String, Object> unprefixedParsedConfig = new HashMap<>(parsedConfig.size());\n                for (Map.Entry<String, Object> e : parsedConfig.entrySet()) {\n                    if (e.getKey().startsWith(keyPrefix)) {\n                        unprefixedParsedConfig.put(unprefixed(e.getKey()), e.getValue());\n                    }\n                }\n                return unprefixedParsedConfig;\n            }\n\n            @Override\n            public List<Object> validValues(String name, Map<String, Object> parsedConfig) {\n                return base.validValues(unprefixed(name), unprefixed(parsedConfig));\n            }\n\n            @Override\n            public boolean visible(String name, Map<String, Object> parsedConfig) {\n                return base.visible(unprefixed(name), unprefixed(parsedConfig));\n            }\n        };\n    }\n\n    public String toHtml() {\n        return toHtml(Collections.emptyMap());\n    }\n\n    /**\n     * Converts this config into an HTML list that can be embedded into docs.\n     * @param headerDepth The top level header depth in the generated HTML.\n     * @param idGenerator A function for computing the HTML id attribute in the generated HTML from a given config name.\n     */\n    public String toHtml(int headerDepth, Function<String, String> idGenerator) {\n        return toHtml(headerDepth, idGenerator, Collections.emptyMap());\n    }\n\n    /**\n     * Converts this config into an HTML list that can be embedded into docs.\n     * If <code>dynamicUpdateModes</code> is non-empty, a \"Dynamic Update Mode\" label\n     * will be included in the config details with the value of the update mode. Default\n     * mode is \"read-only\".\n     * @param dynamicUpdateModes Config name -&gt; update mode mapping.\n     */\n    public String toHtml(Map<String, String> dynamicUpdateModes) {\n        return toHtml(4, Function.identity(), dynamicUpdateModes);\n    }\n\n    /**\n     * Converts this config into an HTML list that can be embedded into docs.\n     * If <code>dynamicUpdateModes</code> is non-empty, a \"Dynamic Update Mode\" label\n     * will be included in the config details with the value of the update mode. Default\n     * mode is \"read-only\".\n     * @param headerDepth The top level header depth in the generated HTML.\n     * @param idGenerator A function for computing the HTML id attribute in the generated HTML from a given config name.\n     * @param dynamicUpdateModes Config name -&gt; update mode mapping.\n     */\n    public String toHtml(int headerDepth, Function<String, String> idGenerator,\n                         Map<String, String> dynamicUpdateModes) {\n        boolean hasUpdateModes = !dynamicUpdateModes.isEmpty();\n        List<ConfigKey> configs = sortedConfigs();\n        StringBuilder b = new StringBuilder();\n        b.append(\"<ul class=\\\"config-list\\\">\\n\");\n        for (ConfigKey key : configs) {\n            if (key.internalConfig) {\n                continue;\n            }\n            b.append(\"<li>\\n\");\n            b.append(String.format(\"<h%1$d>\" +\n                    \"<a id=\\\"%3$s\\\"></a><a id=\\\"%2$s\\\" href=\\\"#%2$s\\\">%3$s</a>\" +\n                    \"</h%1$d>%n\", headerDepth, idGenerator.apply(key.name), key.name));\n            b.append(\"<p>\");\n            if (key.documentation != null) {\n                b.append(key.documentation.replaceAll(\"\\n\", \"<br>\"));\n            }\n            b.append(\"</p>\\n\");\n\n            b.append(\"<table>\" +\n                    \"<tbody>\\n\");\n            for (String detail : headers()) {\n                if (detail.equals(\"Name\") || detail.equals(\"Description\")) continue;\n                if (detail.equals(\"Default\") && key.alternativeString != null) {\n                    addConfigDetail(b, detail, key.alternativeString);\n                    continue;\n                }\n                addConfigDetail(b, detail, getConfigValue(key, detail));\n            }\n            if (hasUpdateModes) {\n                String updateMode = dynamicUpdateModes.get(key.name);\n                if (updateMode == null)\n                    updateMode = \"read-only\";\n                addConfigDetail(b, \"Update Mode\", updateMode);\n            }\n            b.append(\"</tbody></table>\\n\");\n            b.append(\"</li>\\n\");\n        }\n        b.append(\"</ul>\\n\");\n        return b.toString();\n    }\n\n    private static void addConfigDetail(StringBuilder builder, String name, String value) {\n        builder.append(\"<tr>\" +\n                \"<th>\" + name + \":</th>\" +\n                \"<td>\" + value + \"</td>\" +\n                \"</tr>\\n\");\n    }\n\n}\n",
        "methodName": null,
        "exampleID": 40,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java",
        "line": 762,
        "sink": "'.",
        "source": "-",
        "sourceLine": 762,
        "qualifier": "This [regular expression](1) that depends on a [user-provided value](2) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](3) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](4) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](5) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](6) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](7) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](8) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](9) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](10) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](11) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](12) may run slow on strings with many repetitions of ' '.",
        "line_number": 762,
        "steps": [
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 41
            },
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 41
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 41
            },
            {
                "line": 232,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 41
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosShortNamer.java#L53",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.security.kerberos;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n/**\n * This class implements parsing and handling of Kerberos principal names. In\n * particular, it splits them apart and translates them down into local\n * operating system names.\n */\npublic class KerberosShortNamer {\n\n    /**\n     * A pattern for parsing a auth_to_local rule.\n     */\n    private static final Pattern RULE_PARSER = Pattern.compile(\"((DEFAULT)|((RULE:\\\\[(\\\\d*):([^\\\\]]*)](\\\\(([^)]*)\\\\))?(s/([^/]*)/([^/]*)/(g)?)?/?(L|U)?)))\");\n\n    /* Rules for the translation of the principal name into an operating system name */\n    private final List<KerberosRule> principalToLocalRules;\n\n    public KerberosShortNamer(List<KerberosRule> principalToLocalRules) {\n        this.principalToLocalRules = principalToLocalRules;\n    }\n\n    public static KerberosShortNamer fromUnparsedRules(String defaultRealm, List<String> principalToLocalRules) {\n        List<String> rules = principalToLocalRules == null ? Collections.singletonList(\"DEFAULT\") : principalToLocalRules;\n        return new KerberosShortNamer(parseRules(defaultRealm, rules));\n    }\n\n    private static List<KerberosRule> parseRules(String defaultRealm, List<String> rules) {\n        List<KerberosRule> result = new ArrayList<>();\n        for (String rule : rules) {\n            Matcher matcher = RULE_PARSER.matcher(rule);\n            if (!matcher.lookingAt()) {\n                throw new IllegalArgumentException(\"Invalid rule: \" + rule);\n            }\n            if (rule.length() != matcher.end())\n                throw new IllegalArgumentException(\"Invalid rule: `\" + rule + \"`, unmatched substring: `\" + rule.substring(matcher.end()) + \"`\");\n            if (matcher.group(2) != null) {\n                result.add(new KerberosRule(defaultRealm));\n            } else {\n                result.add(new KerberosRule(defaultRealm,\n                        Integer.parseInt(matcher.group(5)),\n                        matcher.group(6),\n                        matcher.group(8),\n                        matcher.group(10),\n                        matcher.group(11),\n                        \"g\".equals(matcher.group(12)),\n                        \"L\".equals(matcher.group(13)),\n                        \"U\".equals(matcher.group(13))));\n\n            }\n        }\n        return result;\n    }\n\n    /**\n     * Get the translation of the principal name into an operating system\n     * user name.\n     * @return the short name\n     * @throws IOException\n     */\n    public String shortName(KerberosName kerberosName) throws IOException {\n        String[] params;\n        if (kerberosName.hostName() == null) {\n            // if it is already simple, just return it\n            if (kerberosName.realm() == null)\n                return kerberosName.serviceName();\n            params = new String[]{kerberosName.realm(), kerberosName.serviceName()};\n        } else {\n            params = new String[]{kerberosName.realm(), kerberosName.serviceName(), kerberosName.hostName()};\n        }\n        for (KerberosRule r : principalToLocalRules) {\n            String result = r.apply(params);\n            if (result != null)\n                return result;\n        }\n        throw new NoMatchingRule(\"No rules apply to \" + kerberosName + \", rules \" + principalToLocalRules);\n    }\n\n    @Override\n    public String toString() {\n        return \"KerberosShortNamer(principalToLocalRules = \" + principalToLocalRules + \")\";\n    }\n\n}\n",
        "methodName": null,
        "exampleID": 42,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosShortNamer.java",
        "line": 53,
        "sink": "'RULE:\\[:\\\\'.",
        "source": "-",
        "sourceLine": 53,
        "qualifier": "This [regular expression](1) that depends on a [user-provided value](2) may run slow on strings starting with 'RULE:\\[:' and with many repetitions of 'RULE:\\[:\\\\'.\nThis [regular expression](3) that depends on a [user-provided value](2) may run slow on strings starting with 'RULE:\\[:\\](' and with many repetitions of 'RULE:\\[:\\](('.\nThis [regular expression](1) that depends on a [user-provided value](4) may run slow on strings starting with 'RULE:\\[:' and with many repetitions of 'RULE:\\[:\\\\'.\nThis [regular expression](3) that depends on a [user-provided value](4) may run slow on strings starting with 'RULE:\\[:\\](' and with many repetitions of 'RULE:\\[:\\](('.\nThis [regular expression](1) that depends on a [user-provided value](5) may run slow on strings starting with 'RULE:\\[:' and with many repetitions of 'RULE:\\[:\\\\'.\nThis [regular expression](3) that depends on a [user-provided value](5) may run slow on strings starting with 'RULE:\\[:\\](' and with many repetitions of 'RULE:\\[:\\](('.\nThis [regular expression](1) that depends on a [user-provided value](6) may run slow on strings starting with 'RULE:\\[:' and with many repetitions of 'RULE:\\[:\\\\'.\nThis [regular expression](3) that depends on a [user-provided value](6) may run slow on strings starting with 'RULE:\\[:\\](' and with many repetitions of 'RULE:\\[:\\](('.\nThis [regular expression](1) that depends on a [user-provided value](7) may run slow on strings starting with 'RULE:\\[:' and with many repetitions of 'RULE:\\[:\\\\'.\nThis [regular expression](3) that depends on a [user-provided value](7) may run slow on strings starting with 'RULE:\\[:\\](' and with many repetitions of 'RULE:\\[:\\](('.\nThis [regular expression](1) that depends on a [user-provided value](8) may run slow on strings starting with 'RULE:\\[:' and with many repetitions of 'RULE:\\[:\\\\'.\nThis [regular expression](3) that depends on a [user-provided value](8) may run slow on strings starting with 'RULE:\\[:\\](' and with many repetitions of 'RULE:\\[:\\](('.\nThis [regular expression](1) that depends on a [user-provided value](9) may run slow on strings starting with 'RULE:\\[:' and with many repetitions of 'RULE:\\[:\\\\'.\nThis [regular expression](3) that depends on a [user-provided value](9) may run slow on strings starting with 'RULE:\\[:\\](' and with many repetitions of 'RULE:\\[:\\](('.\nThis [regular expression](1) that depends on a [user-provided value](10) may run slow on strings starting with 'RULE:\\[:' and with many repetitions of 'RULE:\\[:\\\\'.\nThis [regular expression](3) that depends on a [user-provided value](10) may run slow on strings starting with 'RULE:\\[:\\](' and with many repetitions of 'RULE:\\[:\\](('.",
        "line_number": 53,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 43
            },
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 43
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 43
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 43
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/security/ssl/SslPrincipalMapper.java#L61",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.security.ssl;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport static org.apache.kafka.common.config.internals.BrokerSecurityConfigs.DEFAULT_SSL_PRINCIPAL_MAPPING_RULES;\n\npublic class SslPrincipalMapper {\n\n    private static final String RULE_PATTERN = \"(DEFAULT)|RULE:((\\\\\\\\.|[^\\\\\\\\/])*)/((\\\\\\\\.|[^\\\\\\\\/])*)/([LU]?).*?|(.*?)\";\n    private static final Pattern RULE_SPLITTER = Pattern.compile(\"\\\\s*(\" + RULE_PATTERN + \")\\\\s*(,\\\\s*|$)\");\n    private static final Pattern RULE_PARSER = Pattern.compile(RULE_PATTERN);\n\n    private final List<Rule> rules;\n\n    public SslPrincipalMapper(String sslPrincipalMappingRules) {\n        this.rules = parseRules(splitRules(sslPrincipalMappingRules));\n    }\n\n    public static SslPrincipalMapper fromRules(String sslPrincipalMappingRules) {\n        return new SslPrincipalMapper(sslPrincipalMappingRules);\n    }\n\n    private static List<String> splitRules(String sslPrincipalMappingRules) {\n        if (sslPrincipalMappingRules == null) {\n            sslPrincipalMappingRules = DEFAULT_SSL_PRINCIPAL_MAPPING_RULES;\n        }\n\n        List<String> result = new ArrayList<>();\n        Matcher matcher = RULE_SPLITTER.matcher(sslPrincipalMappingRules.trim());\n        while (matcher.find()) {\n            result.add(matcher.group(1));\n        }\n\n        return result;\n    }\n\n    private static List<Rule> parseRules(List<String> rules) {\n        List<Rule> result = new ArrayList<>();\n        for (String rule : rules) {\n            Matcher matcher = RULE_PARSER.matcher(rule);\n            if (!matcher.lookingAt()) {\n                throw new IllegalArgumentException(\"Invalid rule: \" + rule);\n            }\n            if (rule.length() != matcher.end()) {\n                throw new IllegalArgumentException(\"Invalid rule: `\" + rule + \"`, unmatched substring: `\" + rule.substring(matcher.end()) + \"`\");\n            }\n\n            // empty rules are ignored\n            if (matcher.group(1) != null) {\n                result.add(new Rule());\n            } else if (matcher.group(2) != null) {\n                result.add(new Rule(matcher.group(2),\n                                    matcher.group(4),\n                                    \"L\".equals(matcher.group(6)),\n                                    \"U\".equals(matcher.group(6))));\n            }\n        }\n\n        return result;\n    }\n\n    public String getName(String distinguishedName) throws IOException {\n        for (Rule r : rules) {\n            String principalName = r.apply(distinguishedName);\n            if (principalName != null) {\n                return principalName;\n            }\n        }\n        throw new NoMatchingRule(\"No rules apply to \" + distinguishedName + \", rules \" + rules);\n    }\n\n    @Override\n    public String toString() {\n        return \"SslPrincipalMapper(rules = \" + rules + \")\";\n    }\n\n    public static class NoMatchingRule extends IOException {\n        NoMatchingRule(String msg) {\n            super(msg);\n        }\n    }\n\n    private static class Rule {\n        private static final Pattern BACK_REFERENCE_PATTERN = Pattern.compile(\"\\\\$(\\\\d+)\");\n\n        private final boolean isDefault;\n        private final Pattern pattern;\n        private final String replacement;\n        private final boolean toLowerCase;\n        private final boolean toUpperCase;\n\n        Rule() {\n            isDefault = true;\n            pattern = null;\n            replacement = null;\n            toLowerCase = false;\n            toUpperCase = false;\n        }\n\n        Rule(String pattern, String replacement, boolean toLowerCase, boolean toUpperCase) {\n            isDefault = false;\n            this.pattern = pattern == null ? null : Pattern.compile(pattern);\n            this.replacement = replacement;\n            this.toLowerCase = toLowerCase;\n            this.toUpperCase = toUpperCase;\n        }\n\n        String apply(String distinguishedName) {\n            if (isDefault) {\n                return distinguishedName;\n            }\n\n            String result = null;\n            final Matcher m = pattern.matcher(distinguishedName);\n\n            if (m.matches()) {\n                result = distinguishedName.replaceAll(pattern.pattern(), escapeLiteralBackReferences(replacement, m.groupCount()));\n            }\n\n            if (toLowerCase && result != null) {\n                result = result.toLowerCase(Locale.ENGLISH);\n            } else if (toUpperCase && result != null) {\n                result = result.toUpperCase(Locale.ENGLISH);\n            }\n\n            return result;\n        }\n\n        //If we find a back reference that is not valid, then we will treat it as a literal string. For example, if we have 3 capturing\n        //groups and the Replacement Value has the value is \"$1@$4\", then we want to treat the $4 as a literal \"$4\", rather\n        //than attempting to use it as a back reference.\n        //This method was taken from Apache Nifi project : org.apache.nifi.authorization.util.IdentityMappingUtil\n        private String escapeLiteralBackReferences(final String unescaped, final int numCapturingGroups) {\n            if (numCapturingGroups == 0) {\n                return unescaped;\n            }\n\n            String value = unescaped;\n            final Matcher backRefMatcher = BACK_REFERENCE_PATTERN.matcher(value);\n            while (backRefMatcher.find()) {\n                final String backRefNum = backRefMatcher.group(1);\n                if (backRefNum.startsWith(\"0\")) {\n                    continue;\n                }\n                int backRefIndex = Integer.parseInt(backRefNum);\n\n\n                // if we have a replacement value like $123, and we have less than 123 capturing groups, then\n                // we want to truncate the 3 and use capturing group 12; if we have less than 12 capturing groups,\n                // then we want to truncate the 2 and use capturing group 1; if we don't have a capturing group then\n                // we want to truncate the 1 and get 0.\n                while (backRefIndex > numCapturingGroups && backRefIndex >= 10) {\n                    backRefIndex /= 10;\n                }\n\n                if (backRefIndex > numCapturingGroups) {\n                    final StringBuilder sb = new StringBuilder(value.length() + 1);\n                    final int groupStart = backRefMatcher.start(1);\n\n                    sb.append(value, 0, groupStart - 1);\n                    sb.append(\"\\\\\");\n                    sb.append(value.substring(groupStart - 1));\n                    value = sb.toString();\n                }\n            }\n\n            return value;\n        }\n\n        @Override\n        public String toString() {\n            StringBuilder buf = new StringBuilder();\n            if (isDefault) {\n                buf.append(\"DEFAULT\");\n            } else {\n                buf.append(\"RULE:\");\n                if (pattern != null) {\n                    buf.append(pattern);\n                }\n                if (replacement != null) {\n                    buf.append(\"/\");\n                    buf.append(replacement);\n                }\n                if (toLowerCase) {\n                    buf.append(\"/L\");\n                } else if (toUpperCase) {\n                    buf.append(\"/U\");\n                }\n            }\n            return buf.toString();\n        }\n\n    }\n}\n",
        "methodName": null,
        "exampleID": 44,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/security/ssl/SslPrincipalMapper.java",
        "line": 61,
        "sink": "'RULE:.'.",
        "source": "-",
        "sourceLine": 61,
        "qualifier": "This [regular expression](1) that depends on a [user-provided value](2) may run slow on strings starting with 'RULE:' and with many repetitions of 'RULE:.'.\nThis [regular expression](1) that depends on a [user-provided value](3) may run slow on strings starting with 'RULE:' and with many repetitions of 'RULE:.'.\nThis [regular expression](1) that depends on a [user-provided value](4) may run slow on strings starting with 'RULE:' and with many repetitions of 'RULE:.'.\nThis [regular expression](1) that depends on a [user-provided value](5) may run slow on strings starting with 'RULE:' and with many repetitions of 'RULE:.'.\nThis [regular expression](1) that depends on a [user-provided value](6) may run slow on strings starting with 'RULE:' and with many repetitions of 'RULE:.'.\nThis [regular expression](1) that depends on a [user-provided value](7) may run slow on strings starting with 'RULE:' and with many repetitions of 'RULE:.'.\nThis [regular expression](1) that depends on a [user-provided value](8) may run slow on strings starting with 'RULE:' and with many repetitions of 'RULE:.'.\nThis [regular expression](1) that depends on a [user-provided value](9) may run slow on strings starting with 'RULE:' and with many repetitions of 'RULE:.'.",
        "line_number": 61,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 45
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 45
            },
            {
                "line": 251,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 45
            },
            {
                "line": 265,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 45
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1868",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.streams;\n\nimport org.apache.kafka.clients.CommonClientConfigs;\nimport org.apache.kafka.clients.admin.Admin;\nimport org.apache.kafka.clients.admin.AdminClientConfig;\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.common.config.AbstractConfig;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigDef.Importance;\nimport org.apache.kafka.common.config.ConfigDef.Type;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.config.TopicConfig;\nimport org.apache.kafka.common.metrics.Sensor;\nimport org.apache.kafka.common.metrics.Sensor.RecordingLevel;\nimport org.apache.kafka.common.security.auth.SecurityProtocol;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.utils.Utils;\nimport org.apache.kafka.streams.errors.DefaultProductionExceptionHandler;\nimport org.apache.kafka.streams.errors.DeserializationExceptionHandler;\nimport org.apache.kafka.streams.errors.LogAndFailExceptionHandler;\nimport org.apache.kafka.streams.errors.LogAndFailProcessingExceptionHandler;\nimport org.apache.kafka.streams.errors.ProcessingExceptionHandler;\nimport org.apache.kafka.streams.errors.ProductionExceptionHandler;\nimport org.apache.kafka.streams.errors.StreamsException;\nimport org.apache.kafka.streams.internals.StreamsConfigUtils;\nimport org.apache.kafka.streams.internals.UpgradeFromValues;\nimport org.apache.kafka.streams.processor.FailOnInvalidTimestamp;\nimport org.apache.kafka.streams.processor.TimestampExtractor;\nimport org.apache.kafka.streams.processor.assignment.TaskAssignor;\nimport org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\nimport org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor;\nimport org.apache.kafka.streams.processor.internals.assignment.RackAwareTaskAssignor;\nimport org.apache.kafka.streams.state.BuiltInDslStoreSuppliers;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.File;\nimport java.time.Duration;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Properties;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport static org.apache.kafka.common.IsolationLevel.READ_COMMITTED;\nimport static org.apache.kafka.common.config.ConfigDef.ListSize.atMostOfSize;\nimport static org.apache.kafka.common.config.ConfigDef.Range.atLeast;\nimport static org.apache.kafka.common.config.ConfigDef.Range.between;\nimport static org.apache.kafka.common.config.ConfigDef.ValidString.in;\nimport static org.apache.kafka.common.config.ConfigDef.parseType;\n\n/**\n * Configuration for a {@link KafkaStreams} instance.\n * Can also be used to configure the Kafka Streams internal {@link KafkaConsumer}, {@link KafkaProducer} and {@link Admin}.\n * To avoid consumer/producer/admin property conflicts, you should prefix those properties using\n * {@link #consumerPrefix(String)}, {@link #producerPrefix(String)} and {@link #adminClientPrefix(String)}, respectively.\n * <p>\n * Example:\n * <pre>{@code\n * // potentially wrong: sets \"metadata.max.age.ms\" to 1 minute for producer AND consumer\n * Properties streamsProperties = new Properties();\n * streamsProperties.put(ConsumerConfig.METADATA_MAX_AGE_CONFIG, 60000);\n * // or\n * streamsProperties.put(ProducerConfig.METADATA_MAX_AGE_CONFIG, 60000);\n *\n * // suggested:\n * Properties streamsProperties = new Properties();\n * // sets \"metadata.max.age.ms\" to 1 minute for consumer only\n * streamsProperties.put(StreamsConfig.consumerPrefix(ConsumerConfig.METADATA_MAX_AGE_CONFIG), 60000);\n * // sets \"metadata.max.age.ms\" to 1 minute for producer only\n * streamsProperties.put(StreamsConfig.producerPrefix(ProducerConfig.METADATA_MAX_AGE_CONFIG), 60000);\n *\n * StreamsConfig streamsConfig = new StreamsConfig(streamsProperties);\n * }</pre>\n *\n * This instance can also be used to pass in custom configurations to different modules (e.g. passing a special config in your customized serde class).\n * The consumer/producer/admin prefix can also be used to distinguish these custom config values passed to different clients with the same config name.\n * * Example:\n * <pre>{@code\n * Properties streamsProperties = new Properties();\n * // sets \"my.custom.config\" to \"foo\" for consumer only\n * streamsProperties.put(StreamsConfig.consumerPrefix(\"my.custom.config\"), \"foo\");\n * // sets \"my.custom.config\" to \"bar\" for producer only\n * streamsProperties.put(StreamsConfig.producerPrefix(\"my.custom.config\"), \"bar\");\n * // sets \"my.custom.config2\" to \"boom\" for all clients universally\n * streamsProperties.put(\"my.custom.config2\", \"boom\");\n *\n * // as a result, inside producer's serde class configure(..) function,\n * // users can now read both key-value pairs \"my.custom.config\" -> \"foo\"\n * // and \"my.custom.config2\" -> \"boom\" from the config map\n * StreamsConfig streamsConfig = new StreamsConfig(streamsProperties);\n * }</pre>\n *\n *\n * When increasing {@link ProducerConfig#MAX_BLOCK_MS_CONFIG} to be more resilient to non-available brokers you should also\n * increase {@link ConsumerConfig#MAX_POLL_INTERVAL_MS_CONFIG} using the following guidance:\n * <pre>\n *     max.poll.interval.ms &gt; max.block.ms\n * </pre>\n *\n *\n * Kafka Streams requires at least the following properties to be set:\n * <ul>\n *  <li>{@link #APPLICATION_ID_CONFIG \"application.id\"}</li>\n *  <li>{@link #BOOTSTRAP_SERVERS_CONFIG \"bootstrap.servers\"}</li>\n * </ul>\n *\n * By default, Kafka Streams does not allow users to overwrite the following properties (Streams setting shown in parentheses):\n * <ul>\n *   <li>{@link ConsumerConfig#GROUP_ID_CONFIG \"group.id\"} (&lt;application.id&gt;) - Streams client will always use the application ID a consumer group ID</li>\n *   <li>{@link ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG \"enable.auto.commit\"} (false) - Streams client will always disable/turn off auto committing</li>\n *   <li>{@link ConsumerConfig#PARTITION_ASSIGNMENT_STRATEGY_CONFIG \"partition.assignment.strategy\"} (<code>StreamsPartitionAssignor</code>) - Streams client will always use its own partition assignor</li>\n * </ul>\n *\n * If {@link #PROCESSING_GUARANTEE_CONFIG \"processing.guarantee\"} is set to {@link #EXACTLY_ONCE_V2 \"exactly_once_v2\"},\n * {@link #EXACTLY_ONCE \"exactly_once\"} (deprecated), or {@link #EXACTLY_ONCE_BETA \"exactly_once_beta\"} (deprecated), Kafka Streams does not\n * allow users to overwrite the following properties (Streams setting shown in parentheses):\n * <ul>\n *   <li>{@link ConsumerConfig#ISOLATION_LEVEL_CONFIG \"isolation.level\"} (read_committed) - Consumers will always read committed data only</li>\n *   <li>{@link ProducerConfig#ENABLE_IDEMPOTENCE_CONFIG \"enable.idempotence\"} (true) - Producer will always have idempotency enabled</li>\n * </ul>\n *\n * @see KafkaStreams#KafkaStreams(org.apache.kafka.streams.Topology, Properties)\n * @see ConsumerConfig\n * @see ProducerConfig\n */\npublic class StreamsConfig extends AbstractConfig {\n\n    private static final Logger log = LoggerFactory.getLogger(StreamsConfig.class);\n\n    private static final ConfigDef CONFIG;\n\n    private final boolean eosEnabled;\n    private static final long DEFAULT_COMMIT_INTERVAL_MS = 30000L;\n    private static final long EOS_DEFAULT_COMMIT_INTERVAL_MS = 100L;\n    private static final int DEFAULT_TRANSACTION_TIMEOUT = 10000;\n\n    @SuppressWarnings(\"unused\")\n    public static final int DUMMY_THREAD_INDEX = 1;\n    public static final long MAX_TASK_IDLE_MS_DISABLED = -1;\n\n    // We impose these limitations because client tags are encoded into the subscription info,\n    // which is part of the group metadata message that is persisted into the internal topic.\n    public static final int MAX_RACK_AWARE_ASSIGNMENT_TAG_LIST_SIZE = 5;\n    public static final int MAX_RACK_AWARE_ASSIGNMENT_TAG_KEY_LENGTH = 20;\n    public static final int MAX_RACK_AWARE_ASSIGNMENT_TAG_VALUE_LENGTH = 30;\n\n    /**\n     * Prefix used to provide default topic configs to be applied when creating internal topics.\n     * These should be valid properties from {@link org.apache.kafka.common.config.TopicConfig TopicConfig}.\n     * It is recommended to use {@link #topicPrefix(String)}.\n     */\n    // TODO: currently we cannot get the full topic configurations and hence cannot allow topic configs without the prefix,\n    //       this can be lifted once kafka.log.LogConfig is completely deprecated by org.apache.kafka.common.config.TopicConfig\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String TOPIC_PREFIX = \"topic.\";\n\n    /**\n     * Prefix used to isolate {@link KafkaConsumer consumer} configs from other client configs.\n     * It is recommended to use {@link #consumerPrefix(String)} to add this prefix to {@link ConsumerConfig consumer\n     * properties}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String CONSUMER_PREFIX = \"consumer.\";\n\n    /**\n     * Prefix used to override {@link KafkaConsumer consumer} configs for the main consumer client from\n     * the general consumer client configs. The override precedence is the following (from highest to lowest precedence):\n     * 1. main.consumer.[config-name]\n     * 2. consumer.[config-name]\n     * 3. [config-name]\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String MAIN_CONSUMER_PREFIX = \"main.consumer.\";\n\n    /**\n     * Prefix used to override {@link KafkaConsumer consumer} configs for the restore consumer client from\n     * the general consumer client configs. The override precedence is the following (from highest to lowest precedence):\n     * 1. restore.consumer.[config-name]\n     * 2. consumer.[config-name]\n     * 3. [config-name]\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String RESTORE_CONSUMER_PREFIX = \"restore.consumer.\";\n\n    /**\n     * Prefix used to override {@link KafkaConsumer consumer} configs for the global consumer client from\n     * the general consumer client configs. The override precedence is the following (from highest to lowest precedence):\n     * 1. global.consumer.[config-name]\n     * 2. consumer.[config-name]\n     * 3. [config-name]\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String GLOBAL_CONSUMER_PREFIX = \"global.consumer.\";\n\n    /**\n     * Prefix used to isolate {@link KafkaProducer producer} configs from other client configs.\n     * It is recommended to use {@link #producerPrefix(String)} to add this prefix to {@link ProducerConfig producer\n     * properties}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String PRODUCER_PREFIX = \"producer.\";\n\n    /**\n     * Prefix used to isolate {@link Admin admin} configs from other client configs.\n     * It is recommended to use {@link #adminClientPrefix(String)} to add this prefix to {@link AdminClientConfig admin\n     * client properties}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String ADMIN_CLIENT_PREFIX = \"admin.\";\n\n    /**\n     * Prefix used to add arbitrary tags to a Kafka Stream's instance as key-value pairs.\n     * Example:\n     * client.tag.zone=zone1\n     * client.tag.cluster=cluster1\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String CLIENT_TAG_PREFIX = \"client.tag.\";\n\n    /** {@code topology.optimization} */\n    public static final String TOPOLOGY_OPTIMIZATION_CONFIG = \"topology.optimization\";\n    private static final String CONFIG_ERROR_MSG = \"Acceptable values are:\"\n            + \" \\\"+NO_OPTIMIZATION+\\\", \\\"+OPTIMIZE+\\\", \"\n            + \"or a comma separated list of specific optimizations: \"\n            + \"(\\\"+REUSE_KTABLE_SOURCE_TOPICS+\\\", \\\"+MERGE_REPARTITION_TOPICS+\\\" + \"\n            + \"\\\"SINGLE_STORE_SELF_JOIN+\\\").\";\n    private static final String TOPOLOGY_OPTIMIZATION_DOC = \"A configuration telling Kafka \"\n        + \"Streams if it should optimize the topology and what optimizations to apply. \"\n        + CONFIG_ERROR_MSG\n        + \"\\\"NO_OPTIMIZATION\\\" by default.\";\n\n    /**\n     * Config value for parameter {@link #TOPOLOGY_OPTIMIZATION_CONFIG \"topology.optimization\"} for disabling topology optimization\n     */\n    public static final String NO_OPTIMIZATION = \"none\";\n\n    /**\n     * Config value for parameter {@link #TOPOLOGY_OPTIMIZATION_CONFIG \"topology.optimization\"} for enabling topology optimization\n     */\n    public static final String OPTIMIZE = \"all\";\n\n    /**\n     * Config value for parameter {@link #TOPOLOGY_OPTIMIZATION_CONFIG \"topology.optimization\"}\n     * for enabling the specific optimization that reuses source topic as changelog topic\n     * for KTables.\n     */\n    public static final String REUSE_KTABLE_SOURCE_TOPICS = \"reuse.ktable.source.topics\";\n\n    /**\n     * Config value for parameter {@link #TOPOLOGY_OPTIMIZATION_CONFIG \"topology.optimization\"}\n     * for enabling the specific optimization that merges duplicated repartition topics.\n     */\n    public static final String MERGE_REPARTITION_TOPICS = \"merge.repartition.topics\";\n\n    /**\n     * Config value for parameter {@link #TOPOLOGY_OPTIMIZATION_CONFIG \"topology.optimization\"}\n     * for enabling the optimization that optimizes inner stream-stream joins into self-joins when\n     * both arguments are the same stream.\n     */\n    public static final String SINGLE_STORE_SELF_JOIN = \"single.store.self.join\";\n\n    private static final List<String> TOPOLOGY_OPTIMIZATION_CONFIGS = Arrays.asList(\n        OPTIMIZE, NO_OPTIMIZATION, REUSE_KTABLE_SOURCE_TOPICS, MERGE_REPARTITION_TOPICS,\n        SINGLE_STORE_SELF_JOIN);\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 0.10.0.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_0100 = UpgradeFromValues.UPGRADE_FROM_0100.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 0.10.1.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_0101 = UpgradeFromValues.UPGRADE_FROM_0101.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 0.10.2.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_0102 = UpgradeFromValues.UPGRADE_FROM_0102.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 0.11.0.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_0110 = UpgradeFromValues.UPGRADE_FROM_0110.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 1.0.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_10 = UpgradeFromValues.UPGRADE_FROM_10.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 1.1.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_11 = UpgradeFromValues.UPGRADE_FROM_11.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.0.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_20 = UpgradeFromValues.UPGRADE_FROM_20.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.1.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_21 = UpgradeFromValues.UPGRADE_FROM_21.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.2.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_22 = UpgradeFromValues.UPGRADE_FROM_22.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.3.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_23 = UpgradeFromValues.UPGRADE_FROM_23.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.4.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_24 = UpgradeFromValues.UPGRADE_FROM_24.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.5.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_25 = UpgradeFromValues.UPGRADE_FROM_25.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.6.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_26 = UpgradeFromValues.UPGRADE_FROM_26.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.7.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_27 = UpgradeFromValues.UPGRADE_FROM_27.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 2.8.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_28 = UpgradeFromValues.UPGRADE_FROM_28.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 3.0.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_30 = UpgradeFromValues.UPGRADE_FROM_30.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 3.1.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_31 = UpgradeFromValues.UPGRADE_FROM_31.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 3.2.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_32 = UpgradeFromValues.UPGRADE_FROM_32.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 3.3.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_33 = UpgradeFromValues.UPGRADE_FROM_33.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 3.4.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_34 = UpgradeFromValues.UPGRADE_FROM_34.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 3.5.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_35 = UpgradeFromValues.UPGRADE_FROM_35.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 3.6.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_36 = UpgradeFromValues.UPGRADE_FROM_36.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 3.7.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_37 = UpgradeFromValues.UPGRADE_FROM_37.toString();\n\n    /**\n     * Config value for parameter {@link #UPGRADE_FROM_CONFIG \"upgrade.from\"} for upgrading an application from version {@code 3.8.x}.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_38 = UpgradeFromValues.UPGRADE_FROM_38.toString();\n\n\n    /**\n     * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG \"processing.guarantee\"} for at-least-once processing guarantees.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String AT_LEAST_ONCE = \"at_least_once\";\n\n    /**\n     * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG \"processing.guarantee\"} for exactly-once processing guarantees.\n     * <p>\n     * Enabling exactly-once processing semantics requires broker version 0.11.0 or higher.\n     * If you enable this feature Kafka Streams will use more resources (like broker connections)\n     * compared to {@link #AT_LEAST_ONCE \"at_least_once\"} and {@link #EXACTLY_ONCE_V2 \"exactly_once_v2\"}.\n     *\n     * @deprecated since 3.0.0, will be removed in 4.0. Use {@link #EXACTLY_ONCE_V2 \"exactly_once_v2\"} instead.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @Deprecated\n    public static final String EXACTLY_ONCE = \"exactly_once\";\n\n    /**\n     * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG \"processing.guarantee\"} for exactly-once processing guarantees.\n     * <p>\n     * Enabling exactly-once (beta) requires broker version 2.5 or higher.\n     * If you enable this feature Kafka Streams will use fewer resources (like broker connections)\n     * compared to the {@link #EXACTLY_ONCE} (deprecated) case.\n     *\n     * @deprecated since 3.0.0, will be removed in 4.0. Use {@link #EXACTLY_ONCE_V2 \"exactly_once_v2\"} instead.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @Deprecated\n    public static final String EXACTLY_ONCE_BETA = \"exactly_once_beta\";\n\n    /**\n     * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG \"processing.guarantee\"} for exactly-once processing guarantees.\n     * <p>\n     * Enabling exactly-once-v2 requires broker version 2.5 or higher.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String EXACTLY_ONCE_V2 = \"exactly_once_v2\";\n\n    /**\n     * Config value for parameter {@link #BUILT_IN_METRICS_VERSION_CONFIG \"built.in.metrics.version\"} for the latest built-in metrics version.\n     */\n    public static final String METRICS_LATEST = \"latest\";\n\n    /** {@code acceptable.recovery.lag} */\n    public static final String ACCEPTABLE_RECOVERY_LAG_CONFIG = \"acceptable.recovery.lag\";\n    private static final String ACCEPTABLE_RECOVERY_LAG_DOC = \"The maximum acceptable lag (number of offsets to catch up) for a client to be considered caught-up enough\" +\n                                                                  \" to receive an active task assignment. Upon assignment, it will still restore the rest of the changelog\" +\n                                                                  \" before processing. To avoid a pause in processing during rebalances, this config\" +\n                                                                  \" should correspond to a recovery time of well under a minute for a given workload. Must be at least 0.\";\n\n    /** {@code application.id} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String APPLICATION_ID_CONFIG = \"application.id\";\n    private static final String APPLICATION_ID_DOC = \"An identifier for the stream processing application. Must be unique within the Kafka cluster. It is used as 1) the default client-id prefix, 2) the group-id for membership management, 3) the changelog topic prefix.\";\n\n    /**{@code application.server} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String APPLICATION_SERVER_CONFIG = \"application.server\";\n    private static final String APPLICATION_SERVER_DOC = \"A host:port pair pointing to a user-defined endpoint that can be used for state store discovery and interactive queries on this KafkaStreams instance.\";\n\n    /** {@code bootstrap.servers} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String BOOTSTRAP_SERVERS_CONFIG = CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG;\n\n    /** {@code buffered.records.per.partition} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String BUFFERED_RECORDS_PER_PARTITION_CONFIG = \"buffered.records.per.partition\";\n    public static final String BUFFERED_RECORDS_PER_PARTITION_DOC = \"Maximum number of records to buffer per partition.\";\n\n    /** {@code built.in.metrics.version} */\n    public static final String BUILT_IN_METRICS_VERSION_CONFIG = \"built.in.metrics.version\";\n    private static final String BUILT_IN_METRICS_VERSION_DOC = \"Version of the built-in metrics to use.\";\n\n    /** {@code cache.max.bytes.buffering}\n     * @deprecated since 3.4.0 Use {@link #STATESTORE_CACHE_MAX_BYTES_CONFIG \"statestore.cache.max.bytes\"} instead. */\n    @SuppressWarnings(\"WeakerAccess\")\n    @Deprecated\n    public static final String CACHE_MAX_BYTES_BUFFERING_CONFIG = \"cache.max.bytes.buffering\";\n    public static final String CACHE_MAX_BYTES_BUFFERING_DOC = \"Maximum number of memory bytes to be used for buffering across all threads\";\n\n    /** {@code statestore.cache.max.bytes} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String STATESTORE_CACHE_MAX_BYTES_CONFIG = \"statestore.cache.max.bytes\";\n    public static final String STATESTORE_CACHE_MAX_BYTES_DOC = \"Maximum number of memory bytes to be used for statestore cache across all threads\";\n\n    /** {@code client.id} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String CLIENT_ID_CONFIG = CommonClientConfigs.CLIENT_ID_CONFIG;\n    private static final String CLIENT_ID_DOC = \"An ID prefix string used for the client IDs of internal (main, restore, and global) consumers , producers, and admin clients\" +\n        \" with pattern <code>&lt;client.id&gt;-[Global]StreamThread[-&lt;threadSequenceNumber&gt;]-&lt;consumer|producer|restore-consumer|global-consumer&gt;</code>.\";\n\n    /** {@code enable.metrics.push} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static  final String ENABLE_METRICS_PUSH_CONFIG = CommonClientConfigs.ENABLE_METRICS_PUSH_CONFIG;\n    public static final String ENABLE_METRICS_PUSH_DOC = \"Whether to enable pushing of internal client metrics for (main, restore, and global) consumers, producers, and admin clients.\" + \n        \" The cluster must have a client metrics subscription which corresponds to a client.\";\n\n    /** {@code commit.interval.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String COMMIT_INTERVAL_MS_CONFIG = \"commit.interval.ms\";\n    private static final String COMMIT_INTERVAL_MS_DOC = \"The frequency in milliseconds with which to commit processing progress.\" +\n        \" For at-least-once processing, committing means to save the position (ie, offsets) of the processor.\" +\n        \" For exactly-once processing, it means to commit the transaction which includes to save the position and to make the committed data in the output topic visible to consumers with isolation level read_committed.\" +\n        \" (Note, if <code>processing.guarantee</code> is set to <code>\" + EXACTLY_ONCE_V2 + \"</code>, <code>\" + EXACTLY_ONCE + \"</code>,the default value is <code>\" + EOS_DEFAULT_COMMIT_INTERVAL_MS + \"</code>,\" +\n        \" otherwise the default value is <code>\" + DEFAULT_COMMIT_INTERVAL_MS + \"</code>.\";\n\n    /** {@code repartition.purge.interval.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String REPARTITION_PURGE_INTERVAL_MS_CONFIG = \"repartition.purge.interval.ms\";\n    private static final String REPARTITION_PURGE_INTERVAL_MS_DOC = \"The frequency in milliseconds with which to delete fully consumed records from repartition topics.\" +\n            \" Purging will occur after at least this value since the last purge, but may be delayed until later.\" +\n            \" (Note, unlike <code>commit.interval.ms</code>, the default for this value remains unchanged when <code>processing.guarantee</code> is set to <code>\" + EXACTLY_ONCE_V2 + \"</code>).\";\n\n    /** {@code connections.max.idle.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = CommonClientConfigs.CONNECTIONS_MAX_IDLE_MS_CONFIG;\n\n    /** {@code default.deserialization.exception.handler} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG = \"default.deserialization.exception.handler\";\n    public static final String DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_DOC = \"Exception handling class that implements the <code>org.apache.kafka.streams.errors.DeserializationExceptionHandler</code> interface.\";\n\n    /** {@code default.production.exception.handler} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG = \"default.production.exception.handler\";\n    private static final String DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_DOC = \"Exception handling class that implements the <code>org.apache.kafka.streams.errors.ProductionExceptionHandler</code> interface.\";\n\n    /** {@code processing.exception.handler} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String PROCESSING_EXCEPTION_HANDLER_CLASS_CONFIG = \"processing.exception.handler\";\n    public static final String PROCESSING_EXCEPTION_HANDLER_CLASS_DOC = \"Exception handling class that implements the <code>org.apache.kafka.streams.errors.ProcessingExceptionHandler</code> interface.\";\n\n    /** {@code default.dsl.store} */\n    @Deprecated\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String DEFAULT_DSL_STORE_CONFIG = \"default.dsl.store\";\n    @Deprecated\n    public static final String DEFAULT_DSL_STORE_DOC = \"The default state store type used by DSL operators.\";\n\n    @Deprecated\n    public static final String ROCKS_DB = \"rocksDB\";\n    @Deprecated\n    public static final String IN_MEMORY = \"in_memory\";\n    @Deprecated\n    public static final String DEFAULT_DSL_STORE = ROCKS_DB;\n\n    /** {@code dsl.store.suppliers.class } */\n    public static final String DSL_STORE_SUPPLIERS_CLASS_CONFIG = \"dsl.store.suppliers.class\";\n    static final String DSL_STORE_SUPPLIERS_CLASS_DOC = \"Defines which store implementations to plug in to DSL operators. Must implement the <code>org.apache.kafka.streams.state.DslStoreSuppliers</code> interface.\";\n    static final Class<?> DSL_STORE_SUPPLIERS_CLASS_DEFAULT = BuiltInDslStoreSuppliers.RocksDBDslStoreSuppliers.class;\n\n    /** {@code default.windowed.key.serde.inner}\n     * @deprecated since 3.0.0  Use {@link #WINDOWED_INNER_CLASS_SERDE \"windowed.inner.class.serde\"} instead. */\n    @SuppressWarnings(\"WeakerAccess\")\n    @Deprecated\n    public static final String DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS = \"default.windowed.key.serde.inner\";\n    private static final String DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS_DOC = \"Default serializer / deserializer for the inner class of a windowed key. Must implement the \" +\n        \"<code>org.apache.kafka.common.serialization.Serde</code> interface.\";\n\n    /** {@code default.windowed.value.serde.inner}\n     * @deprecated since 3.0.0  Use {@link #WINDOWED_INNER_CLASS_SERDE \"windowed.inner.class.serde\"} instead. */\n    @SuppressWarnings(\"WeakerAccess\")\n    @Deprecated\n    public static final String DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS = \"default.windowed.value.serde.inner\";\n    private static final String DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS_DOC = \"Default serializer / deserializer for the inner class of a windowed value. Must implement the \" +\n        \"<code>org.apache.kafka.common.serialization.Serde</code> interface.\";\n\n    public static final String WINDOWED_INNER_CLASS_SERDE = \"windowed.inner.class.serde\";\n    private static final String WINDOWED_INNER_CLASS_SERDE_DOC = \" Default serializer / deserializer for the inner class of a windowed record. Must implement the \" +\n        \"<code>org.apache.kafka.common.serialization.Serde</code> interface. Note that setting this config in KafkaStreams application would result \" +\n        \"in an error as it is meant to be used only from Plain consumer client.\";\n        \n    /** {@code default key.serde} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String DEFAULT_KEY_SERDE_CLASS_CONFIG = \"default.key.serde\";\n    private static final String DEFAULT_KEY_SERDE_CLASS_DOC = \"Default serializer / deserializer class for key that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface. \"\n            + \"Note when windowed serde class is used, one needs to set the inner serde class that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface via '\"\n            + DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS + \"' or '\" + DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS + \"' as well\";\n\n    /** {@code default value.serde} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String DEFAULT_VALUE_SERDE_CLASS_CONFIG = \"default.value.serde\";\n    private static final String DEFAULT_VALUE_SERDE_CLASS_DOC = \"Default serializer / deserializer class for value that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface. \"\n            + \"Note when windowed serde class is used, one needs to set the inner serde class that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface via '\"\n            + DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS + \"' or '\" + DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS + \"' as well\";\n\n    /** {@code default.timestamp.extractor} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG = \"default.timestamp.extractor\";\n    public static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_DOC = \"Default timestamp extractor class that implements the <code>org.apache.kafka.streams.processor.TimestampExtractor</code> interface.\";\n\n    /** {@code max.task.idle.ms} */\n    public static final String MAX_TASK_IDLE_MS_CONFIG = \"max.task.idle.ms\";\n    public static final String MAX_TASK_IDLE_MS_DOC = \"This config controls whether joins and merges\"\n        + \" may produce out-of-order results.\"\n        + \" The config value is the maximum amount of time in milliseconds a stream task will stay idle\"\n        + \" when it is fully caught up on some (but not all) input partitions\"\n        + \" to wait for producers to send additional records and avoid potential\"\n        + \" out-of-order record processing across multiple input streams.\"\n        + \" The default (zero) does not wait for producers to send more records,\"\n        + \" but it does wait to fetch data that is already present on the brokers.\"\n        + \" This default means that for records that are already present on the brokers,\"\n        + \" Streams will process them in timestamp order.\"\n        + \" Set to -1 to disable idling entirely and process any locally available data,\"\n        + \" even though doing so may produce out-of-order processing.\";\n\n    /** {@code max.warmup.replicas} */\n    public static final String MAX_WARMUP_REPLICAS_CONFIG = \"max.warmup.replicas\";\n    private static final String MAX_WARMUP_REPLICAS_DOC = \"The maximum number of warmup replicas (extra standbys beyond the configured num.standbys) that can be assigned at once for the purpose of keeping \" +\n                                                              \" the task available on one instance while it is warming up on another instance it has been reassigned to. Used to throttle how much extra broker \" +\n                                                              \" traffic and cluster state can be used for high availability. Must be at least 1.\" +\n                                                              \"Note that one warmup replica corresponds to one Stream Task. Furthermore, note that each warmup replica can only be promoted to an active task \" +\n                                                              \"during a rebalance (normally during a so-called probing rebalance, which occur at a frequency specified by the `probing.rebalance.interval.ms` config). This means \" +\n                                                              \"that the maximum rate at which active tasks can be migrated from one Kafka Streams Instance to another instance can be determined by \" +\n                                                              \"(`max.warmup.replicas` / `probing.rebalance.interval.ms`).\";\n\n    /** {@code metadata.max.age.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String METADATA_MAX_AGE_CONFIG = CommonClientConfigs.METADATA_MAX_AGE_CONFIG;\n\n    /** {@code metrics.num.samples} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String METRICS_NUM_SAMPLES_CONFIG = CommonClientConfigs.METRICS_NUM_SAMPLES_CONFIG;\n\n    /** {@code metrics.record.level} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String METRICS_RECORDING_LEVEL_CONFIG = CommonClientConfigs.METRICS_RECORDING_LEVEL_CONFIG;\n\n    /** {@code metric.reporters} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String METRIC_REPORTER_CLASSES_CONFIG = CommonClientConfigs.METRIC_REPORTER_CLASSES_CONFIG;\n\n    /** {@code metrics.sample.window.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String METRICS_SAMPLE_WINDOW_MS_CONFIG = CommonClientConfigs.METRICS_SAMPLE_WINDOW_MS_CONFIG;\n\n    /** {@code auto.include.jmx.reporter}\n     * @deprecated and will be removed in 4.0.0 */\n    @Deprecated\n    public static final String AUTO_INCLUDE_JMX_REPORTER_CONFIG = CommonClientConfigs.AUTO_INCLUDE_JMX_REPORTER_CONFIG;\n\n    /** {@code num.standby.replicas} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String NUM_STANDBY_REPLICAS_CONFIG = \"num.standby.replicas\";\n    private static final String NUM_STANDBY_REPLICAS_DOC = \"The number of standby replicas for each task.\";\n\n    /** {@code num.stream.threads} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String NUM_STREAM_THREADS_CONFIG = \"num.stream.threads\";\n    private static final String NUM_STREAM_THREADS_DOC = \"The number of threads to execute stream processing.\";\n\n    /** {@code poll.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String POLL_MS_CONFIG = \"poll.ms\";\n    private static final String POLL_MS_DOC = \"The amount of time in milliseconds to block waiting for input.\";\n\n    /** {@code probing.rebalance.interval.ms} */\n    public static final String PROBING_REBALANCE_INTERVAL_MS_CONFIG = \"probing.rebalance.interval.ms\";\n    private static final String PROBING_REBALANCE_INTERVAL_MS_DOC = \"The maximum time in milliseconds to wait before triggering a rebalance to probe for warmup replicas that have finished warming up and are ready to become active.\" +\n        \" Probing rebalances will continue to be triggered until the assignment is balanced. Must be at least 1 minute.\";\n\n    /** {@code processing.guarantee} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String PROCESSING_GUARANTEE_CONFIG = \"processing.guarantee\";\n    private static final String PROCESSING_GUARANTEE_DOC = \"The processing guarantee that should be used. \" +\n        \"Possible values are <code>\" + AT_LEAST_ONCE + \"</code> (default) \" +\n        \"and <code>\" + EXACTLY_ONCE_V2 + \"</code> (requires brokers version 2.5 or higher). \" +\n        \"Deprecated options are <code>\" + EXACTLY_ONCE + \"</code> (requires brokers version 0.11.0 or higher) \" +\n        \"and <code>\" + EXACTLY_ONCE_BETA + \"</code> (requires brokers version 2.5 or higher). \" +\n        \"Note that exactly-once processing requires a cluster of at least three brokers by default what is the \" +\n        \"recommended setting for production; for development you can change this, by adjusting broker setting \" +\n        \"<code>transaction.state.log.replication.factor</code> and <code>transaction.state.log.min.isr</code>.\";\n\n    /** {@code receive.buffer.bytes} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String RECEIVE_BUFFER_CONFIG = CommonClientConfigs.RECEIVE_BUFFER_CONFIG;\n\n    /** {@code rack.aware.assignment.tags} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String RACK_AWARE_ASSIGNMENT_TAGS_CONFIG = \"rack.aware.assignment.tags\";\n    private static final String RACK_AWARE_ASSIGNMENT_TAGS_DOC = \"List of client tag keys used to distribute standby replicas across Kafka Streams instances.\" +\n                                                                 \" When configured, Kafka Streams will make a best-effort to distribute\" +\n                                                                 \" the standby tasks over each client tag dimension.\";\n\n    /** {@code reconnect.backoff.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String RECONNECT_BACKOFF_MS_CONFIG = CommonClientConfigs.RECONNECT_BACKOFF_MS_CONFIG;\n\n    /** {@code reconnect.backoff.max} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String RECONNECT_BACKOFF_MAX_MS_CONFIG = CommonClientConfigs.RECONNECT_BACKOFF_MAX_MS_CONFIG;\n\n    /** {@code replication.factor} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String REPLICATION_FACTOR_CONFIG = \"replication.factor\";\n    private static final String REPLICATION_FACTOR_DOC = \"The replication factor for change log topics and repartition topics created by the stream processing application.\"\n        + \" The default of <code>-1</code> (meaning: use broker default replication factor) requires broker version 2.4 or newer\";\n\n    /** {@code request.timeout.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String REQUEST_TIMEOUT_MS_CONFIG = CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG;\n\n    /**\n     * {@code retries}\n     * <p>\n     * This config is ignored by Kafka Streams. Note, that the internal clients (producer, admin) are still impacted by this config.\n     *\n     * @deprecated since 2.7\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @Deprecated\n    public static final String RETRIES_CONFIG = CommonClientConfigs.RETRIES_CONFIG;\n\n    /** {@code retry.backoff.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String RETRY_BACKOFF_MS_CONFIG = CommonClientConfigs.RETRY_BACKOFF_MS_CONFIG;\n\n    /** {@code rocksdb.config.setter} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String ROCKSDB_CONFIG_SETTER_CLASS_CONFIG = \"rocksdb.config.setter\";\n    private static final String ROCKSDB_CONFIG_SETTER_CLASS_DOC = \"A Rocks DB config setter class or class name that implements the <code>org.apache.kafka.streams.state.RocksDBConfigSetter</code> interface\";\n\n    /** {@code security.protocol} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String SECURITY_PROTOCOL_CONFIG = CommonClientConfigs.SECURITY_PROTOCOL_CONFIG;\n\n    /** {@code send.buffer.bytes} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String SEND_BUFFER_CONFIG = CommonClientConfigs.SEND_BUFFER_CONFIG;\n\n    /** {@code state.cleanup.delay} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String STATE_CLEANUP_DELAY_MS_CONFIG = \"state.cleanup.delay.ms\";\n    private static final String STATE_CLEANUP_DELAY_MS_DOC = \"The amount of time in milliseconds to wait before deleting state when a partition has migrated. Only state directories that have not been modified for at least <code>state.cleanup.delay.ms</code> will be removed\";\n\n    /** {@code state.dir} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String STATE_DIR_CONFIG = \"state.dir\";\n    private static final String STATE_DIR_DOC = \"Directory location for state store. This path must be unique for each streams instance sharing the same underlying filesystem. Note that if not configured, then the default location will be different in each environment as it is computed using System.getProperty(\\\"java.io.tmpdir\\\")\";\n\n    /** {@code task.timeout.ms} */\n    public static final String TASK_TIMEOUT_MS_CONFIG = \"task.timeout.ms\";\n    public static final String TASK_TIMEOUT_MS_DOC = \"The maximum amount of time in milliseconds a task might stall due to internal errors and retries until an error is raised. \" +\n        \"For a timeout of 0ms, a task would raise an error for the first internal error. \" +\n        \"For any timeout larger than 0ms, a task will retry at least once before an error is raised.\";\n\n\n    /** {@code window.size.ms} */\n    public static final String WINDOW_SIZE_MS_CONFIG = \"window.size.ms\";\n    private static final String WINDOW_SIZE_MS_DOC = \"Sets window size for the deserializer in order to calculate window end times.\";\n\n    /** {@code upgrade.from} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String UPGRADE_FROM_CONFIG = \"upgrade.from\";\n    private static final String UPGRADE_FROM_DOC = \"Allows upgrading in a backward compatible way. \" +\n        \"This is needed when upgrading from [0.10.0, 1.1] to 2.0+, or when upgrading from [2.0, 2.3] to 2.4+. \" +\n        \"When upgrading from 3.3 to a newer version it is not required to specify this config. Default is `null`. \" +\n        \"Accepted values are \\\"\" + UPGRADE_FROM_0100 + \"\\\", \\\"\" + UPGRADE_FROM_0101 + \"\\\", \\\"\" +\n        UPGRADE_FROM_0102 + \"\\\", \\\"\" + UPGRADE_FROM_0110 + \"\\\", \\\"\" + UPGRADE_FROM_10 + \"\\\", \\\"\" +\n        UPGRADE_FROM_11 + \"\\\", \\\"\" + UPGRADE_FROM_20 + \"\\\", \\\"\" + UPGRADE_FROM_21 + \"\\\", \\\"\" +\n        UPGRADE_FROM_22 + \"\\\", \\\"\" + UPGRADE_FROM_23 + \"\\\", \\\"\" + UPGRADE_FROM_24 + \"\\\", \\\"\" +\n        UPGRADE_FROM_25 + \"\\\", \\\"\" + UPGRADE_FROM_26 + \"\\\", \\\"\" + UPGRADE_FROM_27 + \"\\\", \\\"\" +\n        UPGRADE_FROM_28 + \"\\\", \\\"\" + UPGRADE_FROM_30 + \"\\\", \\\"\" + UPGRADE_FROM_31 + \"\\\", \\\"\" +\n        UPGRADE_FROM_32 + \"\\\", \\\"\" + UPGRADE_FROM_33 + \"\\\", \\\"\" + UPGRADE_FROM_34 + \"\\\", \\\"\" +\n        UPGRADE_FROM_35 + \"\\\", \\\"\" + UPGRADE_FROM_36 + \"\\\", \\\"\" + UPGRADE_FROM_37 + \"\\\", \\\"\" +\n        UPGRADE_FROM_38 + \"(for upgrading from the corresponding old version).\";\n\n    /** {@code windowstore.changelog.additional.retention.ms} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG = \"windowstore.changelog.additional.retention.ms\";\n    private static final String WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_DOC = \"Added to a windows maintainMs to ensure data is not deleted from the log prematurely. Allows for clock drift. Default is 1 day\";\n\n    /** {@code default.client.supplier} */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String DEFAULT_CLIENT_SUPPLIER_CONFIG = \"default.client.supplier\";\n    public static final String DEFAULT_CLIENT_SUPPLIER_DOC = \"Client supplier class that implements the <code>org.apache.kafka.streams.KafkaClientSupplier</code> interface.\";\n\n    public static final String RACK_AWARE_ASSIGNMENT_STRATEGY_NONE = \"none\";\n    public static final String RACK_AWARE_ASSIGNMENT_STRATEGY_MIN_TRAFFIC = \"min_traffic\";\n    public static final String RACK_AWARE_ASSIGNMENT_STRATEGY_BALANCE_SUBTOPOLOGY = \"balance_subtopology\";\n\n    /** {@code } rack.aware.assignment.strategy */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String RACK_AWARE_ASSIGNMENT_STRATEGY_CONFIG = \"rack.aware.assignment.strategy\";\n    public static final String RACK_AWARE_ASSIGNMENT_STRATEGY_DOC = \"The strategy we use for rack aware assignment. Rack aware assignment will take <code>client.rack</code> and <code>racks</code> of <code>TopicPartition</code> into account when assigning\"\n        + \" tasks to minimize cross rack traffic. Valid settings are : <code>\" + RACK_AWARE_ASSIGNMENT_STRATEGY_NONE + \"</code> (default), which will disable rack aware assignment; <code>\" + RACK_AWARE_ASSIGNMENT_STRATEGY_MIN_TRAFFIC\n        + \"</code>, which will compute minimum cross rack traffic assignment; <code>\" + RACK_AWARE_ASSIGNMENT_STRATEGY_BALANCE_SUBTOPOLOGY + \"</code>, which will compute minimum cross rack traffic and try to balance the tasks of same subtopolgies across different clients\";\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String RACK_AWARE_ASSIGNMENT_TRAFFIC_COST_CONFIG = \"rack.aware.assignment.traffic_cost\";\n    public static final String RACK_AWARE_ASSIGNMENT_TRAFFIC_COST_DOC = \"Cost associated with cross rack traffic. This config and <code>rack.aware.assignment.non_overlap_cost</code> controls whether the \"\n        + \"optimization algorithm favors minimizing cross rack traffic or minimize the movement of tasks in existing assignment. If set a larger value <code>\" + RackAwareTaskAssignor.class.getName() + \"</code> will \"\n        + \"optimize for minimizing cross rack traffic. The default value is null which means it will use default traffic cost values in different assignors.\";\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String RACK_AWARE_ASSIGNMENT_NON_OVERLAP_COST_CONFIG = \"rack.aware.assignment.non_overlap_cost\";\n    public static final String RACK_AWARE_ASSIGNMENT_NON_OVERLAP_COST_DOC = \"Cost associated with moving tasks from existing assignment. This config and <code>\" + RACK_AWARE_ASSIGNMENT_TRAFFIC_COST_CONFIG + \"</code> controls whether the \"\n        + \"optimization algorithm favors minimizing cross rack traffic or minimize the movement of tasks in existing assignment. If set a larger value <code>\" + RackAwareTaskAssignor.class.getName() + \"</code> will \"\n        + \"optimize to maintain the existing assignment. The default value is null which means it will use default non_overlap cost values in different assignors.\";\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final String TASK_ASSIGNOR_CLASS_CONFIG = \"task.assignor.class\";\n    private static final String TASK_ASSIGNOR_CLASS_DOC = \"A task assignor class or class name implementing the <code>\" +\n                                                          TaskAssignor.class.getName() + \"</code> interface. Defaults to the <code>HighAvailabilityTaskAssignor</code> class.\";\n\n    public static final String LOG_SUMMARY_INTERVAL_MS_CONFIG = \"log.summary.interval.ms\";\n    private static final String LOG_SUMMARY_INTERVAL_MS_DOC = \"This configuration controls the output interval for summary information.\\n\" +\n            \"If greater or equal to 0, the summary log will be output according to the set time interval;\\n\" +\n            \"If less than 0, summary output is disabled.\";\n    /**\n     * {@code topology.optimization}\n     * @deprecated since 2.7; use {@link #TOPOLOGY_OPTIMIZATION_CONFIG} instead\n     */\n    @Deprecated\n    public static final String TOPOLOGY_OPTIMIZATION = TOPOLOGY_OPTIMIZATION_CONFIG;\n\n\n    private static final String[] NON_CONFIGURABLE_CONSUMER_DEFAULT_CONFIGS =\n        new String[] {ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG};\n    private static final String[] NON_CONFIGURABLE_CONSUMER_EOS_CONFIGS =\n        new String[] {ConsumerConfig.ISOLATION_LEVEL_CONFIG};\n    private static final String[] NON_CONFIGURABLE_PRODUCER_EOS_CONFIGS =\n        new String[] {\n            ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG,\n            ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION,\n            ProducerConfig.TRANSACTIONAL_ID_CONFIG\n        };\n\n    static {\n        CONFIG = new ConfigDef()\n\n            // HIGH\n\n            .define(APPLICATION_ID_CONFIG, // required with no default value\n                    Type.STRING,\n                    Importance.HIGH,\n                    APPLICATION_ID_DOC)\n            .define(BOOTSTRAP_SERVERS_CONFIG, // required with no default value\n                    Type.LIST,\n                    Importance.HIGH,\n                    CommonClientConfigs.BOOTSTRAP_SERVERS_DOC)\n            .define(NUM_STANDBY_REPLICAS_CONFIG,\n                    Type.INT,\n                    0,\n                    Importance.HIGH,\n                    NUM_STANDBY_REPLICAS_DOC)\n            .define(STATE_DIR_CONFIG,\n                    Type.STRING,\n                    System.getProperty(\"java.io.tmpdir\") + File.separator + \"kafka-streams\",\n                    Importance.HIGH,\n                    STATE_DIR_DOC,\n                    \"${java.io.tmpdir}\")\n\n            // MEDIUM\n\n            .define(ACCEPTABLE_RECOVERY_LAG_CONFIG,\n                    Type.LONG,\n                    10_000L,\n                    atLeast(0),\n                    Importance.MEDIUM,\n                    ACCEPTABLE_RECOVERY_LAG_DOC)\n            .define(CACHE_MAX_BYTES_BUFFERING_CONFIG,\n                    Type.LONG,\n                    10 * 1024 * 1024L,\n                    atLeast(0),\n                    Importance.MEDIUM,\n                    CACHE_MAX_BYTES_BUFFERING_DOC)\n            .define(STATESTORE_CACHE_MAX_BYTES_CONFIG,\n                    Type.LONG,\n                    10 * 1024 * 1024L,\n                    atLeast(0),\n                    Importance.MEDIUM,\n                    STATESTORE_CACHE_MAX_BYTES_DOC)\n            .define(CLIENT_ID_CONFIG,\n                    Type.STRING,\n                    \"\",\n                    Importance.MEDIUM,\n                    CLIENT_ID_DOC,\n                    \"<code>&lt;application.id&gt-&lt;random-UUID&gt</code>\")\n            .define(DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG,\n                    Type.CLASS,\n                    LogAndFailExceptionHandler.class.getName(),\n                    Importance.MEDIUM,\n                    DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_DOC)\n            .define(DEFAULT_KEY_SERDE_CLASS_CONFIG,\n                    Type.CLASS,\n                    null,\n                    Importance.MEDIUM,\n                    DEFAULT_KEY_SERDE_CLASS_DOC)\n            .define(CommonClientConfigs.DEFAULT_LIST_KEY_SERDE_INNER_CLASS,\n                    Type.CLASS,\n                    null,\n                    Importance.MEDIUM,\n                    CommonClientConfigs.DEFAULT_LIST_KEY_SERDE_INNER_CLASS_DOC)\n            .define(CommonClientConfigs.DEFAULT_LIST_VALUE_SERDE_INNER_CLASS,\n                    Type.CLASS,\n                    null,\n                    Importance.MEDIUM,\n                    CommonClientConfigs.DEFAULT_LIST_VALUE_SERDE_INNER_CLASS_DOC)\n            .define(CommonClientConfigs.DEFAULT_LIST_KEY_SERDE_TYPE_CLASS,\n                    Type.CLASS,\n                    null,\n                    Importance.MEDIUM,\n                    CommonClientConfigs.DEFAULT_LIST_KEY_SERDE_TYPE_CLASS_DOC)\n            .define(CommonClientConfigs.DEFAULT_LIST_VALUE_SERDE_TYPE_CLASS,\n                    Type.CLASS,\n                    null,\n                    Importance.MEDIUM,\n                    CommonClientConfigs.DEFAULT_LIST_VALUE_SERDE_TYPE_CLASS_DOC)\n            .define(DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG,\n                    Type.CLASS,\n                    DefaultProductionExceptionHandler.class.getName(),\n                    Importance.MEDIUM,\n                    DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_DOC)\n            .define(PROCESSING_EXCEPTION_HANDLER_CLASS_CONFIG,\n                    Type.CLASS,\n                    LogAndFailProcessingExceptionHandler.class.getName(),\n                    Importance.MEDIUM,\n                    PROCESSING_EXCEPTION_HANDLER_CLASS_DOC)\n            .define(DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG,\n                    Type.CLASS,\n                    FailOnInvalidTimestamp.class.getName(),\n                    Importance.MEDIUM,\n                    DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_DOC)\n            .define(DEFAULT_VALUE_SERDE_CLASS_CONFIG,\n                    Type.CLASS,\n                    null,\n                    Importance.MEDIUM,\n                    DEFAULT_VALUE_SERDE_CLASS_DOC)\n            .define(MAX_TASK_IDLE_MS_CONFIG,\n                    Type.LONG,\n                    0L,\n                    Importance.MEDIUM,\n                    MAX_TASK_IDLE_MS_DOC)\n            .define(MAX_WARMUP_REPLICAS_CONFIG,\n                    Type.INT,\n                    2,\n                    atLeast(1),\n                    Importance.MEDIUM,\n                    MAX_WARMUP_REPLICAS_DOC)\n            .define(NUM_STREAM_THREADS_CONFIG,\n                    Type.INT,\n                    1,\n                    Importance.MEDIUM,\n                    NUM_STREAM_THREADS_DOC)\n            .define(PROCESSING_GUARANTEE_CONFIG,\n                    Type.STRING,\n                    AT_LEAST_ONCE,\n                    in(AT_LEAST_ONCE, EXACTLY_ONCE, EXACTLY_ONCE_BETA, EXACTLY_ONCE_V2),\n                    Importance.MEDIUM,\n                    PROCESSING_GUARANTEE_DOC)\n            .define(RACK_AWARE_ASSIGNMENT_NON_OVERLAP_COST_CONFIG,\n                    Type.INT,\n                    null,\n                    Importance.MEDIUM,\n                    RACK_AWARE_ASSIGNMENT_NON_OVERLAP_COST_DOC)\n            .define(RACK_AWARE_ASSIGNMENT_STRATEGY_CONFIG,\n                    Type.STRING,\n                    RACK_AWARE_ASSIGNMENT_STRATEGY_NONE,\n                    in(RACK_AWARE_ASSIGNMENT_STRATEGY_NONE, RACK_AWARE_ASSIGNMENT_STRATEGY_MIN_TRAFFIC, RACK_AWARE_ASSIGNMENT_STRATEGY_BALANCE_SUBTOPOLOGY),\n                    Importance.MEDIUM,\n                    RACK_AWARE_ASSIGNMENT_STRATEGY_DOC)\n            .define(RACK_AWARE_ASSIGNMENT_TAGS_CONFIG,\n                    Type.LIST,\n                    Collections.emptyList(),\n                    atMostOfSize(MAX_RACK_AWARE_ASSIGNMENT_TAG_LIST_SIZE),\n                    Importance.MEDIUM,\n                    RACK_AWARE_ASSIGNMENT_TAGS_DOC)\n            .define(RACK_AWARE_ASSIGNMENT_TRAFFIC_COST_CONFIG,\n                    Type.INT,\n                    null,\n                    Importance.MEDIUM,\n                    RACK_AWARE_ASSIGNMENT_TRAFFIC_COST_DOC)\n            .define(TASK_ASSIGNOR_CLASS_CONFIG,\n                    Type.STRING,\n                    null,\n                    Importance.MEDIUM,\n                    TASK_ASSIGNOR_CLASS_DOC)\n            .define(REPLICATION_FACTOR_CONFIG,\n                    Type.INT,\n                    -1,\n                    Importance.MEDIUM,\n                    REPLICATION_FACTOR_DOC)\n            .define(SECURITY_PROTOCOL_CONFIG,\n                    Type.STRING,\n                    CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL,\n                    ConfigDef.CaseInsensitiveValidString.in(Utils.enumOptions(SecurityProtocol.class)),\n                    Importance.MEDIUM,\n                    CommonClientConfigs.SECURITY_PROTOCOL_DOC)\n            .define(TASK_TIMEOUT_MS_CONFIG,\n                    Type.LONG,\n                    Duration.ofMinutes(5L).toMillis(),\n                    atLeast(0L),\n                    Importance.MEDIUM,\n                    TASK_TIMEOUT_MS_DOC)\n            .define(TOPOLOGY_OPTIMIZATION_CONFIG,\n                    Type.STRING,\n                    NO_OPTIMIZATION,\n                    (name, value) -> verifyTopologyOptimizationConfigs((String) value),\n                    Importance.MEDIUM,\n                    TOPOLOGY_OPTIMIZATION_DOC)\n\n            // LOW\n\n            .define(APPLICATION_SERVER_CONFIG,\n                    Type.STRING,\n                    \"\",\n                    Importance.LOW,\n                    APPLICATION_SERVER_DOC)\n            .define(BUFFERED_RECORDS_PER_PARTITION_CONFIG,\n                    Type.INT,\n                    1000,\n                    Importance.LOW,\n                    BUFFERED_RECORDS_PER_PARTITION_DOC)\n            .define(BUILT_IN_METRICS_VERSION_CONFIG,\n                    Type.STRING,\n                    METRICS_LATEST,\n                    in(\n                        METRICS_LATEST\n                    ),\n                    Importance.LOW,\n                    BUILT_IN_METRICS_VERSION_DOC)\n            .define(COMMIT_INTERVAL_MS_CONFIG,\n                    Type.LONG,\n                    DEFAULT_COMMIT_INTERVAL_MS,\n                    atLeast(0),\n                    Importance.LOW,\n                    COMMIT_INTERVAL_MS_DOC)\n            .define(ENABLE_METRICS_PUSH_CONFIG,\n                    Type.BOOLEAN,\n                    true,\n                    Importance.LOW,\n                    ENABLE_METRICS_PUSH_DOC)\n            .define(REPARTITION_PURGE_INTERVAL_MS_CONFIG,\n                    Type.LONG,\n                    DEFAULT_COMMIT_INTERVAL_MS,\n                    atLeast(0),\n                    Importance.LOW,\n                    REPARTITION_PURGE_INTERVAL_MS_DOC)\n            .define(CONNECTIONS_MAX_IDLE_MS_CONFIG,\n                    Type.LONG,\n                    9 * 60 * 1000L,\n                    Importance.LOW,\n                    CommonClientConfigs.CONNECTIONS_MAX_IDLE_MS_DOC)\n            .define(DEFAULT_DSL_STORE_CONFIG,\n                    Type.STRING,\n                    DEFAULT_DSL_STORE,\n                    in(ROCKS_DB, IN_MEMORY),\n                    Importance.LOW,\n                    DEFAULT_DSL_STORE_DOC)\n            .define(DSL_STORE_SUPPLIERS_CLASS_CONFIG,\n                    Type.CLASS,\n                    DSL_STORE_SUPPLIERS_CLASS_DEFAULT,\n                    Importance.LOW,\n                    DSL_STORE_SUPPLIERS_CLASS_DOC)\n            .define(DEFAULT_CLIENT_SUPPLIER_CONFIG,\n                    Type.CLASS,\n                    DefaultKafkaClientSupplier.class.getName(),\n                    Importance.LOW,\n                    DEFAULT_CLIENT_SUPPLIER_DOC)\n            .define(METADATA_MAX_AGE_CONFIG,\n                    Type.LONG,\n                    5 * 60 * 1000L,\n                    atLeast(0),\n                    Importance.LOW,\n                    CommonClientConfigs.METADATA_MAX_AGE_DOC)\n            .define(METRICS_NUM_SAMPLES_CONFIG,\n                    Type.INT,\n                    2,\n                    atLeast(1),\n                    Importance.LOW,\n                    CommonClientConfigs.METRICS_NUM_SAMPLES_DOC)\n            .define(METRIC_REPORTER_CLASSES_CONFIG,\n                    Type.LIST,\n                    \"\",\n                    Importance.LOW,\n                    CommonClientConfigs.METRIC_REPORTER_CLASSES_DOC)\n            .define(METRICS_RECORDING_LEVEL_CONFIG,\n                    Type.STRING,\n                    Sensor.RecordingLevel.INFO.toString(),\n                    in(Sensor.RecordingLevel.INFO.toString(), Sensor.RecordingLevel.DEBUG.toString(), RecordingLevel.TRACE.toString()),\n                    Importance.LOW,\n                    CommonClientConfigs.METRICS_RECORDING_LEVEL_DOC)\n            .define(METRICS_SAMPLE_WINDOW_MS_CONFIG,\n                    Type.LONG,\n                    30000L,\n                    atLeast(0),\n                    Importance.LOW,\n                    CommonClientConfigs.METRICS_SAMPLE_WINDOW_MS_DOC)\n            .define(AUTO_INCLUDE_JMX_REPORTER_CONFIG,\n                    Type.BOOLEAN,\n                    true,\n                    Importance.LOW,\n                    CommonClientConfigs.AUTO_INCLUDE_JMX_REPORTER_DOC)\n            .define(POLL_MS_CONFIG,\n                    Type.LONG,\n                    100L,\n                    Importance.LOW,\n                    POLL_MS_DOC)\n            .define(PROBING_REBALANCE_INTERVAL_MS_CONFIG,\n                    Type.LONG,\n                    10 * 60 * 1000L,\n                    atLeast(60 * 1000L),\n                    Importance.LOW,\n                    PROBING_REBALANCE_INTERVAL_MS_DOC)\n            .define(RECEIVE_BUFFER_CONFIG,\n                    Type.INT,\n                    32 * 1024,\n                    atLeast(CommonClientConfigs.RECEIVE_BUFFER_LOWER_BOUND),\n                    Importance.LOW,\n                    CommonClientConfigs.RECEIVE_BUFFER_DOC)\n            .define(RECONNECT_BACKOFF_MS_CONFIG,\n                    Type.LONG,\n                    50L,\n                    atLeast(0L),\n                    Importance.LOW,\n                    CommonClientConfigs.RECONNECT_BACKOFF_MS_DOC)\n            .define(RECONNECT_BACKOFF_MAX_MS_CONFIG,\n                    Type.LONG,\n                    1000L,\n                    atLeast(0L),\n                    Importance.LOW,\n                    CommonClientConfigs.RECONNECT_BACKOFF_MAX_MS_DOC)\n            .define(RETRIES_CONFIG,\n                    Type.INT,\n                    0,\n                    between(0, Integer.MAX_VALUE),\n                    Importance.LOW,\n                    CommonClientConfigs.RETRIES_DOC)\n            .define(RETRY_BACKOFF_MS_CONFIG,\n                    Type.LONG,\n                    100L,\n                    atLeast(0L),\n                    Importance.LOW,\n                    CommonClientConfigs.RETRY_BACKOFF_MS_DOC)\n            .define(REQUEST_TIMEOUT_MS_CONFIG,\n                    Type.INT,\n                    40 * 1000,\n                    atLeast(0),\n                    Importance.LOW,\n                    CommonClientConfigs.REQUEST_TIMEOUT_MS_DOC)\n            .define(ROCKSDB_CONFIG_SETTER_CLASS_CONFIG,\n                    Type.CLASS,\n                    null,\n                    Importance.LOW,\n                    ROCKSDB_CONFIG_SETTER_CLASS_DOC)\n            .define(SEND_BUFFER_CONFIG,\n                    Type.INT,\n                    128 * 1024,\n                    atLeast(CommonClientConfigs.SEND_BUFFER_LOWER_BOUND),\n                    Importance.LOW,\n                    CommonClientConfigs.SEND_BUFFER_DOC)\n            .define(STATE_CLEANUP_DELAY_MS_CONFIG,\n                    Type.LONG,\n                    10 * 60 * 1000L,\n                    Importance.LOW,\n                    STATE_CLEANUP_DELAY_MS_DOC)\n            .define(UPGRADE_FROM_CONFIG,\n                    Type.STRING,\n                    null,\n                    in(Stream.concat(\n                            Stream.of((String) null),\n                            Arrays.stream(UpgradeFromValues.values()).map(UpgradeFromValues::toString)\n                        ).toArray(String[]::new)\n                    ),\n                    Importance.LOW,\n                    UPGRADE_FROM_DOC)\n            .define(WINDOWED_INNER_CLASS_SERDE,\n                Type.STRING,\n                null,\n                Importance.LOW,\n                WINDOWED_INNER_CLASS_SERDE_DOC)\n            .define(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,\n                    Type.LONG,\n                    24 * 60 * 60 * 1000L,\n                    Importance.LOW,\n                    WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_DOC)\n            .define(WINDOW_SIZE_MS_CONFIG,\n                    Type.LONG,\n                    null,\n                    Importance.LOW,\n                    WINDOW_SIZE_MS_DOC)\n            .define(LOG_SUMMARY_INTERVAL_MS_CONFIG,\n                    Type.LONG,\n                    2 * 60 * 1000L,\n                    Importance.LOW,\n                    LOG_SUMMARY_INTERVAL_MS_DOC);\n    }\n\n    // this is the list of configs for underlying clients\n    // that streams prefer different default values\n    private static final Map<String, Object> PRODUCER_DEFAULT_OVERRIDES;\n    static {\n        final Map<String, Object> tempProducerDefaultOverrides = new HashMap<>();\n        tempProducerDefaultOverrides.put(ProducerConfig.LINGER_MS_CONFIG, \"100\");\n        PRODUCER_DEFAULT_OVERRIDES = Collections.unmodifiableMap(tempProducerDefaultOverrides);\n    }\n\n    private static final Map<String, Object> PRODUCER_EOS_OVERRIDES;\n    static {\n        final Map<String, Object> tempProducerDefaultOverrides = new HashMap<>(PRODUCER_DEFAULT_OVERRIDES);\n        tempProducerDefaultOverrides.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        tempProducerDefaultOverrides.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n        // Reduce the transaction timeout for quicker pending offset expiration on broker side.\n        tempProducerDefaultOverrides.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, DEFAULT_TRANSACTION_TIMEOUT);\n\n        PRODUCER_EOS_OVERRIDES = Collections.unmodifiableMap(tempProducerDefaultOverrides);\n    }\n\n    private static final Map<String, Object> CONSUMER_DEFAULT_OVERRIDES;\n    static {\n        final Map<String, Object> tempConsumerDefaultOverrides = new HashMap<>();\n        tempConsumerDefaultOverrides.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"1000\");\n        tempConsumerDefaultOverrides.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        tempConsumerDefaultOverrides.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");\n        tempConsumerDefaultOverrides.put(\"internal.leave.group.on.close\", false);\n        CONSUMER_DEFAULT_OVERRIDES = Collections.unmodifiableMap(tempConsumerDefaultOverrides);\n    }\n\n    private static final Map<String, Object> CONSUMER_EOS_OVERRIDES;\n    static {\n        final Map<String, Object> tempConsumerDefaultOverrides = new HashMap<>(CONSUMER_DEFAULT_OVERRIDES);\n        tempConsumerDefaultOverrides.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, READ_COMMITTED.toString());\n        CONSUMER_EOS_OVERRIDES = Collections.unmodifiableMap(tempConsumerDefaultOverrides);\n    }\n\n    public static class InternalConfig {\n        // This is settable in the main Streams config, but it's a private API for now\n        public static final String INTERNAL_TASK_ASSIGNOR_CLASS = \"internal.task.assignor.class\";\n\n        // These are not settable in the main Streams config; they are set by the StreamThread to pass internal\n        // state into the assignor.\n        public static final String REFERENCE_CONTAINER_PARTITION_ASSIGNOR = \"__reference.container.instance__\";\n\n        // This is settable in the main Streams config, but it's a private API for testing\n        public static final String ASSIGNMENT_LISTENER = \"__assignment.listener__\";\n\n        // Private API used to control the emit latency for left/outer join results (https://issues.apache.org/jira/browse/KAFKA-10847)\n        public static final String EMIT_INTERVAL_MS_KSTREAMS_OUTER_JOIN_SPURIOUS_RESULTS_FIX = \"__emit.interval.ms.kstreams.outer.join.spurious.results.fix__\";\n\n        // Private API used to control the emit latency for windowed aggregation results for ON_WINDOW_CLOSE emit strategy\n        public static final String EMIT_INTERVAL_MS_KSTREAMS_WINDOWED_AGGREGATION = \"__emit.interval.ms.kstreams.windowed.aggregation__\";\n\n        // Private API used to control the usage of consistency offset vectors\n        public static final String IQ_CONSISTENCY_OFFSET_VECTOR_ENABLED = \"__iq.consistency.offset\"\n            + \".vector.enabled__\";\n\n        // Private API used to control the prefix of the auto created topics\n        public static final String TOPIC_PREFIX_ALTERNATIVE = \"__internal.override.topic.prefix__\";\n\n        // Private API to enable the state updater (i.e. state updating on a dedicated thread)\n        public static final String STATE_UPDATER_ENABLED = \"__state.updater.enabled__\";\n\n        public static boolean getStateUpdaterEnabled(final Map<String, Object> configs) {\n            return InternalConfig.getBoolean(configs, InternalConfig.STATE_UPDATER_ENABLED, true);\n        }\n        \n        // Private API to enable processing threads (i.e. polling is decoupled from processing)\n        public static final String PROCESSING_THREADS_ENABLED = \"__processing.threads.enabled__\";\n\n        public static boolean getProcessingThreadsEnabled(final Map<String, Object> configs) {\n            return InternalConfig.getBoolean(configs, InternalConfig.PROCESSING_THREADS_ENABLED, false);\n        }\n\n        public static boolean getBoolean(final Map<String, Object> configs, final String key, final boolean defaultValue) {\n            final Object value = configs.getOrDefault(key, defaultValue);\n            if (value instanceof Boolean) {\n                return (boolean) value;\n            } else if (value instanceof String) {\n                return Boolean.parseBoolean((String) value);\n            } else {\n                log.warn(\"Invalid value (\" + value + \") on internal configuration '\" + key + \"'. Please specify a true/false value.\");\n                return defaultValue;\n            }\n        }\n\n        public static long getLong(final Map<String, Object> configs, final String key, final long defaultValue) {\n            final Object value = configs.getOrDefault(key, defaultValue);\n            if (value instanceof Number) {\n                return ((Number) value).longValue();\n            } else if (value instanceof String) {\n                return Long.parseLong((String) value);\n            } else {\n                log.warn(\"Invalid value (\" + value + \") on internal configuration '\" + key + \"'. Please specify a numeric value.\");\n                return defaultValue;\n            }\n        }\n\n        public static String getString(final Map<String, Object> configs, final String key, final String defaultValue) {\n            final Object value = configs.getOrDefault(key, defaultValue);\n            if (value instanceof String) {\n                return (String) value;\n            } else {\n                log.warn(\"Invalid value (\" + value + \") on internal configuration '\" + key + \"'. Please specify a String value.\");\n                return defaultValue;\n            }\n        }\n    }\n\n    /**\n     * Prefix a property with {@link #CONSUMER_PREFIX}. This is used to isolate {@link ConsumerConfig consumer configs}\n     * from other client configs.\n     *\n     * @param consumerProp the consumer property to be masked\n     * @return {@link #CONSUMER_PREFIX} + {@code consumerProp}\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static String consumerPrefix(final String consumerProp) {\n        return CONSUMER_PREFIX + consumerProp;\n    }\n\n    /**\n     * Prefix a property with {@link #MAIN_CONSUMER_PREFIX}. This is used to isolate {@link ConsumerConfig main consumer configs}\n     * from other client configs.\n     *\n     * @param consumerProp the consumer property to be masked\n     * @return {@link #MAIN_CONSUMER_PREFIX} + {@code consumerProp}\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static String mainConsumerPrefix(final String consumerProp) {\n        return MAIN_CONSUMER_PREFIX + consumerProp;\n    }\n\n    /**\n     * Prefix a property with {@link #RESTORE_CONSUMER_PREFIX}. This is used to isolate {@link ConsumerConfig restore consumer configs}\n     * from other client configs.\n     *\n     * @param consumerProp the consumer property to be masked\n     * @return {@link #RESTORE_CONSUMER_PREFIX} + {@code consumerProp}\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static String restoreConsumerPrefix(final String consumerProp) {\n        return RESTORE_CONSUMER_PREFIX + consumerProp;\n    }\n\n    /**\n     * Prefix a client tag key with {@link #CLIENT_TAG_PREFIX}.\n     *\n     * @param clientTagKey client tag key\n     * @return {@link #CLIENT_TAG_PREFIX} + {@code clientTagKey}\n     */\n    public static String clientTagPrefix(final String clientTagKey) {\n        return CLIENT_TAG_PREFIX + clientTagKey;\n    }\n\n    /**\n     * Prefix a property with {@link #GLOBAL_CONSUMER_PREFIX}. This is used to isolate {@link ConsumerConfig global consumer configs}\n     * from other client configs.\n     *\n     * @param consumerProp the consumer property to be masked\n     * @return {@link #GLOBAL_CONSUMER_PREFIX} + {@code consumerProp}\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static String globalConsumerPrefix(final String consumerProp) {\n        return GLOBAL_CONSUMER_PREFIX + consumerProp;\n    }\n\n    /**\n     * Prefix a property with {@link #PRODUCER_PREFIX}. This is used to isolate {@link ProducerConfig producer configs}\n     * from other client configs.\n     *\n     * @param producerProp the producer property to be masked\n     * @return PRODUCER_PREFIX + {@code producerProp}\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static String producerPrefix(final String producerProp) {\n        return PRODUCER_PREFIX + producerProp;\n    }\n\n    /**\n     * Prefix a property with {@link #ADMIN_CLIENT_PREFIX}. This is used to isolate {@link AdminClientConfig admin configs}\n     * from other client configs.\n     *\n     * @param adminClientProp the admin client property to be masked\n     * @return ADMIN_CLIENT_PREFIX + {@code adminClientProp}\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static String adminClientPrefix(final String adminClientProp) {\n        return ADMIN_CLIENT_PREFIX + adminClientProp;\n    }\n\n    /**\n     * Prefix a property with {@link #TOPIC_PREFIX}\n     * used to provide default topic configs to be applied when creating internal topics.\n     *\n     * @param topicProp the topic property to be masked\n     * @return TOPIC_PREFIX + {@code topicProp}\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static String topicPrefix(final String topicProp) {\n        return TOPIC_PREFIX + topicProp;\n    }\n\n    /**\n     * Return a copy of the config definition.\n     *\n     * @return a copy of the config definition\n     */\n    @SuppressWarnings(\"unused\")\n    public static ConfigDef configDef() {\n        return new ConfigDef(CONFIG);\n    }\n\n    /**\n     * Create a new {@code StreamsConfig} using the given properties.\n     *\n     * @param props properties that specify Kafka Streams and internal consumer/producer configuration\n     */\n    public StreamsConfig(final Map<?, ?> props) {\n        this(props, true);\n    }\n\n    @SuppressWarnings(\"this-escape\")\n    protected StreamsConfig(final Map<?, ?> props,\n                            final boolean doLog) {\n        super(CONFIG, props, doLog);\n        eosEnabled = StreamsConfigUtils.eosEnabled(this);\n\n        final String processingModeConfig = getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG);\n        if (processingModeConfig.equals(EXACTLY_ONCE)) {\n            log.warn(\"Configuration parameter `{}` is deprecated and will be removed in the 4.0.0 release. \" +\n                         \"Please use `{}` instead. Note that this requires broker version 2.5+ so you should prepare \"\n                         + \"to upgrade your brokers if necessary.\", EXACTLY_ONCE, EXACTLY_ONCE_V2);\n        }\n        if (processingModeConfig.equals(EXACTLY_ONCE_BETA)) {\n            log.warn(\"Configuration parameter `{}` is deprecated and will be removed in the 4.0.0 release. \" +\n                         \"Please use `{}` instead.\", EXACTLY_ONCE_BETA, EXACTLY_ONCE_V2);\n        }\n\n        if (props.containsKey(RETRIES_CONFIG)) {\n            log.warn(\"Configuration parameter `{}` is deprecated and will be removed in the 4.0.0 release.\", RETRIES_CONFIG);\n        }\n\n        if (eosEnabled) {\n            verifyEOSTransactionTimeoutCompatibility();\n        }\n        verifyTopologyOptimizationConfigs(getString(TOPOLOGY_OPTIMIZATION_CONFIG));\n    }\n\n    private void verifyEOSTransactionTimeoutCompatibility() {\n        final long commitInterval = getLong(COMMIT_INTERVAL_MS_CONFIG);\n        final String transactionTimeoutConfigKey = producerPrefix(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG);\n        final int transactionTimeout =\n                originals().containsKey(transactionTimeoutConfigKey) ?\n                    (int) Objects.requireNonNull(\n                        parseType(transactionTimeoutConfigKey, originals().get(transactionTimeoutConfigKey), Type.INT),\n                        \"Could not parse config `\" + COMMIT_INTERVAL_MS_CONFIG + \"` because it's set to `null`\") :\n                    DEFAULT_TRANSACTION_TIMEOUT;\n\n        if (transactionTimeout < commitInterval) {\n            throw new IllegalArgumentException(String.format(\"Transaction timeout %d was set lower than \" +\n                \"streams commit interval %d. This will cause ongoing transaction always timeout due to inactivity \" +\n                \"caused by long commit interval. Consider reconfiguring commit interval to match \" +\n                \"transaction timeout by tuning 'commit.interval.ms' config, or increase the transaction timeout to match \" +\n                \"commit interval by tuning `producer.transaction.timeout.ms` config.\",\n                transactionTimeout, commitInterval));\n        }\n    }\n\n    @Override\n    protected Map<String, Object> postProcessParsedConfig(final Map<String, Object> parsedValues) {\n        final Map<String, Object> configUpdates =\n            CommonClientConfigs.postProcessReconnectBackoffConfigs(this, parsedValues);\n\n        if (StreamsConfigUtils.eosEnabled(this) && !originals().containsKey(COMMIT_INTERVAL_MS_CONFIG)) {\n            log.debug(\"Using {} default value of {} as exactly once is enabled.\",\n                    COMMIT_INTERVAL_MS_CONFIG, EOS_DEFAULT_COMMIT_INTERVAL_MS);\n            configUpdates.put(COMMIT_INTERVAL_MS_CONFIG, EOS_DEFAULT_COMMIT_INTERVAL_MS);\n        }\n\n        validateRackAwarenessConfiguration();\n\n        return configUpdates;\n    }\n\n    private void validateRackAwarenessConfiguration() {\n        final List<String> rackAwareAssignmentTags = getList(RACK_AWARE_ASSIGNMENT_TAGS_CONFIG);\n        final Map<String, String> clientTags = getClientTags();\n\n        if (clientTags.size() > MAX_RACK_AWARE_ASSIGNMENT_TAG_LIST_SIZE) {\n            throw new ConfigException(\"At most \" + MAX_RACK_AWARE_ASSIGNMENT_TAG_LIST_SIZE + \" client tags \" +\n                                      \"can be specified using \" + CLIENT_TAG_PREFIX + \" prefix.\");\n        }\n\n        for (final String rackAwareAssignmentTag : rackAwareAssignmentTags) {\n            if (!clientTags.containsKey(rackAwareAssignmentTag)) {\n                throw new ConfigException(RACK_AWARE_ASSIGNMENT_TAGS_CONFIG,\n                                          rackAwareAssignmentTags,\n                                          \"Contains invalid value [\" + rackAwareAssignmentTag + \"] \" +\n                                          \"which doesn't have corresponding tag set via [\" + CLIENT_TAG_PREFIX + \"] prefix.\");\n            }\n        }\n\n        clientTags.forEach((tagKey, tagValue) -> {\n            if (tagKey.length() > MAX_RACK_AWARE_ASSIGNMENT_TAG_KEY_LENGTH) {\n                throw new ConfigException(CLIENT_TAG_PREFIX,\n                                          tagKey,\n                                          \"Tag key exceeds maximum length of \" + MAX_RACK_AWARE_ASSIGNMENT_TAG_KEY_LENGTH + \".\");\n            }\n            if (tagValue.length() > MAX_RACK_AWARE_ASSIGNMENT_TAG_VALUE_LENGTH) {\n                throw new ConfigException(CLIENT_TAG_PREFIX,\n                                          tagValue,\n                                          \"Tag value exceeds maximum length of \" + MAX_RACK_AWARE_ASSIGNMENT_TAG_VALUE_LENGTH + \".\");\n            }\n        });\n    }\n\n    private Map<String, Object> getCommonConsumerConfigs() {\n        final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(CONSUMER_PREFIX, ConsumerConfig.configNames());\n\n        checkIfUnexpectedUserSpecifiedConsumerConfig(clientProvidedProps, NON_CONFIGURABLE_CONSUMER_DEFAULT_CONFIGS);\n        checkIfUnexpectedUserSpecifiedConsumerConfig(clientProvidedProps, NON_CONFIGURABLE_CONSUMER_EOS_CONFIGS);\n\n        final Map<String, Object> consumerProps = new HashMap<>(eosEnabled ? CONSUMER_EOS_OVERRIDES : CONSUMER_DEFAULT_OVERRIDES);\n        if (StreamsConfigUtils.processingMode(this) == StreamsConfigUtils.ProcessingMode.EXACTLY_ONCE_V2) {\n            consumerProps.put(\"internal.throw.on.fetch.stable.offset.unsupported\", true);\n        }\n        consumerProps.putAll(getClientCustomProps());\n        consumerProps.putAll(clientProvidedProps);\n\n        // bootstrap.servers should be from StreamsConfig\n        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, originals().get(BOOTSTRAP_SERVERS_CONFIG));\n\n        return consumerProps;\n    }\n\n    private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Object> clientProvidedProps,\n                                                              final String[] nonConfigurableConfigs) {\n        // Streams does not allow users to configure certain consumer/producer configurations, for example,\n        // enable.auto.commit. In cases where user tries to override such non-configurable\n        // consumer/producer configurations, log a warning and remove the user defined value from the Map.\n        // Thus the default values for these consumer/producer configurations that are suitable for\n        // Streams will be used instead.\n\n        final String nonConfigurableConfigMessage = \"Unexpected user-specified %s config: %s found. %sUser setting (%s) will be ignored and the Streams default setting (%s) will be used \";\n        final String eosMessage = PROCESSING_GUARANTEE_CONFIG + \" is set to \" + getString(PROCESSING_GUARANTEE_CONFIG) + \". Hence, \";\n\n        for (final String config: nonConfigurableConfigs) {\n            if (clientProvidedProps.containsKey(config)) {\n\n                if (CONSUMER_DEFAULT_OVERRIDES.containsKey(config)) {\n                    if (!clientProvidedProps.get(config).equals(CONSUMER_DEFAULT_OVERRIDES.get(config))) {\n                        log.warn(String.format(nonConfigurableConfigMessage, \"consumer\", config, \"\", clientProvidedProps.get(config),  CONSUMER_DEFAULT_OVERRIDES.get(config)));\n                        clientProvidedProps.remove(config);\n                    }\n                } else if (eosEnabled) {\n                    if (CONSUMER_EOS_OVERRIDES.containsKey(config)) {\n                        if (!clientProvidedProps.get(config).equals(CONSUMER_EOS_OVERRIDES.get(config))) {\n                            log.warn(String.format(nonConfigurableConfigMessage,\n                                    \"consumer\", config, eosMessage, clientProvidedProps.get(config), CONSUMER_EOS_OVERRIDES.get(config)));\n                            clientProvidedProps.remove(config);\n                        }\n                    } else if (PRODUCER_EOS_OVERRIDES.containsKey(config)) {\n                        if (!clientProvidedProps.get(config).equals(PRODUCER_EOS_OVERRIDES.get(config))) {\n                            log.warn(String.format(nonConfigurableConfigMessage,\n                                    \"producer\", config, eosMessage, clientProvidedProps.get(config), PRODUCER_EOS_OVERRIDES.get(config)));\n                            clientProvidedProps.remove(config);\n                        }\n                    } else if (ProducerConfig.TRANSACTIONAL_ID_CONFIG.equals(config)) {\n                        log.warn(String.format(nonConfigurableConfigMessage,\n                            \"producer\", config, eosMessage, clientProvidedProps.get(config), \"<appId>-<generatedSuffix>\"));\n                        clientProvidedProps.remove(config);\n                    }\n                }\n            }\n        }\n\n        if (eosEnabled) {\n            verifyMaxInFlightRequestPerConnection(clientProvidedProps.get(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION));\n        }\n    }\n\n    private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightRequests) {\n        if (maxInFlightRequests != null) {\n            final int maxInFlightRequestsAsInteger;\n            if (maxInFlightRequests instanceof Integer) {\n                maxInFlightRequestsAsInteger = (Integer) maxInFlightRequests;\n            } else if (maxInFlightRequests instanceof String) {\n                try {\n                    maxInFlightRequestsAsInteger = Integer.parseInt(((String) maxInFlightRequests).trim());\n                } catch (final NumberFormatException e) {\n                    throw new ConfigException(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequests, \"String value could not be parsed as 32-bit integer\");\n                }\n            } else {\n                throw new ConfigException(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequests, \"Expected value to be a 32-bit integer, but it was a \" + maxInFlightRequests.getClass().getName());\n            }\n\n            if (maxInFlightRequestsAsInteger > 5) {\n                throw new ConfigException(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsAsInteger, \"Can't exceed 5 when exactly-once processing is enabled\");\n            }\n        }\n    }\n\n    /**\n     * Get the configs to the {@link KafkaConsumer main consumer}.\n     * Properties using the prefix {@link #MAIN_CONSUMER_PREFIX} will be used in favor over\n     * the properties prefixed with {@link #CONSUMER_PREFIX} and the non-prefixed versions\n     * (read the override precedence ordering in {@link #MAIN_CONSUMER_PREFIX})\n     * except in the case of {@link ConsumerConfig#BOOTSTRAP_SERVERS_CONFIG} where we always use the non-prefixed\n     * version as we only support reading/writing from/to the same Kafka Cluster.\n     * If not specified by {@link #MAIN_CONSUMER_PREFIX}, main consumer will share the general consumer configs\n     * prefixed by {@link #CONSUMER_PREFIX}.\n     *\n     * @param groupId      consumer groupId\n     * @param clientId     clientId\n     * @param threadIdx    stream thread index\n     * @return Map of the consumer configuration.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public Map<String, Object> getMainConsumerConfigs(final String groupId, final String clientId, final int threadIdx) {\n        final Map<String, Object> consumerProps = getCommonConsumerConfigs();\n\n        // Get main consumer override configs\n        final Map<String, Object> mainConsumerProps = originalsWithPrefix(MAIN_CONSUMER_PREFIX);\n        consumerProps.putAll(mainConsumerProps);\n\n        // this is a hack to work around StreamsConfig constructor inside StreamsPartitionAssignor to avoid casting\n        consumerProps.put(APPLICATION_ID_CONFIG, groupId);\n\n        // add group id, client id with stream client id prefix, and group instance id\n        consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n        consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId);\n        final String groupInstanceId = (String) consumerProps.get(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG);\n        // Suffix each thread consumer with thread.id to enforce uniqueness of group.instance.id.\n        if (groupInstanceId != null) {\n            consumerProps.put(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG, groupInstanceId + \"-\" + threadIdx);\n        }\n\n        // add configs required for stream partition assignor\n        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));\n        consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n        consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n        consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));\n        consumerProps.put(MAX_WARMUP_REPLICAS_CONFIG, getInt(MAX_WARMUP_REPLICAS_CONFIG));\n        consumerProps.put(PROBING_REBALANCE_INTERVAL_MS_CONFIG, getLong(PROBING_REBALANCE_INTERVAL_MS_CONFIG));\n        consumerProps.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, StreamsPartitionAssignor.class.getName());\n        consumerProps.put(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG, getLong(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG));\n        consumerProps.put(RACK_AWARE_ASSIGNMENT_NON_OVERLAP_COST_CONFIG, getInt(RACK_AWARE_ASSIGNMENT_NON_OVERLAP_COST_CONFIG));\n        consumerProps.put(RACK_AWARE_ASSIGNMENT_STRATEGY_CONFIG, getString(RACK_AWARE_ASSIGNMENT_STRATEGY_CONFIG));\n        consumerProps.put(RACK_AWARE_ASSIGNMENT_TAGS_CONFIG, getList(RACK_AWARE_ASSIGNMENT_TAGS_CONFIG));\n        consumerProps.put(RACK_AWARE_ASSIGNMENT_TRAFFIC_COST_CONFIG, getInt(RACK_AWARE_ASSIGNMENT_TRAFFIC_COST_CONFIG));\n        consumerProps.put(TASK_ASSIGNOR_CLASS_CONFIG, getString(TASK_ASSIGNOR_CLASS_CONFIG));\n\n        // disable auto topic creation\n        consumerProps.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, \"false\");\n\n        // verify that producer batch config is no larger than segment size, then add topic configs required for creating topics\n        final Map<String, Object> topicProps = originalsWithPrefix(TOPIC_PREFIX, false);\n        final Map<String, Object> producerProps = getClientPropsWithPrefix(PRODUCER_PREFIX, ProducerConfig.configNames());\n\n        if (topicProps.containsKey(topicPrefix(TopicConfig.SEGMENT_BYTES_CONFIG)) &&\n            producerProps.containsKey(ProducerConfig.BATCH_SIZE_CONFIG)) {\n            final int segmentSize = Integer.parseInt(topicProps.get(topicPrefix(TopicConfig.SEGMENT_BYTES_CONFIG)).toString());\n            final int batchSize = Integer.parseInt(producerProps.get(ProducerConfig.BATCH_SIZE_CONFIG).toString());\n\n            if (segmentSize < batchSize) {\n                throw new IllegalArgumentException(String.format(\"Specified topic segment size %d is is smaller than the configured producer batch size %d, this will cause produced batch not able to be appended to the topic\",\n                        segmentSize,\n                        batchSize));\n            }\n        }\n\n        consumerProps.putAll(topicProps);\n\n        return consumerProps;\n    }\n\n    /**\n     * Get the configs for the {@link KafkaConsumer restore-consumer}.\n     * Properties using the prefix {@link #RESTORE_CONSUMER_PREFIX} will be used in favor over\n     * the properties prefixed with {@link #CONSUMER_PREFIX} and the non-prefixed versions\n     * (read the override precedence ordering in {@link #RESTORE_CONSUMER_PREFIX})\n     * except in the case of {@link ConsumerConfig#BOOTSTRAP_SERVERS_CONFIG} where we always use the non-prefixed\n     * version as we only support reading/writing from/to the same Kafka Cluster.\n     * If not specified by {@link #RESTORE_CONSUMER_PREFIX}, restore consumer will share the general consumer configs\n     * prefixed by {@link #CONSUMER_PREFIX}.\n     *\n     * @param clientId clientId\n     * @return Map of the restore consumer configuration.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public Map<String, Object> getRestoreConsumerConfigs(final String clientId) {\n        final Map<String, Object> baseConsumerProps = getCommonConsumerConfigs();\n\n        // Get restore consumer override configs\n        final Map<String, Object> restoreConsumerProps = originalsWithPrefix(RESTORE_CONSUMER_PREFIX);\n        baseConsumerProps.putAll(restoreConsumerProps);\n\n        // no need to set group id for a restore consumer\n        baseConsumerProps.remove(ConsumerConfig.GROUP_ID_CONFIG);\n        // no need to set instance id for a restore consumer\n        baseConsumerProps.remove(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG);\n\n        // add client id with stream client id prefix\n        baseConsumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId);\n        baseConsumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\");\n\n        return baseConsumerProps;\n    }\n\n    /**\n     * Get the configs for the {@link KafkaConsumer global consumer}.\n     * Properties using the prefix {@link #GLOBAL_CONSUMER_PREFIX} will be used in favor over\n     * the properties prefixed with {@link #CONSUMER_PREFIX} and the non-prefixed versions\n     * (read the override precedence ordering in {@link #GLOBAL_CONSUMER_PREFIX})\n     * except in the case of {@link ConsumerConfig#BOOTSTRAP_SERVERS_CONFIG} where we always use the non-prefixed\n     * version as we only support reading/writing from/to the same Kafka Cluster.\n     * If not specified by {@link #GLOBAL_CONSUMER_PREFIX}, global consumer will share the general consumer configs\n     * prefixed by {@link #CONSUMER_PREFIX}.\n     *\n     * @param clientId clientId\n     * @return Map of the global consumer configuration.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public Map<String, Object> getGlobalConsumerConfigs(final String clientId) {\n        final Map<String, Object> baseConsumerProps = getCommonConsumerConfigs();\n\n        // Get global consumer override configs\n        final Map<String, Object> globalConsumerProps = originalsWithPrefix(GLOBAL_CONSUMER_PREFIX);\n        baseConsumerProps.putAll(globalConsumerProps);\n\n        // no need to set group id for a global consumer\n        baseConsumerProps.remove(ConsumerConfig.GROUP_ID_CONFIG);\n        // no need to set instance id for a restore consumer\n        baseConsumerProps.remove(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG);\n\n        // add client id with stream client id prefix\n        baseConsumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + \"-global-consumer\");\n        baseConsumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\");\n\n        return baseConsumerProps;\n    }\n\n    /**\n     * Get the configs for the {@link KafkaProducer producer}.\n     * Properties using the prefix {@link #PRODUCER_PREFIX} will be used in favor over their non-prefixed versions\n     * except in the case of {@link ProducerConfig#BOOTSTRAP_SERVERS_CONFIG} where we always use the non-prefixed\n     * version as we only support reading/writing from/to the same Kafka Cluster.\n     *\n     * @param clientId clientId\n     * @return Map of the producer configuration.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public Map<String, Object> getProducerConfigs(final String clientId) {\n        final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(PRODUCER_PREFIX, ProducerConfig.configNames());\n\n        checkIfUnexpectedUserSpecifiedConsumerConfig(clientProvidedProps, NON_CONFIGURABLE_PRODUCER_EOS_CONFIGS);\n\n        // generate producer configs from original properties and overridden maps\n        final Map<String, Object> props = new HashMap<>(eosEnabled ? PRODUCER_EOS_OVERRIDES : PRODUCER_DEFAULT_OVERRIDES);\n        props.putAll(getClientCustomProps());\n        props.putAll(clientProvidedProps);\n\n        // When using EOS alpha, stream should auto-downgrade the transactional commit protocol to be compatible with older brokers.\n        if (StreamsConfigUtils.processingMode(this) == StreamsConfigUtils.ProcessingMode.EXACTLY_ONCE_ALPHA) {\n            props.put(\"internal.auto.downgrade.txn.commit\", true);\n        }\n\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, originals().get(BOOTSTRAP_SERVERS_CONFIG));\n        // add client id with stream client id prefix\n        props.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId);\n\n        return props;\n    }\n\n    /**\n     * Get the configs for the {@link Admin admin client}.\n     * @param clientId clientId\n     * @return Map of the admin client configuration.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public Map<String, Object> getAdminConfigs(final String clientId) {\n        final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(ADMIN_CLIENT_PREFIX, AdminClientConfig.configNames());\n\n        final Map<String, Object> props = new HashMap<>();\n        props.putAll(getClientCustomProps());\n        props.putAll(clientProvidedProps);\n\n        // add client id with stream client id prefix\n        props.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId);\n\n        return props;\n    }\n\n    /**\n     * Get the configured client tags set with {@link #CLIENT_TAG_PREFIX} prefix.\n     *\n     * @return Map of the client tags.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public Map<String, String> getClientTags() {\n        return originalsWithPrefix(CLIENT_TAG_PREFIX).entrySet().stream().collect(\n            Collectors.toMap(\n                Map.Entry::getKey,\n                tagEntry -> Objects.toString(tagEntry.getValue())\n            )\n        );\n    }\n\n    private Map<String, Object> getClientPropsWithPrefix(final String prefix,\n                                                         final Set<String> configNames) {\n        final Map<String, Object> props = clientProps(configNames, originals());\n        props.putAll(originalsWithPrefix(prefix));\n        return props;\n    }\n\n    /**\n     * Get a map of custom configs by removing from the originals all the Streams, Consumer, Producer, and AdminClient configs.\n     * Prefixed properties are also removed because they are already added by {@link #getClientPropsWithPrefix(String, Set)}.\n     * This allows to set a custom property for a specific client alone if specified using a prefix, or for all\n     * when no prefix is used.\n     *\n     * @return a map with the custom properties\n     */\n    private Map<String, Object> getClientCustomProps() {\n        final Map<String, Object> props = originals();\n        props.keySet().removeAll(CONFIG.names());\n        props.keySet().removeAll(ConsumerConfig.configNames());\n        props.keySet().removeAll(ProducerConfig.configNames());\n        props.keySet().removeAll(AdminClientConfig.configNames());\n        props.keySet().removeAll(originalsWithPrefix(CONSUMER_PREFIX, false).keySet());\n        props.keySet().removeAll(originalsWithPrefix(PRODUCER_PREFIX, false).keySet());\n        props.keySet().removeAll(originalsWithPrefix(ADMIN_CLIENT_PREFIX, false).keySet());\n        return props;\n    }\n\n    public static Set<String> verifyTopologyOptimizationConfigs(final String config) {\n        final List<String> configs = Arrays.asList(config.split(\"\\\\s*,\\\\s*\"));\n        final Set<String> verifiedConfigs = new HashSet<>();\n        // Verify it doesn't contain none or all plus a list of optimizations\n        if (configs.contains(NO_OPTIMIZATION) || configs.contains(OPTIMIZE)) {\n            if (configs.size() > 1) {\n                throw new ConfigException(\"\\\"\" + config + \"\\\" is not a valid optimization config. \" + CONFIG_ERROR_MSG);\n            }\n        }\n        for (final String conf: configs) {\n            if (!TOPOLOGY_OPTIMIZATION_CONFIGS.contains(conf)) {\n                throw new ConfigException(\"Unrecognized config. \" + CONFIG_ERROR_MSG);\n            }\n        }\n        if (configs.contains(OPTIMIZE)) {\n            verifiedConfigs.add(REUSE_KTABLE_SOURCE_TOPICS);\n            verifiedConfigs.add(MERGE_REPARTITION_TOPICS);\n            verifiedConfigs.add(SINGLE_STORE_SELF_JOIN);\n        } else if (!configs.contains(NO_OPTIMIZATION)) {\n            verifiedConfigs.addAll(configs);\n        }\n        return verifiedConfigs;\n    }\n\n    /**\n     * Return configured KafkaClientSupplier\n     * @return Configured KafkaClientSupplier\n     */\n    public KafkaClientSupplier getKafkaClientSupplier() {\n        return getConfiguredInstance(StreamsConfig.DEFAULT_CLIENT_SUPPLIER_CONFIG,\n            KafkaClientSupplier.class);\n    }\n\n    /**\n     * Return an {@link Serde#configure(Map, boolean) configured} instance of {@link #DEFAULT_KEY_SERDE_CLASS_CONFIG key Serde\n     * class}.\n     *\n     * @return a configured instance of key Serde class\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public Serde<?> defaultKeySerde() {\n        final Object keySerdeConfigSetting = get(DEFAULT_KEY_SERDE_CLASS_CONFIG);\n        if (keySerdeConfigSetting == null) {\n            throw new ConfigException(\"Please specify a key serde or set one through StreamsConfig#DEFAULT_KEY_SERDE_CLASS_CONFIG\");\n        }\n        try {\n            final Serde<?> serde = getConfiguredInstance(DEFAULT_KEY_SERDE_CLASS_CONFIG, Serde.class);\n            serde.configure(originals(), true);\n            return serde;\n        } catch (final Exception e) {\n            throw new StreamsException(\n                String.format(\"Failed to configure key serde %s\", keySerdeConfigSetting), e);\n        }\n    }\n\n    /**\n     * Return an {@link Serde#configure(Map, boolean) configured} instance of {@link #DEFAULT_VALUE_SERDE_CLASS_CONFIG value\n     * Serde class}.\n     *\n     * @return an configured instance of value Serde class\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public Serde<?> defaultValueSerde() {\n        final Object valueSerdeConfigSetting = get(DEFAULT_VALUE_SERDE_CLASS_CONFIG);\n        if (valueSerdeConfigSetting == null) {\n            throw new ConfigException(\"Please specify a value serde or set one through StreamsConfig#DEFAULT_VALUE_SERDE_CLASS_CONFIG\");\n        }\n        try {\n            final Serde<?> serde = getConfiguredInstance(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serde.class);\n            serde.configure(originals(), false);\n            return serde;\n        } catch (final Exception e) {\n            throw new StreamsException(\n                String.format(\"Failed to configure value serde %s\", valueSerdeConfigSetting), e);\n        }\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public TimestampExtractor defaultTimestampExtractor() {\n        return getConfiguredInstance(DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, TimestampExtractor.class);\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public DeserializationExceptionHandler defaultDeserializationExceptionHandler() {\n        return getConfiguredInstance(DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, DeserializationExceptionHandler.class);\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public ProductionExceptionHandler defaultProductionExceptionHandler() {\n        return getConfiguredInstance(DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, ProductionExceptionHandler.class);\n    }\n\n    public ProcessingExceptionHandler processingExceptionHandler() {\n        return getConfiguredInstance(PROCESSING_EXCEPTION_HANDLER_CLASS_CONFIG, ProcessingExceptionHandler.class);\n    }\n\n    /**\n     * Override any client properties in the original configs with overrides\n     *\n     * @param configNames The given set of configuration names.\n     * @param originals   The original configs to be filtered.\n     * @return client config with any overrides\n     */\n    private Map<String, Object> clientProps(final Set<String> configNames,\n                                            final Map<String, Object> originals) {\n        // iterate all client config names, filter out non-client configs from the original\n        // property map and use the overridden values when they are not specified by users\n        final Map<String, Object> parsed = new HashMap<>();\n        for (final String configName: configNames) {\n            if (originals.containsKey(configName)) {\n                parsed.put(configName, originals.get(configName));\n            }\n        }\n\n        return parsed;\n    }\n\n    public static void main(final String[] args) {\n        System.out.println(CONFIG.toHtml(4, config -> \"streamsconfigs_\" + config));\n    }\n}\n",
        "methodName": null,
        "exampleID": 46,
        "dataset": "codeql",
        "filepath": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java",
        "line": 1868,
        "sink": "'.",
        "source": "-",
        "sourceLine": 1868,
        "qualifier": "This [regular expression](1) that depends on a [user-provided value](2) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](3) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](4) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](5) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](6) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](7) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](8) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](9) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](10) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](11) may run slow on strings with many repetitions of ' '.\nThis [regular expression](1) that depends on a [user-provided value](12) may run slow on strings with many repetitions of ' '.",
        "line_number": 1868,
        "steps": [
            {
                "line": 96,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java",
                "methodName": null,
                "exampleID": 47
            },
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 47
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 47
            },
            {
                "line": 232,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 47
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/ConfigurationUtils.java#L78",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.kafka.common.security.oauthbearer.internals.secured;\n\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.network.ListenerName;\n\nimport java.io.File;\nimport java.net.MalformedURLException;\nimport java.net.URISyntaxException;\nimport java.net.URL;\nimport java.nio.file.Path;\nimport java.util.Locale;\nimport java.util.Map;\n\n/**\n * <code>ConfigurationUtils</code> is a utility class to perform basic configuration-related\n * logic and is separated out here for easier, more direct testing.\n */\n\npublic class ConfigurationUtils {\n\n    private final Map<String, ?> configs;\n\n    private final String prefix;\n\n    public ConfigurationUtils(Map<String, ?> configs) {\n        this(configs, null);\n    }\n\n    public ConfigurationUtils(Map<String, ?> configs, String saslMechanism) {\n        this.configs = configs;\n\n        if (saslMechanism != null && !saslMechanism.trim().isEmpty())\n            this.prefix = ListenerName.saslMechanismPrefix(saslMechanism.trim());\n        else\n            this.prefix = null;\n    }\n\n    /**\n     * Validates that, if a value is supplied, is a file that:\n     *\n     * <li>\n     *     <ul>exists</ul>\n     *     <ul>has read permission</ul>\n     *     <ul>points to a file</ul>\n     * </li>\n     *\n     * If the value is null or an empty string, it is assumed to be an \"empty\" value and thus.\n     * ignored. Any whitespace is trimmed off of the beginning and end.\n     */\n\n    public Path validateFile(String name) {\n        URL url = validateUrl(name);\n        File file;\n\n        try {\n            file = new File(url.toURI().getRawPath()).getAbsoluteFile();\n        } catch (URISyntaxException e) {\n            throw new ConfigException(name, url.toString(), String.format(\"The OAuth configuration option %s contains a URL (%s) that is malformed: %s\", name, url, e.getMessage()));\n        }\n\n        if (!file.exists())\n            throw new ConfigException(name, file, String.format(\"The OAuth configuration option %s contains a file (%s) that doesn't exist\", name, file));\n\n        if (!file.canRead())\n            throw new ConfigException(name, file, String.format(\"The OAuth configuration option %s contains a file (%s) that doesn't have read permission\", name, file));\n\n        if (file.isDirectory())\n            throw new ConfigException(name, file, String.format(\"The OAuth configuration option %s references a directory (%s), not a file\", name, file));\n\n        return file.toPath();\n    }\n\n    /**\n     * Validates that, if a value is supplied, is a value that:\n     *\n     * <li>\n     *     <ul>is an Integer</ul>\n     *     <ul>has a value that is not less than the provided minimum value</ul>\n     * </li>\n     *\n     * If the value is null or an empty string, it is assumed to be an \"empty\" value and thus\n     * ignored. Any whitespace is trimmed off of the beginning and end.\n     */\n\n    public Integer validateInteger(String name, boolean isRequired) {\n        Integer value = get(name);\n\n        if (value == null) {\n            if (isRequired)\n                throw new ConfigException(name, null, String.format(\"The OAuth configuration option %s must be non-null\", name));\n            else\n                return null;\n        }\n\n        return value;\n    }\n\n    /**\n     * Validates that, if a value is supplied, is a value that:\n     *\n     * <li>\n     *     <ul>is an Integer</ul>\n     *     <ul>has a value that is not less than the provided minimum value</ul>\n     * </li>\n     *\n     * If the value is null or an empty string, it is assumed to be an \"empty\" value and thus\n     * ignored. Any whitespace is trimmed off of the beginning and end.\n     */\n\n    public Long validateLong(String name) {\n        return validateLong(name, true);\n    }\n\n    public Long validateLong(String name, boolean isRequired) {\n        return validateLong(name, isRequired, null);\n    }\n\n    public Long validateLong(String name, boolean isRequired, Long min) {\n        Long value = get(name);\n\n        if (value == null) {\n            if (isRequired)\n                throw new ConfigException(name, null, String.format(\"The OAuth configuration option %s must be non-null\", name));\n            else\n                return null;\n        }\n\n        if (min != null && value < min)\n            throw new ConfigException(name, value, String.format(\"The OAuth configuration option %s value must be at least %s\", name, min));\n\n        return value;\n    }\n\n    /**\n     * Validates that the configured URL that:\n     *\n     * <li>\n     *     <ul>is well-formed</ul>\n     *     <ul>contains a scheme</ul>\n     *     <ul>uses either HTTP, HTTPS, or file protocols</ul>\n     * </li>\n     *\n     * No effort is made to connect to the URL in the validation step.\n     */\n\n    public URL validateUrl(String name) {\n        String value = validateString(name);\n        URL url;\n\n        try {\n            url = new URL(value);\n        } catch (MalformedURLException e) {\n            throw new ConfigException(name, value, String.format(\"The OAuth configuration option %s contains a URL (%s) that is malformed: %s\", name, value, e.getMessage()));\n        }\n\n        String protocol = url.getProtocol();\n\n        if (protocol == null || protocol.trim().isEmpty())\n            throw new ConfigException(name, value, String.format(\"The OAuth configuration option %s contains a URL (%s) that is missing the protocol\", name, value));\n\n        protocol = protocol.toLowerCase(Locale.ROOT);\n\n        if (!(protocol.equals(\"http\") || protocol.equals(\"https\") || protocol.equals(\"file\")))\n            throw new ConfigException(name, value, String.format(\"The OAuth configuration option %s contains a URL (%s) that contains an invalid protocol (%s); only \\\"http\\\", \\\"https\\\", and \\\"file\\\" protocol are supported\", name, value, protocol));\n\n        return url;\n    }\n\n    public String validateString(String name) throws ValidateException {\n        return validateString(name, true);\n    }\n\n    public String validateString(String name, boolean isRequired) throws ValidateException {\n        String value = get(name);\n\n        if (value == null) {\n            if (isRequired)\n                throw new ConfigException(String.format(\"The OAuth configuration option %s value must be non-null\", name));\n            else\n                return null;\n        }\n\n        value = value.trim();\n\n        if (value.isEmpty()) {\n            if (isRequired)\n                throw new ConfigException(String.format(\"The OAuth configuration option %s value must not contain only whitespace\", name));\n            else\n                return null;\n        }\n\n        return value;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    public <T> T get(String name) {\n        T value = (T) configs.get(prefix + name);\n\n        if (value != null)\n            return value;\n\n        return (T) configs.get(name);\n    }\n\n}\n",
        "methodName": null,
        "exampleID": 48,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/secured/ConfigurationUtils.java",
        "line": 78,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 78,
        "qualifier": "This path depends on a [user-provided value](1).\nThis path depends on a [user-provided value](2).\nThis path depends on a [user-provided value](3).\nThis path depends on a [user-provided value](4).\nThis path depends on a [user-provided value](5).\nThis path depends on a [user-provided value](6).\nThis path depends on a [user-provided value](7).\nThis path depends on a [user-provided value](8).",
        "line_number": 78,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 49
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 49
            },
            {
                "line": 251,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 49
            },
            {
                "line": 265,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 49
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactory.java#L373",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.security.ssl;\n\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.config.SslClientAuth;\nimport org.apache.kafka.common.config.SslConfigs;\nimport org.apache.kafka.common.config.internals.BrokerSecurityConfigs;\nimport org.apache.kafka.common.config.types.Password;\nimport org.apache.kafka.common.errors.InvalidConfigurationException;\nimport org.apache.kafka.common.network.ConnectionMode;\nimport org.apache.kafka.common.security.auth.SslEngineFactory;\nimport org.apache.kafka.common.utils.SecurityUtils;\nimport org.apache.kafka.common.utils.Utils;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.security.GeneralSecurityException;\nimport java.security.Key;\nimport java.security.KeyFactory;\nimport java.security.KeyStore;\nimport java.security.KeyStoreException;\nimport java.security.NoSuchAlgorithmException;\nimport java.security.PrivateKey;\nimport java.security.SecureRandom;\nimport java.security.cert.Certificate;\nimport java.security.cert.CertificateFactory;\nimport java.security.spec.InvalidKeySpecException;\nimport java.security.spec.PKCS8EncodedKeySpec;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Base64;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Set;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\n\nimport javax.crypto.Cipher;\nimport javax.crypto.EncryptedPrivateKeyInfo;\nimport javax.crypto.SecretKey;\nimport javax.crypto.SecretKeyFactory;\nimport javax.crypto.spec.PBEKeySpec;\nimport javax.net.ssl.KeyManager;\nimport javax.net.ssl.KeyManagerFactory;\nimport javax.net.ssl.SSLContext;\nimport javax.net.ssl.SSLEngine;\nimport javax.net.ssl.SSLParameters;\nimport javax.net.ssl.TrustManager;\nimport javax.net.ssl.TrustManagerFactory;\n\npublic class DefaultSslEngineFactory implements SslEngineFactory {\n\n    private static final Logger log = LoggerFactory.getLogger(DefaultSslEngineFactory.class);\n    public static final String PEM_TYPE = \"PEM\";\n\n    private Map<String, ?> configs;\n    private String protocol;\n    private String provider;\n    private String kmfAlgorithm;\n    private String tmfAlgorithm;\n    private SecurityStore keystore;\n    private SecurityStore truststore;\n    private String[] cipherSuites;\n    private String[] enabledProtocols;\n    private SecureRandom secureRandomImplementation;\n    private SSLContext sslContext;\n    private SslClientAuth sslClientAuth;\n\n\n    @Override\n    public SSLEngine createClientSslEngine(String peerHost, int peerPort, String endpointIdentification) {\n        return createSslEngine(ConnectionMode.CLIENT, peerHost, peerPort, endpointIdentification);\n    }\n\n    @Override\n    public SSLEngine createServerSslEngine(String peerHost, int peerPort) {\n        return createSslEngine(ConnectionMode.SERVER, peerHost, peerPort, null);\n    }\n\n    @Override\n    public boolean shouldBeRebuilt(Map<String, Object> nextConfigs) {\n        if (!nextConfigs.equals(configs)) {\n            return true;\n        }\n        if (truststore != null && truststore.modified()) {\n            return true;\n        }\n        return keystore != null && keystore.modified();\n    }\n\n    @Override\n    public Set<String> reconfigurableConfigs() {\n        return SslConfigs.RECONFIGURABLE_CONFIGS;\n    }\n\n    @Override\n    public KeyStore keystore() {\n        return this.keystore != null ? this.keystore.get() : null;\n    }\n\n    @Override\n    public KeyStore truststore() {\n        return this.truststore != null ? this.truststore.get() : null;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @Override\n    public void configure(Map<String, ?> configs) {\n        this.configs = Collections.unmodifiableMap(configs);\n        this.protocol = (String) configs.get(SslConfigs.SSL_PROTOCOL_CONFIG);\n        this.provider = (String) configs.get(SslConfigs.SSL_PROVIDER_CONFIG);\n        SecurityUtils.addConfiguredSecurityProviders(this.configs);\n\n        List<String> cipherSuitesList = (List<String>) configs.get(SslConfigs.SSL_CIPHER_SUITES_CONFIG);\n        if (cipherSuitesList != null && !cipherSuitesList.isEmpty()) {\n            this.cipherSuites = cipherSuitesList.toArray(new String[0]);\n        } else {\n            this.cipherSuites = null;\n        }\n\n        List<String> enabledProtocolsList = (List<String>) configs.get(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG);\n        if (enabledProtocolsList != null && !enabledProtocolsList.isEmpty()) {\n            this.enabledProtocols = enabledProtocolsList.toArray(new String[0]);\n        } else {\n            this.enabledProtocols = null;\n        }\n\n        this.secureRandomImplementation = createSecureRandom((String)\n                configs.get(SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG));\n\n        this.sslClientAuth = createSslClientAuth((String) configs.get(\n                BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG));\n\n        this.kmfAlgorithm = (String) configs.get(SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG);\n        this.tmfAlgorithm = (String) configs.get(SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG);\n\n        this.keystore = createKeystore((String) configs.get(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG),\n                (String) configs.get(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEY_PASSWORD_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEYSTORE_KEY_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_KEYSTORE_CERTIFICATE_CHAIN_CONFIG));\n\n        this.truststore = createTruststore((String) configs.get(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG),\n                (String) configs.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG),\n                (Password) configs.get(SslConfigs.SSL_TRUSTSTORE_CERTIFICATES_CONFIG));\n\n        this.sslContext = createSSLContext(keystore, truststore);\n    }\n\n    @Override\n    public void close() {\n        this.sslContext = null;\n    }\n\n    //For Test only\n    public SSLContext sslContext() {\n        return this.sslContext;\n    }\n\n    private SSLEngine createSslEngine(ConnectionMode connectionMode, String peerHost, int peerPort, String endpointIdentification) {\n        SSLEngine sslEngine = sslContext.createSSLEngine(peerHost, peerPort);\n        if (cipherSuites != null) sslEngine.setEnabledCipherSuites(cipherSuites);\n        if (enabledProtocols != null) sslEngine.setEnabledProtocols(enabledProtocols);\n\n        if (connectionMode == ConnectionMode.SERVER) {\n            sslEngine.setUseClientMode(false);\n            switch (sslClientAuth) {\n                case REQUIRED:\n                    sslEngine.setNeedClientAuth(true);\n                    break;\n                case REQUESTED:\n                    sslEngine.setWantClientAuth(true);\n                    break;\n                case NONE:\n                    break;\n            }\n        } else {\n            sslEngine.setUseClientMode(true);\n            SSLParameters sslParams = sslEngine.getSSLParameters();\n            // SSLParameters#setEndpointIdentificationAlgorithm enables endpoint validation\n            // only in client mode. Hence, validation is enabled only for clients.\n            sslParams.setEndpointIdentificationAlgorithm(endpointIdentification);\n            sslEngine.setSSLParameters(sslParams);\n        }\n        return sslEngine;\n    }\n    private static SslClientAuth createSslClientAuth(String key) {\n        SslClientAuth auth = SslClientAuth.forConfig(key);\n        if (auth != null) {\n            return auth;\n        }\n        log.warn(\"Unrecognized client authentication configuration {}.  Falling \" +\n                \"back to NONE.  Recognized client authentication configurations are {}.\",\n                key, SslClientAuth.VALUES.stream().\n                        map(Enum::name).collect(Collectors.joining(\", \")));\n        return SslClientAuth.NONE;\n    }\n\n    private static SecureRandom createSecureRandom(String key) {\n        if (key == null) {\n            return null;\n        }\n        try {\n            return SecureRandom.getInstance(key);\n        } catch (GeneralSecurityException e) {\n            throw new KafkaException(e);\n        }\n    }\n\n    private SSLContext createSSLContext(SecurityStore keystore, SecurityStore truststore) {\n        try {\n            SSLContext sslContext;\n            if (provider != null)\n                sslContext = SSLContext.getInstance(protocol, provider);\n            else\n                sslContext = SSLContext.getInstance(protocol);\n\n            KeyManager[] keyManagers = null;\n            if (keystore != null || kmfAlgorithm != null) {\n                String kmfAlgorithm = this.kmfAlgorithm != null ?\n                        this.kmfAlgorithm : KeyManagerFactory.getDefaultAlgorithm();\n                KeyManagerFactory kmf = KeyManagerFactory.getInstance(kmfAlgorithm);\n                if (keystore != null) {\n                    kmf.init(keystore.get(), keystore.keyPassword());\n                } else {\n                    kmf.init(null, null);\n                }\n                keyManagers = kmf.getKeyManagers();\n            }\n\n            String tmfAlgorithm = this.tmfAlgorithm != null ? this.tmfAlgorithm : TrustManagerFactory.getDefaultAlgorithm();\n            TrustManager[] trustManagers = getTrustManagers(truststore, tmfAlgorithm);\n\n            sslContext.init(keyManagers, trustManagers, this.secureRandomImplementation);\n            log.debug(\"Created SSL context with keystore {}, truststore {}, provider {}.\",\n                    keystore, truststore, sslContext.getProvider().getName());\n            return sslContext;\n        } catch (Exception e) {\n            throw new KafkaException(e);\n        }\n    }\n\n    protected TrustManager[] getTrustManagers(SecurityStore truststore, String tmfAlgorithm) throws NoSuchAlgorithmException, KeyStoreException {\n        TrustManagerFactory tmf = TrustManagerFactory.getInstance(tmfAlgorithm);\n        KeyStore ts = truststore == null ? null : truststore.get();\n        tmf.init(ts);\n        return tmf.getTrustManagers();\n    }\n\n    // Visibility to override for testing\n    protected SecurityStore createKeystore(String type, String path, Password password, Password keyPassword, Password privateKey, Password certificateChain) {\n        if (privateKey != null) {\n            if (!PEM_TYPE.equals(type))\n                throw new InvalidConfigurationException(\"SSL private key can be specified only for PEM, but key store type is \" + type + \".\");\n            else if (certificateChain == null)\n                throw new InvalidConfigurationException(\"SSL private key is specified, but certificate chain is not specified.\");\n            else if (path != null)\n                throw new InvalidConfigurationException(\"Both SSL key store location and separate private key are specified.\");\n            else if (password != null)\n                throw new InvalidConfigurationException(\"SSL key store password cannot be specified with PEM format, only key password may be specified.\");\n            else\n                return new PemStore(certificateChain, privateKey, keyPassword);\n        } else if (certificateChain != null) {\n            throw new InvalidConfigurationException(\"SSL certificate chain is specified, but private key is not specified\");\n        } else if (PEM_TYPE.equals(type) && path != null) {\n            if (password != null)\n                throw new InvalidConfigurationException(\"SSL key store password cannot be specified with PEM format, only key password may be specified\");\n            else\n                return new FileBasedPemStore(path, keyPassword, true);\n        } else if (path == null && password != null) {\n            throw new InvalidConfigurationException(\"SSL key store is not specified, but key store password is specified.\");\n        } else if (path != null && password == null) {\n            throw new InvalidConfigurationException(\"SSL key store is specified, but key store password is not specified.\");\n        } else if (path != null && password != null) {\n            return new FileBasedStore(type, path, password, keyPassword, true);\n        } else\n            return null; // path == null, clients may use this path with brokers that don't require client auth\n    }\n\n    private static SecurityStore createTruststore(String type, String path, Password password, Password trustStoreCerts) {\n        if (trustStoreCerts != null) {\n            if (!PEM_TYPE.equals(type))\n                throw new InvalidConfigurationException(\"SSL trust store certs can be specified only for PEM, but trust store type is \" + type + \".\");\n            else if (path != null)\n                throw new InvalidConfigurationException(\"Both SSL trust store location and separate trust certificates are specified.\");\n            else if (password != null)\n                throw new InvalidConfigurationException(\"SSL trust store password cannot be specified for PEM format.\");\n            else\n                return new PemStore(trustStoreCerts);\n        } else if (PEM_TYPE.equals(type) && path != null) {\n            if (password != null)\n                throw new InvalidConfigurationException(\"SSL trust store password cannot be specified for PEM format.\");\n            else\n                return new FileBasedPemStore(path, null, false);\n        } else if (path == null && password != null) {\n            throw new InvalidConfigurationException(\"SSL trust store is not specified, but trust store password is specified.\");\n        } else if (path != null) {\n            return new FileBasedStore(type, path, password, null, false);\n        } else\n            return null;\n    }\n\n    interface SecurityStore {\n        KeyStore get();\n        char[] keyPassword();\n        boolean modified();\n    }\n\n    // package access for testing\n    static class FileBasedStore implements SecurityStore {\n        private final String type;\n        protected final String path;\n        private final Password password;\n        protected final Password keyPassword;\n        private final Long fileLastModifiedMs;\n        private final KeyStore keyStore;\n\n        FileBasedStore(String type, String path, Password password, Password keyPassword, boolean isKeyStore) {\n            Objects.requireNonNull(type, \"type must not be null\");\n            this.type = type;\n            this.path = path;\n            this.password = password;\n            this.keyPassword = keyPassword;\n            fileLastModifiedMs = lastModifiedMs(path);\n            this.keyStore = load(isKeyStore);\n        }\n\n        @Override\n        public KeyStore get() {\n            return keyStore;\n        }\n\n        @Override\n        public char[] keyPassword() {\n            Password passwd = keyPassword != null ? keyPassword : password;\n            return passwd == null ? null : passwd.value().toCharArray();\n        }\n\n        /**\n         * Loads this keystore\n         * @return the keystore\n         * @throws KafkaException if the file could not be read or if the keystore could not be loaded\n         *   using the specified configs (e.g. if the password or keystore type is invalid)\n         */\n        protected KeyStore load(boolean isKeyStore) {\n            try (InputStream in = Files.newInputStream(Paths.get(path))) {\n                KeyStore ks = KeyStore.getInstance(type);\n                // If a password is not set access to the truststore is still available, but integrity checking is disabled.\n                char[] passwordChars = password != null ? password.value().toCharArray() : null;\n                ks.load(in, passwordChars);\n                return ks;\n            } catch (GeneralSecurityException | IOException e) {\n                throw new KafkaException(\"Failed to load SSL keystore \" + path + \" of type \" + type, e);\n            }\n        }\n\n        private Long lastModifiedMs(String path) {\n            try {\n                return Files.getLastModifiedTime(Paths.get(path)).toMillis();\n            } catch (IOException e) {\n                log.error(\"Modification time of key store could not be obtained: \" + path, e);\n                return null;\n            }\n        }\n\n        public boolean modified() {\n            Long modifiedMs = lastModifiedMs(path);\n            return modifiedMs != null && !Objects.equals(modifiedMs, this.fileLastModifiedMs);\n        }\n\n        @Override\n        public String toString() {\n            return \"SecurityStore(\" +\n                    \"path=\" + path +\n                    \", modificationTime=\" + (fileLastModifiedMs == null ? null : new Date(fileLastModifiedMs)) + \")\";\n        }\n    }\n\n    static class FileBasedPemStore extends FileBasedStore {\n        FileBasedPemStore(String path, Password keyPassword, boolean isKeyStore) {\n            super(PEM_TYPE, path, null, keyPassword, isKeyStore);\n        }\n\n        @Override\n        protected KeyStore load(boolean isKeyStore) {\n            try {\n                Password storeContents = new Password(Utils.readFileAsString(path));\n                PemStore pemStore = isKeyStore ? new PemStore(storeContents, storeContents, keyPassword) :\n                    new PemStore(storeContents);\n                return pemStore.keyStore;\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Failed to load PEM SSL keystore \" + path, e);\n            }\n        }\n    }\n\n    static class PemStore implements SecurityStore {\n        private static final PemParser CERTIFICATE_PARSER = new PemParser(\"CERTIFICATE\");\n        private static final PemParser PRIVATE_KEY_PARSER = new PemParser(\"PRIVATE KEY\");\n        private static final List<KeyFactory> KEY_FACTORIES = Arrays.asList(\n                keyFactory(\"RSA\"),\n                keyFactory(\"DSA\"),\n                keyFactory(\"EC\")\n        );\n\n        private final char[] keyPassword;\n        private final KeyStore keyStore;\n\n        PemStore(Password certificateChain, Password privateKey, Password keyPassword) {\n            this.keyPassword = keyPassword == null ? null : keyPassword.value().toCharArray();\n            keyStore = createKeyStoreFromPem(privateKey.value(), certificateChain.value(), this.keyPassword);\n        }\n\n        PemStore(Password trustStoreCerts) {\n            this.keyPassword = null;\n            keyStore = createTrustStoreFromPem(trustStoreCerts.value());\n        }\n\n        @Override\n        public KeyStore get() {\n            return keyStore;\n        }\n\n        @Override\n        public char[] keyPassword() {\n            return keyPassword;\n        }\n\n        @Override\n        public boolean modified() {\n            return false;\n        }\n\n        private KeyStore createKeyStoreFromPem(String privateKeyPem, String certChainPem, char[] keyPassword) {\n            try {\n                KeyStore ks = KeyStore.getInstance(\"PKCS12\");\n                ks.load(null, null);\n                Key key = privateKey(privateKeyPem, keyPassword);\n                Certificate[] certChain = certs(certChainPem);\n                ks.setKeyEntry(\"kafka\", key, keyPassword, certChain);\n                return ks;\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Invalid PEM keystore configs\", e);\n            }\n        }\n\n        private KeyStore createTrustStoreFromPem(String trustedCertsPem) {\n            try {\n                KeyStore ts = KeyStore.getInstance(\"PKCS12\");\n                ts.load(null, null);\n                Certificate[] certs = certs(trustedCertsPem);\n                for (int i = 0; i < certs.length; i++) {\n                    ts.setCertificateEntry(\"kafka\" + i, certs[i]);\n                }\n                return ts;\n            } catch (InvalidConfigurationException e) {\n                throw e;\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Invalid PEM truststore configs\", e);\n            }\n        }\n\n        private Certificate[] certs(String pem) throws GeneralSecurityException {\n            List<byte[]> certEntries = CERTIFICATE_PARSER.pemEntries(pem);\n            if (certEntries.isEmpty())\n                throw new InvalidConfigurationException(\"At least one certificate expected, but none found\");\n\n            Certificate[] certs = new Certificate[certEntries.size()];\n            for (int i = 0; i < certs.length; i++) {\n                certs[i] = CertificateFactory.getInstance(\"X.509\")\n                    .generateCertificate(new ByteArrayInputStream(certEntries.get(i)));\n            }\n            return certs;\n        }\n\n        private PrivateKey privateKey(String pem, char[] keyPassword) throws Exception {\n            List<byte[]> keyEntries = PRIVATE_KEY_PARSER.pemEntries(pem);\n            if (keyEntries.isEmpty())\n                throw new InvalidConfigurationException(\"Private key not provided\");\n            if (keyEntries.size() != 1)\n                throw new InvalidConfigurationException(\"Expected one private key, but found \" + keyEntries.size());\n\n            byte[] keyBytes = keyEntries.get(0);\n            PKCS8EncodedKeySpec keySpec;\n            if (keyPassword == null) {\n                keySpec = new PKCS8EncodedKeySpec(keyBytes);\n            } else {\n                EncryptedPrivateKeyInfo keyInfo = new EncryptedPrivateKeyInfo(keyBytes);\n                String algorithm = keyInfo.getAlgName();\n                SecretKeyFactory keyFactory = SecretKeyFactory.getInstance(algorithm);\n                SecretKey pbeKey = keyFactory.generateSecret(new PBEKeySpec(keyPassword));\n                Cipher cipher = Cipher.getInstance(algorithm);\n                cipher.init(Cipher.DECRYPT_MODE, pbeKey, keyInfo.getAlgParameters());\n                keySpec = keyInfo.getKeySpec(cipher);\n            }\n\n            InvalidKeySpecException firstException = null;\n            for (KeyFactory factory : KEY_FACTORIES) {\n                try {\n                    return factory.generatePrivate(keySpec);\n                } catch (InvalidKeySpecException e) {\n                    if (firstException == null)\n                        firstException = e;\n                }\n            }\n            throw new InvalidConfigurationException(\"Private key could not be loaded\", firstException);\n        }\n\n        private static KeyFactory keyFactory(String algorithm) {\n            try {\n                return KeyFactory.getInstance(algorithm);\n            } catch (Exception e) {\n                throw new InvalidConfigurationException(\"Could not create key factory for algorithm \" + algorithm, e);\n            }\n        }\n    }\n\n    /**\n     * Parser to process certificate/private key entries from PEM files\n     * Examples:\n     *   -----BEGIN CERTIFICATE-----\n     *   Base64 cert\n     *   -----END CERTIFICATE-----\n     *\n     *   -----BEGIN ENCRYPTED PRIVATE KEY-----\n     *   Base64 private key\n     *   -----END ENCRYPTED PRIVATE KEY-----\n     *   Additional data may be included before headers, so we match all entries within the PEM.\n     */\n    static class PemParser {\n        private final String name;\n        private final Pattern pattern;\n\n        PemParser(String name) {\n            this.name = name;\n            String beginOrEndFormat = \"-+%s\\\\s*.*%s[^-]*-+\\\\s+\";\n            String nameIgnoreSpace = name.replace(\" \", \"\\\\s+\");\n\n            String encodingParams = \"\\\\s*[^\\\\r\\\\n]*:[^\\\\r\\\\n]*[\\\\r\\\\n]+\";\n            String base64Pattern = \"([a-zA-Z0-9/+=\\\\s]*)\";\n            String patternStr =  String.format(beginOrEndFormat, \"BEGIN\", nameIgnoreSpace) +\n                String.format(\"(?:%s)*\", encodingParams) +\n                base64Pattern +\n                String.format(beginOrEndFormat, \"END\", nameIgnoreSpace);\n            pattern = Pattern.compile(patternStr);\n        }\n\n        private List<byte[]> pemEntries(String pem) {\n            Matcher matcher = pattern.matcher(pem + \"\\n\"); // allow last newline to be omitted in value\n            List<byte[]>  entries = new ArrayList<>();\n            while (matcher.find()) {\n                String base64Str = matcher.group(1).replaceAll(\"\\\\s\", \"\");\n                entries.add(Base64.getDecoder().decode(base64Str));\n            }\n            if (entries.isEmpty())\n                throw new InvalidConfigurationException(\"No matching \" + name + \" entries in PEM file\");\n            return entries;\n        }\n    }\n}\n",
        "methodName": null,
        "exampleID": 50,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactory.java",
        "line": 373,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 373,
        "qualifier": "This path depends on a [user-provided value](1).\nThis path depends on a [user-provided value](2).\nThis path depends on a [user-provided value](3).\nThis path depends on a [user-provided value](4).\nThis path depends on a [user-provided value](5).\nThis path depends on a [user-provided value](6).\nThis path depends on a [user-provided value](7).\nThis path depends on a [user-provided value](8).",
        "line_number": 373,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 51
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 51
            },
            {
                "line": 251,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 51
            },
            {
                "line": 265,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 51
            }
        ]
    },
    {
        "url": "apache/kafka/blob/main/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L753",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.kafka.common.utils;\n\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.network.TransferableChannel;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.slf4j.event.Level;\n\nimport java.io.Closeable;\nimport java.io.DataOutput;\nimport java.io.EOFException;\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.PrintWriter;\nimport java.io.StringWriter;\nimport java.lang.reflect.Constructor;\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Modifier;\nimport java.nio.BufferUnderflowException;\nimport java.nio.ByteBuffer;\nimport java.nio.ByteOrder;\nimport java.nio.channels.FileChannel;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.FileVisitResult;\nimport java.nio.file.Files;\nimport java.nio.file.NoSuchFileException;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.nio.file.SimpleFileVisitor;\nimport java.nio.file.StandardCopyOption;\nimport java.nio.file.StandardOpenOption;\nimport java.nio.file.attribute.BasicFileAttributes;\nimport java.text.DecimalFormat;\nimport java.text.DecimalFormatSymbols;\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.time.Instant;\nimport java.time.ZoneId;\nimport java.time.format.DateTimeFormatter;\nimport java.util.AbstractMap;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport java.util.Objects;\nimport java.util.Properties;\nimport java.util.Set;\nimport java.util.SortedSet;\nimport java.util.TreeSet;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.BiConsumer;\nimport java.util.function.BinaryOperator;\nimport java.util.function.Function;\nimport java.util.function.Predicate;\nimport java.util.function.Supplier;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collector;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\npublic final class Utils {\n\n    private Utils() {}\n\n    // This matches URIs of formats: host:port and protocol://host:port\n    // IPv6 is supported with [ip] pattern\n    private static final Pattern HOST_PORT_PATTERN = Pattern.compile(\"^(?:[0-9a-zA-Z\\\\-%._]*://)?\\\\[?([0-9a-zA-Z\\\\-%._:]*)]?:([0-9]+)\");\n\n    private static final Pattern VALID_HOST_CHARACTERS = Pattern.compile(\"([0-9a-zA-Z\\\\-%._:]*)\");\n\n    // Prints up to 2 decimal digits. Used for human readable printing\n    private static final DecimalFormat TWO_DIGIT_FORMAT = new DecimalFormat(\"0.##\",\n        DecimalFormatSymbols.getInstance(Locale.ENGLISH));\n\n    private static final String[] BYTE_SCALE_SUFFIXES = new String[] {\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\"};\n\n    public static final String NL = System.lineSeparator();\n\n    private static final Logger log = LoggerFactory.getLogger(Utils.class);\n\n    /**\n     * Get a sorted list representation of a collection.\n     * @param collection The collection to sort\n     * @param <T> The class of objects in the collection\n     * @return An unmodifiable sorted list with the contents of the collection\n     */\n    public static <T extends Comparable<? super T>> List<T> sorted(Collection<T> collection) {\n        List<T> res = new ArrayList<>(collection);\n        Collections.sort(res);\n        return Collections.unmodifiableList(res);\n    }\n\n    /**\n     * Turn the given UTF8 byte array into a string\n     *\n     * @param bytes The byte array\n     * @return The string\n     */\n    public static String utf8(byte[] bytes) {\n        return new String(bytes, StandardCharsets.UTF_8);\n    }\n\n    /**\n     * Read a UTF8 string from a byte buffer. Note that the position of the byte buffer is not affected\n     * by this method.\n     *\n     * @param buffer The buffer to read from\n     * @param length The length of the string in bytes\n     * @return The UTF8 string\n     */\n    public static String utf8(ByteBuffer buffer, int length) {\n        return utf8(buffer, 0, length);\n    }\n\n    /**\n     * Read a UTF8 string from the current position till the end of a byte buffer. The position of the byte buffer is\n     * not affected by this method.\n     *\n     * @param buffer The buffer to read from\n     * @return The UTF8 string\n     */\n    public static String utf8(ByteBuffer buffer) {\n        return utf8(buffer, buffer.remaining());\n    }\n\n    /**\n     * Read a UTF8 string from a byte buffer at a given offset. Note that the position of the byte buffer\n     * is not affected by this method.\n     *\n     * @param buffer The buffer to read from\n     * @param offset The offset relative to the current position in the buffer\n     * @param length The length of the string in bytes\n     * @return The UTF8 string\n     */\n    public static String utf8(ByteBuffer buffer, int offset, int length) {\n        if (buffer.hasArray())\n            return new String(buffer.array(), buffer.arrayOffset() + buffer.position() + offset, length, StandardCharsets.UTF_8);\n        else\n            return utf8(toArray(buffer, offset, length));\n    }\n\n    /**\n     * Turn a string into a utf8 byte[]\n     *\n     * @param string The string\n     * @return The byte[]\n     */\n    public static byte[] utf8(String string) {\n        return string.getBytes(StandardCharsets.UTF_8);\n    }\n\n    /**\n     * Get the absolute value of the given number. If the number is Int.MinValue return 0. This is different from\n     * java.lang.Math.abs or scala.math.abs in that they return Int.MinValue (!).\n     */\n    public static int abs(int n) {\n        return (n == Integer.MIN_VALUE) ? 0 : Math.abs(n);\n    }\n\n    /**\n     * Get the minimum of some long values.\n     * @param first Used to ensure at least one value\n     * @param rest The remaining values to compare\n     * @return The minimum of all passed values\n     */\n    public static long min(long first, long... rest) {\n        long min = first;\n        for (long r : rest) {\n            if (r < min)\n                min = r;\n        }\n        return min;\n    }\n\n    /**\n     * Get the maximum of some long values.\n     * @param first Used to ensure at least one value\n     * @param rest The remaining values to compare\n     * @return The maximum of all passed values\n     */\n    public static long max(long first, long... rest) {\n        long max = first;\n        for (long r : rest) {\n            if (r > max)\n                max = r;\n        }\n        return max;\n    }\n\n\n    public static short min(short first, short second) {\n        return (short) Math.min(first, second);\n    }\n\n    /**\n     * Get the length for UTF8-encoding a string without encoding it first\n     *\n     * @param s The string to calculate the length for\n     * @return The length when serialized\n     */\n    public static int utf8Length(CharSequence s) {\n        int count = 0;\n        for (int i = 0, len = s.length(); i < len; i++) {\n            char ch = s.charAt(i);\n            if (ch <= 0x7F) {\n                count++;\n            } else if (ch <= 0x7FF) {\n                count += 2;\n            } else if (Character.isHighSurrogate(ch)) {\n                count += 4;\n                ++i;\n            } else {\n                count += 3;\n            }\n        }\n        return count;\n    }\n\n    /**\n     * Read the given byte buffer from its current position to its limit into a byte array.\n     * @param buffer The buffer to read from\n     */\n    public static byte[] toArray(ByteBuffer buffer) {\n        return toArray(buffer, 0, buffer.remaining());\n    }\n\n    /**\n     * Read a byte array from its current position given the size in the buffer\n     * @param buffer The buffer to read from\n     * @param size The number of bytes to read into the array\n     */\n    public static byte[] toArray(ByteBuffer buffer, int size) {\n        return toArray(buffer, 0, size);\n    }\n\n    /**\n     * Convert a ByteBuffer to a nullable array.\n     * @param buffer The buffer to convert\n     * @return The resulting array or null if the buffer is null\n     */\n    public static byte[] toNullableArray(ByteBuffer buffer) {\n        return buffer == null ? null : toArray(buffer);\n    }\n\n    /**\n     * Wrap an array as a nullable ByteBuffer.\n     * @param array The nullable array to wrap\n     * @return The wrapping ByteBuffer or null if array is null\n     */\n    public static ByteBuffer wrapNullable(byte[] array) {\n        return array == null ? null : ByteBuffer.wrap(array);\n    }\n\n    /**\n     * Read a byte array from the given offset and size in the buffer\n     * @param buffer The buffer to read from\n     * @param offset The offset relative to the current position of the buffer\n     * @param size The number of bytes to read into the array\n     */\n    public static byte[] toArray(ByteBuffer buffer, int offset, int size) {\n        byte[] dest = new byte[size];\n        if (buffer.hasArray()) {\n            System.arraycopy(buffer.array(), buffer.position() + buffer.arrayOffset() + offset, dest, 0, size);\n        } else {\n            int pos = buffer.position();\n            buffer.position(pos + offset);\n            buffer.get(dest);\n            buffer.position(pos);\n        }\n        return dest;\n    }\n\n    /**\n     * Starting from the current position, read an integer indicating the size of the byte array to read,\n     * then read the array. Consumes the buffer: upon returning, the buffer's position is after the array\n     * that is returned.\n     * @param buffer The buffer to read a size-prefixed array from\n     * @return The array\n     */\n    public static byte[] getNullableSizePrefixedArray(final ByteBuffer buffer) {\n        final int size = buffer.getInt();\n        return getNullableArray(buffer, size);\n    }\n\n    /**\n     * Read a byte array of the given size. Consumes the buffer: upon returning, the buffer's position\n     * is after the array that is returned.\n     * @param buffer The buffer to read a size-prefixed array from\n     * @param size The number of bytes to read out of the buffer\n     * @return The array\n     */\n    public static byte[] getNullableArray(final ByteBuffer buffer, final int size) {\n        if (size > buffer.remaining()) {\n            // preemptively throw this when the read is doomed to fail, so we don't have to allocate the array.\n            throw new BufferUnderflowException();\n        }\n        final byte[] oldBytes = size == -1 ? null : new byte[size];\n        if (oldBytes != null) {\n            buffer.get(oldBytes);\n        }\n        return oldBytes;\n    }\n\n    /**\n     * Returns a copy of src byte array\n     * @param src The byte array to copy\n     * @return The copy\n     */\n    public static byte[] copyArray(byte[] src) {\n        return Arrays.copyOf(src, src.length);\n    }\n\n    /**\n     * Compares two character arrays for equality using a constant-time algorithm, which is needed\n     * for comparing passwords. Two arrays are equal if they have the same length and all\n     * characters at corresponding positions are equal.\n     *\n     * All characters in the first array are examined to determine equality.\n     * The calculation time depends only on the length of this first character array; it does not\n     * depend on the length of the second character array or the contents of either array.\n     *\n     * @param first the first array to compare\n     * @param second the second array to compare\n     * @return true if the arrays are equal, or false otherwise\n     */\n    public static boolean isEqualConstantTime(char[] first, char[] second) {\n        if (first == second) {\n            return true;\n        }\n        if (first == null || second == null) {\n            return false;\n        }\n\n        if (second.length == 0) {\n            return first.length == 0;\n        }\n\n        // time-constant comparison that always compares all characters in first array\n        boolean matches = first.length == second.length;\n        for (int i = 0; i < first.length; ++i) {\n            int j = i < second.length ? i : 0;\n            if (first[i] != second[j]) {\n                matches = false;\n            }\n        }\n        return matches;\n    }\n\n    /**\n     * Sleep for a bit\n     * @param ms The duration of the sleep\n     */\n    public static void sleep(long ms) {\n        try {\n            Thread.sleep(ms);\n        } catch (InterruptedException e) {\n            // this is okay, we just wake up early\n            Thread.currentThread().interrupt();\n        }\n    }\n\n    /**\n     * Instantiate the class\n     */\n    public static <T> T newInstance(Class<T> c) {\n        if (c == null)\n            throw new KafkaException(\"class cannot be null\");\n        try {\n            return c.getDeclaredConstructor().newInstance();\n        } catch (NoSuchMethodException e) {\n            throw new KafkaException(\"Could not find a public no-argument constructor for \" + c.getName(), e);\n        } catch (ReflectiveOperationException | RuntimeException e) {\n            throw new KafkaException(\"Could not instantiate class \" + c.getName(), e);\n        }\n    }\n\n    /**\n     * Look up the class by name and instantiate it.\n     * @param klass class name\n     * @param base super class of the class to be instantiated\n     * @param <T> the type of the base class\n     * @return the new instance\n     */\n    public static <T> T newInstance(String klass, Class<T> base) throws ClassNotFoundException {\n        return Utils.newInstance(loadClass(klass, base));\n    }\n\n    /**\n     * Look up a class by name.\n     * @param klass class name\n     * @param base super class of the class for verification\n     * @param <T> the type of the base class\n     * @return the new class\n     */\n    public static <T> Class<? extends T> loadClass(String klass, Class<T> base) throws ClassNotFoundException {\n        ClassLoader contextOrKafkaClassLoader = Utils.getContextOrKafkaClassLoader();\n        // Use loadClass here instead of Class.forName because the name we use here may be an alias\n        // and not match the name of the class that gets loaded. If that happens, Class.forName can\n        // throw an exception.\n        Class<?> loadedClass = contextOrKafkaClassLoader.loadClass(klass);\n        // Invoke forName here with the true name of the requested class to cause class\n        // initialization to take place.\n        return Class.forName(loadedClass.getName(), true, contextOrKafkaClassLoader).asSubclass(base);\n    }\n\n    /**\n     * Cast {@code klass} to {@code base} and instantiate it.\n     * @param klass The class to instantiate\n     * @param base A know baseclass of klass.\n     * @param <T> the type of the base class\n     * @throws ClassCastException If {@code klass} is not a subclass of {@code base}.\n     * @return the new instance.\n     */\n    public static <T> T newInstance(Class<?> klass, Class<T> base) {\n        return Utils.newInstance(klass.asSubclass(base));\n    }\n\n    /**\n     * Construct a new object using a class name and parameters.\n     *\n     * @param className                 The full name of the class to construct.\n     * @param params                    A sequence of (type, object) elements.\n     * @param <T>                       The type of object to construct.\n     * @return                          The new object.\n     * @throws ClassNotFoundException   If there was a problem constructing the object.\n     */\n    public static <T> T newParameterizedInstance(String className, Object... params)\n            throws ClassNotFoundException {\n        Class<?>[] argTypes = new Class<?>[params.length / 2];\n        Object[] args = new Object[params.length / 2];\n        try {\n            Class<?> c = Utils.loadClass(className, Object.class);\n            for (int i = 0; i < params.length / 2; i++) {\n                argTypes[i] = (Class<?>) params[2 * i];\n                args[i] = params[(2 * i) + 1];\n            }\n            @SuppressWarnings(\"unchecked\")\n            Constructor<T> constructor = (Constructor<T>) c.getConstructor(argTypes);\n            return constructor.newInstance(args);\n        } catch (NoSuchMethodException e) {\n            throw new ClassNotFoundException(String.format(\"Failed to find \" +\n                \"constructor with %s for %s\", Arrays.stream(argTypes).map(Object::toString).collect(Collectors.joining(\", \")), className), e);\n        } catch (InstantiationException e) {\n            throw new ClassNotFoundException(String.format(\"Failed to instantiate \" +\n                \"%s\", className), e);\n        } catch (IllegalAccessException e) {\n            throw new ClassNotFoundException(String.format(\"Unable to access \" +\n                \"constructor of %s\", className), e);\n        } catch (InvocationTargetException e) {\n            throw new KafkaException(String.format(\"The constructor of %s threw an exception\", className), e.getCause());\n        }\n    }\n\n    /**\n     * Generates 32 bit murmur2 hash from byte array\n     * @param data byte array to hash\n     * @return 32 bit hash of the given array\n     */\n    @SuppressWarnings(\"fallthrough\")\n    public static int murmur2(final byte[] data) {\n        int length = data.length;\n        int seed = 0x9747b28c;\n        // 'm' and 'r' are mixing constants generated offline.\n        // They're not really 'magic', they just happen to work well.\n        final int m = 0x5bd1e995;\n        final int r = 24;\n\n        // Initialize the hash to a random value\n        int h = seed ^ length;\n        int length4 = length / 4;\n\n        for (int i = 0; i < length4; i++) {\n            final int i4 = i * 4;\n            int k = (data[i4 + 0] & 0xff) + ((data[i4 + 1] & 0xff) << 8) + ((data[i4 + 2] & 0xff) << 16) + ((data[i4 + 3] & 0xff) << 24);\n            k *= m;\n            k ^= k >>> r;\n            k *= m;\n            h *= m;\n            h ^= k;\n        }\n\n        // Handle the last few bytes of the input array\n        switch (length % 4) {\n            case 3:\n                h ^= (data[(length & ~3) + 2] & 0xff) << 16;\n            case 2:\n                h ^= (data[(length & ~3) + 1] & 0xff) << 8;\n            case 1:\n                h ^= data[length & ~3] & 0xff;\n                h *= m;\n        }\n\n        h ^= h >>> 13;\n        h *= m;\n        h ^= h >>> 15;\n\n        return h;\n    }\n\n    /**\n     * Extracts the hostname from a \"host:port\" address string.\n     * @param address address string to parse\n     * @return hostname or null if the given address is incorrect\n     */\n    public static String getHost(String address) {\n        Matcher matcher = HOST_PORT_PATTERN.matcher(address);\n        return matcher.matches() ? matcher.group(1) : null;\n    }\n\n    /**\n     * Extracts the port number from a \"host:port\" address string.\n     * @param address address string to parse\n     * @return port number or null if the given address is incorrect\n     */\n    public static Integer getPort(String address) {\n        Matcher matcher = HOST_PORT_PATTERN.matcher(address);\n        return matcher.matches() ? Integer.parseInt(matcher.group(2)) : null;\n    }\n\n    /**\n     * Basic validation of the supplied address. checks for valid characters\n     * @param address hostname string to validate\n     * @return true if address contains valid characters\n     */\n    public static boolean validHostPattern(String address) {\n        return VALID_HOST_CHARACTERS.matcher(address).matches();\n    }\n\n    /**\n     * Formats hostname and port number as a \"host:port\" address string,\n     * surrounding IPv6 addresses with braces '[', ']'\n     * @param host hostname\n     * @param port port number\n     * @return address string\n     */\n    public static String formatAddress(String host, Integer port) {\n        return host.contains(\":\")\n                ? \"[\" + host + \"]:\" + port // IPv6\n                : host + \":\" + port;\n    }\n\n    /**\n     * Formats a byte number as a human-readable String (\"3.2 MB\")\n     * @param bytes some size in bytes\n     * @return\n     */\n    public static String formatBytes(long bytes) {\n        if (bytes < 0) {\n            return String.valueOf(bytes);\n        }\n        double asDouble = (double) bytes;\n        int ordinal = (int) Math.floor(Math.log(asDouble) / Math.log(1024.0));\n        double scale = Math.pow(1024.0, ordinal);\n        double scaled = asDouble / scale;\n        String formatted = TWO_DIGIT_FORMAT.format(scaled);\n        try {\n            return formatted + \" \" + BYTE_SCALE_SUFFIXES[ordinal];\n        } catch (IndexOutOfBoundsException e) {\n            //huge number?\n            return String.valueOf(asDouble);\n        }\n    }\n\n    /**\n     *  Converts a {@code Map} class into a string, concatenating keys and values\n     *  Example:\n     *      {@code mkString({ key: \"hello\", keyTwo: \"hi\" }, \"|START|\", \"|END|\", \"=\", \",\")\n     *          => \"|START|key=hello,keyTwo=hi|END|\"}\n     */\n    public static <K, V> String mkString(Map<K, V> map, String begin, String end,\n                                         String keyValueSeparator, String elementSeparator) {\n        StringBuilder bld = new StringBuilder();\n        bld.append(begin);\n        String prefix = \"\";\n        for (Map.Entry<K, V> entry : map.entrySet()) {\n            bld.append(prefix).append(entry.getKey()).\n                    append(keyValueSeparator).append(entry.getValue());\n            prefix = elementSeparator;\n        }\n        bld.append(end);\n        return bld.toString();\n    }\n\n    /**\n     *  Converts an extensions string into a {@code Map<String, String>}.\n     *\n     *  Example:\n     *      {@code parseMap(\"key=hey,keyTwo=hi,keyThree=hello\", \"=\", \",\") => { key: \"hey\", keyTwo: \"hi\", keyThree: \"hello\" }}\n     *\n     */\n    public static Map<String, String> parseMap(String mapStr, String keyValueSeparator, String elementSeparator) {\n        Map<String, String> map = new HashMap<>();\n\n        if (!mapStr.isEmpty()) {\n            String[] attrvals = mapStr.split(elementSeparator);\n            for (String attrval : attrvals) {\n                String[] array = attrval.split(keyValueSeparator, 2);\n                map.put(array[0], array[1]);\n            }\n        }\n        return map;\n    }\n\n    /**\n     * Read a properties file from the given path\n     * @param filename The path of the file to read\n     * @return the loaded properties\n     */\n    public static Properties loadProps(String filename) throws IOException {\n        return loadProps(filename, null);\n    }\n\n    /**\n     * Read a properties file from the given path\n     * @param filename The path of the file to read\n     * @param onlyIncludeKeys When non-null, only return values associated with these keys and ignore all others\n     * @return the loaded properties\n     */\n    public static Properties loadProps(String filename, List<String> onlyIncludeKeys) throws IOException {\n        Properties props = new Properties();\n\n        if (filename != null) {\n            try (InputStream propStream = Files.newInputStream(Paths.get(filename))) {\n                props.load(propStream);\n            }\n        } else {\n            System.out.println(\"Did not load any properties since the property file is not specified\");\n        }\n\n        if (onlyIncludeKeys == null || onlyIncludeKeys.isEmpty())\n            return props;\n        Properties requestedProps = new Properties();\n        onlyIncludeKeys.forEach(key -> {\n            String value = props.getProperty(key);\n            if (value != null)\n                requestedProps.setProperty(key, value);\n        });\n        return requestedProps;\n    }\n\n    /**\n     * Converts a Properties object to a Map<String, String>, calling {@link #toString} to ensure all keys and values\n     * are Strings.\n     */\n    public static Map<String, String> propsToStringMap(Properties props) {\n        Map<String, String> result = new HashMap<>();\n        for (Map.Entry<Object, Object> entry : props.entrySet())\n            result.put(entry.getKey().toString(), entry.getValue().toString());\n        return result;\n    }\n\n    /**\n     * Get the stack trace from an exception as a string\n     */\n    public static String stackTrace(Throwable e) {\n        StringWriter sw = new StringWriter();\n        PrintWriter pw = new PrintWriter(sw);\n        e.printStackTrace(pw);\n        return sw.toString();\n    }\n\n    /**\n     * Read a buffer into a Byte array for the given offset and length\n     */\n    public static byte[] readBytes(ByteBuffer buffer, int offset, int length) {\n        byte[] dest = new byte[length];\n        if (buffer.hasArray()) {\n            System.arraycopy(buffer.array(), buffer.arrayOffset() + offset, dest, 0, length);\n        } else {\n            buffer.mark();\n            buffer.position(offset);\n            buffer.get(dest);\n            buffer.reset();\n        }\n        return dest;\n    }\n\n    /**\n     * Read the given byte buffer into a Byte array\n     */\n    public static byte[] readBytes(ByteBuffer buffer) {\n        return Utils.readBytes(buffer, 0, buffer.limit());\n    }\n\n    /**\n     * Reads bytes from a source buffer and returns a new buffer.\n     * <p> The content of the new buffer will start at this buffer's current\n     * position.  Changes to this buffer's content will be visible in the new\n     * buffer, and vice versa; the two buffers' position, limit, and mark\n     * values will be independent.\n     *\n     * <p> The new buffer's position will be zero, its limit will be the number of bytes\n     * read i.e. <code>bytesToRead</code>, it's capacity will be the number of bytes remaining in\n     * source buffer , its mark will be undefined, and its byte order will be {@link ByteOrder#BIG_ENDIAN BIG_ENDIAN}.\n     *\n     * <p>Since JDK 13, this method could be replaced with slice(int index, int length).\n     *\n     * @param srcBuf Source buffer where data is read from\n     * @param bytesToRead Number of bytes to read\n     * @return Destination buffer or null if bytesToRead is < 0\n     *\n     * @see ByteBuffer#slice()\n     */\n    public static ByteBuffer readBytes(ByteBuffer srcBuf, int bytesToRead) {\n        if (bytesToRead < 0)\n            return null;\n\n        final ByteBuffer dstBuf = srcBuf.slice();\n        dstBuf.limit(bytesToRead);\n        srcBuf.position(srcBuf.position() + bytesToRead);\n\n        return dstBuf;\n    }\n\n    /**\n     * Read a file as string and return the content. The file is treated as a stream and no seek is performed.\n     * This allows the program to read from a regular file as well as from a pipe/fifo.\n     */\n    public static String readFileAsString(String path) throws IOException {\n        try {\n            byte[] allBytes = Files.readAllBytes(Paths.get(path));\n            return new String(allBytes, StandardCharsets.UTF_8);\n        } catch (IOException ex) {\n            throw new IOException(\"Unable to read file \" + path, ex);\n        }\n    }\n\n    /**\n     * Check if the given ByteBuffer capacity\n     * @param existingBuffer ByteBuffer capacity to check\n     * @param newLength new length for the ByteBuffer.\n     * returns ByteBuffer\n     */\n    public static ByteBuffer ensureCapacity(ByteBuffer existingBuffer, int newLength) {\n        if (newLength > existingBuffer.capacity()) {\n            ByteBuffer newBuffer = ByteBuffer.allocate(newLength);\n            existingBuffer.flip();\n            newBuffer.put(existingBuffer);\n            return newBuffer;\n        }\n        return existingBuffer;\n    }\n\n    /**\n     * Creates a set\n     * @param elems the elements\n     * @param <T> the type of element\n     * @return Set\n     */\n    @SafeVarargs\n    public static <T> Set<T> mkSet(T... elems) {\n        Set<T> result = new HashSet<>((int) (elems.length / 0.75) + 1);\n        for (T elem : elems)\n            result.add(elem);\n        return result;\n    }\n\n    /**\n     * Creates a sorted set\n     * @param elems the elements\n     * @param <T> the type of element, must be comparable\n     * @return SortedSet\n     */\n    @SafeVarargs\n    public static <T extends Comparable<T>> SortedSet<T> mkSortedSet(T... elems) {\n        SortedSet<T> result = new TreeSet<>();\n        for (T elem : elems)\n            result.add(elem);\n        return result;\n    }\n\n    /**\n     * Creates a map entry (for use with {@link Utils#mkMap(java.util.Map.Entry[])})\n     *\n     * @param k   The key\n     * @param v   The value\n     * @param <K> The key type\n     * @param <V> The value type\n     * @return An entry\n     */\n    public static <K, V> Map.Entry<K, V> mkEntry(final K k, final V v) {\n        return new AbstractMap.SimpleEntry<>(k, v);\n    }\n\n    /**\n     * Creates a map from a sequence of entries\n     *\n     * @param entries The entries to map\n     * @param <K>     The key type\n     * @param <V>     The value type\n     * @return A map\n     */\n    @SafeVarargs\n    public static <K, V> Map<K, V> mkMap(final Map.Entry<K, V>... entries) {\n        final LinkedHashMap<K, V> result = new LinkedHashMap<>();\n        for (final Map.Entry<K, V> entry : entries) {\n            result.put(entry.getKey(), entry.getValue());\n        }\n        return result;\n    }\n\n    /**\n     * Creates a {@link Properties} from a map\n     *\n     * @param properties A map of properties to add\n     * @return The properties object\n     */\n    public static Properties mkProperties(final Map<String, String> properties) {\n        final Properties result = new Properties();\n        for (final Map.Entry<String, String> entry : properties.entrySet()) {\n            result.setProperty(entry.getKey(), entry.getValue());\n        }\n        return result;\n    }\n\n    /**\n     * Creates a {@link Properties} from a map\n     *\n     * @param properties A map of properties to add\n     * @return The properties object\n     */\n    public static Properties mkObjectProperties(final Map<String, Object> properties) {\n        final Properties result = new Properties();\n        for (final Map.Entry<String, Object> entry : properties.entrySet()) {\n            result.put(entry.getKey(), entry.getValue());\n        }\n        return result;\n    }\n\n    /**\n     * Recursively delete the given file/directory and any subfiles (if any exist)\n     *\n     * @param rootFile The root file at which to begin deleting\n     */\n    public static void delete(final File rootFile) throws IOException {\n        if (rootFile == null)\n            return;\n        Files.walkFileTree(rootFile.toPath(), new SimpleFileVisitor<Path>() {\n            @Override\n            public FileVisitResult visitFileFailed(Path path, IOException exc) throws IOException {\n                if (exc instanceof NoSuchFileException) {\n                    if (path.toFile().equals(rootFile)) {\n                        // If the root path did not exist, ignore the error and terminate;\n                        return FileVisitResult.TERMINATE;\n                    } else {\n                        // Otherwise, just continue walking as the file might already be deleted by other threads.\n                        return FileVisitResult.CONTINUE;\n                    }\n                }\n                throw exc;\n            }\n\n            @Override\n            public FileVisitResult visitFile(Path path, BasicFileAttributes attrs) throws IOException {\n                Files.deleteIfExists(path);\n                return FileVisitResult.CONTINUE;\n            }\n\n            @Override\n            public FileVisitResult postVisitDirectory(Path path, IOException exc) throws IOException {\n                // KAFKA-8999: if there's an exception thrown previously already, we should throw it\n                if (exc != null) {\n                    throw exc;\n                }\n\n                Files.deleteIfExists(path);\n                return FileVisitResult.CONTINUE;\n            }\n        });\n    }\n\n    /**\n     * Returns an empty collection if this list is null\n     * @param other\n     * @return\n     */\n    public static <T> List<T> safe(List<T> other) {\n        return other == null ? Collections.emptyList() : other;\n    }\n\n   /**\n    * Get the ClassLoader which loaded Kafka.\n    */\n    public static ClassLoader getKafkaClassLoader() {\n        return Utils.class.getClassLoader();\n    }\n\n    /**\n     * Get the Context ClassLoader on this thread or, if not present, the ClassLoader that\n     * loaded Kafka.\n     *\n     * This should be used whenever passing a ClassLoader to Class.forName\n     */\n    public static ClassLoader getContextOrKafkaClassLoader() {\n        ClassLoader cl = Thread.currentThread().getContextClassLoader();\n        if (cl == null)\n            return getKafkaClassLoader();\n        else\n            return cl;\n    }\n\n    /**\n     * Attempts to move source to target atomically and falls back to a non-atomic move if it fails.\n     * This function also flushes the parent directory to guarantee crash consistency.\n     *\n     * @throws IOException if both atomic and non-atomic moves fail, or parent dir flush fails.\n     */\n    public static void atomicMoveWithFallback(Path source, Path target) throws IOException {\n        atomicMoveWithFallback(source, target, true);\n    }\n\n    /**\n     * Attempts to move source to target atomically and falls back to a non-atomic move if it fails.\n     * This function allows callers to decide whether to flush the parent directory. This is needed\n     * when a sequence of atomicMoveWithFallback is called for the same directory and we don't want\n     * to repeatedly flush the same parent directory.\n     *\n     * @throws IOException if both atomic and non-atomic moves fail,\n     * or parent dir flush fails if needFlushParentDir is true.\n     */\n    public static void atomicMoveWithFallback(Path source, Path target, boolean needFlushParentDir) throws IOException {\n        try {\n            Files.move(source, target, StandardCopyOption.ATOMIC_MOVE);\n        } catch (IOException outer) {\n            try {\n                log.warn(\"Failed atomic move of {} to {} retrying with a non-atomic move\", source, target, outer);\n                Files.move(source, target, StandardCopyOption.REPLACE_EXISTING);\n                log.debug(\"Non-atomic move of {} to {} succeeded after atomic move failed\", source, target);\n            } catch (IOException inner) {\n                inner.addSuppressed(outer);\n                throw inner;\n            }\n        } finally {\n            if (needFlushParentDir) {\n                flushDir(target.toAbsolutePath().normalize().getParent());\n            }\n        }\n    }\n\n    /**\n     * Flushes dirty directories to guarantee crash consistency.\n     *\n     * Note: We don't fsync directories on Windows OS because otherwise it'll throw AccessDeniedException (KAFKA-13391)\n     *\n     * @throws IOException if flushing the directory fails.\n     */\n    public static void flushDir(Path path) throws IOException {\n        if (path != null && !OperatingSystem.IS_WINDOWS && !OperatingSystem.IS_ZOS) {\n            try (FileChannel dir = FileChannel.open(path, StandardOpenOption.READ)) {\n                dir.force(true);\n            }\n        }\n    }\n\n    /**\n     * Flushes dirty directories to guarantee crash consistency with swallowing {@link NoSuchFileException}\n     *\n     * @throws IOException if flushing the directory fails.\n     */\n    public static void flushDirIfExists(Path path) throws IOException {\n        try {\n            flushDir(path);\n        } catch (NoSuchFileException e) {\n            log.warn(\"Failed to flush directory {}\", path);\n        }\n    }\n\n    /**\n     * Flushes dirty file with swallowing {@link NoSuchFileException}\n     */\n    public static void flushFileIfExists(Path path) throws IOException {\n        try (FileChannel fileChannel = FileChannel.open(path, StandardOpenOption.READ)) {\n            fileChannel.force(true);\n        } catch (NoSuchFileException e) {\n            log.warn(\"Failed to flush file {}\", path, e);\n        }\n    }\n\n    /**\n     * Closes all the provided closeables.\n     * @throws IOException if any of the close methods throws an IOException.\n     *         The first IOException is thrown with subsequent exceptions\n     *         added as suppressed exceptions.\n     */\n    public static void closeAll(Closeable... closeables) throws IOException {\n        IOException exception = null;\n        for (Closeable closeable : closeables) {\n            try {\n                if (closeable != null)\n                    closeable.close();\n            } catch (IOException e) {\n                if (exception != null)\n                    exception.addSuppressed(e);\n                else\n                    exception = e;\n            }\n        }\n        if (exception != null)\n            throw exception;\n    }\n\n    @FunctionalInterface\n    public interface SwallowAction {\n        void run() throws Throwable;\n    }\n\n    public static void swallow(final Logger log, final Level level, final String what, final SwallowAction code) {\n        swallow(log, level, what, code, null);\n    }\n\n    /**\n     * Run the supplied code. If an exception is thrown, it is swallowed and registered to the firstException parameter.\n     */\n    public static void swallow(final Logger log, final Level level, final String what, final SwallowAction code,\n                               final AtomicReference<Throwable> firstException) {\n        if (code != null) {\n            try {\n                code.run();\n            } catch (Throwable t) {\n                switch (level) {\n                    case INFO:\n                        log.info(what, t);\n                        break;\n                    case DEBUG:\n                        log.debug(what, t);\n                        break;\n                    case ERROR:\n                        log.error(what, t);\n                        break;\n                    case TRACE:\n                        log.trace(what, t);\n                        break;\n                    case WARN:\n                    default:\n                        log.warn(what, t);\n                }\n                if (firstException != null)\n                    firstException.compareAndSet(null, t);\n            }\n        }\n    }\n\n    /**\n     * An {@link AutoCloseable} interface without a throws clause in the signature\n     *\n     * This is used with lambda expressions in try-with-resources clauses\n     * to avoid casting un-checked exceptions to checked exceptions unnecessarily.\n     */\n    @FunctionalInterface\n    public interface UncheckedCloseable extends AutoCloseable {\n        @Override\n        void close();\n    }\n\n    /**\n     * Closes {@code maybeCloseable} if it implements the {@link AutoCloseable} interface,\n     * and if an exception is thrown, it is logged at the WARN level.\n     */\n    public static void maybeCloseQuietly(Object maybeCloseable, String name) {\n        if (maybeCloseable instanceof AutoCloseable)\n            closeQuietly((AutoCloseable) maybeCloseable, name);\n    }\n\n    /**\n     * Closes {@code closeable} and if an exception is thrown, it is logged at the WARN level.\n     * <b>Be cautious when passing method references as an argument.</b> For example:\n     * <p>\n     * {@code closeQuietly(task::stop, \"source task\");}\n     * <p>\n     * Although this method gracefully handles null {@link AutoCloseable} objects, attempts to take a method\n     * reference from a null object will result in a {@link NullPointerException}. In the example code above,\n     * it would be the caller's responsibility to ensure that {@code task} was non-null before attempting to\n     * use a method reference from it.\n     */\n    public static void closeQuietly(AutoCloseable closeable, String name) {\n        closeQuietly(closeable, name, log);\n    }\n\n    /**\n     * Closes {@code closeable} and if an exception is thrown, it is logged with the provided logger at the WARN level.\n     * <b>Be cautious when passing method references as an argument.</b> For example:\n     * <p>\n     * {@code closeQuietly(task::stop, \"source task\");}\n     * <p>\n     * Although this method gracefully handles null {@link AutoCloseable} objects, attempts to take a method\n     * reference from a null object will result in a {@link NullPointerException}. In the example code above,\n     * it would be the caller's responsibility to ensure that {@code task} was non-null before attempting to\n     * use a method reference from it.\n     */\n    public static void closeQuietly(AutoCloseable closeable, String name, Logger logger) {\n        if (closeable != null) {\n            try {\n                closeable.close();\n            } catch (Throwable t) {\n                logger.warn(\"Failed to close {} with type {}\", name, closeable.getClass().getName(), t);\n            }\n        }\n    }\n\n    /**\n    * Closes {@code closeable} and if an exception is thrown, it is registered to the firstException parameter.\n    * <b>Be cautious when passing method references as an argument.</b> For example:\n    * <p>\n    * {@code closeQuietly(task::stop, \"source task\");}\n    * <p>\n    * Although this method gracefully handles null {@link AutoCloseable} objects, attempts to take a method\n    * reference from a null object will result in a {@link NullPointerException}. In the example code above,\n    * it would be the caller's responsibility to ensure that {@code task} was non-null before attempting to\n    * use a method reference from it.\n    */\n    public static void closeQuietly(AutoCloseable closeable, String name, AtomicReference<Throwable> firstException) {\n        if (closeable != null) {\n            try {\n                closeable.close();\n            } catch (Throwable t) {\n                firstException.compareAndSet(null, t);\n                log.error(\"Failed to close {} with type {}\", name, closeable.getClass().getName(), t);\n            }\n        }\n    }\n\n    /**\n     * close all closable objects even if one of them throws exception.\n     * @param firstException keeps the first exception\n     * @param name message of closing those objects\n     * @param closeables closable objects\n     */\n    public static void closeAllQuietly(AtomicReference<Throwable> firstException, String name, AutoCloseable... closeables) {\n        for (AutoCloseable closeable : closeables) closeQuietly(closeable, name, firstException);\n    }\n\n    /**\n     * Invokes every function in `all` even if one or more functions throws an exception.\n     *\n     * If any of the functions throws an exception, the first one will be rethrown at the end with subsequent exceptions\n     * added as suppressed exceptions.\n     */\n    // Note that this is a generalised version of `closeAll`. We could potentially make it more general by\n    // changing the signature to `public <R> List<R> tryAll(all: List[Callable<R>])`\n    public static void tryAll(List<Callable<Void>> all) throws Throwable {\n        Throwable exception = null;\n        for (Callable<Void> call : all) {\n            try {\n                call.call();\n            } catch (Throwable t) {\n                if (exception != null)\n                    exception.addSuppressed(t);\n                else\n                    exception = t;\n            }\n        }\n        if (exception != null)\n            throw exception;\n    }\n\n    /**\n     * A cheap way to deterministically convert a number to a positive value. When the input is\n     * positive, the original value is returned. When the input number is negative, the returned\n     * positive value is the original value bit AND against 0x7fffffff which is not its absolute\n     * value.\n     *\n     * Note: changing this method in the future will possibly cause partition selection not to be\n     * compatible with the existing messages already placed on a partition since it is used\n     * in producer's partition selection logic {@link org.apache.kafka.clients.producer.KafkaProducer}\n     *\n     * @param number a given number\n     * @return a positive number.\n     */\n    public static int toPositive(int number) {\n        return number & 0x7fffffff;\n    }\n\n    /**\n     * Read a size-delimited byte buffer starting at the given offset.\n     * @param buffer Buffer containing the size and data\n     * @param start Offset in the buffer to read from\n     * @return A slice of the buffer containing only the delimited data (excluding the size)\n     */\n    public static ByteBuffer sizeDelimited(ByteBuffer buffer, int start) {\n        int size = buffer.getInt(start);\n        if (size < 0) {\n            return null;\n        } else {\n            ByteBuffer b = buffer.duplicate();\n            b.position(start + 4);\n            b = b.slice();\n            b.limit(size);\n            b.rewind();\n            return b;\n        }\n    }\n\n    /**\n     * Read data from the channel to the given byte buffer until there are no bytes remaining in the buffer. If the end\n     * of the file is reached while there are bytes remaining in the buffer, an EOFException is thrown.\n     *\n     * @param channel File channel containing the data to read from\n     * @param destinationBuffer The buffer into which bytes are to be transferred\n     * @param position The file position at which the transfer is to begin; it must be non-negative\n     * @param description A description of what is being read, this will be included in the EOFException if it is thrown\n     *\n     * @throws IllegalArgumentException If position is negative\n     * @throws EOFException If the end of the file is reached while there are remaining bytes in the destination buffer\n     * @throws IOException If an I/O error occurs, see {@link FileChannel#read(ByteBuffer, long)} for details on the\n     * possible exceptions\n     */\n    public static void readFullyOrFail(FileChannel channel, ByteBuffer destinationBuffer, long position,\n                                       String description) throws IOException {\n        if (position < 0) {\n            throw new IllegalArgumentException(\"The file channel position cannot be negative, but it is \" + position);\n        }\n        int expectedReadBytes = destinationBuffer.remaining();\n        readFully(channel, destinationBuffer, position);\n        if (destinationBuffer.hasRemaining()) {\n            throw new EOFException(String.format(\"Failed to read `%s` from file channel `%s`. Expected to read %d bytes, \" +\n                    \"but reached end of file after reading %d bytes. Started read from position %d.\",\n                    description, channel, expectedReadBytes, expectedReadBytes - destinationBuffer.remaining(), position));\n        }\n    }\n\n    /**\n     * Read data from the channel to the given byte buffer until there are no bytes remaining in the buffer or the end\n     * of the file has been reached.\n     *\n     * @param channel File channel containing the data to read from\n     * @param destinationBuffer The buffer into which bytes are to be transferred\n     * @param position The file position at which the transfer is to begin; it must be non-negative\n     *\n     * @throws IllegalArgumentException If position is negative\n     * @throws IOException If an I/O error occurs, see {@link FileChannel#read(ByteBuffer, long)} for details on the\n     * possible exceptions\n     */\n    public static void readFully(FileChannel channel, ByteBuffer destinationBuffer, long position) throws IOException {\n        if (position < 0) {\n            throw new IllegalArgumentException(\"The file channel position cannot be negative, but it is \" + position);\n        }\n        long currentPosition = position;\n        int bytesRead;\n        do {\n            bytesRead = channel.read(destinationBuffer, currentPosition);\n            currentPosition += bytesRead;\n        } while (bytesRead != -1 && destinationBuffer.hasRemaining());\n    }\n\n    /**\n     * Read data from the input stream to the given byte buffer until there are no bytes remaining in the buffer or the\n     * end of the stream has been reached.\n     *\n     * @param inputStream       Input stream to read from\n     * @param destinationBuffer The buffer into which bytes are to be transferred (it must be backed by an array)\n     * @return number of byte read from the input stream\n     * @throws IOException If an I/O error occurs\n     */\n    public static int readFully(InputStream inputStream, ByteBuffer destinationBuffer) throws IOException {\n        if (!destinationBuffer.hasArray())\n            throw new IllegalArgumentException(\"destinationBuffer must be backed by an array\");\n        int initialOffset = destinationBuffer.arrayOffset() + destinationBuffer.position();\n        byte[] array = destinationBuffer.array();\n        int length = destinationBuffer.remaining();\n        int totalBytesRead = 0;\n        do {\n            int bytesRead = inputStream.read(array, initialOffset + totalBytesRead, length - totalBytesRead);\n            if (bytesRead == -1)\n                break;\n            totalBytesRead += bytesRead;\n        } while (length > totalBytesRead);\n        destinationBuffer.position(destinationBuffer.position() + totalBytesRead);\n        return totalBytesRead;\n    }\n\n    public static void writeFully(FileChannel channel, ByteBuffer sourceBuffer) throws IOException {\n        while (sourceBuffer.hasRemaining())\n            channel.write(sourceBuffer);\n    }\n\n    /**\n     * Trying to write data in source buffer to a {@link TransferableChannel}, we may need to call this method multiple\n     * times since this method doesn't ensure the data in the source buffer can be fully written to the destination channel.\n     *\n     * @param destChannel The destination channel\n     * @param position From which the source buffer will be written\n     * @param length The max size of bytes can be written\n     * @param sourceBuffer The source buffer\n     *\n     * @return The length of the actual written data\n     * @throws IOException If an I/O error occurs\n     */\n    public static int tryWriteTo(TransferableChannel destChannel,\n                                  int position,\n                                  int length,\n                                  ByteBuffer sourceBuffer) throws IOException {\n\n        ByteBuffer dup = sourceBuffer.duplicate();\n        dup.position(position);\n        dup.limit(position + length);\n        return destChannel.write(dup);\n    }\n\n    /**\n     * Write the contents of a buffer to an output stream. The bytes are copied from the current position\n     * in the buffer.\n     * @param out The output to write to\n     * @param buffer The buffer to write from\n     * @param length The number of bytes to write\n     * @throws IOException For any errors writing to the output\n     */\n    public static void writeTo(DataOutput out, ByteBuffer buffer, int length) throws IOException {\n        if (buffer.hasArray()) {\n            out.write(buffer.array(), buffer.position() + buffer.arrayOffset(), length);\n        } else {\n            int pos = buffer.position();\n            for (int i = pos; i < length + pos; i++)\n                out.writeByte(buffer.get(i));\n        }\n    }\n\n    public static <T> List<T> toList(Iterable<T> iterable) {\n        return toList(iterable.iterator());\n    }\n\n    public static <T> List<T> toList(Iterator<T> iterator) {\n        List<T> res = new ArrayList<>();\n        while (iterator.hasNext())\n            res.add(iterator.next());\n        return res;\n    }\n\n    public static <T> List<T> toList(Iterator<T> iterator, Predicate<T> predicate) {\n        List<T> res = new ArrayList<>();\n        while (iterator.hasNext()) {\n            T e = iterator.next();\n            if (predicate.test(e)) {\n                res.add(e);\n            }\n        }\n        return res;\n    }\n\n    public static int to32BitField(final Set<Byte> bytes) {\n        int value = 0;\n        for (final byte b : bytes)\n            value |= 1 << checkRange(b);\n        return value;\n    }\n\n    private static byte checkRange(final byte i) {\n        if (i > 31)\n            throw new IllegalArgumentException(\"out of range: i>31, i = \" + i);\n        if (i < 0)\n            throw new IllegalArgumentException(\"out of range: i<0, i = \" + i);\n        return i;\n    }\n\n    public static Set<Byte> from32BitField(final int intValue) {\n        Set<Byte> result = new HashSet<>();\n        for (int itr = intValue, count = 0; itr != 0; itr >>>= 1) {\n            if ((itr & 1) != 0)\n                result.add((byte) count);\n            count++;\n        }\n        return result;\n    }\n\n    /**\n     * A Collector that offers two kinds of convenience:\n     * 1. You can specify the concrete type of the returned Map\n     * 2. You can turn a stream of Entries directly into a Map without having to mess with a key function\n     *    and a value function. In particular, this is handy if all you need to do is apply a filter to a Map's entries.\n     *\n     *\n     * One thing to be wary of: These types are too \"distant\" for IDE type checkers to warn you if you\n     * try to do something like build a TreeMap of non-Comparable elements. You'd get a runtime exception for that.\n     *\n     * @param mapSupplier The constructor for your concrete map type.\n     * @param <K> The Map key type\n     * @param <V> The Map value type\n     * @param <M> The type of the Map itself.\n     * @return new Collector<Map.Entry<K, V>, M, M>\n     */\n    public static <K, V, M extends Map<K, V>> Collector<Map.Entry<K, V>, M, M> entriesToMap(final Supplier<M> mapSupplier) {\n        return new Collector<Map.Entry<K, V>, M, M>() {\n            @Override\n            public Supplier<M> supplier() {\n                return mapSupplier;\n            }\n\n            @Override\n            public BiConsumer<M, Map.Entry<K, V>> accumulator() {\n                return (map, entry) -> map.put(entry.getKey(), entry.getValue());\n            }\n\n            @Override\n            public BinaryOperator<M> combiner() {\n                return (map, map2) -> {\n                    map.putAll(map2);\n                    return map;\n                };\n            }\n\n            @Override\n            public Function<M, M> finisher() {\n                return map -> map;\n            }\n\n            @Override\n            public Set<Characteristics> characteristics() {\n                return EnumSet.of(Characteristics.UNORDERED, Characteristics.IDENTITY_FINISH);\n            }\n        };\n    }\n\n    @SafeVarargs\n    public static <E> Set<E> union(final Supplier<Set<E>> constructor, final Set<E>... set) {\n        final Set<E> result = constructor.get();\n        for (final Set<E> s : set) {\n            result.addAll(s);\n        }\n        return result;\n    }\n\n    @SafeVarargs\n    public static <E> Set<E> intersection(final Supplier<Set<E>> constructor, final Set<E> first, final Set<E>... set) {\n        final Set<E> result = constructor.get();\n        result.addAll(first);\n        for (final Set<E> s : set) {\n            result.retainAll(s);\n        }\n        return result;\n    }\n\n    public static <E> Set<E> diff(final Supplier<Set<E>> constructor, final Set<E> left, final Set<E> right) {\n        final Set<E> result = constructor.get();\n        result.addAll(left);\n        result.removeAll(right);\n        return result;\n    }\n\n    public static <K, V> Map<K, V> filterMap(final Map<K, V> map, final Predicate<Entry<K, V>> filterPredicate) {\n        return map.entrySet().stream().filter(filterPredicate).collect(Collectors.toMap(Entry::getKey, Entry::getValue));\n    }\n\n    /**\n     * Convert a properties to map. All keys in properties must be string type. Otherwise, a ConfigException is thrown.\n     * @param properties to be converted\n     * @return a map including all elements in properties\n     */\n    public static Map<String, Object> propsToMap(Properties properties) {\n        return castToStringObjectMap(properties);\n    }\n\n    /**\n     * Cast a map with arbitrary type keys to be keyed on String.\n     * @param inputMap A map with unknown type keys\n     * @return A map with the same contents as the input map, but with String keys\n     * @throws ConfigException if any key is not a String\n     */\n    public static Map<String, Object> castToStringObjectMap(Map<?, ?> inputMap) {\n        Map<String, Object> map = new HashMap<>(inputMap.size());\n        for (Map.Entry<?, ?> entry : inputMap.entrySet()) {\n            if (entry.getKey() instanceof String) {\n                String k = (String) entry.getKey();\n                map.put(k, entry.getValue());\n            } else {\n                throw new ConfigException(String.valueOf(entry.getKey()), entry.getValue(), \"Key must be a string.\");\n            }\n        }\n        return map;\n    }\n\n    /**\n     * Convert timestamp to an epoch value\n     * @param timestamp the timestamp to be converted, the accepted formats are:\n     *                 (1) yyyy-MM-dd'T'HH:mm:ss.SSS, ex: 2020-11-10T16:51:38.198\n     *                 (2) yyyy-MM-dd'T'HH:mm:ss.SSSZ, ex: 2020-11-10T16:51:38.198+0800\n     *                 (3) yyyy-MM-dd'T'HH:mm:ss.SSSX, ex: 2020-11-10T16:51:38.198+08\n     *                 (4) yyyy-MM-dd'T'HH:mm:ss.SSSXX, ex: 2020-11-10T16:51:38.198+0800\n     *                 (5) yyyy-MM-dd'T'HH:mm:ss.SSSXXX, ex: 2020-11-10T16:51:38.198+08:00\n     *\n     * @return epoch value of a given timestamp (i.e. the number of milliseconds since January 1, 1970, 00:00:00 GMT)\n     * @throws ParseException for timestamp that doesn't follow ISO8601 format or the format is not expected\n     */\n    public static long getDateTime(String timestamp) throws ParseException, IllegalArgumentException {\n        if (timestamp == null) {\n            throw new IllegalArgumentException(\"Error parsing timestamp with null value\");\n        }\n\n        final String[] timestampParts = timestamp.split(\"T\");\n        if (timestampParts.length < 2) {\n            throw new ParseException(\"Error parsing timestamp. It does not contain a 'T' according to ISO8601 format\", timestamp.length());\n        }\n\n        final String secondPart = timestampParts[1];\n        if (!(secondPart.contains(\"+\") || secondPart.contains(\"-\") || secondPart.contains(\"Z\"))) {\n            timestamp = timestamp + \"Z\";\n        }\n\n        SimpleDateFormat simpleDateFormat = new SimpleDateFormat();\n        // strictly parsing the date/time format\n        simpleDateFormat.setLenient(false);\n        try {\n            simpleDateFormat.applyPattern(\"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\");\n            final Date date = simpleDateFormat.parse(timestamp);\n            return date.getTime();\n        } catch (final ParseException e) {\n            simpleDateFormat.applyPattern(\"yyyy-MM-dd'T'HH:mm:ss.SSSX\");\n            final Date date = simpleDateFormat.parse(timestamp);\n            return date.getTime();\n        }\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    public static <S> Iterator<S> covariantCast(Iterator<? extends S> iterator) {\n        return (Iterator<S>) iterator;\n    }\n\n    /**\n     * Checks if a string is null, empty or whitespace only.\n     * @param str a string to be checked\n     * @return true if the string is null, empty or whitespace only; otherwise, return false.\n     */\n    public static boolean isBlank(String str) {\n        return str == null || str.trim().isEmpty();\n    }\n\n    /**\n     * Get an array containing all of the {@link Object#toString string representations} of a given enumerable type.\n     * @param enumClass the enum class; may not be null\n     * @return an array with the names of every value for the enum class; never null, but may be empty\n     * if there are no values defined for the enum\n     */\n    public static String[] enumOptions(Class<? extends Enum<?>> enumClass) {\n        Objects.requireNonNull(enumClass);\n        if (!enumClass.isEnum()) {\n            throw new IllegalArgumentException(\"Class \" + enumClass + \" is not an enumerable type\");\n        }\n\n        return Stream.of(enumClass.getEnumConstants())\n                .map(Object::toString)\n                .toArray(String[]::new);\n    }\n\n    /**\n     * Ensure that the class is concrete (i.e., not abstract), and that it subclasses a given base class.\n     * If it is abstract or does not subclass the given base class, throw a {@link ConfigException}\n     * with a friendly error message suggesting a list of concrete child subclasses (if any are known).\n     * @param baseClass the expected superclass; may not be null\n     * @param klass the class to check; may not be null\n     * @throws ConfigException if the class is not concrete\n     */\n    public static void ensureConcreteSubclass(Class<?> baseClass, Class<?> klass) {\n        Objects.requireNonNull(baseClass);\n        Objects.requireNonNull(klass);\n\n        if (!baseClass.isAssignableFrom(klass)) {\n            String inheritFrom = baseClass.isInterface() ? \"implement\" : \"extend\";\n            String baseClassType = baseClass.isInterface() ? \"interface\" : \"class\";\n            throw new ConfigException(\"Class \" + klass + \" does not \" + inheritFrom + \" the \" + baseClass.getSimpleName() + \" \" + baseClassType);\n        }\n\n        if (Modifier.isAbstract(klass.getModifiers())) {\n            String childClassNames = Stream.of(klass.getClasses())\n                    .filter(baseClass::isAssignableFrom)\n                    .filter(c -> !Modifier.isAbstract(c.getModifiers()))\n                    .filter(c -> Modifier.isPublic(c.getModifiers()))\n                    .map(Class::getName)\n                    .collect(Collectors.joining(\", \"));\n            String message = \"This class is abstract and cannot be created.\";\n            if (!Utils.isBlank(childClassNames))\n                message += \" Did you mean \" + childClassNames + \"?\";\n            throw new ConfigException(message);\n        }\n    }\n\n    /**\n     * Convert time instant to readable string for logging\n     * @param timestamp the timestamp of the instant to be converted.\n     *\n     * @return string value of a given timestamp in the format \"yyyy-MM-dd HH:mm:ss,SSS\"\n     */\n    public static String toLogDateTimeFormat(long timestamp) {\n        final DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss,SSS XXX\");\n        return Instant.ofEpochMilli(timestamp).atZone(ZoneId.systemDefault()).format(dateTimeFormatter);\n    }\n\n    /**\n     * Replace the given string suffix with the new suffix. If the string doesn't end with the given suffix throw an exception.\n     */\n    public static String replaceSuffix(String str, String oldSuffix, String newSuffix) {\n        if (!str.endsWith(oldSuffix))\n            throw new IllegalArgumentException(\"Expected string to end with \" + oldSuffix + \" but string is \" + str);\n        return str.substring(0, str.length() - oldSuffix.length()) + newSuffix;\n    }\n\n    /**\n     * Find all key/value pairs whose keys begin with the given prefix, and remove that prefix from all\n     * resulting keys.\n     * @param map the map to filter key/value pairs from\n     * @param prefix the prefix to search keys for\n     * @return a {@link Map} containing a key/value pair for every key/value pair in the {@code map}\n     * parameter whose key begins with the given {@code prefix} and whose corresponding keys have\n     * the prefix stripped from them; may be empty, but never null\n     * @param <V> the type of values stored in the map\n     */\n    public static <V> Map<String, V> entriesWithPrefix(Map<String, V> map, String prefix) {\n        return entriesWithPrefix(map, prefix, true);\n    }\n\n    /**\n     * Find all key/value pairs whose keys begin with the given prefix, optionally removing that prefix\n     * from all resulting keys.\n     * @param map the map to filter key/value pairs from\n     * @param prefix the prefix to search keys for\n     * @param strip whether the keys of the returned map should not include the prefix\n     * @return a {@link Map} containing a key/value pair for every key/value pair in the {@code map}\n     * parameter whose key begins with the given {@code prefix}; may be empty, but never null\n     * @param <V> the type of values stored in the map\n     */\n    public static <V> Map<String, V> entriesWithPrefix(Map<String, V> map, String prefix, boolean strip) {\n        return entriesWithPrefix(map, prefix, strip, false);\n    }\n\n    /**\n     * Find all key/value pairs whose keys begin with the given prefix, optionally removing that prefix\n     * from all resulting keys.\n     * @param map the map to filter key/value pairs from\n     * @param prefix the prefix to search keys for\n     * @param strip whether the keys of the returned map should not include the prefix\n     * @param allowMatchingLength whether to include keys that are exactly the same length as the prefix\n     * @return a {@link Map} containing a key/value pair for every key/value pair in the {@code map}\n     * parameter whose key begins with the given {@code prefix}; may be empty, but never null\n     * @param <V> the type of values stored in the map\n     */\n    public static <V> Map<String, V> entriesWithPrefix(Map<String, V> map, String prefix, boolean strip, boolean allowMatchingLength) {\n        Map<String, V> result = new HashMap<>();\n        for (Map.Entry<String, V> entry : map.entrySet()) {\n            if (entry.getKey().startsWith(prefix) && (allowMatchingLength || entry.getKey().length() > prefix.length())) {\n                if (strip)\n                    result.put(entry.getKey().substring(prefix.length()), entry.getValue());\n                else\n                    result.put(entry.getKey(), entry.getValue());\n            }\n        }\n        return result;\n    }\n\n    /**\n     * Checks requirement. Throw {@link IllegalArgumentException} if {@code requirement} failed.\n     * @param requirement Requirement to check.\n     */\n    public static void require(boolean requirement) {\n        if (!requirement)\n            throw new IllegalArgumentException(\"requirement failed\");\n    }\n\n    /**\n     * Checks requirement. Throw {@link IllegalArgumentException} if {@code requirement} failed.\n     * @param requirement Requirement to check.\n     * @param errorMessage String to include in the failure message\n     */\n    public static void require(boolean requirement, String errorMessage) {\n        if (!requirement)\n            throw new IllegalArgumentException(errorMessage);\n    }\n\n    /**\n     * Merge multiple {@link ConfigDef} into one\n     * @param configDefs List of {@link ConfigDef}\n     */\n    public static ConfigDef mergeConfigs(List<ConfigDef> configDefs) {\n        ConfigDef all = new ConfigDef();\n        configDefs.forEach(configDef -> configDef.configKeys().values().forEach(all::define));\n        return all;\n    }\n    /**\n     * A runnable that can throw checked exception.\n     */\n    @FunctionalInterface\n    public interface ThrowingRunnable {\n        void run() throws Exception;\n    }\n}\n",
        "methodName": null,
        "exampleID": 52,
        "dataset": "codeql",
        "filepath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
        "line": 753,
        "sink": "value](1).",
        "source": "-",
        "sourceLine": 753,
        "qualifier": "This path depends on a [user-provided value](1).\nThis path depends on a [user-provided value](2).\nThis path depends on a [user-provided value](3).\nThis path depends on a [user-provided value](4).\nThis path depends on a [user-provided value](5).\nThis path depends on a [user-provided value](6).\nThis path depends on a [user-provided value](7).\nThis path depends on a [user-provided value](8).",
        "line_number": 753,
        "steps": [
            {
                "line": 143,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 53
            },
            {
                "line": 229,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 53
            },
            {
                "line": 251,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 53
            },
            {
                "line": 265,
                "source": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "filepath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java",
                "methodName": null,
                "exampleID": 53
            }
        ]
    }
]