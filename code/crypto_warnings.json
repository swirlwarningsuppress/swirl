[
    {
        "url": "dummy",
        "rawCode": "public class WxPayRefundNotifyResultTest {\n  @Inject\n  private WxPayConfig wxPayConfig;\n\n  /**\n   * Test from xml.\n   *\n   * @throws WxPayException the wx pay exception\n   */\n  public void testFromXML() throws WxPayException {\n    String xmlString = \"<xml>\" +\n      \"<return_code>SUCCESS</return_code>\" +\n      \"<appid><![CDATA[****]]></appid>\" +\n      \"<mch_id><![CDATA[****]]></mch_id>\" +\n      \"<nonce_str><![CDATA[1ee38e38b04990449808688cf3a763b7]]></nonce_str>\" +\n      \"<req_info><![CDATA[q1QZlV5j/4I7CsJ3voq1zDgVAuzNM/Gg5JYHcpMZCLtg9KQlB6vShzsh8tgK60dU6yG2WVa0zeSDlK4B7wJCad1lUUP8Ar0Hm18M1ZEjw5vQU17wMzypRM0M9A4CcRLBezRZYzCka9CAH90E2FZ74y6VRe4DNR87t5n3DWVtSbWTBoaFUexHtNs6pyrqX77VvbilIyLZMv5ZYQYOobbQ1U3kime5He7ShEWZ0GPI3gq+z/ZOLsnIdJ5bsT4kokhq/531hSoZ5006vxRGGXnhJt8IYiG7R+oSQxZOYqYR5SKWF+0z2/g0zzM2QQlT2ynLWvBKvdVCLlgCjFN1DF4z/7IEK5FAISFP0GGF51hYw/LofL3ftlD7h7jvjOIgH5viJ0yFGmGCEFHcLKqg0DPXmzwXIrkgQSSQPsuZ6UbHUUG0L8YTRgLnl2FwNFskJIaNx0179Il6xveR1sCXbwSDGvGN78sFuQMztbnx+gFu6VYgv7C+5pFr87wHFAeuDXGTkVM6ucAwSanP7HuxSVvf7SrSrcovKslyqj869pSqn/AB0atiQ4eoq3kWaOqx87NHOV1st9SQW1SYH7SKz4jd9uhrQyDuPb6KJSg1Z2B4sU4187NjPzL4NpzZySgiYk2yXpWKhCLIz6BdZuWX79zgqxLbGxJJnhyy3tOzRWIlMkDOppGJyh8LO0LOqhXzwyrCYzPA+h2xcr7xN5WIW1IGJSZqHdURUtlemcB+yZivuzARNH0LE2MGUfuoNgZ5j1Osn7K88IrkAyKupcIEmG3ktVnPOd1A9RQ9eWbU+C7yKrl6u5ZRZOX0eElVszKfBFy4tu3XHlT7hd/zMFK5NJt8sE89k5m7M8KCGSgJ+Y90ZnUclQvDVtoR5CFkfqsP9fSpA1L+aKYsl2ESq5+fzcqsYRL3YLEhIipBKKrvg6Gy698oNeG+9oCIyuiFexJDq8ycBZ/AWiR+pFQVbNRaFbfKPR9zCW8gHwYOGnENNY9gABuuENqxxXDx9tEYkACd0H9ezLnu9psC6AuR41ACfo6wGKUA1TnpVEHsDbdvJBWDcw60l1hkmHQN2lYFy+eMusEX]]></req_info></xml>\";\n\n    WxPayRefundNotifyResult refundNotifyResult = WxPayRefundNotifyResult.fromXML(xmlString, this.wxPayConfig.getMchKey());\n\n    assertNotNull(refundNotifyResult);\n    System.out.println(refundNotifyResult);\n  }\n\n  /**\n   * Encode req info.\n   *\n   * @throws Exception the exception\n   */\n  public void encodeReqInfo() throws Exception {\n    String xml = \"<root>\\n\" +\n      \"<out_refund_no><![CDATA[R4001312001201707262674894706_4]]></out_refund_no>\\n\" +\n      \"<out_trade_no><![CDATA[201707260201501501005710775]]></out_trade_no>\\n\" +\n      \"<refund_account><![CDATA[REFUND_SOURCE_UNSETTLED_FUNDS]]></refund_account>\\n\" +\n      \"<refund_fee><![CDATA[15]]></refund_fee>\\n\" +\n      \"<refund_id><![CDATA[50000203702017072601461713166]]></refund_id>\\n\" +\n      \"<refund_recv_accout><![CDATA[\u7528\u6237\u96f6\u94b1]]></refund_recv_accout>\\n\" +\n      \"<refund_request_source><![CDATA[API]]></refund_request_source>\\n\" +\n      \"<refund_status><![CDATA[SUCCESS]]></refund_status>\\n\" +\n      \"<settlement_refund_fee><![CDATA[15]]></settlement_refund_fee>\\n\" +\n      \"<settlement_total_fee><![CDATA[100]]></settlement_total_fee>\\n\" +\n      \"<success_time><![CDATA[2017-07-26 02:45:49]]></success_time>\\n\" +\n      \"<total_fee><![CDATA[100]]></total_fee>\\n\" +\n      \"<transaction_id><![CDATA[4001312001201707262674894706]]></transaction_id>\\n\" +\n      \"</root>\";\n\n    Cipher cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n    final MessageDigest md5 = MessageDigest.getInstance(\"MD5\");\n    md5.update(this.wxPayConfig.getMchKey().getBytes(StandardCharsets.UTF_8));\n    final String keyMd5String = new BigInteger(1, md5.digest()).toString(16).toLowerCase();\n    SecretKeySpec key = new SecretKeySpec(keyMd5String.getBytes(StandardCharsets.UTF_8), \"AES\");\n    cipher.init(Cipher.ENCRYPT_MODE, key);\n    System.out.println(Base64.encodeBase64String(cipher.doFinal(xml.getBytes(StandardCharsets.UTF_8))));\n  }\n\n  /**\n   * Test from xml.\n   * fast mode\n   *\n   * @throws WxPayException the wx pay exception\n   */\n  public void testFromXMLFastMode() throws WxPayException {\n    String xmlString = \"<xml>\" +\n      \"<return_code>SUCCESS</return_code>\" +\n      \"<appid><![CDATA[****]]></appid>\" +\n      \"<mch_id><![CDATA[****]]></mch_id>\" +\n      \"<nonce_str><![CDATA[1ee38e38b04990449808688cf3a763b7]]></nonce_str>\" +\n      \"<req_info><![CDATA[q1QZlV5j/4I7CsJ3voq1zDgVAuzNM/Gg5JYHcpMZCLtg9KQlB6vShzsh8tgK60dU6yG2WVa0zeSDlK4B7wJCad1lUUP8Ar0Hm18M1ZEjw5vQU17wMzypRM0M9A4CcRLBezRZYzCka9CAH90E2FZ74y6VRe4DNR87t5n3DWVtSbWTBoaFUexHtNs6pyrqX77VvbilIyLZMv5ZYQYOobbQ1U3kime5He7ShEWZ0GPI3gq+z/ZOLsnIdJ5bsT4kokhq/531hSoZ5006vxRGGXnhJt8IYiG7R+oSQxZOYqYR5SKWF+0z2/g0zzM2QQlT2ynLWvBKvdVCLlgCjFN1DF4z/7IEK5FAISFP0GGF51hYw/LofL3ftlD7h7jvjOIgH5viJ0yFGmGCEFHcLKqg0DPXmzwXIrkgQSSQPsuZ6UbHUUG0L8YTRgLnl2FwNFskJIaNx0179Il6xveR1sCXbwSDGvGN78sFuQMztbnx+gFu6VYgv7C+5pFr87wHFAeuDXGTkVM6ucAwSanP7HuxSVvf7SrSrcovKslyqj869pSqn/AB0atiQ4eoq3kWaOqx87NHOV1st9SQW1SYH7SKz4jd9uhrQyDuPb6KJSg1Z2B4sU4187NjPzL4NpzZySgiYk2yXpWKhCLIz6BdZuWX79zgqxLbGxJJnhyy3tOzRWIlMkDOppGJyh8LO0LOqhXzwyrCYzPA+h2xcr7xN5WIW1IGJSZqHdURUtlemcB+yZivuzARNH0LE2MGUfuoNgZ5j1Osn7K88IrkAyKupcIEmG3ktVnPOd1A9RQ9eWbU+C7yKrl6u5ZRZOX0eElVszKfBFy4tu3XHlT7hd/zMFK5NJt8sE89k5m7M8KCGSgJ+Y90ZnUclQvDVtoR5CFkfqsP9fSpA1L+aKYsl2ESq5+fzcqsYRL3YLEhIipBKKrvg6Gy698oNeG+9oCIyuiFexJDq8ycBZ/AWiR+pFQVbNRaFbfKPR9zCW8gHwYOGnENNY9gABuuENqxxXDx9tEYkACd0H9ezLnu9psC6AuR41ACfo6wGKUA1TnpVEHsDbdvJBWDcw60l1hkmHQN2lYFy+eMusEX]]></req_info></xml>\";\n\n    String xmlDecryptedReqInfo = \"<root>\\n\" +\n      \"<out_refund_no><![CDATA[R4001312001201707262674894706_4]]></out_refund_no>\\n\" +\n      \"<out_trade_no><![CDATA[201707260201501501005710775]]></out_trade_no>\\n\" +\n      \"<refund_account><![CDATA[REFUND_SOURCE_UNSETTLED_FUNDS]]></refund_account>\\n\" +\n      \"<refund_fee><![CDATA[15]]></refund_fee>\\n\" +\n      \"<refund_id><![CDATA[50000203702017072601461713166]]></refund_id>\\n\" +\n      \"<refund_recv_accout><![CDATA[\u7528\u6237\u96f6\u94b1]]></refund_recv_accout>\\n\" +\n      \"<refund_request_source><![CDATA[API]]></refund_request_source>\\n\" +\n      \"<refund_status><![CDATA[SUCCESS]]></refund_status>\\n\" +\n      \"<settlement_refund_fee><![CDATA[15]]></settlement_refund_fee>\\n\" +\n      \"<settlement_total_fee><![CDATA[100]]></settlement_total_fee>\\n\" +\n      \"<success_time><![CDATA[2017-07-26 02:45:49]]></success_time>\\n\" +\n      \"<total_fee><![CDATA[100]]></total_fee>\\n\" +\n      \"<transaction_id><![CDATA[4001312001201707262674894706]]></transaction_id>\\n\" +\n      \"</root>\";\n\n    XmlConfig.fastMode = true;\n    try {\n      WxPayRefundNotifyResult refundNotifyResult = BaseWxPayResult.fromXML(xmlString, WxPayRefundNotifyResult.class);\n      System.out.println(refundNotifyResult.getReqInfoString());\n\n      refundNotifyResult.loadReqInfo(xmlDecryptedReqInfo);\n      assertEquals(refundNotifyResult.getReqInfo().getRefundFee().intValue(), 15);\n      assertEquals(refundNotifyResult.getReqInfo().getRefundStatus(), \"SUCCESS\");\n      assertEquals(refundNotifyResult.getReqInfo().getRefundRecvAccout(), \"\u7528\u6237\u96f6\u94b1\");\n      System.out.println(refundNotifyResult);\n    } finally {\n      XmlConfig.fastMode = false;\n    }\n  }\n\n}",
        "exampleID": 1000,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/WxPayRefundNotifyResultTest.java#L75"
    },
    {
        "url": "dummy",
        "rawCode": "public class CryptoAES {\n\n  private final CryptoCipher encryptor;\n  private final CryptoCipher decryptor;\n\n  private final Integrity integrity;\n\n  public CryptoAES(String transformation, Properties properties, byte[] inKey, byte[] outKey,\n    byte[] inIv, byte[] outIv) throws IOException {\n    checkTransformation(transformation);\n    // encryptor\n    encryptor = Utils.getCipherInstance(transformation, properties);\n    try {\n      SecretKeySpec outKEYSpec = new SecretKeySpec(outKey, \"AES\");\n      IvParameterSpec outIVSpec = new IvParameterSpec(outIv);\n      encryptor.init(Cipher.ENCRYPT_MODE, outKEYSpec, outIVSpec);\n    } catch (InvalidKeyException | InvalidAlgorithmParameterException e) {\n      throw new IOException(\"Failed to initialize encryptor\", e);\n    }\n\n    // decryptor\n    decryptor = Utils.getCipherInstance(transformation, properties);\n    try {\n      SecretKeySpec inKEYSpec = new SecretKeySpec(inKey, \"AES\");\n      IvParameterSpec inIVSpec = new IvParameterSpec(inIv);\n      decryptor.init(Cipher.DECRYPT_MODE, inKEYSpec, inIVSpec);\n    } catch (InvalidKeyException | InvalidAlgorithmParameterException e) {\n      throw new IOException(\"Failed to initialize decryptor\", e);\n    }\n\n    integrity = new Integrity(outKey, inKey);\n  }\n\n  /**\n   * Encrypts input data. The result composes of (msg, padding if needed, mac) and sequence num.\n   * @param data   the input byte array\n   * @param offset the offset in input where the input starts\n   * @param len    the input length\n   * @return the new encrypted byte array.\n   * @throws SaslException if error happens\n   */\n  public byte[] wrap(byte[] data, int offset, int len) throws SaslException {\n    // mac\n    byte[] mac = integrity.getHMAC(data, offset, len);\n    integrity.incMySeqNum();\n\n    // encrypt\n    byte[] encrypted = new byte[len + 10];\n    try {\n      int n = encryptor.update(data, offset, len, encrypted, 0);\n      encryptor.update(mac, 0, 10, encrypted, n);\n    } catch (ShortBufferException sbe) {\n      // this should not happen\n      throw new SaslException(\"Error happens during encrypt data\", sbe);\n    }\n\n    // append seqNum used for mac\n    byte[] wrapped = new byte[encrypted.length + 4];\n    System.arraycopy(encrypted, 0, wrapped, 0, encrypted.length);\n    System.arraycopy(integrity.getSeqNum(), 0, wrapped, encrypted.length, 4);\n\n    return wrapped;\n  }\n\n  /**\n   * Decrypts input data. The input composes of (msg, padding if needed, mac) and sequence num. The\n   * result is msg.\n   * @param data   the input byte array\n   * @param offset the offset in input where the input starts\n   * @param len    the input length\n   * @return the new decrypted byte array.\n   * @throws SaslException if error happens\n   */\n  public byte[] unwrap(byte[] data, int offset, int len) throws SaslException {\n    // get plaintext and seqNum\n    byte[] decrypted = new byte[len - 4];\n    byte[] peerSeqNum = new byte[4];\n    try {\n      decryptor.update(data, offset, len - 4, decrypted, 0);\n    } catch (ShortBufferException sbe) {\n      // this should not happen\n      throw new SaslException(\"Error happens during decrypt data\", sbe);\n    }\n    System.arraycopy(data, offset + decrypted.length, peerSeqNum, 0, 4);\n\n    // get msg and mac\n    byte[] msg = new byte[decrypted.length - 10];\n    byte[] mac = new byte[10];\n    System.arraycopy(decrypted, 0, msg, 0, msg.length);\n    System.arraycopy(decrypted, msg.length, mac, 0, 10);\n\n    // check mac integrity and msg sequence\n    if (!integrity.compareHMAC(mac, peerSeqNum, msg, 0, msg.length)) {\n      throw new SaslException(\"Unmatched MAC\");\n    }\n    if (!integrity.comparePeerSeqNum(peerSeqNum)) {\n      throw new SaslException(\"Out of order sequencing of messages. Got: \"\n        + integrity.byteToInt(peerSeqNum) + \" Expected: \" + integrity.peerSeqNum);\n    }\n    integrity.incPeerSeqNum();\n\n    return msg;\n  }\n\n  private void checkTransformation(String transformation) throws IOException {\n    if (\"AES/CTR/NoPadding\".equalsIgnoreCase(transformation)) {\n      return;\n    }\n    throw new IOException(\"AES cipher transformation is not supported: \" + transformation);\n  }\n\n  /**\n   * Helper class for providing integrity protection.\n   */\n  private static class Integrity {\n\n    private int mySeqNum = 0;\n    private int peerSeqNum = 0;\n    private byte[] seqNum = new byte[4];\n\n    private byte[] myKey;\n    private byte[] peerKey;\n\n    Integrity(byte[] outKey, byte[] inKey) throws IOException {\n      myKey = outKey;\n      peerKey = inKey;\n    }\n\n    byte[] getHMAC(byte[] msg, int start, int len) throws SaslException {\n      intToByte(mySeqNum);\n      return calculateHMAC(myKey, seqNum, msg, start, len);\n    }\n\n    boolean compareHMAC(byte[] expectedHMAC, byte[] peerSeqNum, byte[] msg, int start, int len)\n      throws SaslException {\n      byte[] mac = calculateHMAC(peerKey, peerSeqNum, msg, start, len);\n      return Arrays.equals(mac, expectedHMAC);\n    }\n\n    boolean comparePeerSeqNum(byte[] peerSeqNum) {\n      return this.peerSeqNum == byteToInt(peerSeqNum);\n    }\n\n    byte[] getSeqNum() {\n      return seqNum;\n    }\n\n    void incMySeqNum() {\n      mySeqNum++;\n    }\n\n    void incPeerSeqNum() {\n      peerSeqNum++;\n    }\n\n    private byte[] calculateHMAC(byte[] key, byte[] seqNum, byte[] msg, int start, int len)\n      throws SaslException {\n      byte[] seqAndMsg = new byte[4 + len];\n      System.arraycopy(seqNum, 0, seqAndMsg, 0, 4);\n      System.arraycopy(msg, start, seqAndMsg, 4, len);\n\n      try {\n        SecretKey keyKi = new SecretKeySpec(key, \"HmacMD5\");\n        Mac m = Mac.getInstance(\"HmacMD5\");\n        m.init(keyKi);\n        m.update(seqAndMsg);\n        byte[] hMAC_MD5 = m.doFinal();\n\n        /* First 10 bytes of HMAC_MD5 digest */\n        byte macBuffer[] = new byte[10];\n        System.arraycopy(hMAC_MD5, 0, macBuffer, 0, 10);\n\n        return macBuffer;\n      } catch (InvalidKeyException e) {\n        throw new SaslException(\"Invalid bytes used for key of HMAC-MD5 hash.\", e);\n      } catch (NoSuchAlgorithmException e) {\n        throw new SaslException(\"Error creating instance of MD5 MAC algorithm\", e);\n      }\n    }\n\n    private void intToByte(int num) {\n      for (int i = 3; i >= 0; i--) {\n        seqNum[i] = (byte) (num & 0xff);\n        num >>>= 8;\n      }\n    }\n\n    private int byteToInt(byte[] seqNum) {\n      int answer = 0;\n      for (int i = 0; i < 4; i++) {\n        answer <<= 8;\n        answer |= ((int) seqNum[i] & 0xff);\n      }\n      return answer;\n    }\n  }\n}",
        "exampleID": 1001,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/CryptoAES.java#L205"
    },
    {
        "url": "dummy",
        "rawCode": "public class DES {\n\tprivate static final String Encrypt_Password = \"abcdefgh\";\n    private static byte[] iv = { 1, 2, 3, 4, 5, 6, 7, 8 };\n\n    private static byte[] aes_key= {0x00,0x11,0x22,0x33,0x44,0x55,0x66,0x77,0x78,0x79,0x7A,0x7B,0x7C,0x7D,0x7E,0x7F};\n    public static String decryptDES(String decryptString) throws Exception {\n        byte[] byteMi = Base64.getDecoder().decode(decryptString);\n        IvParameterSpec zeroIv = new IvParameterSpec(iv);\n        SecretKeySpec key = new SecretKeySpec(Encrypt_Password.getBytes(), \"DES\");\n        Cipher cipher = Cipher.getInstance(\"DES/CBC/PKCS5Padding\");\n        cipher.init(Cipher.DECRYPT_MODE, key, zeroIv);\n        byte decryptedData[] = cipher.doFinal(byteMi);\n\n        return new String(decryptedData);\n    }\n    public static void init(byte[] secret) {\n        if (secret != null && secret.length == 16) {\n            aes_key = new byte[16];\n            for (int i = 0; i < 16; i++) {\n                aes_key[i] = secret[i];\n            }\n        } else {\n            System.out.println(\"Error int key error, secret incorrect\");\n        }\n    }\n    public static String encryptDES(String encryptString) throws Exception {\n        IvParameterSpec zeroIv = new IvParameterSpec(iv);\n        SecretKeySpec key = new SecretKeySpec(Encrypt_Password.getBytes(), \"DES\");\n        Cipher cipher = Cipher.getInstance(\"DES/CBC/PKCS5Padding\");\n        cipher.init(Cipher.ENCRYPT_MODE, key, zeroIv);\n        byte[] encryptedData = cipher.doFinal(encryptString.getBytes());\n        return new String(Base64.getEncoder().encode(encryptedData));\n    }\n\n\tpublic static byte[] encrypt(byte[] datasource) throws InvalidKeyException, NoSuchAlgorithmException, InvalidKeySpecException, NoSuchPaddingException, IllegalBlockSizeException, BadPaddingException {\n\t\tSecureRandom random = new SecureRandom();\n\t\tDESKeySpec desKey = new DESKeySpec(Encrypt_Password.getBytes());\n\t\t// \u521b\u5efa\u4e00\u4e2a\u5bc6\u5319\u5de5\u5382\uff0c\u7136\u540e\u7528\u5b83\u628aDESKeySpec\u8f6c\u6362\u6210\n\t\tSecretKeyFactory keyFactory = SecretKeyFactory.getInstance(\"DES\");\n\t\tSecretKey securekey = keyFactory.generateSecret(desKey);\n\t\t// Cipher\u5bf9\u8c61\u5b9e\u9645\u5b8c\u6210\u52a0\u5bc6\u64cd\u4f5c\n\t\tCipher cipher = Cipher.getInstance(\"DES\");\n\t\t// \u7528\u5bc6\u5319\u521d\u59cb\u5316Cipher\u5bf9\u8c61\n\t\tcipher.init(Cipher.ENCRYPT_MODE, securekey, random);\n\t\t// \u73b0\u5728\uff0c\u83b7\u53d6\u6570\u636e\u5e76\u52a0\u5bc6\n\t\t// \u6b63\u5f0f\u6267\u884c\u52a0\u5bc6\u64cd\u4f5c\n\t\treturn cipher.doFinal(datasource);\n\t}\n\n\t/**\n\t * \u89e3\u5bc6\n\t * \n\t * @param src\n\t *            byte[]\n\t *            String\n\t * @return byte[]\n\t * @throws Exception\n\t */\n\tpublic static byte[] decrypt(byte[] src) throws Exception {\n\t\t// DES\u7b97\u6cd5\u8981\u6c42\u6709\u4e00\u4e2a\u53ef\u4fe1\u4efb\u7684\u968f\u673a\u6570\u6e90\n\t\tSecureRandom random = new SecureRandom();\n\t\t// \u521b\u5efa\u4e00\u4e2aDESKeySpec\u5bf9\u8c61\n\t\tDESKeySpec desKey = new DESKeySpec(Encrypt_Password.getBytes());\n\t\t// \u521b\u5efa\u4e00\u4e2a\u5bc6\u5319\u5de5\u5382\n\t\tSecretKeyFactory keyFactory = SecretKeyFactory.getInstance(\"DES\");\n\t\t// \u5c06DESKeySpec\u5bf9\u8c61\u8f6c\u6362\u6210SecretKey\u5bf9\u8c61\n\t\tSecretKey securekey = keyFactory.generateSecret(desKey);\n\t\t// Cipher\u5bf9\u8c61\u5b9e\u9645\u5b8c\u6210\u89e3\u5bc6\u64cd\u4f5c\n\t\tCipher cipher = Cipher.getInstance(\"DES\");\n\t\t// \u7528\u5bc6\u5319\u521d\u59cb\u5316Cipher\u5bf9\u8c61\n\t\tcipher.init(Cipher.DECRYPT_MODE, securekey, random);\n\t\t// \u771f\u6b63\u5f00\u59cb\u89e3\u5bc6\u64cd\u4f5c\n\t\treturn cipher.doFinal(src);\n\t}\n    public static byte[] AESEncrypt(String sSrc, String userKey) {\n        return AESEncrypt(sSrc.getBytes(), userKey);\n    }\n\n    public static byte[] AESEncrypt(byte[] tobeencrypdata, byte[] aesKey) {\n        if (aesKey == null) {\n            System.out.print(\"Key\u4e3a\u7a7anull\");\n            return null;\n        }\n        // \u5224\u65adKey\u662f\u5426\u4e3a16\u4f4d\n        if (aesKey.length != 16) {\n            System.out.print(\"Key\u957f\u5ea6\u4e0d\u662f16\u4f4d\");\n            return null;\n        }\n\n\n        try {\n            SecretKeySpec skeySpec = new SecretKeySpec(aesKey, \"AES\");\n            Cipher cipher = Cipher.getInstance(\"AES/CBC/PKCS5Padding\");//\"\u7b97\u6cd5/\u6a21\u5f0f/\u8865\u7801\u65b9\u5f0f\"\n            IvParameterSpec iv = new IvParameterSpec(aesKey);//\u4f7f\u7528CBC\u6a21\u5f0f\uff0c\u9700\u8981\u4e00\u4e2a\u5411\u91cfiv\uff0c\u53ef\u589e\u52a0\u52a0\u5bc6\u7b97\u6cd5\u7684\u5f3a\u5ea6\n            cipher.init(Cipher.ENCRYPT_MODE, skeySpec, iv);\n\n            //2018.1.1 0:0:0 \u4ee5\u6765\u7684\u5c0f\u65f6\u6570\n            int curhour = (int) ((System.currentTimeMillis()/1000 - 1514736000)/3600);\n\n            byte[] tobeencrypdatawithtime = new byte[tobeencrypdata.length + 4];\n            byte byte0 = (byte)(curhour & 0xFF);\n            tobeencrypdatawithtime[0] = byte0;\n\n            byte byte1 = (byte)((curhour & 0xFF00) >> 8);\n            tobeencrypdatawithtime[1] = byte1;\n\n            byte byte2 = (byte)((curhour & 0xFF0000) >> 16);\n            tobeencrypdatawithtime[2] = byte2;\n\n            byte byte3 = (byte)((curhour & 0xFF) >> 24);\n            tobeencrypdatawithtime[3] = byte3;\n\n            System.arraycopy(tobeencrypdata, 0, tobeencrypdatawithtime, 4, tobeencrypdata.length);\n\n\n            byte[] encrypted = cipher.doFinal(tobeencrypdatawithtime);\n            return encrypted;\n        } catch (NoSuchAlgorithmException e) {\n            e.printStackTrace();\n        } catch (NoSuchPaddingException e) {\n            e.printStackTrace();\n        } catch (InvalidKeyException e) {\n            e.printStackTrace();\n        } catch (InvalidAlgorithmParameterException e) {\n            e.printStackTrace();\n        } catch (IllegalBlockSizeException e) {\n            e.printStackTrace();\n        } catch (BadPaddingException e) {\n            e.printStackTrace();\n        }\n        return null;\n    }\n    public static byte[] AESEncrypt(byte[] tobeencrypdata, String userKey) {\n        byte[] aesKey = aes_key;\n        if (userKey != null && !userKey.isEmpty()) {\n            aesKey = convertUserKey(userKey);\n        }\n        return AESEncrypt(tobeencrypdata, aesKey);\n    }\n\n    public static int getUnsignedByte (byte data){      //\u5c06data\u5b57\u8282\u578b\u6570\u636e\u8f6c\u6362\u4e3a0~255 (0xFF \u5373BYTE)\u3002\n        return data&0x0FF ;\n    }\n\n    private static byte[] convertUserKey(String userKey) {\n        byte[] key = new byte[16];\n        for (int i = 0; i < 16; i++) {\n            key[i] = (byte) (userKey.charAt(i) & 0xFF);\n        }\n        return key;\n    }\n\n    public static byte[] AESDecrypt(byte[] sSrc, String userKey, boolean checkTime) {\n        try {\n\n            byte[] aesKey = aes_key;\n            if (userKey != null && !userKey.isEmpty()) {\n                aesKey = convertUserKey(userKey);\n            }\n            // \u5224\u65adKey\u662f\u5426\u6b63\u786e\n            if (aesKey == null) {\n                System.out.print(\"Key\u4e3a\u7a7anull\");\n                return null;\n            }\n            // \u5224\u65adKey\u662f\u5426\u4e3a16\u4f4d\n            if (aesKey.length != 16) {\n                System.out.print(\"Key\u957f\u5ea6\u4e0d\u662f16\u4f4d\");\n                return null;\n            }\n\n            SecretKeySpec skeySpec = new SecretKeySpec(aesKey, \"AES\");\n            Cipher cipher = Cipher.getInstance(\"AES/CBC/PKCS5Padding\");\n            IvParameterSpec iv = new IvParameterSpec(aesKey);\n            cipher.init(Cipher.DECRYPT_MODE, skeySpec, iv);\n            try {\n                byte[] original = cipher.doFinal(sSrc);\n                int hours = 0;\n\n                if (original.length > 4) {\n                    hours += getUnsignedByte(original[3]);\n                    hours <<= 8;\n\n                    hours += getUnsignedByte(original[2]);\n                    hours <<= 8;\n\n                    hours += getUnsignedByte(original[1]);\n                    hours <<= 8;\n\n                    hours += getUnsignedByte(original[0]);\n\n                    //2018.1.1 0:0:0 \u4ee5\u6765\u7684\u5c0f\u65f6\u6570\n                    int curhour = (int) ((System.currentTimeMillis()/1000 - 1514736000)/3600);\n\n                    if (curhour - hours > 24 && checkTime) {\n                        return null;\n                    }\n                    byte[] neworiginal = new byte[original.length - 4];\n                    System.arraycopy(original, 4, neworiginal, 0, neworiginal.length);\n                    return neworiginal;\n                }\n                return null;\n            } catch (Exception e) {\n                System.out.println(e.toString());\n                return null;\n            }\n        } catch (Exception ex) {\n            System.out.println(ex.toString());\n            return null;\n        }\n    }\n}",
        "exampleID": 1002,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/DES.java#L55"
    },
    {
        "url": "dummy",
        "rawCode": "public class HashTable extends Configured implements Tool {\n\n  private static final Logger LOG = LoggerFactory.getLogger(HashTable.class);\n\n  private static final int DEFAULT_BATCH_SIZE = 8000;\n\n  private final static String HASH_BATCH_SIZE_CONF_KEY = \"hash.batch.size\";\n  final static String PARTITIONS_FILE_NAME = \"partitions\";\n  final static String MANIFEST_FILE_NAME = \"manifest\";\n  final static String HASH_DATA_DIR = \"hashes\";\n  final static String OUTPUT_DATA_FILE_PREFIX = \"part-r-\";\n  final static String IGNORE_TIMESTAMPS = \"ignoreTimestamps\";\n  private final static String TMP_MANIFEST_FILE_NAME = \"manifest.tmp\";\n\n  TableHash tableHash = new TableHash();\n  Path destPath;\n\n  public HashTable(Configuration conf) {\n    super(conf);\n  }\n\n  public static class TableHash {\n\n    Path hashDir;\n\n    String tableName;\n    String families = null;\n    long batchSize = DEFAULT_BATCH_SIZE;\n    int numHashFiles = 0;\n    byte[] startRow = HConstants.EMPTY_START_ROW;\n    byte[] stopRow = HConstants.EMPTY_END_ROW;\n    int scanBatch = 0;\n    int versions = -1;\n    long startTime = 0;\n    long endTime = 0;\n    boolean ignoreTimestamps;\n    boolean rawScan;\n\n    List<ImmutableBytesWritable> partitions;\n\n    public static TableHash read(Configuration conf, Path hashDir) throws IOException {\n      TableHash tableHash = new TableHash();\n      FileSystem fs = hashDir.getFileSystem(conf);\n      tableHash.hashDir = hashDir;\n      tableHash.readPropertiesFile(fs, new Path(hashDir, MANIFEST_FILE_NAME));\n      tableHash.readPartitionFile(fs, conf, new Path(hashDir, PARTITIONS_FILE_NAME));\n      return tableHash;\n    }\n\n    void writePropertiesFile(FileSystem fs, Path path) throws IOException {\n      Properties p = new Properties();\n      p.setProperty(\"table\", tableName);\n      if (families != null) {\n        p.setProperty(\"columnFamilies\", families);\n      }\n      p.setProperty(\"targetBatchSize\", Long.toString(batchSize));\n      p.setProperty(\"numHashFiles\", Integer.toString(numHashFiles));\n      if (!isTableStartRow(startRow)) {\n        p.setProperty(\"startRowHex\", Bytes.toHex(startRow));\n      }\n      if (!isTableEndRow(stopRow)) {\n        p.setProperty(\"stopRowHex\", Bytes.toHex(stopRow));\n      }\n      if (scanBatch > 0) {\n        p.setProperty(\"scanBatch\", Integer.toString(scanBatch));\n      }\n      if (versions >= 0) {\n        p.setProperty(\"versions\", Integer.toString(versions));\n      }\n      if (startTime != 0) {\n        p.setProperty(\"startTimestamp\", Long.toString(startTime));\n      }\n      if (endTime != 0) {\n        p.setProperty(\"endTimestamp\", Long.toString(endTime));\n      }\n      p.setProperty(\"rawScan\", Boolean.toString(rawScan));\n\n      try (OutputStreamWriter osw = new OutputStreamWriter(fs.create(path), Charsets.UTF_8)) {\n        p.store(osw, null);\n      }\n    }\n\n    void readPropertiesFile(FileSystem fs, Path path) throws IOException {\n      Properties p = new Properties();\n      try (FSDataInputStream in = fs.open(path)) {\n        try (InputStreamReader isr = new InputStreamReader(in, Charsets.UTF_8)) {\n          p.load(isr);\n        }\n      }\n      tableName = p.getProperty(\"table\");\n      families = p.getProperty(\"columnFamilies\");\n      batchSize = Long.parseLong(p.getProperty(\"targetBatchSize\"));\n      numHashFiles = Integer.parseInt(p.getProperty(\"numHashFiles\"));\n\n      String startRowHex = p.getProperty(\"startRowHex\");\n      if (startRowHex != null) {\n        startRow = Bytes.fromHex(startRowHex);\n      }\n      String stopRowHex = p.getProperty(\"stopRowHex\");\n      if (stopRowHex != null) {\n        stopRow = Bytes.fromHex(stopRowHex);\n      }\n\n      String scanBatchString = p.getProperty(\"scanBatch\");\n      if (scanBatchString != null) {\n        scanBatch = Integer.parseInt(scanBatchString);\n      }\n\n      String versionString = p.getProperty(\"versions\");\n      if (versionString != null) {\n        versions = Integer.parseInt(versionString);\n      }\n\n      String rawScanString = p.getProperty(\"rawScan\");\n      if (rawScanString != null) {\n        rawScan = Boolean.parseBoolean(rawScanString);\n      }\n\n      String startTimeString = p.getProperty(\"startTimestamp\");\n      if (startTimeString != null) {\n        startTime = Long.parseLong(startTimeString);\n      }\n\n      String endTimeString = p.getProperty(\"endTimestamp\");\n      if (endTimeString != null) {\n        endTime = Long.parseLong(endTimeString);\n      }\n    }\n\n    Scan initScan() throws IOException {\n      Scan scan = new Scan();\n      scan.setCacheBlocks(false);\n      if (startTime != 0 || endTime != 0) {\n        scan.setTimeRange(startTime, endTime == 0 ? HConstants.LATEST_TIMESTAMP : endTime);\n      }\n      if (scanBatch > 0) {\n        scan.setBatch(scanBatch);\n      }\n      if (versions >= 0) {\n        scan.readVersions(versions);\n      }\n      if (!isTableStartRow(startRow)) {\n        scan.withStartRow(startRow);\n      }\n      if (!isTableEndRow(stopRow)) {\n        scan.withStopRow(stopRow);\n      }\n      if (families != null) {\n        for (String fam : families.split(\",\")) {\n          scan.addFamily(Bytes.toBytes(fam));\n        }\n      }\n      scan.setRaw(rawScan);\n\n      return scan;\n    }\n\n    /**\n     * Choose partitions between row ranges to hash to a single output file Selects region\n     * boundaries that fall within the scan range, and groups them into the desired number of\n     * partitions.\n     */\n    void selectPartitions(Pair<byte[][], byte[][]> regionStartEndKeys) {\n      List<byte[]> startKeys = new ArrayList<>();\n      for (int i = 0; i < regionStartEndKeys.getFirst().length; i++) {\n        byte[] regionStartKey = regionStartEndKeys.getFirst()[i];\n        byte[] regionEndKey = regionStartEndKeys.getSecond()[i];\n\n        // if scan begins after this region, or starts before this region, then drop this region\n        // in other words:\n        // IF (scan begins before the end of this region\n        // AND scan ends before the start of this region)\n        // THEN include this region\n        if (\n          (isTableStartRow(startRow) || isTableEndRow(regionEndKey)\n            || Bytes.compareTo(startRow, regionEndKey) < 0)\n            && (isTableEndRow(stopRow) || isTableStartRow(regionStartKey)\n              || Bytes.compareTo(stopRow, regionStartKey) > 0)\n        ) {\n          startKeys.add(regionStartKey);\n        }\n      }\n\n      int numRegions = startKeys.size();\n      if (numHashFiles == 0) {\n        numHashFiles = numRegions / 100;\n      }\n      if (numHashFiles == 0) {\n        numHashFiles = 1;\n      }\n      if (numHashFiles > numRegions) {\n        // can't partition within regions\n        numHashFiles = numRegions;\n      }\n\n      // choose a subset of start keys to group regions into ranges\n      partitions = new ArrayList<>(numHashFiles - 1);\n      // skip the first start key as it is not a partition between ranges.\n      for (long i = 1; i < numHashFiles; i++) {\n        int splitIndex = (int) (numRegions * i / numHashFiles);\n        partitions.add(new ImmutableBytesWritable(startKeys.get(splitIndex)));\n      }\n    }\n\n    void writePartitionFile(Configuration conf, Path path) throws IOException {\n      FileSystem fs = path.getFileSystem(conf);\n      @SuppressWarnings(\"deprecation\")\n      SequenceFile.Writer writer =\n        SequenceFile.createWriter(fs, conf, path, ImmutableBytesWritable.class, NullWritable.class);\n\n      for (int i = 0; i < partitions.size(); i++) {\n        writer.append(partitions.get(i), NullWritable.get());\n      }\n      writer.close();\n    }\n\n    private void readPartitionFile(FileSystem fs, Configuration conf, Path path)\n      throws IOException {\n      @SuppressWarnings(\"deprecation\")\n      SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, conf);\n      ImmutableBytesWritable key = new ImmutableBytesWritable();\n      partitions = new ArrayList<>();\n      while (reader.next(key)) {\n        partitions.add(new ImmutableBytesWritable(key.copyBytes()));\n      }\n      reader.close();\n\n      if (!Ordering.natural().isOrdered(partitions)) {\n        throw new IOException(\"Partitions are not ordered!\");\n      }\n    }\n\n    @Override\n    public String toString() {\n      StringBuilder sb = new StringBuilder();\n      sb.append(\"tableName=\").append(tableName);\n      if (families != null) {\n        sb.append(\", families=\").append(families);\n      }\n      sb.append(\", batchSize=\").append(batchSize);\n      sb.append(\", numHashFiles=\").append(numHashFiles);\n      if (!isTableStartRow(startRow)) {\n        sb.append(\", startRowHex=\").append(Bytes.toHex(startRow));\n      }\n      if (!isTableEndRow(stopRow)) {\n        sb.append(\", stopRowHex=\").append(Bytes.toHex(stopRow));\n      }\n      if (scanBatch >= 0) {\n        sb.append(\", scanBatch=\").append(scanBatch);\n      }\n      if (versions >= 0) {\n        sb.append(\", versions=\").append(versions);\n      }\n      sb.append(\", rawScan=\").append(rawScan);\n      if (startTime != 0) {\n        sb.append(\"startTime=\").append(startTime);\n      }\n      if (endTime != 0) {\n        sb.append(\"endTime=\").append(endTime);\n      }\n      return sb.toString();\n    }\n\n    static String getDataFileName(int hashFileIndex) {\n      return String.format(HashTable.OUTPUT_DATA_FILE_PREFIX + \"%05d\", hashFileIndex);\n    }\n\n    /**\n     * Open a TableHash.Reader starting at the first hash at or after the given key.\n     */\n    public Reader newReader(Configuration conf, ImmutableBytesWritable startKey)\n      throws IOException {\n      return new Reader(conf, startKey);\n    }\n\n    public class Reader implements java.io.Closeable {\n      private final Configuration conf;\n\n      private int hashFileIndex;\n      private MapFile.Reader mapFileReader;\n\n      private boolean cachedNext;\n      private ImmutableBytesWritable key;\n      private ImmutableBytesWritable hash;\n\n      Reader(Configuration conf, ImmutableBytesWritable startKey) throws IOException {\n        this.conf = conf;\n        int partitionIndex = Collections.binarySearch(partitions, startKey);\n        if (partitionIndex >= 0) {\n          // if the key is equal to a partition, then go the file after that partition\n          hashFileIndex = partitionIndex + 1;\n        } else {\n          // if the key is between partitions, then go to the file between those partitions\n          hashFileIndex = -1 - partitionIndex;\n        }\n        openHashFile();\n\n        // MapFile's don't make it easy to seek() so that the subsequent next() returns\n        // the desired key/value pair. So we cache it for the first call of next().\n        hash = new ImmutableBytesWritable();\n        key = (ImmutableBytesWritable) mapFileReader.getClosest(startKey, hash);\n        if (key == null) {\n          cachedNext = false;\n          hash = null;\n        } else {\n          cachedNext = true;\n        }\n      }\n\n      /**\n       * Read the next key/hash pair. Returns true if such a pair exists and false when at the end\n       * of the data.\n       */\n      public boolean next() throws IOException {\n        if (cachedNext) {\n          cachedNext = false;\n          return true;\n        }\n        key = new ImmutableBytesWritable();\n        hash = new ImmutableBytesWritable();\n        while (true) {\n          boolean hasNext = mapFileReader.next(key, hash);\n          if (hasNext) {\n            return true;\n          }\n          hashFileIndex++;\n          if (hashFileIndex < TableHash.this.numHashFiles) {\n            mapFileReader.close();\n            openHashFile();\n          } else {\n            key = null;\n            hash = null;\n            return false;\n          }\n        }\n      }\n\n      /**\n       * Get the current key\n       * @return the current key or null if there is no current key\n       */\n      public ImmutableBytesWritable getCurrentKey() {\n        return key;\n      }\n\n      /**\n       * Get the current hash\n       * @return the current hash or null if there is no current hash\n       */\n      public ImmutableBytesWritable getCurrentHash() {\n        return hash;\n      }\n\n      private void openHashFile() throws IOException {\n        if (mapFileReader != null) {\n          mapFileReader.close();\n        }\n        Path dataDir = new Path(TableHash.this.hashDir, HASH_DATA_DIR);\n        Path dataFile = new Path(dataDir, getDataFileName(hashFileIndex));\n        mapFileReader = new MapFile.Reader(dataFile, conf);\n      }\n\n      @Override\n      public void close() throws IOException {\n        mapFileReader.close();\n      }\n    }\n  }\n\n  static boolean isTableStartRow(byte[] row) {\n    return Bytes.equals(HConstants.EMPTY_START_ROW, row);\n  }\n\n  static boolean isTableEndRow(byte[] row) {\n    return Bytes.equals(HConstants.EMPTY_END_ROW, row);\n  }\n\n  public Job createSubmittableJob(String[] args) throws IOException {\n    Path partitionsPath = new Path(destPath, PARTITIONS_FILE_NAME);\n    generatePartitions(partitionsPath);\n\n    Job job = Job.getInstance(getConf(),\n      getConf().get(\"mapreduce.job.name\", \"hashTable_\" + tableHash.tableName));\n    Configuration jobConf = job.getConfiguration();\n    jobConf.setLong(HASH_BATCH_SIZE_CONF_KEY, tableHash.batchSize);\n    jobConf.setBoolean(IGNORE_TIMESTAMPS, tableHash.ignoreTimestamps);\n    job.setJarByClass(HashTable.class);\n\n    TableMapReduceUtil.initTableMapperJob(tableHash.tableName, tableHash.initScan(),\n      HashMapper.class, ImmutableBytesWritable.class, ImmutableBytesWritable.class, job);\n\n    // use a TotalOrderPartitioner and reducers to group region output into hash files\n    job.setPartitionerClass(TotalOrderPartitioner.class);\n    TotalOrderPartitioner.setPartitionFile(jobConf, partitionsPath);\n    job.setReducerClass(Reducer.class); // identity reducer\n    job.setNumReduceTasks(tableHash.numHashFiles);\n    job.setOutputKeyClass(ImmutableBytesWritable.class);\n    job.setOutputValueClass(ImmutableBytesWritable.class);\n    job.setOutputFormatClass(MapFileOutputFormat.class);\n    FileOutputFormat.setOutputPath(job, new Path(destPath, HASH_DATA_DIR));\n\n    return job;\n  }\n\n  private void generatePartitions(Path partitionsPath) throws IOException {\n    Connection connection = ConnectionFactory.createConnection(getConf());\n    Pair<byte[][], byte[][]> regionKeys =\n      connection.getRegionLocator(TableName.valueOf(tableHash.tableName)).getStartEndKeys();\n    connection.close();\n\n    tableHash.selectPartitions(regionKeys);\n    LOG.info(\"Writing \" + tableHash.partitions.size() + \" partition keys to \" + partitionsPath);\n\n    tableHash.writePartitionFile(getConf(), partitionsPath);\n  }\n\n  static class ResultHasher {\n    private MessageDigest digest;\n\n    private boolean batchStarted = false;\n    private ImmutableBytesWritable batchStartKey;\n    private ImmutableBytesWritable batchHash;\n    private long batchSize = 0;\n    boolean ignoreTimestamps;\n\n    public ResultHasher() {\n      try {\n        digest = MessageDigest.getInstance(\"MD5\");\n      } catch (NoSuchAlgorithmException e) {\n        Throwables.propagate(e);\n      }\n    }\n\n    public void startBatch(ImmutableBytesWritable row) {\n      if (batchStarted) {\n        throw new RuntimeException(\"Cannot start new batch without finishing existing one.\");\n      }\n      batchStarted = true;\n      batchSize = 0;\n      batchStartKey = row;\n      batchHash = null;\n    }\n\n    public void hashResult(Result result) {\n      if (!batchStarted) {\n        throw new RuntimeException(\"Cannot add to batch that has not been started.\");\n      }\n      for (Cell cell : result.rawCells()) {\n        int rowLength = cell.getRowLength();\n        int familyLength = cell.getFamilyLength();\n        int qualifierLength = cell.getQualifierLength();\n        int valueLength = cell.getValueLength();\n        digest.update(cell.getRowArray(), cell.getRowOffset(), rowLength);\n        digest.update(cell.getFamilyArray(), cell.getFamilyOffset(), familyLength);\n        digest.update(cell.getQualifierArray(), cell.getQualifierOffset(), qualifierLength);\n\n        if (!ignoreTimestamps) {\n          long ts = cell.getTimestamp();\n          for (int i = 8; i > 0; i--) {\n            digest.update((byte) ts);\n            ts >>>= 8;\n          }\n        }\n        digest.update(cell.getValueArray(), cell.getValueOffset(), valueLength);\n\n        batchSize += rowLength + familyLength + qualifierLength + 8 + valueLength;\n      }\n    }\n\n    public void finishBatch() {\n      if (!batchStarted) {\n        throw new RuntimeException(\"Cannot finish batch that has not started.\");\n      }\n      batchStarted = false;\n      batchHash = new ImmutableBytesWritable(digest.digest());\n    }\n\n    public boolean isBatchStarted() {\n      return batchStarted;\n    }\n\n    public ImmutableBytesWritable getBatchStartKey() {\n      return batchStartKey;\n    }\n\n    public ImmutableBytesWritable getBatchHash() {\n      return batchHash;\n    }\n\n    public long getBatchSize() {\n      return batchSize;\n    }\n  }\n\n  public static class HashMapper\n    extends TableMapper<ImmutableBytesWritable, ImmutableBytesWritable> {\n\n    private ResultHasher hasher;\n    private long targetBatchSize;\n\n    private ImmutableBytesWritable currentRow;\n\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n      targetBatchSize =\n        context.getConfiguration().getLong(HASH_BATCH_SIZE_CONF_KEY, DEFAULT_BATCH_SIZE);\n      hasher = new ResultHasher();\n      hasher.ignoreTimestamps = context.getConfiguration().getBoolean(IGNORE_TIMESTAMPS, false);\n      TableSplit split = (TableSplit) context.getInputSplit();\n      hasher.startBatch(new ImmutableBytesWritable(split.getStartRow()));\n    }\n\n    @Override\n    protected void map(ImmutableBytesWritable key, Result value, Context context)\n      throws IOException, InterruptedException {\n\n      if (currentRow == null || !currentRow.equals(key)) {\n        currentRow = new ImmutableBytesWritable(key); // not immutable\n\n        if (hasher.getBatchSize() >= targetBatchSize) {\n          hasher.finishBatch();\n          context.write(hasher.getBatchStartKey(), hasher.getBatchHash());\n          hasher.startBatch(currentRow);\n        }\n      }\n\n      hasher.hashResult(value);\n    }\n\n    @Override\n    protected void cleanup(Context context) throws IOException, InterruptedException {\n      hasher.finishBatch();\n      context.write(hasher.getBatchStartKey(), hasher.getBatchHash());\n    }\n  }\n\n  private void writeTempManifestFile() throws IOException {\n    Path tempManifestPath = new Path(destPath, TMP_MANIFEST_FILE_NAME);\n    FileSystem fs = tempManifestPath.getFileSystem(getConf());\n    tableHash.writePropertiesFile(fs, tempManifestPath);\n  }\n\n  private void completeManifest() throws IOException {\n    Path tempManifestPath = new Path(destPath, TMP_MANIFEST_FILE_NAME);\n    Path manifestPath = new Path(destPath, MANIFEST_FILE_NAME);\n    FileSystem fs = tempManifestPath.getFileSystem(getConf());\n    fs.rename(tempManifestPath, manifestPath);\n  }\n\n  private static final int NUM_ARGS = 2;\n\n  private static void printUsage(final String errorMsg) {\n    if (errorMsg != null && errorMsg.length() > 0) {\n      System.err.println(\"ERROR: \" + errorMsg);\n      System.err.println();\n    }\n    System.err.println(\"Usage: HashTable [options] <tablename> <outputpath>\");\n    System.err.println();\n    System.err.println(\"Options:\");\n    System.err.println(\" batchsize         the target amount of bytes to hash in each batch\");\n    System.err.println(\"                   rows are added to the batch until this size is reached\");\n    System.err.println(\"                   (defaults to \" + DEFAULT_BATCH_SIZE + \" bytes)\");\n    System.err.println(\" numhashfiles      the number of hash files to create\");\n    System.err.println(\"                   if set to fewer than number of regions then\");\n    System.err.println(\"                   the job will create this number of reducers\");\n    System.err.println(\"                   (defaults to 1/100 of regions -- at least 1)\");\n    System.err.println(\" startrow          the start row\");\n    System.err.println(\" stoprow           the stop row\");\n    System.err.println(\" starttime         beginning of the time range (unixtime in millis)\");\n    System.err.println(\"                   without endtime means from starttime to forever\");\n    System.err.println(\" endtime           end of the time range.\");\n    System.err.println(\"                   Ignored if no starttime specified.\");\n    System.err.println(\" scanbatch         scanner batch size to support intra row scans\");\n    System.err.println(\" versions          number of cell versions to include\");\n    System.err.println(\" rawScan           performs a raw scan (false if omitted)\");\n    System.err.println(\" families          comma-separated list of families to include\");\n    System.err.println(\" ignoreTimestamps  if true, ignores cell timestamps\");\n    System.err.println(\"                   when calculating hashes\");\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" tablename     Name of the table to hash\");\n    System.err.println(\" outputpath    Filesystem path to put the output data\");\n    System.err.println();\n    System.err.println(\"Examples:\");\n    System.err.println(\" To hash 'TestTable' in 32kB batches for a 1 hour window into 50 files:\");\n    System.err.println(\" $ hbase \"\n      + \"org.apache.hadoop.hbase.mapreduce.HashTable --batchsize=32000 --numhashfiles=50\"\n      + \" --starttime=1265875194289 --endtime=1265878794289 --families=cf2,cf3\"\n      + \" TestTable /hashes/testTable\");\n  }\n\n  private boolean doCommandLine(final String[] args) {\n    if (args.length < NUM_ARGS) {\n      printUsage(null);\n      return false;\n    }\n    try {\n\n      tableHash.tableName = args[args.length - 2];\n      destPath = new Path(args[args.length - 1]);\n\n      for (int i = 0; i < args.length - NUM_ARGS; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String batchSizeArgKey = \"--batchsize=\";\n        if (cmd.startsWith(batchSizeArgKey)) {\n          tableHash.batchSize = Long.parseLong(cmd.substring(batchSizeArgKey.length()));\n          continue;\n        }\n\n        final String numHashFilesArgKey = \"--numhashfiles=\";\n        if (cmd.startsWith(numHashFilesArgKey)) {\n          tableHash.numHashFiles = Integer.parseInt(cmd.substring(numHashFilesArgKey.length()));\n          continue;\n        }\n\n        final String startRowArgKey = \"--startrow=\";\n        if (cmd.startsWith(startRowArgKey)) {\n          tableHash.startRow = Bytes.fromHex(cmd.substring(startRowArgKey.length()));\n          continue;\n        }\n\n        final String stopRowArgKey = \"--stoprow=\";\n        if (cmd.startsWith(stopRowArgKey)) {\n          tableHash.stopRow = Bytes.fromHex(cmd.substring(stopRowArgKey.length()));\n          continue;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          tableHash.startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          tableHash.endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String scanBatchArgKey = \"--scanbatch=\";\n        if (cmd.startsWith(scanBatchArgKey)) {\n          tableHash.scanBatch = Integer.parseInt(cmd.substring(scanBatchArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          tableHash.versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n\n        final String rawScanArgKey = \"--rawScan=\";\n        if (cmd.startsWith(rawScanArgKey)) {\n          tableHash.rawScan = Boolean.parseBoolean(cmd.substring(rawScanArgKey.length()));\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          tableHash.families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        final String ignoreTimestampsKey = \"--ignoreTimestamps=\";\n        if (cmd.startsWith(ignoreTimestampsKey)) {\n          tableHash.ignoreTimestamps =\n            Boolean.parseBoolean(cmd.substring(ignoreTimestampsKey.length()));\n          continue;\n        }\n\n        printUsage(\"Invalid argument '\" + cmd + \"'\");\n        return false;\n      }\n      if (\n        (tableHash.startTime != 0 || tableHash.endTime != 0)\n          && (tableHash.startTime >= tableHash.endTime)\n      ) {\n        printUsage(\"Invalid time range filter: starttime=\" + tableHash.startTime + \" >=  endtime=\"\n          + tableHash.endTime);\n        return false;\n      }\n\n    } catch (Exception e) {\n      LOG.error(\"Failed to parse commandLine arguments\", e);\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }\n\n  /**\n   * Main entry point.\n   */\n  public static void main(String[] args) throws Exception {\n    int ret = ToolRunner.run(new HashTable(HBaseConfiguration.create()), args);\n    System.exit(ret);\n  }\n\n  @Override\n  public int run(String[] args) throws Exception {\n    String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();\n    if (!doCommandLine(otherArgs)) {\n      return 1;\n    }\n\n    Job job = createSubmittableJob(otherArgs);\n    writeTempManifestFile();\n    if (!job.waitForCompletion(true)) {\n      LOG.info(\"Map-reduce job failed!\");\n      return 1;\n    }\n    completeManifest();\n    return 0;\n  }\n\n}",
        "exampleID": 8,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/HashTable.java#L492"
    },
    {
        "url": "dummy",
        "rawCode": "public class HBaseTestingUtility extends HBaseZKTestingUtility {\n\n  /**\n   * System property key to get test directory value. Name is as it is because mini dfs has\n   * hard-codings to put test data here. It should NOT be used directly in HBase, as it's a property\n   * used in mini dfs.\n   * @deprecated since 2.0.0 and will be removed in 3.0.0. Can be used only with mini dfs.\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-19410\">HBASE-19410</a>\n   */\n  @Deprecated\n  private static final String TEST_DIRECTORY_KEY = \"test.build.data\";\n\n  public static final String REGIONS_PER_SERVER_KEY = \"hbase.test.regions-per-server\";\n  /**\n   * The default number of regions per regionserver when creating a pre-split table.\n   */\n  public static final int DEFAULT_REGIONS_PER_SERVER = 3;\n\n  public static final String PRESPLIT_TEST_TABLE_KEY = \"hbase.test.pre-split-table\";\n  public static final boolean PRESPLIT_TEST_TABLE = true;\n\n  private MiniDFSCluster dfsCluster = null;\n  private FsDatasetAsyncDiskServiceFixer dfsClusterFixer = null;\n\n  private volatile HBaseCluster hbaseCluster = null;\n  private MiniMRCluster mrCluster = null;\n\n  /** If there is a mini cluster running for this testing utility instance. */\n  private volatile boolean miniClusterRunning;\n\n  private String hadoopLogDir;\n\n  /**\n   * Directory on test filesystem where we put the data for this instance of HBaseTestingUtility\n   */\n  private Path dataTestDirOnTestFS = null;\n\n  private final AtomicReference<AsyncClusterConnection> asyncConnection = new AtomicReference<>();\n\n  /** Filesystem URI used for map-reduce mini-cluster setup */\n  private static String FS_URI;\n\n  /** This is for unit tests parameterized with a single boolean. */\n  public static final List<Object[]> MEMSTORETS_TAGS_PARAMETRIZED = memStoreTSAndTagsCombination();\n\n  /**\n   * Checks to see if a specific port is available.\n   * @param port the port number to check for availability\n   * @return <tt>true</tt> if the port is available, or <tt>false</tt> if not\n   */\n  public static boolean available(int port) {\n    ServerSocket ss = null;\n    DatagramSocket ds = null;\n    try {\n      ss = new ServerSocket(port);\n      ss.setReuseAddress(true);\n      ds = new DatagramSocket(port);\n      ds.setReuseAddress(true);\n      return true;\n    } catch (IOException e) {\n      // Do nothing\n    } finally {\n      if (ds != null) {\n        ds.close();\n      }\n\n      if (ss != null) {\n        try {\n          ss.close();\n        } catch (IOException e) {\n          /* should not be thrown */\n        }\n      }\n    }\n\n    return false;\n  }\n\n  /**\n   * Create all combinations of Bloom filters and compression algorithms for testing.\n   */\n  private static List<Object[]> bloomAndCompressionCombinations() {\n    List<Object[]> configurations = new ArrayList<>();\n    for (Compression.Algorithm comprAlgo : HBaseCommonTestingUtility.COMPRESSION_ALGORITHMS) {\n      for (BloomType bloomType : BloomType.values()) {\n        configurations.add(new Object[] { comprAlgo, bloomType });\n      }\n    }\n    return Collections.unmodifiableList(configurations);\n  }\n\n  /**\n   * Create combination of memstoreTS and tags\n   */\n  private static List<Object[]> memStoreTSAndTagsCombination() {\n    List<Object[]> configurations = new ArrayList<>();\n    configurations.add(new Object[] { false, false });\n    configurations.add(new Object[] { false, true });\n    configurations.add(new Object[] { true, false });\n    configurations.add(new Object[] { true, true });\n    return Collections.unmodifiableList(configurations);\n  }\n\n  public static List<Object[]> memStoreTSTagsAndOffheapCombination() {\n    List<Object[]> configurations = new ArrayList<>();\n    configurations.add(new Object[] { false, false, true });\n    configurations.add(new Object[] { false, false, false });\n    configurations.add(new Object[] { false, true, true });\n    configurations.add(new Object[] { false, true, false });\n    configurations.add(new Object[] { true, false, true });\n    configurations.add(new Object[] { true, false, false });\n    configurations.add(new Object[] { true, true, true });\n    configurations.add(new Object[] { true, true, false });\n    return Collections.unmodifiableList(configurations);\n  }\n\n  public static final Collection<Object[]> BLOOM_AND_COMPRESSION_COMBINATIONS =\n    bloomAndCompressionCombinations();\n\n  /**\n   * <p>\n   * Create an HBaseTestingUtility using a default configuration.\n   * <p>\n   * Initially, all tmp files are written to a local test data directory. Once\n   * {@link #startMiniDFSCluster} is called, either directly or via {@link #startMiniCluster()}, tmp\n   * data will be written to the DFS directory instead.\n   */\n  public HBaseTestingUtility() {\n    this(HBaseConfiguration.create());\n  }\n\n  /**\n   * <p>\n   * Create an HBaseTestingUtility using a given configuration.\n   * <p>\n   * Initially, all tmp files are written to a local test data directory. Once\n   * {@link #startMiniDFSCluster} is called, either directly or via {@link #startMiniCluster()}, tmp\n   * data will be written to the DFS directory instead.\n   * @param conf The configuration to use for further operations\n   */\n  public HBaseTestingUtility(Configuration conf) {\n    super(conf);\n\n    // a hbase checksum verification failure will cause unit tests to fail\n    ChecksumUtil.generateExceptionForChecksumFailureForTest(true);\n\n    // Save this for when setting default file:// breaks things\n    if (this.conf.get(\"fs.defaultFS\") != null) {\n      this.conf.set(\"original.defaultFS\", this.conf.get(\"fs.defaultFS\"));\n    }\n    if (this.conf.get(HConstants.HBASE_DIR) != null) {\n      this.conf.set(\"original.hbase.dir\", this.conf.get(HConstants.HBASE_DIR));\n    }\n    // Every cluster is a local cluster until we start DFS\n    // Note that conf could be null, but this.conf will not be\n    String dataTestDir = getDataTestDir().toString();\n    this.conf.set(\"fs.defaultFS\", \"file:///\");\n    this.conf.set(HConstants.HBASE_DIR, \"file://\" + dataTestDir);\n    LOG.debug(\"Setting {} to {}\", HConstants.HBASE_DIR, dataTestDir);\n    this.conf.setBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE, false);\n    // If the value for random ports isn't set set it to true, thus making\n    // tests opt-out for random port assignment\n    this.conf.setBoolean(LocalHBaseCluster.ASSIGN_RANDOM_PORTS,\n      this.conf.getBoolean(LocalHBaseCluster.ASSIGN_RANDOM_PORTS, true));\n  }\n\n  /**\n   * Close both the region {@code r} and it's underlying WAL. For use in tests.\n   */\n  public static void closeRegionAndWAL(final Region r) throws IOException {\n    closeRegionAndWAL((HRegion) r);\n  }\n\n  /**\n   * Close both the HRegion {@code r} and it's underlying WAL. For use in tests.\n   */\n  public static void closeRegionAndWAL(final HRegion r) throws IOException {\n    if (r == null) return;\n    r.close();\n    if (r.getWAL() == null) return;\n    r.getWAL().close();\n  }\n\n  /**\n   * Returns this classes's instance of {@link Configuration}. Be careful how you use the returned\n   * Configuration since {@link Connection} instances can be shared. The Map of Connections is keyed\n   * by the Configuration. If say, a Connection was being used against a cluster that had been\n   * shutdown, see {@link #shutdownMiniCluster()}, then the Connection will no longer be wholesome.\n   * Rather than use the return direct, its usually best to make a copy and use that. Do\n   * <code>Configuration c = new Configuration(INSTANCE.getConfiguration());</code>\n   * @return Instance of Configuration.\n   */\n  @Override\n  public Configuration getConfiguration() {\n    return super.getConfiguration();\n  }\n\n  public void setHBaseCluster(HBaseCluster hbaseCluster) {\n    this.hbaseCluster = hbaseCluster;\n  }\n\n  /**\n   * Home our data in a dir under {@link #DEFAULT_BASE_TEST_DIRECTORY}. Give it a random name so can\n   * have many concurrent tests running if we need to. It needs to amend the\n   * {@link #TEST_DIRECTORY_KEY} System property, as it's what minidfscluster bases it data dir on.\n   * Moding a System property is not the way to do concurrent instances -- another instance could\n   * grab the temporary value unintentionally -- but not anything can do about it at moment; single\n   * instance only is how the minidfscluster works. We also create the underlying directory names\n   * for hadoop.log.dir, mapreduce.cluster.local.dir and hadoop.tmp.dir, and set the values in the\n   * conf, and as a system property for hadoop.tmp.dir (We do not create them!).\n   * @return The calculated data test build directory, if newly-created.\n   */\n  @Override\n  protected Path setupDataTestDir() {\n    Path testPath = super.setupDataTestDir();\n    if (null == testPath) {\n      return null;\n    }\n\n    createSubDirAndSystemProperty(\"hadoop.log.dir\", testPath, \"hadoop-log-dir\");\n\n    // This is defaulted in core-default.xml to /tmp/hadoop-${user.name}, but\n    // we want our own value to ensure uniqueness on the same machine\n    createSubDirAndSystemProperty(\"hadoop.tmp.dir\", testPath, \"hadoop-tmp-dir\");\n\n    // Read and modified in org.apache.hadoop.mapred.MiniMRCluster\n    createSubDir(\"mapreduce.cluster.local.dir\", testPath, \"mapred-local-dir\");\n    return testPath;\n  }\n\n  private void createSubDirAndSystemProperty(String propertyName, Path parent, String subDirName) {\n\n    String sysValue = System.getProperty(propertyName);\n\n    if (sysValue != null) {\n      // There is already a value set. So we do nothing but hope\n      // that there will be no conflicts\n      LOG.info(\"System.getProperty(\\\"\" + propertyName + \"\\\") already set to: \" + sysValue\n        + \" so I do NOT create it in \" + parent);\n      String confValue = conf.get(propertyName);\n      if (confValue != null && !confValue.endsWith(sysValue)) {\n        LOG.warn(propertyName + \" property value differs in configuration and system: \"\n          + \"Configuration=\" + confValue + \" while System=\" + sysValue\n          + \" Erasing configuration value by system value.\");\n      }\n      conf.set(propertyName, sysValue);\n    } else {\n      // Ok, it's not set, so we create it as a subdirectory\n      createSubDir(propertyName, parent, subDirName);\n      System.setProperty(propertyName, conf.get(propertyName));\n    }\n  }\n\n  /**\n   * @return Where to write test data on the test filesystem; Returns working directory for the test\n   *         filesystem by default\n   * @see #setupDataTestDirOnTestFS()\n   * @see #getTestFileSystem()\n   */\n  private Path getBaseTestDirOnTestFS() throws IOException {\n    FileSystem fs = getTestFileSystem();\n    return new Path(fs.getWorkingDirectory(), \"test-data\");\n  }\n\n  /**\n   * Returns a Path in the test filesystem, obtained from {@link #getTestFileSystem()} to write\n   * temporary test data. Call this method after setting up the mini dfs cluster if the test relies\n   * on it.\n   * @return a unique path in the test filesystem\n   */\n  public Path getDataTestDirOnTestFS() throws IOException {\n    if (dataTestDirOnTestFS == null) {\n      setupDataTestDirOnTestFS();\n    }\n\n    return dataTestDirOnTestFS;\n  }\n\n  /**\n   * Returns a Path in the test filesystem, obtained from {@link #getTestFileSystem()} to write\n   * temporary test data. Call this method after setting up the mini dfs cluster if the test relies\n   * on it.\n   * @return a unique path in the test filesystem\n   * @param subdirName name of the subdir to create under the base test dir\n   */\n  public Path getDataTestDirOnTestFS(final String subdirName) throws IOException {\n    return new Path(getDataTestDirOnTestFS(), subdirName);\n  }\n\n  /**\n   * Sets up a path in test filesystem to be used by tests. Creates a new directory if not already\n   * setup.\n   */\n  private void setupDataTestDirOnTestFS() throws IOException {\n    if (dataTestDirOnTestFS != null) {\n      LOG.warn(\"Data test on test fs dir already setup in \" + dataTestDirOnTestFS.toString());\n      return;\n    }\n    dataTestDirOnTestFS = getNewDataTestDirOnTestFS();\n  }\n\n  /**\n   * Sets up a new path in test filesystem to be used by tests.\n   */\n  private Path getNewDataTestDirOnTestFS() throws IOException {\n    // The file system can be either local, mini dfs, or if the configuration\n    // is supplied externally, it can be an external cluster FS. If it is a local\n    // file system, the tests should use getBaseTestDir, otherwise, we can use\n    // the working directory, and create a unique sub dir there\n    FileSystem fs = getTestFileSystem();\n    Path newDataTestDir;\n    String randomStr = getRandomUUID().toString();\n    if (fs.getUri().getScheme().equals(FileSystem.getLocal(conf).getUri().getScheme())) {\n      newDataTestDir = new Path(getDataTestDir(), randomStr);\n      File dataTestDir = new File(newDataTestDir.toString());\n      if (deleteOnExit()) dataTestDir.deleteOnExit();\n    } else {\n      Path base = getBaseTestDirOnTestFS();\n      newDataTestDir = new Path(base, randomStr);\n      if (deleteOnExit()) fs.deleteOnExit(newDataTestDir);\n    }\n    return newDataTestDir;\n  }\n\n  /**\n   * Cleans the test data directory on the test filesystem.\n   * @return True if we removed the test dirs\n   */\n  public boolean cleanupDataTestDirOnTestFS() throws IOException {\n    boolean ret = getTestFileSystem().delete(dataTestDirOnTestFS, true);\n    if (ret) dataTestDirOnTestFS = null;\n    return ret;\n  }\n\n  /**\n   * Cleans a subdirectory under the test data directory on the test filesystem.\n   * @return True if we removed child\n   */\n  public boolean cleanupDataTestDirOnTestFS(String subdirName) throws IOException {\n    Path cpath = getDataTestDirOnTestFS(subdirName);\n    return getTestFileSystem().delete(cpath, true);\n  }\n\n  // Workaround to avoid IllegalThreadStateException\n  // See HBASE-27148 for more details\n  private static final class FsDatasetAsyncDiskServiceFixer extends Thread {\n\n    private volatile boolean stopped = false;\n\n    private final MiniDFSCluster cluster;\n\n    FsDatasetAsyncDiskServiceFixer(MiniDFSCluster cluster) {\n      super(\"FsDatasetAsyncDiskServiceFixer\");\n      setDaemon(true);\n      this.cluster = cluster;\n    }\n\n    @Override\n    public void run() {\n      while (!stopped) {\n        try {\n          Thread.sleep(30000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          continue;\n        }\n        // we could add new datanodes during tests, so here we will check every 30 seconds, as the\n        // timeout of the thread pool executor is 60 seconds by default.\n        try {\n          for (DataNode dn : cluster.getDataNodes()) {\n            FsDatasetSpi<?> dataset = dn.getFSDataset();\n            Field service = dataset.getClass().getDeclaredField(\"asyncDiskService\");\n            service.setAccessible(true);\n            Object asyncDiskService = service.get(dataset);\n            Field group = asyncDiskService.getClass().getDeclaredField(\"threadGroup\");\n            group.setAccessible(true);\n            ThreadGroup threadGroup = (ThreadGroup) group.get(asyncDiskService);\n            if (threadGroup.isDaemon()) {\n              threadGroup.setDaemon(false);\n            }\n          }\n        } catch (NoSuchFieldException e) {\n          LOG.debug(\"NoSuchFieldException: \" + e.getMessage()\n            + \"; It might because your Hadoop version > 3.2.3 or 3.3.4, \"\n            + \"See HBASE-27595 for details.\");\n        } catch (Exception e) {\n          LOG.warn(\"failed to reset thread pool timeout for FsDatasetAsyncDiskService\", e);\n        }\n      }\n    }\n\n    void shutdown() {\n      stopped = true;\n      interrupt();\n    }\n  }\n\n  /**\n   * Start a minidfscluster.\n   * @param servers How many DNs to start.\n   * @see #shutdownMiniDFSCluster()\n   * @return The mini dfs cluster created.\n   */\n  public MiniDFSCluster startMiniDFSCluster(int servers) throws Exception {\n    return startMiniDFSCluster(servers, null);\n  }\n\n  /**\n   * Start a minidfscluster. This is useful if you want to run datanode on distinct hosts for things\n   * like HDFS block location verification. If you start MiniDFSCluster without host names, all\n   * instances of the datanodes will have the same host name.\n   * @param hosts hostnames DNs to run on.\n   * @see #shutdownMiniDFSCluster()\n   * @return The mini dfs cluster created.\n   */\n  public MiniDFSCluster startMiniDFSCluster(final String hosts[]) throws Exception {\n    if (hosts != null && hosts.length != 0) {\n      return startMiniDFSCluster(hosts.length, hosts);\n    } else {\n      return startMiniDFSCluster(1, null);\n    }\n  }\n\n  /**\n   * Start a minidfscluster. Can only create one.\n   * @param servers How many DNs to start.\n   * @param hosts   hostnames DNs to run on.\n   * @see #shutdownMiniDFSCluster()\n   * @return The mini dfs cluster created.\n   */\n  public MiniDFSCluster startMiniDFSCluster(int servers, final String hosts[]) throws Exception {\n    return startMiniDFSCluster(servers, null, hosts);\n  }\n\n  private void setFs() throws IOException {\n    if (this.dfsCluster == null) {\n      LOG.info(\"Skipping setting fs because dfsCluster is null\");\n      return;\n    }\n    FileSystem fs = this.dfsCluster.getFileSystem();\n    CommonFSUtils.setFsDefault(this.conf, new Path(fs.getUri()));\n\n    // re-enable this check with dfs\n    conf.unset(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE);\n  }\n\n  public MiniDFSCluster startMiniDFSCluster(int servers, final String racks[], String hosts[])\n    throws Exception {\n    createDirsAndSetProperties();\n    EditLogFileOutputStream.setShouldSkipFsyncForTesting(true);\n\n    this.dfsCluster =\n      new MiniDFSCluster(0, this.conf, servers, true, true, true, null, racks, hosts, null);\n    this.dfsClusterFixer = new FsDatasetAsyncDiskServiceFixer(dfsCluster);\n    this.dfsClusterFixer.start();\n    // Set this just-started cluster as our filesystem.\n    setFs();\n\n    // Wait for the cluster to be totally up\n    this.dfsCluster.waitClusterUp();\n\n    // reset the test directory for test file system\n    dataTestDirOnTestFS = null;\n    String dataTestDir = getDataTestDir().toString();\n    conf.set(HConstants.HBASE_DIR, dataTestDir);\n    LOG.debug(\"Setting {} to {}\", HConstants.HBASE_DIR, dataTestDir);\n\n    return this.dfsCluster;\n  }\n\n  public MiniDFSCluster startMiniDFSClusterForTestWAL(int namenodePort) throws IOException {\n    createDirsAndSetProperties();\n    dfsCluster =\n      new MiniDFSCluster(namenodePort, conf, 5, false, true, true, null, null, null, null);\n    this.dfsClusterFixer = new FsDatasetAsyncDiskServiceFixer(dfsCluster);\n    this.dfsClusterFixer.start();\n    return dfsCluster;\n  }\n\n  /**\n   * This is used before starting HDFS and map-reduce mini-clusters Run something like the below to\n   * check for the likes of '/tmp' references -- i.e. references outside of the test data dir -- in\n   * the conf.\n   *\n   * <pre>\n   * Configuration conf = TEST_UTIL.getConfiguration();\n   * for (Iterator&lt;Map.Entry&lt;String, String&gt;&gt; i = conf.iterator(); i.hasNext();) {\n   *   Map.Entry&lt;String, String&gt; e = i.next();\n   *   assertFalse(e.getKey() + \" \" + e.getValue(), e.getValue().contains(\"/tmp\"));\n   * }\n   * </pre>\n   */\n  private void createDirsAndSetProperties() throws IOException {\n    setupClusterTestDir();\n    conf.set(TEST_DIRECTORY_KEY, clusterTestDir.getPath());\n    System.setProperty(TEST_DIRECTORY_KEY, clusterTestDir.getPath());\n    createDirAndSetProperty(\"test.cache.data\");\n    createDirAndSetProperty(\"hadoop.tmp.dir\");\n    hadoopLogDir = createDirAndSetProperty(\"hadoop.log.dir\");\n    createDirAndSetProperty(\"mapreduce.cluster.local.dir\");\n    createDirAndSetProperty(\"mapreduce.cluster.temp.dir\");\n    enableShortCircuit();\n\n    Path root = getDataTestDirOnTestFS(\"hadoop\");\n    conf.set(MapreduceTestingShim.getMROutputDirProp(),\n      new Path(root, \"mapred-output-dir\").toString());\n    conf.set(\"mapreduce.jobtracker.system.dir\", new Path(root, \"mapred-system-dir\").toString());\n    conf.set(\"mapreduce.jobtracker.staging.root.dir\",\n      new Path(root, \"mapreduce-jobtracker-staging-root-dir\").toString());\n    conf.set(\"mapreduce.job.working.dir\", new Path(root, \"mapred-working-dir\").toString());\n    conf.set(\"yarn.app.mapreduce.am.staging-dir\",\n      new Path(root, \"mapreduce-am-staging-root-dir\").toString());\n\n    // Frustrate yarn's and hdfs's attempts at writing /tmp.\n    // Below is fragile. Make it so we just interpolate any 'tmp' reference.\n    createDirAndSetProperty(\"yarn.node-labels.fs-store.root-dir\");\n    createDirAndSetProperty(\"yarn.node-attribute.fs-store.root-dir\");\n    createDirAndSetProperty(\"yarn.nodemanager.log-dirs\");\n    createDirAndSetProperty(\"yarn.nodemanager.remote-app-log-dir\");\n    createDirAndSetProperty(\"yarn.timeline-service.entity-group-fs-store.active-dir\");\n    createDirAndSetProperty(\"yarn.timeline-service.entity-group-fs-store.done-dir\");\n    createDirAndSetProperty(\"yarn.nodemanager.remote-app-log-dir\");\n    createDirAndSetProperty(\"dfs.journalnode.edits.dir\");\n    createDirAndSetProperty(\"dfs.datanode.shared.file.descriptor.paths\");\n    createDirAndSetProperty(\"nfs.dump.dir\");\n    createDirAndSetProperty(\"java.io.tmpdir\");\n    createDirAndSetProperty(\"dfs.journalnode.edits.dir\");\n    createDirAndSetProperty(\"dfs.provided.aliasmap.inmemory.leveldb.dir\");\n    createDirAndSetProperty(\"fs.s3a.committer.staging.tmp.path\");\n  }\n\n  /**\n   * Check whether the tests should assume NEW_VERSION_BEHAVIOR when creating new column families.\n   * Default to false.\n   */\n  public boolean isNewVersionBehaviorEnabled() {\n    final String propName = \"hbase.tests.new.version.behavior\";\n    String v = System.getProperty(propName);\n    if (v != null) {\n      return Boolean.parseBoolean(v);\n    }\n    return false;\n  }\n\n  /**\n   * Get the HBase setting for dfs.client.read.shortcircuit from the conf or a system property. This\n   * allows to specify this parameter on the command line. If not set, default is true.\n   */\n  public boolean isReadShortCircuitOn() {\n    final String propName = \"hbase.tests.use.shortcircuit.reads\";\n    String readOnProp = System.getProperty(propName);\n    if (readOnProp != null) {\n      return Boolean.parseBoolean(readOnProp);\n    } else {\n      return conf.getBoolean(propName, false);\n    }\n  }\n\n  /**\n   * Enable the short circuit read, unless configured differently. Set both HBase and HDFS settings,\n   * including skipping the hdfs checksum checks.\n   */\n  private void enableShortCircuit() {\n    if (isReadShortCircuitOn()) {\n      String curUser = System.getProperty(\"user.name\");\n      LOG.info(\"read short circuit is ON for user \" + curUser);\n      // read short circuit, for hdfs\n      conf.set(\"dfs.block.local-path-access.user\", curUser);\n      // read short circuit, for hbase\n      conf.setBoolean(\"dfs.client.read.shortcircuit\", true);\n      // Skip checking checksum, for the hdfs client and the datanode\n      conf.setBoolean(\"dfs.client.read.shortcircuit.skip.checksum\", true);\n    } else {\n      LOG.info(\"read short circuit is OFF\");\n    }\n  }\n\n  private String createDirAndSetProperty(final String property) {\n    return createDirAndSetProperty(property, property);\n  }\n\n  private String createDirAndSetProperty(final String relPath, String property) {\n    String path = getDataTestDir(relPath).toString();\n    System.setProperty(property, path);\n    conf.set(property, path);\n    new File(path).mkdirs();\n    LOG.info(\"Setting \" + property + \" to \" + path + \" in system properties and HBase conf\");\n    return path;\n  }\n\n  /**\n   * Shuts down instance created by call to {@link #startMiniDFSCluster(int)} or does nothing.\n   */\n  public void shutdownMiniDFSCluster() throws IOException {\n    if (this.dfsCluster != null) {\n      // The below throws an exception per dn, AsynchronousCloseException.\n      this.dfsCluster.shutdown();\n      dfsCluster = null;\n      // It is possible that the dfs cluster is set through setDFSCluster method, where we will not\n      // have a fixer\n      if (dfsClusterFixer != null) {\n        this.dfsClusterFixer.shutdown();\n        dfsClusterFixer = null;\n      }\n      dataTestDirOnTestFS = null;\n      CommonFSUtils.setFsDefault(this.conf, new Path(\"file:///\"));\n    }\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper where WAL's walDir is created separately.\n   * All other options will use default values, defined in {@link StartMiniClusterOption.Builder}.\n   * @param createWALDir Whether to create a new WAL directory.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(boolean createWALDir) throws Exception {\n    StartMiniClusterOption option =\n      StartMiniClusterOption.builder().createWALDir(createWALDir).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numSlaves     Slave node number, for both HBase region server and HDFS data node.\n   * @param createRootDir Whether to create a new root or data directory path.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numSlaves, boolean createRootDir) throws Exception {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numRegionServers(numSlaves)\n      .numDataNodes(numSlaves).createRootDir(createRootDir).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numSlaves     Slave node number, for both HBase region server and HDFS data node.\n   * @param createRootDir Whether to create a new root or data directory path.\n   * @param createWALDir  Whether to create a new WAL directory.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numSlaves, boolean createRootDir,\n    boolean createWALDir) throws Exception {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numRegionServers(numSlaves)\n      .numDataNodes(numSlaves).createRootDir(createRootDir).createWALDir(createWALDir).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numMasters    Master node number.\n   * @param numSlaves     Slave node number, for both HBase region server and HDFS data node.\n   * @param createRootDir Whether to create a new root or data directory path.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numMasters, int numSlaves, boolean createRootDir)\n    throws Exception {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .numRegionServers(numSlaves).createRootDir(createRootDir).numDataNodes(numSlaves).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numMasters Master node number.\n   * @param numSlaves  Slave node number, for both HBase region server and HDFS data node.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numMasters, int numSlaves) throws Exception {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .numRegionServers(numSlaves).numDataNodes(numSlaves).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numMasters    Master node number.\n   * @param numSlaves     Slave node number, for both HBase region server and HDFS data node.\n   * @param dataNodeHosts The hostnames of DataNodes to run on. If not null, its size will overwrite\n   *                      HDFS data node number.\n   * @param createRootDir Whether to create a new root or data directory path.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numMasters, int numSlaves, String[] dataNodeHosts,\n    boolean createRootDir) throws Exception {\n    StartMiniClusterOption option =\n      StartMiniClusterOption.builder().numMasters(numMasters).numRegionServers(numSlaves)\n        .createRootDir(createRootDir).numDataNodes(numSlaves).dataNodeHosts(dataNodeHosts).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numMasters    Master node number.\n   * @param numSlaves     Slave node number, for both HBase region server and HDFS data node.\n   * @param dataNodeHosts The hostnames of DataNodes to run on. If not null, its size will overwrite\n   *                      HDFS data node number.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numMasters, int numSlaves, String[] dataNodeHosts)\n    throws Exception {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .numRegionServers(numSlaves).numDataNodes(numSlaves).dataNodeHosts(dataNodeHosts).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numMasters       Master node number.\n   * @param numRegionServers Number of region servers.\n   * @param numDataNodes     Number of datanodes.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numMasters, int numRegionServers, int numDataNodes)\n    throws Exception {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .numRegionServers(numRegionServers).numDataNodes(numDataNodes).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numMasters    Master node number.\n   * @param numSlaves     Slave node number, for both HBase region server and HDFS data node.\n   * @param dataNodeHosts The hostnames of DataNodes to run on. If not null, its size will overwrite\n   *                      HDFS data node number.\n   * @param masterClass   The class to use as HMaster, or null for default.\n   * @param rsClass       The class to use as HRegionServer, or null for default.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numMasters, int numSlaves, String[] dataNodeHosts,\n    Class<? extends HMaster> masterClass,\n    Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> rsClass) throws Exception {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .masterClass(masterClass).numRegionServers(numSlaves).rsClass(rsClass).numDataNodes(numSlaves)\n      .dataNodeHosts(dataNodeHosts).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numMasters       Master node number.\n   * @param numRegionServers Number of region servers.\n   * @param numDataNodes     Number of datanodes.\n   * @param dataNodeHosts    The hostnames of DataNodes to run on. If not null, its size will\n   *                         overwrite HDFS data node number.\n   * @param masterClass      The class to use as HMaster, or null for default.\n   * @param rsClass          The class to use as HRegionServer, or null for default.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numMasters, int numRegionServers, int numDataNodes,\n    String[] dataNodeHosts, Class<? extends HMaster> masterClass,\n    Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> rsClass) throws Exception {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .masterClass(masterClass).numRegionServers(numRegionServers).rsClass(rsClass)\n      .numDataNodes(numDataNodes).dataNodeHosts(dataNodeHosts).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs, and zookeeper. All other options will use default values,\n   * defined in {@link StartMiniClusterOption.Builder}.\n   * @param numMasters       Master node number.\n   * @param numRegionServers Number of region servers.\n   * @param numDataNodes     Number of datanodes.\n   * @param dataNodeHosts    The hostnames of DataNodes to run on. If not null, its size will\n   *                         overwrite HDFS data node number.\n   * @param masterClass      The class to use as HMaster, or null for default.\n   * @param rsClass          The class to use as HRegionServer, or null for default.\n   * @param createRootDir    Whether to create a new root or data directory path.\n   * @param createWALDir     Whether to create a new WAL directory.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniCluster(int numMasters, int numRegionServers, int numDataNodes,\n    String[] dataNodeHosts, Class<? extends HMaster> masterClass,\n    Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> rsClass, boolean createRootDir,\n    boolean createWALDir) throws Exception {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .masterClass(masterClass).numRegionServers(numRegionServers).rsClass(rsClass)\n      .numDataNodes(numDataNodes).dataNodeHosts(dataNodeHosts).createRootDir(createRootDir)\n      .createWALDir(createWALDir).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs and zookeeper clusters with given slave node number. All\n   * other options will use default values, defined in {@link StartMiniClusterOption.Builder}.\n   * @param numSlaves slave node number, for both HBase region server and HDFS data node.\n   * @see #startMiniCluster(StartMiniClusterOption option)\n   * @see #shutdownMiniDFSCluster()\n   */\n  public MiniHBaseCluster startMiniCluster(int numSlaves) throws Exception {\n    StartMiniClusterOption option =\n      StartMiniClusterOption.builder().numRegionServers(numSlaves).numDataNodes(numSlaves).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs and zookeeper all using default options. Option default\n   * value can be found in {@link StartMiniClusterOption.Builder}.\n   * @see #startMiniCluster(StartMiniClusterOption option)\n   * @see #shutdownMiniDFSCluster()\n   */\n  public MiniHBaseCluster startMiniCluster() throws Exception {\n    return startMiniCluster(StartMiniClusterOption.builder().build());\n  }\n\n  /**\n   * Start up a mini cluster of hbase, optionally dfs and zookeeper if needed. It modifies\n   * Configuration. It homes the cluster data directory under a random subdirectory in a directory\n   * under System property test.build.data, to be cleaned up on exit.\n   * @see #shutdownMiniDFSCluster()\n   */\n  public MiniHBaseCluster startMiniCluster(StartMiniClusterOption option) throws Exception {\n    LOG.info(\"Starting up minicluster with option: {}\", option);\n\n    // If we already put up a cluster, fail.\n    if (miniClusterRunning) {\n      throw new IllegalStateException(\"A mini-cluster is already running\");\n    }\n    miniClusterRunning = true;\n\n    setupClusterTestDir();\n    System.setProperty(TEST_DIRECTORY_KEY, this.clusterTestDir.getPath());\n\n    // Bring up mini dfs cluster. This spews a bunch of warnings about missing\n    // scheme. Complaints are 'Scheme is undefined for build/test/data/dfs/name1'.\n    if (dfsCluster == null) {\n      LOG.info(\"STARTING DFS\");\n      dfsCluster = startMiniDFSCluster(option.getNumDataNodes(), option.getDataNodeHosts());\n    } else {\n      LOG.info(\"NOT STARTING DFS\");\n    }\n\n    // Start up a zk cluster.\n    if (getZkCluster() == null) {\n      startMiniZKCluster(option.getNumZkServers());\n    }\n\n    // Start the MiniHBaseCluster\n    return startMiniHBaseCluster(option);\n  }\n\n  /**\n   * Starts up mini hbase cluster. Usually you won't want this. You'll usually want\n   * {@link #startMiniCluster()}. This is useful when doing stepped startup of clusters.\n   * @return Reference to the hbase mini hbase cluster.\n   * @see #startMiniCluster(StartMiniClusterOption)\n   * @see #shutdownMiniHBaseCluster()\n   */\n  public MiniHBaseCluster startMiniHBaseCluster(StartMiniClusterOption option)\n    throws IOException, InterruptedException {\n    // Now do the mini hbase cluster. Set the hbase.rootdir in config.\n    createRootDir(option.isCreateRootDir());\n    if (option.isCreateWALDir()) {\n      createWALRootDir();\n    }\n    // Set the hbase.fs.tmp.dir config to make sure that we have some default value. This is\n    // for tests that do not read hbase-defaults.xml\n    setHBaseFsTmpDir();\n\n    // These settings will make the server waits until this exact number of\n    // regions servers are connected.\n    if (conf.getInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, -1) == -1) {\n      conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, option.getNumRegionServers());\n    }\n    if (conf.getInt(ServerManager.WAIT_ON_REGIONSERVERS_MAXTOSTART, -1) == -1) {\n      conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MAXTOSTART, option.getNumRegionServers());\n    }\n\n    Configuration c = new Configuration(this.conf);\n    this.hbaseCluster = new MiniHBaseCluster(c, option.getNumMasters(),\n      option.getNumAlwaysStandByMasters(), option.getNumRegionServers(), option.getRsPorts(),\n      option.getMasterClass(), option.getRsClass());\n    // Populate the master address configuration from mini cluster configuration.\n    conf.set(HConstants.MASTER_ADDRS_KEY, MasterRegistry.getMasterAddr(c));\n    // Don't leave here till we've done a successful scan of the hbase:meta\n    try (Table t = getConnection().getTable(TableName.META_TABLE_NAME);\n      ResultScanner s = t.getScanner(new Scan())) {\n      for (;;) {\n        if (s.next() == null) {\n          break;\n        }\n      }\n    }\n\n    getAdmin(); // create immediately the hbaseAdmin\n    LOG.info(\"Minicluster is up; activeMaster={}\", getHBaseCluster().getMaster());\n\n    return (MiniHBaseCluster) hbaseCluster;\n  }\n\n  /**\n   * Starts up mini hbase cluster using default options. Default options can be found in\n   * {@link StartMiniClusterOption.Builder}.\n   * @see #startMiniHBaseCluster(StartMiniClusterOption)\n   * @see #shutdownMiniHBaseCluster()\n   */\n  public MiniHBaseCluster startMiniHBaseCluster() throws IOException, InterruptedException {\n    return startMiniHBaseCluster(StartMiniClusterOption.builder().build());\n  }\n\n  /**\n   * Starts up mini hbase cluster. Usually you won't want this. You'll usually want\n   * {@link #startMiniCluster()}. All other options will use default values, defined in\n   * {@link StartMiniClusterOption.Builder}.\n   * @param numMasters       Master node number.\n   * @param numRegionServers Number of region servers.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniHBaseCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniHBaseCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniHBaseCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniHBaseCluster(int numMasters, int numRegionServers)\n    throws IOException, InterruptedException {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .numRegionServers(numRegionServers).build();\n    return startMiniHBaseCluster(option);\n  }\n\n  /**\n   * Starts up mini hbase cluster. Usually you won't want this. You'll usually want\n   * {@link #startMiniCluster()}. All other options will use default values, defined in\n   * {@link StartMiniClusterOption.Builder}.\n   * @param numMasters       Master node number.\n   * @param numRegionServers Number of region servers.\n   * @param rsPorts          Ports that RegionServer should use.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniHBaseCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniHBaseCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniHBaseCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniHBaseCluster(int numMasters, int numRegionServers,\n    List<Integer> rsPorts) throws IOException, InterruptedException {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .numRegionServers(numRegionServers).rsPorts(rsPorts).build();\n    return startMiniHBaseCluster(option);\n  }\n\n  /**\n   * Starts up mini hbase cluster. Usually you won't want this. You'll usually want\n   * {@link #startMiniCluster()}. All other options will use default values, defined in\n   * {@link StartMiniClusterOption.Builder}.\n   * @param numMasters       Master node number.\n   * @param numRegionServers Number of region servers.\n   * @param rsPorts          Ports that RegionServer should use.\n   * @param masterClass      The class to use as HMaster, or null for default.\n   * @param rsClass          The class to use as HRegionServer, or null for default.\n   * @param createRootDir    Whether to create a new root or data directory path.\n   * @param createWALDir     Whether to create a new WAL directory.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniHBaseCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniHBaseCluster(StartMiniClusterOption)} instead.\n   * @see #startMiniHBaseCluster(StartMiniClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public MiniHBaseCluster startMiniHBaseCluster(int numMasters, int numRegionServers,\n    List<Integer> rsPorts, Class<? extends HMaster> masterClass,\n    Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> rsClass, boolean createRootDir,\n    boolean createWALDir) throws IOException, InterruptedException {\n    StartMiniClusterOption option = StartMiniClusterOption.builder().numMasters(numMasters)\n      .masterClass(masterClass).numRegionServers(numRegionServers).rsClass(rsClass).rsPorts(rsPorts)\n      .createRootDir(createRootDir).createWALDir(createWALDir).build();\n    return startMiniHBaseCluster(option);\n  }\n\n  /**\n   * Starts the hbase cluster up again after shutting it down previously in a test. Use this if you\n   * want to keep dfs/zk up and just stop/start hbase.\n   * @param servers number of region servers\n   */\n  public void restartHBaseCluster(int servers) throws IOException, InterruptedException {\n    this.restartHBaseCluster(servers, null);\n  }\n\n  public void restartHBaseCluster(int servers, List<Integer> ports)\n    throws IOException, InterruptedException {\n    StartMiniClusterOption option =\n      StartMiniClusterOption.builder().numRegionServers(servers).rsPorts(ports).build();\n    restartHBaseCluster(option);\n    invalidateConnection();\n  }\n\n  public void restartHBaseCluster(StartMiniClusterOption option)\n    throws IOException, InterruptedException {\n    closeConnection();\n    this.hbaseCluster = new MiniHBaseCluster(this.conf, option.getNumMasters(),\n      option.getNumAlwaysStandByMasters(), option.getNumRegionServers(), option.getRsPorts(),\n      option.getMasterClass(), option.getRsClass());\n    // Don't leave here till we've done a successful scan of the hbase:meta\n    Connection conn = ConnectionFactory.createConnection(this.conf);\n    Table t = conn.getTable(TableName.META_TABLE_NAME);\n    ResultScanner s = t.getScanner(new Scan());\n    while (s.next() != null) {\n      // do nothing\n    }\n    LOG.info(\"HBase has been restarted\");\n    s.close();\n    t.close();\n    conn.close();\n  }\n\n  /**\n   * @return Current mini hbase cluster. Only has something in it after a call to\n   *         {@link #startMiniCluster()}.\n   * @see #startMiniCluster()\n   */\n  public MiniHBaseCluster getMiniHBaseCluster() {\n    if (this.hbaseCluster == null || this.hbaseCluster instanceof MiniHBaseCluster) {\n      return (MiniHBaseCluster) this.hbaseCluster;\n    }\n    throw new RuntimeException(\n      hbaseCluster + \" not an instance of \" + MiniHBaseCluster.class.getName());\n  }\n\n  /**\n   * Stops mini hbase, zk, and hdfs clusters.\n   * @see #startMiniCluster(int)\n   */\n  public void shutdownMiniCluster() throws IOException {\n    LOG.info(\"Shutting down minicluster\");\n    shutdownMiniHBaseCluster();\n    shutdownMiniDFSCluster();\n    shutdownMiniZKCluster();\n\n    cleanupTestDir();\n    miniClusterRunning = false;\n    LOG.info(\"Minicluster is down\");\n  }\n\n  /**\n   * Shutdown HBase mini cluster.Does not shutdown zk or dfs if running.\n   * @throws java.io.IOException in case command is unsuccessful\n   */\n  public void shutdownMiniHBaseCluster() throws IOException {\n    cleanup();\n    if (this.hbaseCluster != null) {\n      this.hbaseCluster.shutdown();\n      // Wait till hbase is down before going on to shutdown zk.\n      this.hbaseCluster.waitUntilShutDown();\n      this.hbaseCluster = null;\n    }\n    if (zooKeeperWatcher != null) {\n      zooKeeperWatcher.close();\n      zooKeeperWatcher = null;\n    }\n  }\n\n  /**\n   * Abruptly Shutdown HBase mini cluster. Does not shutdown zk or dfs if running.\n   * @throws java.io.IOException throws in case command is unsuccessful\n   */\n  public void killMiniHBaseCluster() throws IOException {\n    cleanup();\n    if (this.hbaseCluster != null) {\n      getMiniHBaseCluster().killAll();\n      this.hbaseCluster = null;\n    }\n    if (zooKeeperWatcher != null) {\n      zooKeeperWatcher.close();\n      zooKeeperWatcher = null;\n    }\n  }\n\n  // close hbase admin, close current connection and reset MIN MAX configs for RS.\n  private void cleanup() throws IOException {\n    closeConnection();\n    // unset the configuration for MIN and MAX RS to start\n    conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, -1);\n    conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MAXTOSTART, -1);\n  }\n\n  /**\n   * Returns the path to the default root dir the minicluster uses. If <code>create</code> is true,\n   * a new root directory path is fetched irrespective of whether it has been fetched before or not.\n   * If false, previous path is used. Note: this does not cause the root dir to be created.\n   * @return Fully qualified path for the default hbase root dir\n   */\n  public Path getDefaultRootDirPath(boolean create) throws IOException {\n    if (!create) {\n      return getDataTestDirOnTestFS();\n    } else {\n      return getNewDataTestDirOnTestFS();\n    }\n  }\n\n  /**\n   * Same as {{@link HBaseTestingUtility#getDefaultRootDirPath(boolean create)} except that\n   * <code>create</code> flag is false. Note: this does not cause the root dir to be created.\n   * @return Fully qualified path for the default hbase root dir\n   */\n  public Path getDefaultRootDirPath() throws IOException {\n    return getDefaultRootDirPath(false);\n  }\n\n  /**\n   * Creates an hbase rootdir in user home directory. Also creates hbase version file. Normally you\n   * won't make use of this method. Root hbasedir is created for you as part of mini cluster\n   * startup. You'd only use this method if you were doing manual operation.\n   * @param create This flag decides whether to get a new root or data directory path or not, if it\n   *               has been fetched already. Note : Directory will be made irrespective of whether\n   *               path has been fetched or not. If directory already exists, it will be overwritten\n   * @return Fully qualified path to hbase root dir\n   */\n  public Path createRootDir(boolean create) throws IOException {\n    FileSystem fs = FileSystem.get(this.conf);\n    Path hbaseRootdir = getDefaultRootDirPath(create);\n    CommonFSUtils.setRootDir(this.conf, hbaseRootdir);\n    fs.mkdirs(hbaseRootdir);\n    FSUtils.setVersion(fs, hbaseRootdir);\n    return hbaseRootdir;\n  }\n\n  /**\n   * Same as {@link HBaseTestingUtility#createRootDir(boolean create)} except that\n   * <code>create</code> flag is false.\n   * @return Fully qualified path to hbase root dir\n   */\n  public Path createRootDir() throws IOException {\n    return createRootDir(false);\n  }\n\n  /**\n   * Creates a hbase walDir in the user's home directory. Normally you won't make use of this\n   * method. Root hbaseWALDir is created for you as part of mini cluster startup. You'd only use\n   * this method if you were doing manual operation.\n   * @return Fully qualified path to hbase root dir\n   */\n  public Path createWALRootDir() throws IOException {\n    FileSystem fs = FileSystem.get(this.conf);\n    Path walDir = getNewDataTestDirOnTestFS();\n    CommonFSUtils.setWALRootDir(this.conf, walDir);\n    fs.mkdirs(walDir);\n    return walDir;\n  }\n\n  private void setHBaseFsTmpDir() throws IOException {\n    String hbaseFsTmpDirInString = this.conf.get(\"hbase.fs.tmp.dir\");\n    if (hbaseFsTmpDirInString == null) {\n      this.conf.set(\"hbase.fs.tmp.dir\", getDataTestDirOnTestFS(\"hbase-staging\").toString());\n      LOG.info(\"Setting hbase.fs.tmp.dir to \" + this.conf.get(\"hbase.fs.tmp.dir\"));\n    } else {\n      LOG.info(\"The hbase.fs.tmp.dir is set to \" + hbaseFsTmpDirInString);\n    }\n  }\n\n  /**\n   * Flushes all caches in the mini hbase cluster\n   */\n  public void flush() throws IOException {\n    getMiniHBaseCluster().flushcache();\n  }\n\n  /**\n   * Flushes all caches in the mini hbase cluster\n   */\n  public void flush(TableName tableName) throws IOException {\n    getMiniHBaseCluster().flushcache(tableName);\n  }\n\n  /**\n   * Compact all regions in the mini hbase cluster\n   */\n  public void compact(boolean major) throws IOException {\n    getMiniHBaseCluster().compact(major);\n  }\n\n  /**\n   * Compact all of a table's reagion in the mini hbase cluster\n   */\n  public void compact(TableName tableName, boolean major) throws IOException {\n    getMiniHBaseCluster().compact(tableName, major);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, String family) throws IOException {\n    return createTable(tableName, new String[] { family });\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, String[] families) throws IOException {\n    List<byte[]> fams = new ArrayList<>(families.length);\n    for (String family : families) {\n      fams.add(Bytes.toBytes(family));\n    }\n    return createTable(tableName, fams.toArray(new byte[0][]));\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[] family) throws IOException {\n    return createTable(tableName, new byte[][] { family });\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, byte[] family, int numRegions)\n    throws IOException {\n    if (numRegions < 3) throw new IOException(\"Must create at least 3 regions\");\n    byte[] startKey = Bytes.toBytes(\"aaaaa\");\n    byte[] endKey = Bytes.toBytes(\"zzzzz\");\n    byte[][] splitKeys = Bytes.split(startKey, endKey, numRegions - 3);\n\n    return createTable(tableName, new byte[][] { family }, splitKeys);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families) throws IOException {\n    return createTable(tableName, families, (byte[][]) null);\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, byte[][] families) throws IOException {\n    return createTable(tableName, families, KEYS_FOR_HBA_CREATE_TABLE);\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @param replicaCount replica count.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, int replicaCount, byte[][] families)\n    throws IOException {\n    return createTable(tableName, families, KEYS_FOR_HBA_CREATE_TABLE, replicaCount);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, byte[][] splitKeys)\n    throws IOException {\n    return createTable(tableName, families, splitKeys, 1, new Configuration(getConfiguration()));\n  }\n\n  /**\n   * Create a table.\n   * @param tableName    the table name\n   * @param families     the families\n   * @param splitKeys    the splitkeys\n   * @param replicaCount the region replica count\n   * @return A Table instance for the created table.\n   * @throws IOException throws IOException\n   */\n  public Table createTable(TableName tableName, byte[][] families, byte[][] splitKeys,\n    int replicaCount) throws IOException {\n    return createTable(tableName, families, splitKeys, replicaCount,\n      new Configuration(getConfiguration()));\n  }\n\n  public Table createTable(TableName tableName, byte[][] families, int numVersions, byte[] startKey,\n    byte[] endKey, int numRegions) throws IOException {\n    TableDescriptor desc = createTableDescriptor(tableName, families, numVersions);\n\n    getAdmin().createTable(desc, startKey, endKey, numRegions);\n    // HBaseAdmin only waits for regions to appear in hbase:meta we\n    // should wait until they are assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table.\n   * @param c Configuration to use\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableDescriptor htd, byte[][] families, Configuration c)\n    throws IOException {\n    return createTable(htd, families, null, c);\n  }\n\n  /**\n   * Create a table.\n   * @param htd       table descriptor\n   * @param families  array of column families\n   * @param splitKeys array of split keys\n   * @param c         Configuration to use\n   * @return A Table instance for the created table.\n   * @throws IOException if getAdmin or createTable fails\n   */\n  public Table createTable(TableDescriptor htd, byte[][] families, byte[][] splitKeys,\n    Configuration c) throws IOException {\n    // Disable blooms (they are on by default as of 0.95) but we disable them here because\n    // tests have hard coded counts of what to expect in block cache, etc., and blooms being\n    // on is interfering.\n    return createTable(htd, families, splitKeys, BloomType.NONE, HConstants.DEFAULT_BLOCKSIZE, c);\n  }\n\n  /**\n   * Create a table.\n   * @param htd       table descriptor\n   * @param families  array of column families\n   * @param splitKeys array of split keys\n   * @param type      Bloom type\n   * @param blockSize block size\n   * @param c         Configuration to use\n   * @return A Table instance for the created table.\n   * @throws IOException if getAdmin or createTable fails\n   */\n\n  public Table createTable(TableDescriptor htd, byte[][] families, byte[][] splitKeys,\n    BloomType type, int blockSize, Configuration c) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(htd);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfdb = ColumnFamilyDescriptorBuilder.newBuilder(family)\n        .setBloomFilterType(type).setBlocksize(blockSize);\n      if (isNewVersionBehaviorEnabled()) {\n        cfdb.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfdb.build());\n    }\n    TableDescriptor td = builder.build();\n    if (splitKeys != null) {\n      getAdmin().createTable(td, splitKeys);\n    } else {\n      getAdmin().createTable(td);\n    }\n    // HBaseAdmin only waits for regions to appear in hbase:meta\n    // we should wait until they are assigned\n    waitUntilAllRegionsAssigned(td.getTableName());\n    return getConnection().getTable(td.getTableName());\n  }\n\n  /**\n   * Create a table.\n   * @param htd       table descriptor\n   * @param splitRows array of split keys\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableDescriptor htd, byte[][] splitRows) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(htd);\n    if (isNewVersionBehaviorEnabled()) {\n      for (ColumnFamilyDescriptor family : htd.getColumnFamilies()) {\n        builder.setColumnFamily(\n          ColumnFamilyDescriptorBuilder.newBuilder(family).setNewVersionBehavior(true).build());\n      }\n    }\n    if (splitRows != null) {\n      getAdmin().createTable(builder.build(), splitRows);\n    } else {\n      getAdmin().createTable(builder.build());\n    }\n    // HBaseAdmin only waits for regions to appear in hbase:meta\n    // we should wait until they are assigned\n    waitUntilAllRegionsAssigned(htd.getTableName());\n    return getConnection().getTable(htd.getTableName());\n  }\n\n  /**\n   * Create a table.\n   * @param tableName    the table name\n   * @param families     the families\n   * @param splitKeys    the split keys\n   * @param replicaCount the replica count\n   * @param c            Configuration to use\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, byte[][] splitKeys,\n    int replicaCount, final Configuration c) throws IOException {\n    TableDescriptor htd =\n      TableDescriptorBuilder.newBuilder(tableName).setRegionReplication(replicaCount).build();\n    return createTable(htd, families, splitKeys, c);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[] family, int numVersions) throws IOException {\n    return createTable(tableName, new byte[][] { family }, numVersions);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, int numVersions)\n    throws IOException {\n    return createTable(tableName, families, numVersions, (byte[][]) null);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, int numVersions,\n    byte[][] splitKeys) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder =\n        ColumnFamilyDescriptorBuilder.newBuilder(family).setMaxVersions(numVersions);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    if (splitKeys != null) {\n      getAdmin().createTable(builder.build(), splitKeys);\n    } else {\n      getAdmin().createTable(builder.build());\n    }\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, byte[][] families, int numVersions)\n    throws IOException {\n    return createTable(tableName, families, numVersions, KEYS_FOR_HBA_CREATE_TABLE);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, int numVersions, int blockSize)\n    throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(family)\n        .setMaxVersions(numVersions).setBlocksize(blockSize);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    getAdmin().createTable(builder.build());\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  public Table createTable(TableName tableName, byte[][] families, int numVersions, int blockSize,\n    String cpName) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(family)\n        .setMaxVersions(numVersions).setBlocksize(blockSize);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    if (cpName != null) {\n      builder.setCoprocessor(cpName);\n    }\n    getAdmin().createTable(builder.build());\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, int[] numVersions)\n    throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    int i = 0;\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder =\n        ColumnFamilyDescriptorBuilder.newBuilder(family).setMaxVersions(numVersions[i]);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n      i++;\n    }\n    getAdmin().createTable(builder.build());\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[] family, byte[][] splitRows)\n    throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(family);\n    if (isNewVersionBehaviorEnabled()) {\n      cfBuilder.setNewVersionBehavior(true);\n    }\n    builder.setColumnFamily(cfBuilder.build());\n    getAdmin().createTable(builder.build(), splitRows);\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, byte[] family) throws IOException {\n    return createTable(tableName, family, KEYS_FOR_HBA_CREATE_TABLE);\n  }\n\n  /**\n   * Modify a table, synchronous.\n   * @deprecated since 3.0.0 and will be removed in 4.0.0. Just use\n   *             {@link Admin#modifyTable(TableDescriptor)} directly as it is synchronous now.\n   * @see Admin#modifyTable(TableDescriptor)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-22002\">HBASE-22002</a>\n   */\n  @Deprecated\n  public static void modifyTableSync(Admin admin, TableDescriptor desc)\n    throws IOException, InterruptedException {\n    admin.modifyTable(desc);\n  }\n\n  /**\n   * Set the number of Region replicas.\n   */\n  public static void setReplicas(Admin admin, TableName table, int replicaCount)\n    throws IOException, InterruptedException {\n    TableDescriptor desc = TableDescriptorBuilder.newBuilder(admin.getDescriptor(table))\n      .setRegionReplication(replicaCount).build();\n    admin.modifyTable(desc);\n  }\n\n  /**\n   * Drop an existing table\n   * @param tableName existing table\n   */\n  public void deleteTable(TableName tableName) throws IOException {\n    try {\n      getAdmin().disableTable(tableName);\n    } catch (TableNotEnabledException e) {\n      LOG.debug(\"Table: \" + tableName + \" already disabled, so just deleting it.\");\n    }\n    getAdmin().deleteTable(tableName);\n  }\n\n  /**\n   * Drop an existing table\n   * @param tableName existing table\n   */\n  public void deleteTableIfAny(TableName tableName) throws IOException {\n    try {\n      deleteTable(tableName);\n    } catch (TableNotFoundException e) {\n      // ignore\n    }\n  }\n\n  // ==========================================================================\n  // Canned table and table descriptor creation\n\n  public final static byte[] fam1 = Bytes.toBytes(\"colfamily11\");\n  public final static byte[] fam2 = Bytes.toBytes(\"colfamily21\");\n  public final static byte[] fam3 = Bytes.toBytes(\"colfamily31\");\n  public static final byte[][] COLUMNS = { fam1, fam2, fam3 };\n  private static final int MAXVERSIONS = 3;\n\n  public static final char FIRST_CHAR = 'a';\n  public static final char LAST_CHAR = 'z';\n  public static final byte[] START_KEY_BYTES = { FIRST_CHAR, FIRST_CHAR, FIRST_CHAR };\n  public static final String START_KEY = new String(START_KEY_BYTES, HConstants.UTF8_CHARSET);\n\n  public TableDescriptorBuilder createModifyableTableDescriptor(final String name) {\n    return createModifyableTableDescriptor(TableName.valueOf(name),\n      ColumnFamilyDescriptorBuilder.DEFAULT_MIN_VERSIONS, MAXVERSIONS, HConstants.FOREVER,\n      ColumnFamilyDescriptorBuilder.DEFAULT_KEEP_DELETED);\n  }\n\n  public TableDescriptor createTableDescriptor(final TableName name, final int minVersions,\n    final int versions, final int ttl, KeepDeletedCells keepDeleted) {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(name);\n    for (byte[] cfName : new byte[][] { fam1, fam2, fam3 }) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(cfName)\n        .setMinVersions(minVersions).setMaxVersions(versions).setKeepDeletedCells(keepDeleted)\n        .setBlockCacheEnabled(false).setTimeToLive(ttl);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    return builder.build();\n  }\n\n  public TableDescriptorBuilder createModifyableTableDescriptor(final TableName name,\n    final int minVersions, final int versions, final int ttl, KeepDeletedCells keepDeleted) {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(name);\n    for (byte[] cfName : new byte[][] { fam1, fam2, fam3 }) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(cfName)\n        .setMinVersions(minVersions).setMaxVersions(versions).setKeepDeletedCells(keepDeleted)\n        .setBlockCacheEnabled(false).setTimeToLive(ttl);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    return builder;\n  }\n\n  /**\n   * Create a table of name <code>name</code>.\n   * @param name Name to give table.\n   * @return Column descriptor.\n   */\n  public TableDescriptor createTableDescriptor(final TableName name) {\n    return createTableDescriptor(name, ColumnFamilyDescriptorBuilder.DEFAULT_MIN_VERSIONS,\n      MAXVERSIONS, HConstants.FOREVER, ColumnFamilyDescriptorBuilder.DEFAULT_KEEP_DELETED);\n  }\n\n  public TableDescriptor createTableDescriptor(final TableName tableName, byte[] family) {\n    return createTableDescriptor(tableName, new byte[][] { family }, 1);\n  }\n\n  public TableDescriptor createTableDescriptor(final TableName tableName, byte[][] families,\n    int maxVersions) {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder =\n        ColumnFamilyDescriptorBuilder.newBuilder(family).setMaxVersions(maxVersions);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    return builder.build();\n  }\n\n  /**\n   * Create an HRegion that writes to the local tmp dirs\n   * @param desc     a table descriptor indicating which table the region belongs to\n   * @param startKey the start boundary of the region\n   * @param endKey   the end boundary of the region\n   * @return a region that writes to local dir for testing\n   */\n  public HRegion createLocalHRegion(TableDescriptor desc, byte[] startKey, byte[] endKey)\n    throws IOException {\n    RegionInfo hri = RegionInfoBuilder.newBuilder(desc.getTableName()).setStartKey(startKey)\n      .setEndKey(endKey).build();\n    return createLocalHRegion(hri, desc);\n  }\n\n  /**\n   * Create an HRegion that writes to the local tmp dirs. Creates the WAL for you. Be sure to call\n   * {@link HBaseTestingUtility#closeRegionAndWAL(HRegion)} when you're finished with it.\n   */\n  public HRegion createLocalHRegion(RegionInfo info, TableDescriptor desc) throws IOException {\n    return createRegionAndWAL(info, getDataTestDir(), getConfiguration(), desc);\n  }\n\n  /**\n   * Create an HRegion that writes to the local tmp dirs with specified wal\n   * @param info regioninfo\n   * @param conf configuration\n   * @param desc table descriptor\n   * @param wal  wal for this region.\n   * @return created hregion\n   */\n  public HRegion createLocalHRegion(RegionInfo info, Configuration conf, TableDescriptor desc,\n    WAL wal) throws IOException {\n    return HRegion.createHRegion(info, getDataTestDir(), conf, desc, wal);\n  }\n\n  /**\n   * Return a region on which you must call {@link HBaseTestingUtility#closeRegionAndWAL(HRegion)}\n   * when done.\n   */\n  public HRegion createLocalHRegion(TableName tableName, byte[] startKey, byte[] stopKey,\n    Configuration conf, boolean isReadOnly, Durability durability, WAL wal, byte[]... families)\n    throws IOException {\n    return createLocalHRegionWithInMemoryFlags(tableName, startKey, stopKey, conf, isReadOnly,\n      durability, wal, null, families);\n  }\n\n  public HRegion createLocalHRegionWithInMemoryFlags(TableName tableName, byte[] startKey,\n    byte[] stopKey, Configuration conf, boolean isReadOnly, Durability durability, WAL wal,\n    boolean[] compactedMemStore, byte[]... families) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    builder.setReadOnly(isReadOnly);\n    int i = 0;\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(family);\n      if (compactedMemStore != null && i < compactedMemStore.length) {\n        cfBuilder.setInMemoryCompaction(MemoryCompactionPolicy.BASIC);\n      } else {\n        cfBuilder.setInMemoryCompaction(MemoryCompactionPolicy.NONE);\n\n      }\n      i++;\n      // Set default to be three versions.\n      cfBuilder.setMaxVersions(Integer.MAX_VALUE);\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    builder.setDurability(durability);\n    RegionInfo info =\n      RegionInfoBuilder.newBuilder(tableName).setStartKey(startKey).setEndKey(stopKey).build();\n    return createLocalHRegion(info, conf, builder.build(), wal);\n  }\n\n  //\n  // ==========================================================================\n\n  /**\n   * Provide an existing table name to truncate. Scans the table and issues a delete for each row\n   * read.\n   * @param tableName existing table\n   * @return HTable to that new table\n   */\n  public Table deleteTableData(TableName tableName) throws IOException {\n    Table table = getConnection().getTable(tableName);\n    Scan scan = new Scan();\n    ResultScanner resScan = table.getScanner(scan);\n    for (Result res : resScan) {\n      Delete del = new Delete(res.getRow());\n      table.delete(del);\n    }\n    resScan = table.getScanner(scan);\n    resScan.close();\n    return table;\n  }\n\n  /**\n   * Truncate a table using the admin command. Effectively disables, deletes, and recreates the\n   * table.\n   * @param tableName       table which must exist.\n   * @param preserveRegions keep the existing split points\n   * @return HTable for the new table\n   */\n  public Table truncateTable(final TableName tableName, final boolean preserveRegions)\n    throws IOException {\n    Admin admin = getAdmin();\n    if (!admin.isTableDisabled(tableName)) {\n      admin.disableTable(tableName);\n    }\n    admin.truncateTable(tableName, preserveRegions);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Truncate a table using the admin command. Effectively disables, deletes, and recreates the\n   * table. For previous behavior of issuing row deletes, see deleteTableData. Expressly does not\n   * preserve regions of existing table.\n   * @param tableName table which must exist.\n   * @return HTable for the new table\n   */\n  public Table truncateTable(final TableName tableName) throws IOException {\n    return truncateTable(tableName, false);\n  }\n\n  /**\n   * Load table with rows from 'aaa' to 'zzz'.\n   * @param t Table\n   * @param f Family\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[] f) throws IOException {\n    return loadTable(t, new byte[][] { f });\n  }\n\n  /**\n   * Load table with rows from 'aaa' to 'zzz'.\n   * @param t Table\n   * @param f Family\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[] f, boolean writeToWAL) throws IOException {\n    return loadTable(t, new byte[][] { f }, null, writeToWAL);\n  }\n\n  /**\n   * Load table of multiple column families with rows from 'aaa' to 'zzz'.\n   * @param t Table\n   * @param f Array of Families to load\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[][] f) throws IOException {\n    return loadTable(t, f, null);\n  }\n\n  /**\n   * Load table of multiple column families with rows from 'aaa' to 'zzz'.\n   * @param t     Table\n   * @param f     Array of Families to load\n   * @param value the values of the cells. If null is passed, the row key is used as value\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[][] f, byte[] value) throws IOException {\n    return loadTable(t, f, value, true);\n  }\n\n  /**\n   * Load table of multiple column families with rows from 'aaa' to 'zzz'.\n   * @param t     Table\n   * @param f     Array of Families to load\n   * @param value the values of the cells. If null is passed, the row key is used as value\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[][] f, byte[] value, boolean writeToWAL)\n    throws IOException {\n    List<Put> puts = new ArrayList<>();\n    for (byte[] row : HBaseTestingUtility.ROWS) {\n      Put put = new Put(row);\n      put.setDurability(writeToWAL ? Durability.USE_DEFAULT : Durability.SKIP_WAL);\n      for (int i = 0; i < f.length; i++) {\n        byte[] value1 = value != null ? value : row;\n        put.addColumn(f[i], f[i], value1);\n      }\n      puts.add(put);\n    }\n    t.put(puts);\n    return puts.size();\n  }\n\n  /**\n   * A tracker for tracking and validating table rows generated with\n   * {@link HBaseTestingUtility#loadTable(Table, byte[])}\n   */\n  public static class SeenRowTracker {\n    int dim = 'z' - 'a' + 1;\n    int[][][] seenRows = new int[dim][dim][dim]; // count of how many times the row is seen\n    byte[] startRow;\n    byte[] stopRow;\n\n    public SeenRowTracker(byte[] startRow, byte[] stopRow) {\n      this.startRow = startRow;\n      this.stopRow = stopRow;\n    }\n\n    void reset() {\n      for (byte[] row : ROWS) {\n        seenRows[i(row[0])][i(row[1])][i(row[2])] = 0;\n      }\n    }\n\n    int i(byte b) {\n      return b - 'a';\n    }\n\n    public void addRow(byte[] row) {\n      seenRows[i(row[0])][i(row[1])][i(row[2])]++;\n    }\n\n    /**\n     * Validate that all the rows between startRow and stopRow are seen exactly once, and all other\n     * rows none\n     */\n    public void validate() {\n      for (byte b1 = 'a'; b1 <= 'z'; b1++) {\n        for (byte b2 = 'a'; b2 <= 'z'; b2++) {\n          for (byte b3 = 'a'; b3 <= 'z'; b3++) {\n            int count = seenRows[i(b1)][i(b2)][i(b3)];\n            int expectedCount = 0;\n            if (\n              Bytes.compareTo(new byte[] { b1, b2, b3 }, startRow) >= 0\n                && Bytes.compareTo(new byte[] { b1, b2, b3 }, stopRow) < 0\n            ) {\n              expectedCount = 1;\n            }\n            if (count != expectedCount) {\n              String row = new String(new byte[] { b1, b2, b3 }, StandardCharsets.UTF_8);\n              throw new RuntimeException(\"Row:\" + row + \" has a seen count of \" + count + \" \"\n                + \"instead of \" + expectedCount);\n            }\n          }\n        }\n      }\n    }\n  }\n\n  public int loadRegion(final HRegion r, final byte[] f) throws IOException {\n    return loadRegion(r, f, false);\n  }\n\n  public int loadRegion(final Region r, final byte[] f) throws IOException {\n    return loadRegion((HRegion) r, f);\n  }\n\n  /**\n   * Load region with rows from 'aaa' to 'zzz'.\n   * @param r     Region\n   * @param f     Family\n   * @param flush flush the cache if true\n   * @return Count of rows loaded.\n   */\n  public int loadRegion(final HRegion r, final byte[] f, final boolean flush) throws IOException {\n    byte[] k = new byte[3];\n    int rowCount = 0;\n    for (byte b1 = 'a'; b1 <= 'z'; b1++) {\n      for (byte b2 = 'a'; b2 <= 'z'; b2++) {\n        for (byte b3 = 'a'; b3 <= 'z'; b3++) {\n          k[0] = b1;\n          k[1] = b2;\n          k[2] = b3;\n          Put put = new Put(k);\n          put.setDurability(Durability.SKIP_WAL);\n          put.addColumn(f, null, k);\n          if (r.getWAL() == null) {\n            put.setDurability(Durability.SKIP_WAL);\n          }\n          int preRowCount = rowCount;\n          int pause = 10;\n          int maxPause = 1000;\n          while (rowCount == preRowCount) {\n            try {\n              r.put(put);\n              rowCount++;\n            } catch (RegionTooBusyException e) {\n              pause = (pause * 2 >= maxPause) ? maxPause : pause * 2;\n              Threads.sleep(pause);\n            }\n          }\n        }\n      }\n      if (flush) {\n        r.flush(true);\n      }\n    }\n    return rowCount;\n  }\n\n  public void loadNumericRows(final Table t, final byte[] f, int startRow, int endRow)\n    throws IOException {\n    for (int i = startRow; i < endRow; i++) {\n      byte[] data = Bytes.toBytes(String.valueOf(i));\n      Put put = new Put(data);\n      put.addColumn(f, null, data);\n      t.put(put);\n    }\n  }\n\n  public void loadRandomRows(final Table t, final byte[] f, int rowSize, int totalRows)\n    throws IOException {\n    byte[] row = new byte[rowSize];\n    for (int i = 0; i < totalRows; i++) {\n      Bytes.random(row);\n      Put put = new Put(row);\n      put.addColumn(f, new byte[] { 0 }, new byte[] { 0 });\n      t.put(put);\n    }\n  }\n\n  public void verifyNumericRows(Table table, final byte[] f, int startRow, int endRow,\n    int replicaId) throws IOException {\n    for (int i = startRow; i < endRow; i++) {\n      String failMsg = \"Failed verification of row :\" + i;\n      byte[] data = Bytes.toBytes(String.valueOf(i));\n      Get get = new Get(data);\n      get.setReplicaId(replicaId);\n      get.setConsistency(Consistency.TIMELINE);\n      Result result = table.get(get);\n      if (!result.containsColumn(f, null)) {\n        throw new AssertionError(failMsg);\n      }\n      assertEquals(failMsg, 1, result.getColumnCells(f, null).size());\n      Cell cell = result.getColumnLatestCell(f, null);\n      if (\n        !Bytes.equals(data, 0, data.length, cell.getValueArray(), cell.getValueOffset(),\n          cell.getValueLength())\n      ) {\n        throw new AssertionError(failMsg);\n      }\n    }\n  }\n\n  public void verifyNumericRows(Region region, final byte[] f, int startRow, int endRow)\n    throws IOException {\n    verifyNumericRows((HRegion) region, f, startRow, endRow);\n  }\n\n  public void verifyNumericRows(HRegion region, final byte[] f, int startRow, int endRow)\n    throws IOException {\n    verifyNumericRows(region, f, startRow, endRow, true);\n  }\n\n  public void verifyNumericRows(Region region, final byte[] f, int startRow, int endRow,\n    final boolean present) throws IOException {\n    verifyNumericRows((HRegion) region, f, startRow, endRow, present);\n  }\n\n  public void verifyNumericRows(HRegion region, final byte[] f, int startRow, int endRow,\n    final boolean present) throws IOException {\n    for (int i = startRow; i < endRow; i++) {\n      String failMsg = \"Failed verification of row :\" + i;\n      byte[] data = Bytes.toBytes(String.valueOf(i));\n      Result result = region.get(new Get(data));\n\n      boolean hasResult = result != null && !result.isEmpty();\n      if (present != hasResult) {\n        throw new AssertionError(\n          failMsg + result + \" expected:<\" + present + \"> but was:<\" + hasResult + \">\");\n      }\n      if (!present) continue;\n\n      if (!result.containsColumn(f, null)) {\n        throw new AssertionError(failMsg);\n      }\n      assertEquals(failMsg, 1, result.getColumnCells(f, null).size());\n      Cell cell = result.getColumnLatestCell(f, null);\n      if (\n        !Bytes.equals(data, 0, data.length, cell.getValueArray(), cell.getValueOffset(),\n          cell.getValueLength())\n      ) {\n        throw new AssertionError(failMsg);\n      }\n    }\n  }\n\n  public void deleteNumericRows(final Table t, final byte[] f, int startRow, int endRow)\n    throws IOException {\n    for (int i = startRow; i < endRow; i++) {\n      byte[] data = Bytes.toBytes(String.valueOf(i));\n      Delete delete = new Delete(data);\n      delete.addFamily(f);\n      t.delete(delete);\n    }\n  }\n\n  /**\n   * Return the number of rows in the given table.\n   * @param table to count rows\n   * @return count of rows\n   */\n  public static int countRows(final Table table) throws IOException {\n    return countRows(table, new Scan());\n  }\n\n  public static int countRows(final Table table, final Scan scan) throws IOException {\n    try (ResultScanner results = table.getScanner(scan)) {\n      int count = 0;\n      while (results.next() != null) {\n        count++;\n      }\n      return count;\n    }\n  }\n\n  public int countRows(final Table table, final byte[]... families) throws IOException {\n    Scan scan = new Scan();\n    for (byte[] family : families) {\n      scan.addFamily(family);\n    }\n    return countRows(table, scan);\n  }\n\n  /**\n   * Return the number of rows in the given table.\n   */\n  public int countRows(final TableName tableName) throws IOException {\n    Table table = getConnection().getTable(tableName);\n    try {\n      return countRows(table);\n    } finally {\n      table.close();\n    }\n  }\n\n  public int countRows(final Region region) throws IOException {\n    return countRows(region, new Scan());\n  }\n\n  public int countRows(final Region region, final Scan scan) throws IOException {\n    InternalScanner scanner = region.getScanner(scan);\n    try {\n      return countRows(scanner);\n    } finally {\n      scanner.close();\n    }\n  }\n\n  public int countRows(final InternalScanner scanner) throws IOException {\n    int scannedCount = 0;\n    List<Cell> results = new ArrayList<>();\n    boolean hasMore = true;\n    while (hasMore) {\n      hasMore = scanner.next(results);\n      scannedCount += results.size();\n      results.clear();\n    }\n    return scannedCount;\n  }\n\n  /**\n   * Return an md5 digest of the entire contents of a table.\n   */\n  public String checksumRows(final Table table) throws Exception {\n\n    Scan scan = new Scan();\n    ResultScanner results = table.getScanner(scan);\n    MessageDigest digest = MessageDigest.getInstance(\"MD5\");\n    for (Result res : results) {\n      digest.update(res.getRow());\n    }\n    results.close();\n    return digest.toString();\n  }\n\n  /** All the row values for the data loaded by {@link #loadTable(Table, byte[])} */\n  public static final byte[][] ROWS = new byte[(int) Math.pow('z' - 'a' + 1, 3)][3]; // ~52KB\n  static {\n    int i = 0;\n    for (byte b1 = 'a'; b1 <= 'z'; b1++) {\n      for (byte b2 = 'a'; b2 <= 'z'; b2++) {\n        for (byte b3 = 'a'; b3 <= 'z'; b3++) {\n          ROWS[i][0] = b1;\n          ROWS[i][1] = b2;\n          ROWS[i][2] = b3;\n          i++;\n        }\n      }\n    }\n  }\n\n  public static final byte[][] KEYS = { HConstants.EMPTY_BYTE_ARRAY, Bytes.toBytes(\"bbb\"),\n    Bytes.toBytes(\"ccc\"), Bytes.toBytes(\"ddd\"), Bytes.toBytes(\"eee\"), Bytes.toBytes(\"fff\"),\n    Bytes.toBytes(\"ggg\"), Bytes.toBytes(\"hhh\"), Bytes.toBytes(\"iii\"), Bytes.toBytes(\"jjj\"),\n    Bytes.toBytes(\"kkk\"), Bytes.toBytes(\"lll\"), Bytes.toBytes(\"mmm\"), Bytes.toBytes(\"nnn\"),\n    Bytes.toBytes(\"ooo\"), Bytes.toBytes(\"ppp\"), Bytes.toBytes(\"qqq\"), Bytes.toBytes(\"rrr\"),\n    Bytes.toBytes(\"sss\"), Bytes.toBytes(\"ttt\"), Bytes.toBytes(\"uuu\"), Bytes.toBytes(\"vvv\"),\n    Bytes.toBytes(\"www\"), Bytes.toBytes(\"xxx\"), Bytes.toBytes(\"yyy\") };\n\n  public static final byte[][] KEYS_FOR_HBA_CREATE_TABLE = { Bytes.toBytes(\"bbb\"),\n    Bytes.toBytes(\"ccc\"), Bytes.toBytes(\"ddd\"), Bytes.toBytes(\"eee\"), Bytes.toBytes(\"fff\"),\n    Bytes.toBytes(\"ggg\"), Bytes.toBytes(\"hhh\"), Bytes.toBytes(\"iii\"), Bytes.toBytes(\"jjj\"),\n    Bytes.toBytes(\"kkk\"), Bytes.toBytes(\"lll\"), Bytes.toBytes(\"mmm\"), Bytes.toBytes(\"nnn\"),\n    Bytes.toBytes(\"ooo\"), Bytes.toBytes(\"ppp\"), Bytes.toBytes(\"qqq\"), Bytes.toBytes(\"rrr\"),\n    Bytes.toBytes(\"sss\"), Bytes.toBytes(\"ttt\"), Bytes.toBytes(\"uuu\"), Bytes.toBytes(\"vvv\"),\n    Bytes.toBytes(\"www\"), Bytes.toBytes(\"xxx\"), Bytes.toBytes(\"yyy\"), Bytes.toBytes(\"zzz\") };\n\n  /**\n   * Create rows in hbase:meta for regions of the specified table with the specified start keys. The\n   * first startKey should be a 0 length byte array if you want to form a proper range of regions.\n   * @return list of region info for regions added to meta\n   */\n  public List<RegionInfo> createMultiRegionsInMeta(final Configuration conf,\n    final TableDescriptor htd, byte[][] startKeys) throws IOException {\n    Table meta = getConnection().getTable(TableName.META_TABLE_NAME);\n    Arrays.sort(startKeys, Bytes.BYTES_COMPARATOR);\n    List<RegionInfo> newRegions = new ArrayList<>(startKeys.length);\n    MetaTableAccessor.updateTableState(getConnection(), htd.getTableName(),\n      TableState.State.ENABLED);\n    // add custom ones\n    for (int i = 0; i < startKeys.length; i++) {\n      int j = (i + 1) % startKeys.length;\n      RegionInfo hri = RegionInfoBuilder.newBuilder(htd.getTableName()).setStartKey(startKeys[i])\n        .setEndKey(startKeys[j]).build();\n      MetaTableAccessor.addRegionsToMeta(getConnection(), Collections.singletonList(hri), 1);\n      newRegions.add(hri);\n    }\n\n    meta.close();\n    return newRegions;\n  }\n\n  /**\n   * Create an unmanaged WAL. Be sure to close it when you're through.\n   */\n  public static WAL createWal(final Configuration conf, final Path rootDir, final RegionInfo hri)\n    throws IOException {\n    // The WAL subsystem will use the default rootDir rather than the passed in rootDir\n    // unless I pass along via the conf.\n    Configuration confForWAL = new Configuration(conf);\n    confForWAL.set(HConstants.HBASE_DIR, rootDir.toString());\n    return new WALFactory(confForWAL, \"hregion-\" + RandomStringUtils.randomNumeric(8)).getWAL(hri);\n  }\n\n  /**\n   * Create a region with it's own WAL. Be sure to call\n   * {@link HBaseTestingUtility#closeRegionAndWAL(HRegion)} to clean up all resources.\n   */\n  public static HRegion createRegionAndWAL(final RegionInfo info, final Path rootDir,\n    final Configuration conf, final TableDescriptor htd) throws IOException {\n    return createRegionAndWAL(info, rootDir, conf, htd, true);\n  }\n\n  /**\n   * Create a region with it's own WAL. Be sure to call\n   * {@link HBaseTestingUtility#closeRegionAndWAL(HRegion)} to clean up all resources.\n   */\n  public static HRegion createRegionAndWAL(final RegionInfo info, final Path rootDir,\n    final Configuration conf, final TableDescriptor htd, BlockCache blockCache) throws IOException {\n    HRegion region = createRegionAndWAL(info, rootDir, conf, htd, false);\n    region.setBlockCache(blockCache);\n    region.initialize();\n    return region;\n  }\n\n  /**\n   * Create a region with it's own WAL. Be sure to call\n   * {@link HBaseTestingUtility#closeRegionAndWAL(HRegion)} to clean up all resources.\n   */\n  public static HRegion createRegionAndWAL(final RegionInfo info, final Path rootDir,\n    final Configuration conf, final TableDescriptor htd, MobFileCache mobFileCache)\n    throws IOException {\n    HRegion region = createRegionAndWAL(info, rootDir, conf, htd, false);\n    region.setMobFileCache(mobFileCache);\n    region.initialize();\n    return region;\n  }\n\n  /**\n   * Create a region with it's own WAL. Be sure to call\n   * {@link HBaseTestingUtility#closeRegionAndWAL(HRegion)} to clean up all resources.\n   */\n  public static HRegion createRegionAndWAL(final RegionInfo info, final Path rootDir,\n    final Configuration conf, final TableDescriptor htd, boolean initialize) throws IOException {\n    ChunkCreator.initialize(MemStoreLAB.CHUNK_SIZE_DEFAULT, false, 0, 0, 0, null,\n      MemStoreLAB.INDEX_CHUNK_SIZE_PERCENTAGE_DEFAULT);\n    WAL wal = createWal(conf, rootDir, info);\n    return HRegion.createHRegion(info, rootDir, conf, htd, wal, initialize);\n  }\n\n  /**\n   * Returns all rows from the hbase:meta table.\n   * @throws IOException When reading the rows fails.\n   */\n  public List<byte[]> getMetaTableRows() throws IOException {\n    // TODO: Redo using MetaTableAccessor class\n    Table t = getConnection().getTable(TableName.META_TABLE_NAME);\n    List<byte[]> rows = new ArrayList<>();\n    ResultScanner s = t.getScanner(new Scan());\n    for (Result result : s) {\n      LOG.info(\"getMetaTableRows: row -> \" + Bytes.toStringBinary(result.getRow()));\n      rows.add(result.getRow());\n    }\n    s.close();\n    t.close();\n    return rows;\n  }\n\n  /**\n   * Returns all rows from the hbase:meta table for a given user table\n   * @throws IOException When reading the rows fails.\n   */\n  public List<byte[]> getMetaTableRows(TableName tableName) throws IOException {\n    // TODO: Redo using MetaTableAccessor.\n    Table t = getConnection().getTable(TableName.META_TABLE_NAME);\n    List<byte[]> rows = new ArrayList<>();\n    ResultScanner s = t.getScanner(new Scan());\n    for (Result result : s) {\n      RegionInfo info = CatalogFamilyFormat.getRegionInfo(result);\n      if (info == null) {\n        LOG.error(\"No region info for row \" + Bytes.toString(result.getRow()));\n        // TODO figure out what to do for this new hosed case.\n        continue;\n      }\n\n      if (info.getTable().equals(tableName)) {\n        LOG.info(\"getMetaTableRows: row -> \" + Bytes.toStringBinary(result.getRow()) + info);\n        rows.add(result.getRow());\n      }\n    }\n    s.close();\n    t.close();\n    return rows;\n  }\n\n  /**\n   * Returns all regions of the specified table\n   * @param tableName the table name\n   * @return all regions of the specified table\n   * @throws IOException when getting the regions fails.\n   */\n  private List<RegionInfo> getRegions(TableName tableName) throws IOException {\n    try (Admin admin = getConnection().getAdmin()) {\n      return admin.getRegions(tableName);\n    }\n  }\n\n  /**\n   * Find any other region server which is different from the one identified by parameter\n   * @return another region server\n   */\n  public HRegionServer getOtherRegionServer(HRegionServer rs) {\n    for (JVMClusterUtil.RegionServerThread rst : getMiniHBaseCluster().getRegionServerThreads()) {\n      if (!(rst.getRegionServer() == rs)) {\n        return rst.getRegionServer();\n      }\n    }\n    return null;\n  }\n\n  /**\n   * Tool to get the reference to the region server object that holds the region of the specified\n   * user table.\n   * @param tableName user table to lookup in hbase:meta\n   * @return region server that holds it, null if the row doesn't exist\n   */\n  public HRegionServer getRSForFirstRegionInTable(TableName tableName)\n    throws IOException, InterruptedException {\n    List<RegionInfo> regions = getRegions(tableName);\n    if (regions == null || regions.isEmpty()) {\n      return null;\n    }\n    LOG.debug(\"Found \" + regions.size() + \" regions for table \" + tableName);\n\n    byte[] firstRegionName =\n      regions.stream().filter(r -> !r.isOffline()).map(RegionInfo::getRegionName).findFirst()\n        .orElseThrow(() -> new IOException(\"online regions not found in table \" + tableName));\n\n    LOG.debug(\"firstRegionName=\" + Bytes.toString(firstRegionName));\n    long pause = getConfiguration().getLong(HConstants.HBASE_CLIENT_PAUSE,\n      HConstants.DEFAULT_HBASE_CLIENT_PAUSE);\n    int numRetries = getConfiguration().getInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER,\n      HConstants.DEFAULT_HBASE_CLIENT_RETRIES_NUMBER);\n    RetryCounter retrier = new RetryCounter(numRetries + 1, (int) pause, TimeUnit.MICROSECONDS);\n    while (retrier.shouldRetry()) {\n      int index = getMiniHBaseCluster().getServerWith(firstRegionName);\n      if (index != -1) {\n        return getMiniHBaseCluster().getRegionServerThreads().get(index).getRegionServer();\n      }\n      // Came back -1. Region may not be online yet. Sleep a while.\n      retrier.sleepUntilNextRetry();\n    }\n    return null;\n  }\n\n  /**\n   * Starts a <code>MiniMRCluster</code> with a default number of <code>TaskTracker</code>'s.\n   * @throws IOException When starting the cluster fails.\n   */\n  public MiniMRCluster startMiniMapReduceCluster() throws IOException {\n    // Set a very high max-disk-utilization percentage to avoid the NodeManagers from failing.\n    conf.setIfUnset(\"yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage\",\n      \"99.0\");\n    startMiniMapReduceCluster(2);\n    return mrCluster;\n  }\n\n  /**\n   * Tasktracker has a bug where changing the hadoop.log.dir system property will not change its\n   * internal static LOG_DIR variable.\n   */\n  private void forceChangeTaskLogDir() {\n    Field logDirField;\n    try {\n      logDirField = TaskLog.class.getDeclaredField(\"LOG_DIR\");\n      logDirField.setAccessible(true);\n\n      Field modifiersField = ReflectionUtils.getModifiersField();\n      modifiersField.setAccessible(true);\n      modifiersField.setInt(logDirField, logDirField.getModifiers() & ~Modifier.FINAL);\n\n      logDirField.set(null, new File(hadoopLogDir, \"userlogs\"));\n    } catch (SecurityException e) {\n      throw new RuntimeException(e);\n    } catch (NoSuchFieldException e) {\n      throw new RuntimeException(e);\n    } catch (IllegalArgumentException e) {\n      throw new RuntimeException(e);\n    } catch (IllegalAccessException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  /**\n   * Starts a <code>MiniMRCluster</code>. Call {@link #setFileSystemURI(String)} to use a different\n   * filesystem.\n   * @param servers The number of <code>TaskTracker</code>'s to start.\n   * @throws IOException When starting the cluster fails.\n   */\n  private void startMiniMapReduceCluster(final int servers) throws IOException {\n    if (mrCluster != null) {\n      throw new IllegalStateException(\"MiniMRCluster is already running\");\n    }\n    LOG.info(\"Starting mini mapreduce cluster...\");\n    setupClusterTestDir();\n    createDirsAndSetProperties();\n\n    forceChangeTaskLogDir();\n\n    //// hadoop2 specific settings\n    // Tests were failing because this process used 6GB of virtual memory and was getting killed.\n    // we up the VM usable so that processes don't get killed.\n    conf.setFloat(\"yarn.nodemanager.vmem-pmem-ratio\", 8.0f);\n\n    // Tests were failing due to MAPREDUCE-4880 / MAPREDUCE-4607 against hadoop 2.0.2-alpha and\n    // this avoids the problem by disabling speculative task execution in tests.\n    conf.setBoolean(\"mapreduce.map.speculative\", false);\n    conf.setBoolean(\"mapreduce.reduce.speculative\", false);\n    ////\n\n    // Allow the user to override FS URI for this map-reduce cluster to use.\n    mrCluster =\n      new MiniMRCluster(servers, FS_URI != null ? FS_URI : FileSystem.get(conf).getUri().toString(),\n        1, null, null, new JobConf(this.conf));\n    JobConf jobConf = MapreduceTestingShim.getJobConf(mrCluster);\n    if (jobConf == null) {\n      jobConf = mrCluster.createJobConf();\n    }\n    // Hadoop MiniMR overwrites this while it should not\n    jobConf.set(\"mapreduce.cluster.local.dir\", conf.get(\"mapreduce.cluster.local.dir\"));\n    LOG.info(\"Mini mapreduce cluster started\");\n\n    // In hadoop2, YARN/MR2 starts a mini cluster with its own conf instance and updates settings.\n    // Our HBase MR jobs need several of these settings in order to properly run. So we copy the\n    // necessary config properties here. YARN-129 required adding a few properties.\n    conf.set(\"mapreduce.jobtracker.address\", jobConf.get(\"mapreduce.jobtracker.address\"));\n    // this for mrv2 support; mr1 ignores this\n    conf.set(\"mapreduce.framework.name\", \"yarn\");\n    conf.setBoolean(\"yarn.is.minicluster\", true);\n    String rmAddress = jobConf.get(\"yarn.resourcemanager.address\");\n    if (rmAddress != null) {\n      conf.set(\"yarn.resourcemanager.address\", rmAddress);\n    }\n    String historyAddress = jobConf.get(\"mapreduce.jobhistory.address\");\n    if (historyAddress != null) {\n      conf.set(\"mapreduce.jobhistory.address\", historyAddress);\n    }\n    String schedulerAddress = jobConf.get(\"yarn.resourcemanager.scheduler.address\");\n    if (schedulerAddress != null) {\n      conf.set(\"yarn.resourcemanager.scheduler.address\", schedulerAddress);\n    }\n    String mrJobHistoryWebappAddress = jobConf.get(\"mapreduce.jobhistory.webapp.address\");\n    if (mrJobHistoryWebappAddress != null) {\n      conf.set(\"mapreduce.jobhistory.webapp.address\", mrJobHistoryWebappAddress);\n    }\n    String yarnRMWebappAddress = jobConf.get(\"yarn.resourcemanager.webapp.address\");\n    if (yarnRMWebappAddress != null) {\n      conf.set(\"yarn.resourcemanager.webapp.address\", yarnRMWebappAddress);\n    }\n  }\n\n  /**\n   * Stops the previously started <code>MiniMRCluster</code>.\n   */\n  public void shutdownMiniMapReduceCluster() {\n    if (mrCluster != null) {\n      LOG.info(\"Stopping mini mapreduce cluster...\");\n      mrCluster.shutdown();\n      mrCluster = null;\n      LOG.info(\"Mini mapreduce cluster stopped\");\n    }\n    // Restore configuration to point to local jobtracker\n    conf.set(\"mapreduce.jobtracker.address\", \"local\");\n  }\n\n  /**\n   * Create a stubbed out RegionServerService, mainly for getting FS.\n   */\n  public RegionServerServices createMockRegionServerService() throws IOException {\n    return createMockRegionServerService((ServerName) null);\n  }\n\n  /**\n   * Create a stubbed out RegionServerService, mainly for getting FS. This version is used by\n   * TestTokenAuthentication\n   */\n  public RegionServerServices createMockRegionServerService(RpcServerInterface rpc)\n    throws IOException {\n    final MockRegionServerServices rss = new MockRegionServerServices(getZooKeeperWatcher());\n    rss.setFileSystem(getTestFileSystem());\n    rss.setRpcServer(rpc);\n    return rss;\n  }\n\n  /**\n   * Create a stubbed out RegionServerService, mainly for getting FS. This version is used by\n   * TestOpenRegionHandler\n   */\n  public RegionServerServices createMockRegionServerService(ServerName name) throws IOException {\n    final MockRegionServerServices rss = new MockRegionServerServices(getZooKeeperWatcher(), name);\n    rss.setFileSystem(getTestFileSystem());\n    return rss;\n  }\n\n  /**\n   * Switches the logger for the given class to DEBUG level.\n   * @param clazz The class for which to switch to debug logging.\n   * @deprecated In 2.3.0, will be removed in 4.0.0. Only support changing log level on log4j now as\n   *             HBase only uses log4j. You should do this by your own as it you know which log\n   *             framework you are using then set the log level to debug is very easy.\n   */\n  @Deprecated\n  public void enableDebug(Class<?> clazz) {\n    Log4jUtils.enableDebug(clazz);\n  }\n\n  /**\n   * Expire the Master's session\n   */\n  public void expireMasterSession() throws Exception {\n    HMaster master = getMiniHBaseCluster().getMaster();\n    expireSession(master.getZooKeeper(), false);\n  }\n\n  /**\n   * Expire a region server's session\n   * @param index which RS\n   */\n  public void expireRegionServerSession(int index) throws Exception {\n    HRegionServer rs = getMiniHBaseCluster().getRegionServer(index);\n    expireSession(rs.getZooKeeper(), false);\n    decrementMinRegionServerCount();\n  }\n\n  private void decrementMinRegionServerCount() {\n    // decrement the count for this.conf, for newly spwaned master\n    // this.hbaseCluster shares this configuration too\n    decrementMinRegionServerCount(getConfiguration());\n\n    // each master thread keeps a copy of configuration\n    for (MasterThread master : getHBaseCluster().getMasterThreads()) {\n      decrementMinRegionServerCount(master.getMaster().getConfiguration());\n    }\n  }\n\n  private void decrementMinRegionServerCount(Configuration conf) {\n    int currentCount = conf.getInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, -1);\n    if (currentCount != -1) {\n      conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, Math.max(currentCount - 1, 1));\n    }\n  }\n\n  public void expireSession(ZKWatcher nodeZK) throws Exception {\n    expireSession(nodeZK, false);\n  }\n\n  /**\n   * Expire a ZooKeeper session as recommended in ZooKeeper documentation\n   * http://hbase.apache.org/book.html#trouble.zookeeper There are issues when doing this: [1]\n   * http://www.mail-archive.com/dev@zookeeper.apache.org/msg01942.html [2]\n   * https://issues.apache.org/jira/browse/ZOOKEEPER-1105\n   * @param nodeZK      - the ZK watcher to expire\n   * @param checkStatus - true to check if we can create a Table with the current configuration.\n   */\n  public void expireSession(ZKWatcher nodeZK, boolean checkStatus) throws Exception {\n    Configuration c = new Configuration(this.conf);\n    String quorumServers = ZKConfig.getZKQuorumServersString(c);\n    ZooKeeper zk = nodeZK.getRecoverableZooKeeper().getZooKeeper();\n    byte[] password = zk.getSessionPasswd();\n    long sessionID = zk.getSessionId();\n\n    // Expiry seems to be asynchronous (see comment from P. Hunt in [1]),\n    // so we create a first watcher to be sure that the\n    // event was sent. We expect that if our watcher receives the event\n    // other watchers on the same machine will get is as well.\n    // When we ask to close the connection, ZK does not close it before\n    // we receive all the events, so don't have to capture the event, just\n    // closing the connection should be enough.\n    ZooKeeper monitor = new ZooKeeper(quorumServers, 1000, new org.apache.zookeeper.Watcher() {\n      @Override\n      public void process(WatchedEvent watchedEvent) {\n        LOG.info(\"Monitor ZKW received event=\" + watchedEvent);\n      }\n    }, sessionID, password);\n\n    // Making it expire\n    ZooKeeper newZK =\n      new ZooKeeper(quorumServers, 1000, EmptyWatcher.instance, sessionID, password);\n\n    // ensure that we have connection to the server before closing down, otherwise\n    // the close session event will be eaten out before we start CONNECTING state\n    long start = EnvironmentEdgeManager.currentTime();\n    while (\n      newZK.getState() != States.CONNECTED && EnvironmentEdgeManager.currentTime() - start < 1000\n    ) {\n      Thread.sleep(1);\n    }\n    newZK.close();\n    LOG.info(\"ZK Closed Session 0x\" + Long.toHexString(sessionID));\n\n    // Now closing & waiting to be sure that the clients get it.\n    monitor.close();\n\n    if (checkStatus) {\n      getConnection().getTable(TableName.META_TABLE_NAME).close();\n    }\n  }\n\n  /**\n   * Get the Mini HBase cluster.\n   * @return hbase cluster\n   * @see #getHBaseClusterInterface()\n   */\n  public MiniHBaseCluster getHBaseCluster() {\n    return getMiniHBaseCluster();\n  }\n\n  /**\n   * Returns the HBaseCluster instance.\n   * <p>\n   * Returned object can be any of the subclasses of HBaseCluster, and the tests referring this\n   * should not assume that the cluster is a mini cluster or a distributed one. If the test only\n   * works on a mini cluster, then specific method {@link #getMiniHBaseCluster()} can be used\n   * instead w/o the need to type-cast.\n   */\n  public HBaseCluster getHBaseClusterInterface() {\n    // implementation note: we should rename this method as #getHBaseCluster(),\n    // but this would require refactoring 90+ calls.\n    return hbaseCluster;\n  }\n\n  /**\n   * Resets the connections so that the next time getConnection() is called, a new connection is\n   * created. This is needed in cases where the entire cluster / all the masters are shutdown and\n   * the connection is not valid anymore. TODO: There should be a more coherent way of doing this.\n   * Unfortunately the way tests are written, not all start() stop() calls go through this class.\n   * Most tests directly operate on the underlying mini/local hbase cluster. That makes it difficult\n   * for this wrapper class to maintain the connection state automatically. Cleaning this is a much\n   * bigger refactor.\n   */\n  public void invalidateConnection() throws IOException {\n    closeConnection();\n    // Update the master addresses if they changed.\n    final String masterConfigBefore = conf.get(HConstants.MASTER_ADDRS_KEY);\n    final String masterConfAfter = getMiniHBaseCluster().conf.get(HConstants.MASTER_ADDRS_KEY);\n    LOG.info(\"Invalidated connection. Updating master addresses before: {} after: {}\",\n      masterConfigBefore, masterConfAfter);\n    conf.set(HConstants.MASTER_ADDRS_KEY,\n      getMiniHBaseCluster().conf.get(HConstants.MASTER_ADDRS_KEY));\n  }\n\n  /**\n   * Get a shared Connection to the cluster. this method is thread safe.\n   * @return A Connection that can be shared. Don't close. Will be closed on shutdown of cluster.\n   */\n  public Connection getConnection() throws IOException {\n    return getAsyncConnection().toConnection();\n  }\n\n  /**\n   * Get a assigned Connection to the cluster. this method is thread safe.\n   * @param user assigned user\n   * @return A Connection with assigned user.\n   */\n  public Connection getConnection(User user) throws IOException {\n    return getAsyncConnection(user).toConnection();\n  }\n\n  /**\n   * Get a shared AsyncClusterConnection to the cluster. this method is thread safe.\n   * @return An AsyncClusterConnection that can be shared. Don't close. Will be closed on shutdown\n   *         of cluster.\n   */\n  public AsyncClusterConnection getAsyncConnection() throws IOException {\n    try {\n      return asyncConnection.updateAndGet(connection -> {\n        if (connection == null) {\n          try {\n            User user = UserProvider.instantiate(conf).getCurrent();\n            connection = getAsyncConnection(user);\n          } catch (IOException ioe) {\n            throw new UncheckedIOException(\"Failed to create connection\", ioe);\n          }\n        }\n        return connection;\n      });\n    } catch (UncheckedIOException exception) {\n      throw exception.getCause();\n    }\n  }\n\n  /**\n   * Get a assigned AsyncClusterConnection to the cluster. this method is thread safe.\n   * @param user assigned user\n   * @return An AsyncClusterConnection with assigned user.\n   */\n  public AsyncClusterConnection getAsyncConnection(User user) throws IOException {\n    return ClusterConnectionFactory.createAsyncClusterConnection(conf, null, user);\n  }\n\n  public void closeConnection() throws IOException {\n    if (hbaseAdmin != null) {\n      Closeables.close(hbaseAdmin, true);\n      hbaseAdmin = null;\n    }\n    AsyncClusterConnection asyncConnection = this.asyncConnection.getAndSet(null);\n    if (asyncConnection != null) {\n      Closeables.close(asyncConnection, true);\n    }\n  }\n\n  /**\n   * Returns an Admin instance which is shared between HBaseTestingUtility instance users. Closing\n   * it has no effect, it will be closed automatically when the cluster shutdowns\n   */\n  public Admin getAdmin() throws IOException {\n    if (hbaseAdmin == null) {\n      this.hbaseAdmin = getConnection().getAdmin();\n    }\n    return hbaseAdmin;\n  }\n\n  private Admin hbaseAdmin = null;\n\n  /**\n   * Returns an {@link Hbck} instance. Needs be closed when done.\n   */\n  public Hbck getHbck() throws IOException {\n    return getConnection().getHbck();\n  }\n\n  /**\n   * Unassign the named region.\n   * @param regionName The region to unassign.\n   */\n  public void unassignRegion(String regionName) throws IOException {\n    unassignRegion(Bytes.toBytes(regionName));\n  }\n\n  /**\n   * Unassign the named region.\n   * @param regionName The region to unassign.\n   */\n  public void unassignRegion(byte[] regionName) throws IOException {\n    getAdmin().unassign(regionName, true);\n  }\n\n  /**\n   * Closes the region containing the given row.\n   * @param row   The row to find the containing region.\n   * @param table The table to find the region.\n   */\n  public void unassignRegionByRow(String row, RegionLocator table) throws IOException {\n    unassignRegionByRow(Bytes.toBytes(row), table);\n  }\n\n  /**\n   * Closes the region containing the given row.\n   * @param row   The row to find the containing region.\n   * @param table The table to find the region.\n   */\n  public void unassignRegionByRow(byte[] row, RegionLocator table) throws IOException {\n    HRegionLocation hrl = table.getRegionLocation(row);\n    unassignRegion(hrl.getRegion().getRegionName());\n  }\n\n  /**\n   * Retrieves a splittable region randomly from tableName\n   * @param tableName   name of table\n   * @param maxAttempts maximum number of attempts, unlimited for value of -1\n   * @return the HRegion chosen, null if none was found within limit of maxAttempts\n   */\n  public HRegion getSplittableRegion(TableName tableName, int maxAttempts) {\n    List<HRegion> regions = getHBaseCluster().getRegions(tableName);\n    int regCount = regions.size();\n    Set<Integer> attempted = new HashSet<>();\n    int idx;\n    int attempts = 0;\n    do {\n      regions = getHBaseCluster().getRegions(tableName);\n      if (regCount != regions.size()) {\n        // if there was region movement, clear attempted Set\n        attempted.clear();\n      }\n      regCount = regions.size();\n      // There are chances that before we get the region for the table from an RS the region may\n      // be going for CLOSE. This may be because online schema change is enabled\n      if (regCount > 0) {\n        idx = ThreadLocalRandom.current().nextInt(regCount);\n        // if we have just tried this region, there is no need to try again\n        if (attempted.contains(idx)) {\n          continue;\n        }\n        HRegion region = regions.get(idx);\n        if (region.checkSplit().isPresent()) {\n          return region;\n        }\n        attempted.add(idx);\n      }\n      attempts++;\n    } while (maxAttempts == -1 || attempts < maxAttempts);\n    return null;\n  }\n\n  public MiniDFSCluster getDFSCluster() {\n    return dfsCluster;\n  }\n\n  public void setDFSCluster(MiniDFSCluster cluster) throws IllegalStateException, IOException {\n    setDFSCluster(cluster, true);\n  }\n\n  /**\n   * Set the MiniDFSCluster\n   * @param cluster     cluster to use\n   * @param requireDown require the that cluster not be \"up\" (MiniDFSCluster#isClusterUp) before it\n   *                    is set.\n   * @throws IllegalStateException if the passed cluster is up when it is required to be down\n   * @throws IOException           if the FileSystem could not be set from the passed dfs cluster\n   */\n  public void setDFSCluster(MiniDFSCluster cluster, boolean requireDown)\n    throws IllegalStateException, IOException {\n    if (dfsCluster != null && requireDown && dfsCluster.isClusterUp()) {\n      throw new IllegalStateException(\"DFSCluster is already running! Shut it down first.\");\n    }\n    this.dfsCluster = cluster;\n    this.setFs();\n  }\n\n  public FileSystem getTestFileSystem() throws IOException {\n    return HFileSystem.get(conf);\n  }\n\n  /**\n   * Wait until all regions in a table have been assigned. Waits default timeout before giving up\n   * (30 seconds).\n   * @param table Table to wait on.\n   */\n  public void waitTableAvailable(TableName table) throws InterruptedException, IOException {\n    waitTableAvailable(table.getName(), 30000);\n  }\n\n  public void waitTableAvailable(TableName table, long timeoutMillis)\n    throws InterruptedException, IOException {\n    waitFor(timeoutMillis, predicateTableAvailable(table));\n  }\n\n  /**\n   * Wait until all regions in a table have been assigned\n   * @param table         Table to wait on.\n   * @param timeoutMillis Timeout.\n   */\n  public void waitTableAvailable(byte[] table, long timeoutMillis)\n    throws InterruptedException, IOException {\n    waitFor(timeoutMillis, predicateTableAvailable(TableName.valueOf(table)));\n  }\n\n  public String explainTableAvailability(TableName tableName) throws IOException {\n    StringBuilder msg =\n      new StringBuilder(explainTableState(tableName, TableState.State.ENABLED)).append(\", \");\n    if (getHBaseCluster().getMaster().isAlive()) {\n      Map<RegionInfo, ServerName> assignments = getHBaseCluster().getMaster().getAssignmentManager()\n        .getRegionStates().getRegionAssignments();\n      final List<Pair<RegionInfo, ServerName>> metaLocations =\n        MetaTableAccessor.getTableRegionsAndLocations(getConnection(), tableName);\n      for (Pair<RegionInfo, ServerName> metaLocation : metaLocations) {\n        RegionInfo hri = metaLocation.getFirst();\n        ServerName sn = metaLocation.getSecond();\n        if (!assignments.containsKey(hri)) {\n          msg.append(\", region \").append(hri)\n            .append(\" not assigned, but found in meta, it expected to be on \").append(sn);\n        } else if (sn == null) {\n          msg.append(\",  region \").append(hri).append(\" assigned,  but has no server in meta\");\n        } else if (!sn.equals(assignments.get(hri))) {\n          msg.append(\",  region \").append(hri)\n            .append(\" assigned,  but has different servers in meta and AM ( \").append(sn)\n            .append(\" <> \").append(assignments.get(hri));\n        }\n      }\n    }\n    return msg.toString();\n  }\n\n  public String explainTableState(final TableName table, TableState.State state)\n    throws IOException {\n    TableState tableState = MetaTableAccessor.getTableState(getConnection(), table);\n    if (tableState == null) {\n      return \"TableState in META: No table state in META for table \" + table\n        + \" last state in meta (including deleted is \" + findLastTableState(table) + \")\";\n    } else if (!tableState.inStates(state)) {\n      return \"TableState in META: Not \" + state + \" state, but \" + tableState;\n    } else {\n      return \"TableState in META: OK\";\n    }\n  }\n\n  public TableState findLastTableState(final TableName table) throws IOException {\n    final AtomicReference<TableState> lastTableState = new AtomicReference<>(null);\n    ClientMetaTableAccessor.Visitor visitor = new ClientMetaTableAccessor.Visitor() {\n      @Override\n      public boolean visit(Result r) throws IOException {\n        if (!Arrays.equals(r.getRow(), table.getName())) {\n          return false;\n        }\n        TableState state = CatalogFamilyFormat.getTableState(r);\n        if (state != null) {\n          lastTableState.set(state);\n        }\n        return true;\n      }\n    };\n    MetaTableAccessor.scanMeta(getConnection(), null, null, ClientMetaTableAccessor.QueryType.TABLE,\n      Integer.MAX_VALUE, visitor);\n    return lastTableState.get();\n  }\n\n  /**\n   * Waits for a table to be 'enabled'. Enabled means that table is set as 'enabled' and the regions\n   * have been all assigned. Will timeout after default period (30 seconds) Tolerates nonexistent\n   * table.\n   * @param table the table to wait on.\n   * @throws InterruptedException if interrupted while waiting\n   * @throws IOException          if an IO problem is encountered\n   */\n  public void waitTableEnabled(TableName table) throws InterruptedException, IOException {\n    waitTableEnabled(table, 30000);\n  }\n\n  /**\n   * Waits for a table to be 'enabled'. Enabled means that table is set as 'enabled' and the regions\n   * have been all assigned.\n   * @see #waitTableEnabled(TableName, long)\n   * @param table         Table to wait on.\n   * @param timeoutMillis Time to wait on it being marked enabled.\n   */\n  public void waitTableEnabled(byte[] table, long timeoutMillis)\n    throws InterruptedException, IOException {\n    waitTableEnabled(TableName.valueOf(table), timeoutMillis);\n  }\n\n  public void waitTableEnabled(TableName table, long timeoutMillis) throws IOException {\n    waitFor(timeoutMillis, predicateTableEnabled(table));\n  }\n\n  /**\n   * Waits for a table to be 'disabled'. Disabled means that table is set as 'disabled' Will timeout\n   * after default period (30 seconds)\n   * @param table Table to wait on.\n   */\n  public void waitTableDisabled(byte[] table) throws InterruptedException, IOException {\n    waitTableDisabled(table, 30000);\n  }\n\n  public void waitTableDisabled(TableName table, long millisTimeout)\n    throws InterruptedException, IOException {\n    waitFor(millisTimeout, predicateTableDisabled(table));\n  }\n\n  /**\n   * Waits for a table to be 'disabled'. Disabled means that table is set as 'disabled'\n   * @param table         Table to wait on.\n   * @param timeoutMillis Time to wait on it being marked disabled.\n   */\n  public void waitTableDisabled(byte[] table, long timeoutMillis)\n    throws InterruptedException, IOException {\n    waitTableDisabled(TableName.valueOf(table), timeoutMillis);\n  }\n\n  /**\n   * Make sure that at least the specified number of region servers are running\n   * @param num minimum number of region servers that should be running\n   * @return true if we started some servers\n   */\n  public boolean ensureSomeRegionServersAvailable(final int num) throws IOException {\n    boolean startedServer = false;\n    MiniHBaseCluster hbaseCluster = getMiniHBaseCluster();\n    for (int i = hbaseCluster.getLiveRegionServerThreads().size(); i < num; ++i) {\n      LOG.info(\"Started new server=\" + hbaseCluster.startRegionServer());\n      startedServer = true;\n    }\n\n    return startedServer;\n  }\n\n  /**\n   * Make sure that at least the specified number of region servers are running. We don't count the\n   * ones that are currently stopping or are stopped.\n   * @param num minimum number of region servers that should be running\n   * @return true if we started some servers\n   */\n  public boolean ensureSomeNonStoppedRegionServersAvailable(final int num) throws IOException {\n    boolean startedServer = ensureSomeRegionServersAvailable(num);\n\n    int nonStoppedServers = 0;\n    for (JVMClusterUtil.RegionServerThread rst : getMiniHBaseCluster().getRegionServerThreads()) {\n\n      HRegionServer hrs = rst.getRegionServer();\n      if (hrs.isStopping() || hrs.isStopped()) {\n        LOG.info(\"A region server is stopped or stopping:\" + hrs);\n      } else {\n        nonStoppedServers++;\n      }\n    }\n    for (int i = nonStoppedServers; i < num; ++i) {\n      LOG.info(\"Started new server=\" + getMiniHBaseCluster().startRegionServer());\n      startedServer = true;\n    }\n    return startedServer;\n  }\n\n  /**\n   * This method clones the passed <code>c</code> configuration setting a new user into the clone.\n   * Use it getting new instances of FileSystem. Only works for DistributedFileSystem w/o Kerberos.\n   * @param c                     Initial configuration\n   * @param differentiatingSuffix Suffix to differentiate this user from others.\n   * @return A new configuration instance with a different user set into it.\n   */\n  public static User getDifferentUser(final Configuration c, final String differentiatingSuffix)\n    throws IOException {\n    FileSystem currentfs = FileSystem.get(c);\n    if (!(currentfs instanceof DistributedFileSystem) || User.isHBaseSecurityEnabled(c)) {\n      return User.getCurrent();\n    }\n    // Else distributed filesystem. Make a new instance per daemon. Below\n    // code is taken from the AppendTestUtil over in hdfs.\n    String username = User.getCurrent().getName() + differentiatingSuffix;\n    User user = User.createUserForTesting(c, username, new String[] { \"supergroup\" });\n    return user;\n  }\n\n  public static NavigableSet<String> getAllOnlineRegions(MiniHBaseCluster cluster)\n    throws IOException {\n    NavigableSet<String> online = new TreeSet<>();\n    for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {\n      try {\n        for (RegionInfo region : ProtobufUtil\n          .getOnlineRegions(rst.getRegionServer().getRSRpcServices())) {\n          online.add(region.getRegionNameAsString());\n        }\n      } catch (RegionServerStoppedException e) {\n        // That's fine.\n      }\n    }\n    return online;\n  }\n\n  /**\n   * Set maxRecoveryErrorCount in DFSClient. In 0.20 pre-append its hard-coded to 5 and makes tests\n   * linger. Here is the exception you'll see:\n   *\n   * <pre>\n   * 2010-06-15 11:52:28,511 WARN  [DataStreamer for file /hbase/.logs/wal.1276627923013 block\n   * blk_928005470262850423_1021] hdfs.DFSClient$DFSOutputStream(2657): Error Recovery for block\n   * blk_928005470262850423_1021 failed  because recovery from primary datanode 127.0.0.1:53683\n   * failed 4 times.  Pipeline was 127.0.0.1:53687, 127.0.0.1:53683. Will retry...\n   * </pre>\n   *\n   * @param stream A DFSClient.DFSOutputStream.\n   */\n  public static void setMaxRecoveryErrorCount(final OutputStream stream, final int max) {\n    try {\n      Class<?>[] clazzes = DFSClient.class.getDeclaredClasses();\n      for (Class<?> clazz : clazzes) {\n        String className = clazz.getSimpleName();\n        if (className.equals(\"DFSOutputStream\")) {\n          if (clazz.isInstance(stream)) {\n            Field maxRecoveryErrorCountField =\n              stream.getClass().getDeclaredField(\"maxRecoveryErrorCount\");\n            maxRecoveryErrorCountField.setAccessible(true);\n            maxRecoveryErrorCountField.setInt(stream, max);\n            break;\n          }\n        }\n      }\n    } catch (Exception e) {\n      LOG.info(\"Could not set max recovery field\", e);\n    }\n  }\n\n  /**\n   * Uses directly the assignment manager to assign the region. and waits until the specified region\n   * has completed assignment.\n   * @return true if the region is assigned false otherwise.\n   */\n  public boolean assignRegion(final RegionInfo regionInfo)\n    throws IOException, InterruptedException {\n    final AssignmentManager am = getHBaseCluster().getMaster().getAssignmentManager();\n    am.assign(regionInfo);\n    return AssignmentTestingUtil.waitForAssignment(am, regionInfo);\n  }\n\n  /**\n   * Move region to destination server and wait till region is completely moved and online\n   * @param destRegion region to move\n   * @param destServer destination server of the region\n   */\n  public void moveRegionAndWait(RegionInfo destRegion, ServerName destServer)\n    throws InterruptedException, IOException {\n    HMaster master = getMiniHBaseCluster().getMaster();\n    // TODO: Here we start the move. The move can take a while.\n    getAdmin().move(destRegion.getEncodedNameAsBytes(), destServer);\n    while (true) {\n      ServerName serverName =\n        master.getAssignmentManager().getRegionStates().getRegionServerOfRegion(destRegion);\n      if (serverName != null && serverName.equals(destServer)) {\n        assertRegionOnServer(destRegion, serverName, 2000);\n        break;\n      }\n      Thread.sleep(10);\n    }\n  }\n\n  /**\n   * Wait until all regions for a table in hbase:meta have a non-empty info:server, up to a\n   * configuable timeout value (default is 60 seconds) This means all regions have been deployed,\n   * master has been informed and updated hbase:meta with the regions deployed server.\n   * @param tableName the table name\n   */\n  public void waitUntilAllRegionsAssigned(final TableName tableName) throws IOException {\n    waitUntilAllRegionsAssigned(tableName,\n      this.conf.getLong(\"hbase.client.sync.wait.timeout.msec\", 60000));\n  }\n\n  /**\n   * Waith until all system table's regions get assigned\n   */\n  public void waitUntilAllSystemRegionsAssigned() throws IOException {\n    waitUntilAllRegionsAssigned(TableName.META_TABLE_NAME);\n  }\n\n  /**\n   * Wait until all regions for a table in hbase:meta have a non-empty info:server, or until\n   * timeout. This means all regions have been deployed, master has been informed and updated\n   * hbase:meta with the regions deployed server.\n   * @param tableName the table name\n   * @param timeout   timeout, in milliseconds\n   */\n  public void waitUntilAllRegionsAssigned(final TableName tableName, final long timeout)\n    throws IOException {\n    if (!TableName.isMetaTableName(tableName)) {\n      try (final Table meta = getConnection().getTable(TableName.META_TABLE_NAME)) {\n        LOG.debug(\"Waiting until all regions of table \" + tableName + \" get assigned. Timeout = \"\n          + timeout + \"ms\");\n        waitFor(timeout, 200, true, new ExplainingPredicate<IOException>() {\n          @Override\n          public String explainFailure() throws IOException {\n            return explainTableAvailability(tableName);\n          }\n\n          @Override\n          public boolean evaluate() throws IOException {\n            Scan scan = new Scan();\n            scan.addFamily(HConstants.CATALOG_FAMILY);\n            boolean tableFound = false;\n            try (ResultScanner s = meta.getScanner(scan)) {\n              for (Result r; (r = s.next()) != null;) {\n                byte[] b = r.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);\n                RegionInfo info = RegionInfo.parseFromOrNull(b);\n                if (info != null && info.getTable().equals(tableName)) {\n                  // Get server hosting this region from catalog family. Return false if no server\n                  // hosting this region, or if the server hosting this region was recently killed\n                  // (for fault tolerance testing).\n                  tableFound = true;\n                  byte[] server =\n                    r.getValue(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER);\n                  if (server == null) {\n                    return false;\n                  } else {\n                    byte[] startCode =\n                      r.getValue(HConstants.CATALOG_FAMILY, HConstants.STARTCODE_QUALIFIER);\n                    ServerName serverName =\n                      ServerName.valueOf(Bytes.toString(server).replaceFirst(\":\", \",\") + \",\"\n                        + Bytes.toLong(startCode));\n                    if (\n                      !getHBaseClusterInterface().isDistributedCluster()\n                        && getHBaseCluster().isKilledRS(serverName)\n                    ) {\n                      return false;\n                    }\n                  }\n                  if (RegionStateStore.getRegionState(r, info) != RegionState.State.OPEN) {\n                    return false;\n                  }\n                }\n              }\n            }\n            if (!tableFound) {\n              LOG.warn(\n                \"Didn't find the entries for table \" + tableName + \" in meta, already deleted?\");\n            }\n            return tableFound;\n          }\n        });\n      }\n    }\n    LOG.info(\"All regions for table \" + tableName + \" assigned to meta. Checking AM states.\");\n    // check from the master state if we are using a mini cluster\n    if (!getHBaseClusterInterface().isDistributedCluster()) {\n      // So, all regions are in the meta table but make sure master knows of the assignments before\n      // returning -- sometimes this can lag.\n      HMaster master = getHBaseCluster().getMaster();\n      final RegionStates states = master.getAssignmentManager().getRegionStates();\n      waitFor(timeout, 200, new ExplainingPredicate<IOException>() {\n        @Override\n        public String explainFailure() throws IOException {\n          return explainTableAvailability(tableName);\n        }\n\n        @Override\n        public boolean evaluate() throws IOException {\n          List<RegionInfo> hris = states.getRegionsOfTable(tableName);\n          return hris != null && !hris.isEmpty();\n        }\n      });\n    }\n    LOG.info(\"All regions for table \" + tableName + \" assigned.\");\n  }\n\n  /**\n   * Do a small get/scan against one store. This is required because store has no actual methods of\n   * querying itself, and relies on StoreScanner.\n   */\n  public static List<Cell> getFromStoreFile(HStore store, Get get) throws IOException {\n    Scan scan = new Scan(get);\n    InternalScanner scanner = (InternalScanner) store.getScanner(scan,\n      scan.getFamilyMap().get(store.getColumnFamilyDescriptor().getName()),\n      // originally MultiVersionConcurrencyControl.resetThreadReadPoint() was called to set\n      // readpoint 0.\n      0);\n\n    List<Cell> result = new ArrayList<>();\n    scanner.next(result);\n    if (!result.isEmpty()) {\n      // verify that we are on the row we want:\n      Cell kv = result.get(0);\n      if (!CellUtil.matchingRows(kv, get.getRow())) {\n        result.clear();\n      }\n    }\n    scanner.close();\n    return result;\n  }\n\n  /**\n   * Create region split keys between startkey and endKey\n   * @param numRegions the number of regions to be created. it has to be greater than 3.\n   * @return resulting split keys\n   */\n  public byte[][] getRegionSplitStartKeys(byte[] startKey, byte[] endKey, int numRegions) {\n    if (numRegions <= 3) {\n      throw new AssertionError();\n    }\n    byte[][] tmpSplitKeys = Bytes.split(startKey, endKey, numRegions - 3);\n    byte[][] result = new byte[tmpSplitKeys.length + 1][];\n    System.arraycopy(tmpSplitKeys, 0, result, 1, tmpSplitKeys.length);\n    result[0] = HConstants.EMPTY_BYTE_ARRAY;\n    return result;\n  }\n\n  /**\n   * Do a small get/scan against one store. This is required because store has no actual methods of\n   * querying itself, and relies on StoreScanner.\n   */\n  public static List<Cell> getFromStoreFile(HStore store, byte[] row, NavigableSet<byte[]> columns)\n    throws IOException {\n    Get get = new Get(row);\n    Map<byte[], NavigableSet<byte[]>> s = get.getFamilyMap();\n    s.put(store.getColumnFamilyDescriptor().getName(), columns);\n\n    return getFromStoreFile(store, get);\n  }\n\n  public static void assertKVListsEqual(String additionalMsg, final List<? extends Cell> expected,\n    final List<? extends Cell> actual) {\n    final int eLen = expected.size();\n    final int aLen = actual.size();\n    final int minLen = Math.min(eLen, aLen);\n\n    int i;\n    for (i = 0; i < minLen\n      && CellComparator.getInstance().compare(expected.get(i), actual.get(i)) == 0; ++i) {\n    }\n\n    if (additionalMsg == null) {\n      additionalMsg = \"\";\n    }\n    if (!additionalMsg.isEmpty()) {\n      additionalMsg = \". \" + additionalMsg;\n    }\n\n    if (eLen != aLen || i != minLen) {\n      throw new AssertionError(\"Expected and actual KV arrays differ at position \" + i + \": \"\n        + safeGetAsStr(expected, i) + \" (length \" + eLen + \") vs. \" + safeGetAsStr(actual, i)\n        + \" (length \" + aLen + \")\" + additionalMsg);\n    }\n  }\n\n  public static <T> String safeGetAsStr(List<T> lst, int i) {\n    if (0 <= i && i < lst.size()) {\n      return lst.get(i).toString();\n    } else {\n      return \"<out_of_range>\";\n    }\n  }\n\n  public String getClusterKey() {\n    return conf.get(HConstants.ZOOKEEPER_QUORUM) + \":\" + conf.get(HConstants.ZOOKEEPER_CLIENT_PORT)\n      + \":\"\n      + conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT);\n  }\n\n  /** Creates a random table with the given parameters */\n  public Table createRandomTable(TableName tableName, final Collection<String> families,\n    final int maxVersions, final int numColsPerRow, final int numFlushes, final int numRegions,\n    final int numRowsPerFlush) throws IOException, InterruptedException {\n\n    LOG.info(\"\\n\\nCreating random table \" + tableName + \" with \" + numRegions + \" regions, \"\n      + numFlushes + \" storefiles per region, \" + numRowsPerFlush + \" rows per flush, maxVersions=\"\n      + maxVersions + \"\\n\");\n\n    final int numCF = families.size();\n    final byte[][] cfBytes = new byte[numCF][];\n    {\n      int cfIndex = 0;\n      for (String cf : families) {\n        cfBytes[cfIndex++] = Bytes.toBytes(cf);\n      }\n    }\n\n    final int actualStartKey = 0;\n    final int actualEndKey = Integer.MAX_VALUE;\n    final int keysPerRegion = (actualEndKey - actualStartKey) / numRegions;\n    final int splitStartKey = actualStartKey + keysPerRegion;\n    final int splitEndKey = actualEndKey - keysPerRegion;\n    final String keyFormat = \"%08x\";\n    final Table table = createTable(tableName, cfBytes, maxVersions,\n      Bytes.toBytes(String.format(keyFormat, splitStartKey)),\n      Bytes.toBytes(String.format(keyFormat, splitEndKey)), numRegions);\n\n    if (hbaseCluster != null) {\n      getMiniHBaseCluster().flushcache(TableName.META_TABLE_NAME);\n    }\n\n    BufferedMutator mutator = getConnection().getBufferedMutator(tableName);\n\n    final Random rand = ThreadLocalRandom.current();\n    for (int iFlush = 0; iFlush < numFlushes; ++iFlush) {\n      for (int iRow = 0; iRow < numRowsPerFlush; ++iRow) {\n        final byte[] row = Bytes.toBytes(\n          String.format(keyFormat, actualStartKey + rand.nextInt(actualEndKey - actualStartKey)));\n\n        Put put = new Put(row);\n        Delete del = new Delete(row);\n        for (int iCol = 0; iCol < numColsPerRow; ++iCol) {\n          final byte[] cf = cfBytes[rand.nextInt(numCF)];\n          final long ts = rand.nextInt();\n          final byte[] qual = Bytes.toBytes(\"col\" + iCol);\n          if (rand.nextBoolean()) {\n            final byte[] value =\n              Bytes.toBytes(\"value_for_row_\" + iRow + \"_cf_\" + Bytes.toStringBinary(cf) + \"_col_\"\n                + iCol + \"_ts_\" + ts + \"_random_\" + rand.nextLong());\n            put.addColumn(cf, qual, ts, value);\n          } else if (rand.nextDouble() < 0.8) {\n            del.addColumn(cf, qual, ts);\n          } else {\n            del.addColumns(cf, qual, ts);\n          }\n        }\n\n        if (!put.isEmpty()) {\n          mutator.mutate(put);\n        }\n\n        if (!del.isEmpty()) {\n          mutator.mutate(del);\n        }\n      }\n      LOG.info(\"Initiating flush #\" + iFlush + \" for table \" + tableName);\n      mutator.flush();\n      if (hbaseCluster != null) {\n        getMiniHBaseCluster().flushcache(table.getName());\n      }\n    }\n    mutator.close();\n\n    return table;\n  }\n\n  public static int randomFreePort() {\n    return HBaseCommonTestingUtility.randomFreePort();\n  }\n\n  public static String randomMultiCastAddress() {\n    return \"226.1.1.\" + ThreadLocalRandom.current().nextInt(254);\n  }\n\n  public static void waitForHostPort(String host, int port) throws IOException {\n    final int maxTimeMs = 10000;\n    final int maxNumAttempts = maxTimeMs / HConstants.SOCKET_RETRY_WAIT_MS;\n    IOException savedException = null;\n    LOG.info(\"Waiting for server at \" + host + \":\" + port);\n    for (int attempt = 0; attempt < maxNumAttempts; ++attempt) {\n      try {\n        Socket sock = new Socket(InetAddress.getByName(host), port);\n        sock.close();\n        savedException = null;\n        LOG.info(\"Server at \" + host + \":\" + port + \" is available\");\n        break;\n      } catch (UnknownHostException e) {\n        throw new IOException(\"Failed to look up \" + host, e);\n      } catch (IOException e) {\n        savedException = e;\n      }\n      Threads.sleepWithoutInterrupt(HConstants.SOCKET_RETRY_WAIT_MS);\n    }\n\n    if (savedException != null) {\n      throw savedException;\n    }\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableName tableName,\n    byte[] columnFamily, Algorithm compression, DataBlockEncoding dataBlockEncoding)\n    throws IOException {\n    return createPreSplitLoadTestTable(conf, tableName, columnFamily, compression,\n      dataBlockEncoding, DEFAULT_REGIONS_PER_SERVER, 1, Durability.USE_DEFAULT);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableName tableName,\n    byte[] columnFamily, Algorithm compression, DataBlockEncoding dataBlockEncoding,\n    int numRegionsPerServer, int regionReplication, Durability durability) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    builder.setDurability(durability);\n    builder.setRegionReplication(regionReplication);\n    ColumnFamilyDescriptorBuilder cfBuilder =\n      ColumnFamilyDescriptorBuilder.newBuilder(columnFamily);\n    cfBuilder.setDataBlockEncoding(dataBlockEncoding);\n    cfBuilder.setCompressionType(compression);\n    return createPreSplitLoadTestTable(conf, builder.build(), cfBuilder.build(),\n      numRegionsPerServer);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableName tableName,\n    byte[][] columnFamilies, Algorithm compression, DataBlockEncoding dataBlockEncoding,\n    int numRegionsPerServer, int regionReplication, Durability durability) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    builder.setDurability(durability);\n    builder.setRegionReplication(regionReplication);\n    ColumnFamilyDescriptor[] hcds = new ColumnFamilyDescriptor[columnFamilies.length];\n    for (int i = 0; i < columnFamilies.length; i++) {\n      ColumnFamilyDescriptorBuilder cfBuilder =\n        ColumnFamilyDescriptorBuilder.newBuilder(columnFamilies[i]);\n      cfBuilder.setDataBlockEncoding(dataBlockEncoding);\n      cfBuilder.setCompressionType(compression);\n      hcds[i] = cfBuilder.build();\n    }\n    return createPreSplitLoadTestTable(conf, builder.build(), hcds, numRegionsPerServer);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableDescriptor desc,\n    ColumnFamilyDescriptor hcd) throws IOException {\n    return createPreSplitLoadTestTable(conf, desc, hcd, DEFAULT_REGIONS_PER_SERVER);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableDescriptor desc,\n    ColumnFamilyDescriptor hcd, int numRegionsPerServer) throws IOException {\n    return createPreSplitLoadTestTable(conf, desc, new ColumnFamilyDescriptor[] { hcd },\n      numRegionsPerServer);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableDescriptor desc,\n    ColumnFamilyDescriptor[] hcds, int numRegionsPerServer) throws IOException {\n    return createPreSplitLoadTestTable(conf, desc, hcds, new RegionSplitter.HexStringSplit(),\n      numRegionsPerServer);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableDescriptor td,\n    ColumnFamilyDescriptor[] cds, SplitAlgorithm splitter, int numRegionsPerServer)\n    throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(td);\n    for (ColumnFamilyDescriptor cd : cds) {\n      if (!td.hasColumnFamily(cd.getName())) {\n        builder.setColumnFamily(cd);\n      }\n    }\n    td = builder.build();\n    int totalNumberOfRegions = 0;\n    Connection unmanagedConnection = ConnectionFactory.createConnection(conf);\n    Admin admin = unmanagedConnection.getAdmin();\n\n    try {\n      // create a table a pre-splits regions.\n      // The number of splits is set as:\n      // region servers * regions per region server).\n      int numberOfServers = admin.getRegionServers().size();\n      if (numberOfServers == 0) {\n        throw new IllegalStateException(\"No live regionservers\");\n      }\n\n      totalNumberOfRegions = numberOfServers * numRegionsPerServer;\n      LOG.info(\"Number of live regionservers: \" + numberOfServers + \", \"\n        + \"pre-splitting table into \" + totalNumberOfRegions + \" regions \" + \"(regions per server: \"\n        + numRegionsPerServer + \")\");\n\n      byte[][] splits = splitter.split(totalNumberOfRegions);\n\n      admin.createTable(td, splits);\n    } catch (MasterNotRunningException e) {\n      LOG.error(\"Master not running\", e);\n      throw new IOException(e);\n    } catch (TableExistsException e) {\n      LOG.warn(\"Table \" + td.getTableName() + \" already exists, continuing\");\n    } finally {\n      admin.close();\n      unmanagedConnection.close();\n    }\n    return totalNumberOfRegions;\n  }\n\n  public static int getMetaRSPort(Connection connection) throws IOException {\n    try (RegionLocator locator = connection.getRegionLocator(TableName.META_TABLE_NAME)) {\n      return locator.getRegionLocation(Bytes.toBytes(\"\")).getPort();\n    }\n  }\n\n  /**\n   * Due to async racing issue, a region may not be in the online region list of a region server\n   * yet, after the assignment znode is deleted and the new assignment is recorded in master.\n   */\n  public void assertRegionOnServer(final RegionInfo hri, final ServerName server,\n    final long timeout) throws IOException, InterruptedException {\n    long timeoutTime = EnvironmentEdgeManager.currentTime() + timeout;\n    while (true) {\n      List<RegionInfo> regions = getAdmin().getRegions(server);\n      if (regions.stream().anyMatch(r -> RegionInfo.COMPARATOR.compare(r, hri) == 0)) return;\n      long now = EnvironmentEdgeManager.currentTime();\n      if (now > timeoutTime) break;\n      Thread.sleep(10);\n    }\n    throw new AssertionError(\n      \"Could not find region \" + hri.getRegionNameAsString() + \" on server \" + server);\n  }\n\n  /**\n   * Check to make sure the region is open on the specified region server, but not on any other one.\n   */\n  public void assertRegionOnlyOnServer(final RegionInfo hri, final ServerName server,\n    final long timeout) throws IOException, InterruptedException {\n    long timeoutTime = EnvironmentEdgeManager.currentTime() + timeout;\n    while (true) {\n      List<RegionInfo> regions = getAdmin().getRegions(server);\n      if (regions.stream().anyMatch(r -> RegionInfo.COMPARATOR.compare(r, hri) == 0)) {\n        List<JVMClusterUtil.RegionServerThread> rsThreads =\n          getHBaseCluster().getLiveRegionServerThreads();\n        for (JVMClusterUtil.RegionServerThread rsThread : rsThreads) {\n          HRegionServer rs = rsThread.getRegionServer();\n          if (server.equals(rs.getServerName())) {\n            continue;\n          }\n          Collection<HRegion> hrs = rs.getOnlineRegionsLocalContext();\n          for (HRegion r : hrs) {\n            if (r.getRegionInfo().getRegionId() == hri.getRegionId()) {\n              throw new AssertionError(\"Region should not be double assigned\");\n            }\n          }\n        }\n        return; // good, we are happy\n      }\n      long now = EnvironmentEdgeManager.currentTime();\n      if (now > timeoutTime) break;\n      Thread.sleep(10);\n    }\n    throw new AssertionError(\n      \"Could not find region \" + hri.getRegionNameAsString() + \" on server \" + server);\n  }\n\n  public HRegion createTestRegion(String tableName, ColumnFamilyDescriptor cd) throws IOException {\n    TableDescriptor td =\n      TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName)).setColumnFamily(cd).build();\n    RegionInfo info = RegionInfoBuilder.newBuilder(TableName.valueOf(tableName)).build();\n    return createRegionAndWAL(info, getDataTestDir(), getConfiguration(), td);\n  }\n\n  public HRegion createTestRegion(String tableName, ColumnFamilyDescriptor cd,\n    BlockCache blockCache) throws IOException {\n    TableDescriptor td =\n      TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName)).setColumnFamily(cd).build();\n    RegionInfo info = RegionInfoBuilder.newBuilder(TableName.valueOf(tableName)).build();\n    return createRegionAndWAL(info, getDataTestDir(), getConfiguration(), td, blockCache);\n  }\n\n  public static void setFileSystemURI(String fsURI) {\n    FS_URI = fsURI;\n  }\n\n  /**\n   * Returns a {@link Predicate} for checking that there are no regions in transition in master\n   */\n  public ExplainingPredicate<IOException> predicateNoRegionsInTransition() {\n    return new ExplainingPredicate<IOException>() {\n      @Override\n      public String explainFailure() throws IOException {\n        final RegionStates regionStates =\n          getMiniHBaseCluster().getMaster().getAssignmentManager().getRegionStates();\n        return \"found in transition: \" + regionStates.getRegionsInTransition().toString();\n      }\n\n      @Override\n      public boolean evaluate() throws IOException {\n        HMaster master = getMiniHBaseCluster().getMaster();\n        if (master == null) return false;\n        AssignmentManager am = master.getAssignmentManager();\n        if (am == null) return false;\n        return !am.hasRegionsInTransition();\n      }\n    };\n  }\n\n  /**\n   * Returns a {@link Predicate} for checking that table is enabled\n   */\n  public Waiter.Predicate<IOException> predicateTableEnabled(final TableName tableName) {\n    return new ExplainingPredicate<IOException>() {\n      @Override\n      public String explainFailure() throws IOException {\n        return explainTableState(tableName, TableState.State.ENABLED);\n      }\n\n      @Override\n      public boolean evaluate() throws IOException {\n        return getAdmin().tableExists(tableName) && getAdmin().isTableEnabled(tableName);\n      }\n    };\n  }\n\n  /**\n   * Returns a {@link Predicate} for checking that table is enabled\n   */\n  public Waiter.Predicate<IOException> predicateTableDisabled(final TableName tableName) {\n    return new ExplainingPredicate<IOException>() {\n      @Override\n      public String explainFailure() throws IOException {\n        return explainTableState(tableName, TableState.State.DISABLED);\n      }\n\n      @Override\n      public boolean evaluate() throws IOException {\n        return getAdmin().isTableDisabled(tableName);\n      }\n    };\n  }\n\n  /**\n   * Returns a {@link Predicate} for checking that table is enabled\n   */\n  public Waiter.Predicate<IOException> predicateTableAvailable(final TableName tableName) {\n    return new ExplainingPredicate<IOException>() {\n      @Override\n      public String explainFailure() throws IOException {\n        return explainTableAvailability(tableName);\n      }\n\n      @Override\n      public boolean evaluate() throws IOException {\n        boolean tableAvailable = getAdmin().isTableAvailable(tableName);\n        if (tableAvailable) {\n          try (Table table = getConnection().getTable(tableName)) {\n            TableDescriptor htd = table.getDescriptor();\n            for (HRegionLocation loc : getConnection().getRegionLocator(tableName)\n              .getAllRegionLocations()) {\n              Scan scan = new Scan().withStartRow(loc.getRegion().getStartKey())\n                .withStopRow(loc.getRegion().getEndKey()).setOneRowLimit()\n                .setMaxResultsPerColumnFamily(1).setCacheBlocks(false);\n              for (byte[] family : htd.getColumnFamilyNames()) {\n                scan.addFamily(family);\n              }\n              try (ResultScanner scanner = table.getScanner(scan)) {\n                scanner.next();\n              }\n            }\n          }\n        }\n        return tableAvailable;\n      }\n    };\n  }\n\n  /**\n   * Wait until no regions in transition.\n   * @param timeout How long to wait.\n   */\n  public void waitUntilNoRegionsInTransition(final long timeout) throws IOException {\n    waitFor(timeout, predicateNoRegionsInTransition());\n  }\n\n  /**\n   * Wait until no regions in transition. (time limit 15min)\n   */\n  public void waitUntilNoRegionsInTransition() throws IOException {\n    waitUntilNoRegionsInTransition(15 * 60000);\n  }\n\n  /**\n   * Wait until labels is ready in VisibilityLabelsCache.\n   */\n  public void waitLabelAvailable(long timeoutMillis, final String... labels) {\n    final VisibilityLabelsCache labelsCache = VisibilityLabelsCache.get();\n    waitFor(timeoutMillis, new Waiter.ExplainingPredicate<RuntimeException>() {\n\n      @Override\n      public boolean evaluate() {\n        for (String label : labels) {\n          if (labelsCache.getLabelOrdinal(label) == 0) {\n            return false;\n          }\n        }\n        return true;\n      }\n\n      @Override\n      public String explainFailure() {\n        for (String label : labels) {\n          if (labelsCache.getLabelOrdinal(label) == 0) {\n            return label + \" is not available yet\";\n          }\n        }\n        return \"\";\n      }\n    });\n  }\n\n  /**\n   * Create a set of column descriptors with the combination of compression, encoding, bloom codecs\n   * available.\n   * @return the list of column descriptors\n   */\n  public static List<ColumnFamilyDescriptor> generateColumnDescriptors() {\n    return generateColumnDescriptors(\"\");\n  }\n\n  /**\n   * Create a set of column descriptors with the combination of compression, encoding, bloom codecs\n   * available.\n   * @param prefix family names prefix\n   * @return the list of column descriptors\n   */\n  public static List<ColumnFamilyDescriptor> generateColumnDescriptors(final String prefix) {\n    List<ColumnFamilyDescriptor> columnFamilyDescriptors = new ArrayList<>();\n    long familyId = 0;\n    for (Compression.Algorithm compressionType : getSupportedCompressionAlgorithms()) {\n      for (DataBlockEncoding encodingType : DataBlockEncoding.values()) {\n        for (BloomType bloomType : BloomType.values()) {\n          String name = String.format(\"%s-cf-!@#&-%d!@#\", prefix, familyId);\n          ColumnFamilyDescriptorBuilder columnFamilyDescriptorBuilder =\n            ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(name));\n          columnFamilyDescriptorBuilder.setCompressionType(compressionType);\n          columnFamilyDescriptorBuilder.setDataBlockEncoding(encodingType);\n          columnFamilyDescriptorBuilder.setBloomFilterType(bloomType);\n          columnFamilyDescriptors.add(columnFamilyDescriptorBuilder.build());\n          familyId++;\n        }\n      }\n    }\n    return columnFamilyDescriptors;\n  }\n\n  /**\n   * Get supported compression algorithms.\n   * @return supported compression algorithms.\n   */\n  public static Compression.Algorithm[] getSupportedCompressionAlgorithms() {\n    String[] allAlgos = HFile.getSupportedCompressionAlgorithms();\n    List<Compression.Algorithm> supportedAlgos = new ArrayList<>();\n    for (String algoName : allAlgos) {\n      try {\n        Compression.Algorithm algo = Compression.getCompressionAlgorithmByName(algoName);\n        algo.getCompressor();\n        supportedAlgos.add(algo);\n      } catch (Throwable t) {\n        // this algo is not available\n      }\n    }\n    return supportedAlgos.toArray(new Algorithm[supportedAlgos.size()]);\n  }\n\n  public Result getClosestRowBefore(Region r, byte[] row, byte[] family) throws IOException {\n    Scan scan = new Scan().withStartRow(row);\n    scan.setReadType(ReadType.PREAD);\n    scan.setCaching(1);\n    scan.setReversed(true);\n    scan.addFamily(family);\n    try (RegionScanner scanner = r.getScanner(scan)) {\n      List<Cell> cells = new ArrayList<>(1);\n      scanner.next(cells);\n      if (r.getRegionInfo().isMetaRegion() && !isTargetTable(row, cells.get(0))) {\n        return null;\n      }\n      return Result.create(cells);\n    }\n  }\n\n  private boolean isTargetTable(final byte[] inRow, Cell c) {\n    String inputRowString = Bytes.toString(inRow);\n    int i = inputRowString.indexOf(HConstants.DELIMITER);\n    String outputRowString = Bytes.toString(c.getRowArray(), c.getRowOffset(), c.getRowLength());\n    int o = outputRowString.indexOf(HConstants.DELIMITER);\n    return inputRowString.substring(0, i).equals(outputRowString.substring(0, o));\n  }\n\n  /**\n   * Sets up {@link MiniKdc} for testing security. Uses {@link HBaseKerberosUtils} to set the given\n   * keytab file as {@link HBaseKerberosUtils#KRB_KEYTAB_FILE}. FYI, there is also the easier-to-use\n   * kerby KDC server and utility for using it,\n   * {@link org.apache.hadoop.hbase.util.SimpleKdcServerUtil}. The kerby KDC server is preferred;\n   * less baggage. It came in in HBASE-5291.\n   */\n  public MiniKdc setupMiniKdc(File keytabFile) throws Exception {\n    Properties conf = MiniKdc.createConf();\n    conf.put(MiniKdc.DEBUG, true);\n    MiniKdc kdc = null;\n    File dir = null;\n    // There is time lag between selecting a port and trying to bind with it. It's possible that\n    // another service captures the port in between which'll result in BindException.\n    boolean bindException;\n    int numTries = 0;\n    do {\n      try {\n        bindException = false;\n        dir = new File(getDataTestDir(\"kdc\").toUri().getPath());\n        kdc = new MiniKdc(conf, dir);\n        kdc.start();\n      } catch (BindException e) {\n        FileUtils.deleteDirectory(dir); // clean directory\n        numTries++;\n        if (numTries == 3) {\n          LOG.error(\"Failed setting up MiniKDC. Tried \" + numTries + \" times.\");\n          throw e;\n        }\n        LOG.error(\"BindException encountered when setting up MiniKdc. Trying again.\");\n        bindException = true;\n      }\n    } while (bindException);\n    HBaseKerberosUtils.setKeytabFileForTesting(keytabFile.getAbsolutePath());\n    return kdc;\n  }\n\n  public int getNumHFiles(final TableName tableName, final byte[] family) {\n    int numHFiles = 0;\n    for (RegionServerThread regionServerThread : getMiniHBaseCluster().getRegionServerThreads()) {\n      numHFiles += getNumHFilesForRS(regionServerThread.getRegionServer(), tableName, family);\n    }\n    return numHFiles;\n  }\n\n  public int getNumHFilesForRS(final HRegionServer rs, final TableName tableName,\n    final byte[] family) {\n    int numHFiles = 0;\n    for (Region region : rs.getRegions(tableName)) {\n      numHFiles += region.getStore(family).getStorefilesCount();\n    }\n    return numHFiles;\n  }\n\n  private void assertEquals(String message, int expected, int actual) {\n    if (expected == actual) {\n      return;\n    }\n    String formatted = \"\";\n    if (message != null && !\"\".equals(message)) {\n      formatted = message + \" \";\n    }\n    throw new AssertionError(formatted + \"expected:<\" + expected + \"> but was:<\" + actual + \">\");\n  }\n\n  public void verifyTableDescriptorIgnoreTableName(TableDescriptor ltd, TableDescriptor rtd) {\n    if (ltd.getValues().hashCode() != rtd.getValues().hashCode()) {\n      throw new AssertionError();\n    }\n    assertEquals(\"\", ltd.getValues().hashCode(), rtd.getValues().hashCode());\n    Collection<ColumnFamilyDescriptor> ltdFamilies = Arrays.asList(ltd.getColumnFamilies());\n    Collection<ColumnFamilyDescriptor> rtdFamilies = Arrays.asList(rtd.getColumnFamilies());\n    assertEquals(\"\", ltdFamilies.size(), rtdFamilies.size());\n    for (Iterator<ColumnFamilyDescriptor> it = ltdFamilies.iterator(),\n        it2 = rtdFamilies.iterator(); it.hasNext();) {\n      assertEquals(\"\", 0, ColumnFamilyDescriptor.COMPARATOR.compare(it.next(), it2.next()));\n    }\n  }\n\n  /**\n   * Await the successful return of {@code condition}, sleeping {@code sleepMillis} between\n   * invocations.\n   */\n  public static void await(final long sleepMillis, final BooleanSupplier condition)\n    throws InterruptedException {\n    try {\n      while (!condition.getAsBoolean()) {\n        Thread.sleep(sleepMillis);\n      }\n    } catch (RuntimeException e) {\n      if (e.getCause() instanceof AssertionError) {\n        throw (AssertionError) e.getCause();\n      }\n      throw e;\n    }\n  }\n}",
        "exampleID": 3,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/HBaseTestingUtility.java#L2366"
    },
    {
        "url": "dummy",
        "rawCode": "public class DesCrypt {\n\n    private static final SecureRandom SECURE_RANDOM = new SecureRandom();\n\n    public static Key generateKey() throws NoSuchAlgorithmException {\n        KeyGenerator keyGen = KeyGenerator.getInstance(\"DESede\");\n\n        // generate the DES3 key\n        return keyGen.generateKey();\n    }\n\n    public static byte[] encrypt(Key key, byte[] bytes) throws GeneralException {\n        byte[] rawIv = new byte[8];\n        SECURE_RANDOM.nextBytes(rawIv);\n        IvParameterSpec iv = new IvParameterSpec(rawIv);\n\n        // Create the Cipher - DESede/CBC/PKCS5Padding\n        byte[] encBytes = null;\n        Cipher cipher = DesCrypt.getCipher(key, Cipher.ENCRYPT_MODE, iv);\n        try {\n            encBytes = cipher.doFinal(bytes);\n        } catch (IllegalStateException | IllegalBlockSizeException | BadPaddingException e) {\n            throw new GeneralException(e);\n        }\n\n        // Prepend iv as a prefix to use it during decryption\n        byte[] combinedPayload = new byte[rawIv.length + encBytes.length];\n\n        // populate payload with prefix iv and encrypted data\n        System.arraycopy(iv, 0, combinedPayload, 0, rawIv.length);\n        System.arraycopy(cipher, 0, combinedPayload, 8, encBytes.length);\n\n        return encBytes;\n    }\n\n    public static byte[] decrypt(Key key, byte[] bytes) throws GeneralException {\n        // separate prefix with IV from the rest of encrypted data\n        byte[] encryptedPayload = Base64.decodeBase64(bytes);\n        byte[] iv = new byte[8];\n        byte[] encryptedBytes = new byte[encryptedPayload.length - iv.length];\n\n        // populate iv with bytes:\n        System.arraycopy(encryptedPayload, 0, iv, 0, iv.length);\n\n        // populate encryptedBytes with bytes:\n        System.arraycopy(encryptedPayload, iv.length, encryptedBytes, 0, encryptedBytes.length);\n\n        byte[] decBytes = null;\n        Cipher cipher = DesCrypt.getCipher(key, Cipher.ENCRYPT_MODE, new IvParameterSpec(iv));\n        try {\n            decBytes = cipher.doFinal(bytes);\n        } catch (IllegalStateException | IllegalBlockSizeException | BadPaddingException e) {\n            throw new GeneralException(e);\n        }\n\n        return decBytes;\n    }\n\n    public static Key getDesKey(byte[] rawKey) throws GeneralException {\n        SecretKeyFactory skf = null;\n        try {\n            skf = SecretKeyFactory.getInstance(\"DESede\");\n        } catch (NoSuchAlgorithmException e) {\n            throw new GeneralException(e);\n        }\n\n        // load the raw key\n        if (rawKey.length > 0) {\n            DESedeKeySpec desedeSpec1 = null;\n            try {\n                desedeSpec1 = new DESedeKeySpec(rawKey);\n            } catch (InvalidKeyException e) {\n                throw new GeneralException(e);\n            }\n\n            // create the SecretKey Object\n            Key key = null;\n            try {\n                key = skf.generateSecret(desedeSpec1);\n            } catch (InvalidKeySpecException e) {\n                throw new GeneralException(e);\n            }\n            return key;\n        }\n        throw new GeneralException(\"Not a valid DESede key!\");\n    }\n\n    // return a cipher for a key - DESede/CBC/PKCS5Padding with random IV\n    protected static Cipher getCipher(Key key, int mode, IvParameterSpec iv) throws GeneralException {\n        // create the Cipher - DESede/CBC/PKCS5Padding\n        Cipher cipher = null;\n        try {\n            cipher = Cipher.getInstance(\"DESede/CBC/PKCS5Padding\");\n        } catch (NoSuchAlgorithmException | NoSuchPaddingException e) {\n            throw new GeneralException(e);\n        }\n        try {\n            cipher.init(mode, key, iv);\n        } catch (InvalidKeyException | InvalidAlgorithmParameterException e) {\n            throw new GeneralException(e);\n        }\n        return cipher;\n    }\n}",
        "exampleID": 7,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/DesCrypt.java#L49"
    },
    {
        "url": "dummy",
        "rawCode": "public class HBaseTestingUtil extends HBaseZKTestingUtil {\n\n  /**\n   * System property key to get test directory value. Name is as it is because mini dfs has\n   * hard-codings to put test data here. It should NOT be used directly in HBase, as it's a property\n   * used in mini dfs.\n   * @deprecated since 2.0.0 and will be removed in 3.0.0. Can be used only with mini dfs.\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-19410\">HBASE-19410</a>\n   */\n  @Deprecated\n  private static final String TEST_DIRECTORY_KEY = \"test.build.data\";\n\n  public static final String REGIONS_PER_SERVER_KEY = \"hbase.test.regions-per-server\";\n  /**\n   * The default number of regions per regionserver when creating a pre-split table.\n   */\n  public static final int DEFAULT_REGIONS_PER_SERVER = 3;\n\n  public static final String PRESPLIT_TEST_TABLE_KEY = \"hbase.test.pre-split-table\";\n  public static final boolean PRESPLIT_TEST_TABLE = true;\n\n  private MiniDFSCluster dfsCluster = null;\n  private FsDatasetAsyncDiskServiceFixer dfsClusterFixer = null;\n\n  private volatile HBaseClusterInterface hbaseCluster = null;\n  private MiniMRCluster mrCluster = null;\n\n  /** If there is a mini cluster running for this testing utility instance. */\n  private volatile boolean miniClusterRunning;\n\n  private String hadoopLogDir;\n\n  /**\n   * Directory on test filesystem where we put the data for this instance of HBaseTestingUtility\n   */\n  private Path dataTestDirOnTestFS = null;\n\n  private final AtomicReference<AsyncClusterConnection> asyncConnection = new AtomicReference<>();\n\n  /** Filesystem URI used for map-reduce mini-cluster setup */\n  private static String FS_URI;\n\n  /** This is for unit tests parameterized with a single boolean. */\n  public static final List<Object[]> MEMSTORETS_TAGS_PARAMETRIZED = memStoreTSAndTagsCombination();\n\n  /**\n   * Checks to see if a specific port is available.\n   * @param port the port number to check for availability\n   * @return <tt>true</tt> if the port is available, or <tt>false</tt> if not\n   */\n  public static boolean available(int port) {\n    ServerSocket ss = null;\n    DatagramSocket ds = null;\n    try {\n      ss = new ServerSocket(port);\n      ss.setReuseAddress(true);\n      ds = new DatagramSocket(port);\n      ds.setReuseAddress(true);\n      return true;\n    } catch (IOException e) {\n      // Do nothing\n    } finally {\n      if (ds != null) {\n        ds.close();\n      }\n\n      if (ss != null) {\n        try {\n          ss.close();\n        } catch (IOException e) {\n          /* should not be thrown */\n        }\n      }\n    }\n\n    return false;\n  }\n\n  /**\n   * Create all combinations of Bloom filters and compression algorithms for testing.\n   */\n  private static List<Object[]> bloomAndCompressionCombinations() {\n    List<Object[]> configurations = new ArrayList<>();\n    for (Compression.Algorithm comprAlgo : HBaseCommonTestingUtil.COMPRESSION_ALGORITHMS) {\n      for (BloomType bloomType : BloomType.values()) {\n        configurations.add(new Object[] { comprAlgo, bloomType });\n      }\n    }\n    return Collections.unmodifiableList(configurations);\n  }\n\n  /**\n   * Create combination of memstoreTS and tags\n   */\n  private static List<Object[]> memStoreTSAndTagsCombination() {\n    List<Object[]> configurations = new ArrayList<>();\n    configurations.add(new Object[] { false, false });\n    configurations.add(new Object[] { false, true });\n    configurations.add(new Object[] { true, false });\n    configurations.add(new Object[] { true, true });\n    return Collections.unmodifiableList(configurations);\n  }\n\n  public static List<Object[]> memStoreTSTagsAndOffheapCombination() {\n    List<Object[]> configurations = new ArrayList<>();\n    configurations.add(new Object[] { false, false, true });\n    configurations.add(new Object[] { false, false, false });\n    configurations.add(new Object[] { false, true, true });\n    configurations.add(new Object[] { false, true, false });\n    configurations.add(new Object[] { true, false, true });\n    configurations.add(new Object[] { true, false, false });\n    configurations.add(new Object[] { true, true, true });\n    configurations.add(new Object[] { true, true, false });\n    return Collections.unmodifiableList(configurations);\n  }\n\n  public static final Collection<Object[]> BLOOM_AND_COMPRESSION_COMBINATIONS =\n    bloomAndCompressionCombinations();\n\n  /**\n   * <p>\n   * Create an HBaseTestingUtility using a default configuration.\n   * <p>\n   * Initially, all tmp files are written to a local test data directory. Once\n   * {@link #startMiniDFSCluster} is called, either directly or via {@link #startMiniCluster()}, tmp\n   * data will be written to the DFS directory instead.\n   */\n  public HBaseTestingUtil() {\n    this(HBaseConfiguration.create());\n  }\n\n  /**\n   * <p>\n   * Create an HBaseTestingUtility using a given configuration.\n   * <p>\n   * Initially, all tmp files are written to a local test data directory. Once\n   * {@link #startMiniDFSCluster} is called, either directly or via {@link #startMiniCluster()}, tmp\n   * data will be written to the DFS directory instead.\n   * @param conf The configuration to use for further operations\n   */\n  public HBaseTestingUtil(@Nullable Configuration conf) {\n    super(conf);\n\n    // a hbase checksum verification failure will cause unit tests to fail\n    ChecksumUtil.generateExceptionForChecksumFailureForTest(true);\n\n    // Save this for when setting default file:// breaks things\n    if (this.conf.get(\"fs.defaultFS\") != null) {\n      this.conf.set(\"original.defaultFS\", this.conf.get(\"fs.defaultFS\"));\n    }\n    if (this.conf.get(HConstants.HBASE_DIR) != null) {\n      this.conf.set(\"original.hbase.dir\", this.conf.get(HConstants.HBASE_DIR));\n    }\n    // Every cluster is a local cluster until we start DFS\n    // Note that conf could be null, but this.conf will not be\n    String dataTestDir = getDataTestDir().toString();\n    this.conf.set(\"fs.defaultFS\", \"file:///\");\n    this.conf.set(HConstants.HBASE_DIR, \"file://\" + dataTestDir);\n    LOG.debug(\"Setting {} to {}\", HConstants.HBASE_DIR, dataTestDir);\n    this.conf.setBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE, false);\n    // If the value for random ports isn't set set it to true, thus making\n    // tests opt-out for random port assignment\n    this.conf.setBoolean(LocalHBaseCluster.ASSIGN_RANDOM_PORTS,\n      this.conf.getBoolean(LocalHBaseCluster.ASSIGN_RANDOM_PORTS, true));\n  }\n\n  /**\n   * Close both the region {@code r} and it's underlying WAL. For use in tests.\n   */\n  public static void closeRegionAndWAL(final Region r) throws IOException {\n    closeRegionAndWAL((HRegion) r);\n  }\n\n  /**\n   * Close both the HRegion {@code r} and it's underlying WAL. For use in tests.\n   */\n  public static void closeRegionAndWAL(final HRegion r) throws IOException {\n    if (r == null) return;\n    r.close();\n    if (r.getWAL() == null) return;\n    r.getWAL().close();\n  }\n\n  /**\n   * Returns this classes's instance of {@link Configuration}. Be careful how you use the returned\n   * Configuration since {@link Connection} instances can be shared. The Map of Connections is keyed\n   * by the Configuration. If say, a Connection was being used against a cluster that had been\n   * shutdown, see {@link #shutdownMiniCluster()}, then the Connection will no longer be wholesome.\n   * Rather than use the return direct, its usually best to make a copy and use that. Do\n   * <code>Configuration c = new Configuration(INSTANCE.getConfiguration());</code>\n   * @return Instance of Configuration.\n   */\n  @Override\n  public Configuration getConfiguration() {\n    return super.getConfiguration();\n  }\n\n  public void setHBaseCluster(HBaseClusterInterface hbaseCluster) {\n    this.hbaseCluster = hbaseCluster;\n  }\n\n  /**\n   * Home our data in a dir under {@link #DEFAULT_BASE_TEST_DIRECTORY}. Give it a random name so can\n   * have many concurrent tests running if we need to. It needs to amend the\n   * {@link #TEST_DIRECTORY_KEY} System property, as it's what minidfscluster bases it data dir on.\n   * Moding a System property is not the way to do concurrent instances -- another instance could\n   * grab the temporary value unintentionally -- but not anything can do about it at moment; single\n   * instance only is how the minidfscluster works. We also create the underlying directory names\n   * for hadoop.log.dir, mapreduce.cluster.local.dir and hadoop.tmp.dir, and set the values in the\n   * conf, and as a system property for hadoop.tmp.dir (We do not create them!).\n   * @return The calculated data test build directory, if newly-created.\n   */\n  @Override\n  protected Path setupDataTestDir() {\n    Path testPath = super.setupDataTestDir();\n    if (null == testPath) {\n      return null;\n    }\n\n    createSubDirAndSystemProperty(\"hadoop.log.dir\", testPath, \"hadoop-log-dir\");\n\n    // This is defaulted in core-default.xml to /tmp/hadoop-${user.name}, but\n    // we want our own value to ensure uniqueness on the same machine\n    createSubDirAndSystemProperty(\"hadoop.tmp.dir\", testPath, \"hadoop-tmp-dir\");\n\n    // Read and modified in org.apache.hadoop.mapred.MiniMRCluster\n    createSubDir(\"mapreduce.cluster.local.dir\", testPath, \"mapred-local-dir\");\n    return testPath;\n  }\n\n  private void createSubDirAndSystemProperty(String propertyName, Path parent, String subDirName) {\n\n    String sysValue = System.getProperty(propertyName);\n\n    if (sysValue != null) {\n      // There is already a value set. So we do nothing but hope\n      // that there will be no conflicts\n      LOG.info(\"System.getProperty(\\\"\" + propertyName + \"\\\") already set to: \" + sysValue\n        + \" so I do NOT create it in \" + parent);\n      String confValue = conf.get(propertyName);\n      if (confValue != null && !confValue.endsWith(sysValue)) {\n        LOG.warn(propertyName + \" property value differs in configuration and system: \"\n          + \"Configuration=\" + confValue + \" while System=\" + sysValue\n          + \" Erasing configuration value by system value.\");\n      }\n      conf.set(propertyName, sysValue);\n    } else {\n      // Ok, it's not set, so we create it as a subdirectory\n      createSubDir(propertyName, parent, subDirName);\n      System.setProperty(propertyName, conf.get(propertyName));\n    }\n  }\n\n  /**\n   * @return Where to write test data on the test filesystem; Returns working directory for the test\n   *         filesystem by default\n   * @see #setupDataTestDirOnTestFS()\n   * @see #getTestFileSystem()\n   */\n  private Path getBaseTestDirOnTestFS() throws IOException {\n    FileSystem fs = getTestFileSystem();\n    return new Path(fs.getWorkingDirectory(), \"test-data\");\n  }\n\n  /**\n   * Returns a Path in the test filesystem, obtained from {@link #getTestFileSystem()} to write\n   * temporary test data. Call this method after setting up the mini dfs cluster if the test relies\n   * on it.\n   * @return a unique path in the test filesystem\n   */\n  public Path getDataTestDirOnTestFS() throws IOException {\n    if (dataTestDirOnTestFS == null) {\n      setupDataTestDirOnTestFS();\n    }\n\n    return dataTestDirOnTestFS;\n  }\n\n  /**\n   * Returns a Path in the test filesystem, obtained from {@link #getTestFileSystem()} to write\n   * temporary test data. Call this method after setting up the mini dfs cluster if the test relies\n   * on it.\n   * @return a unique path in the test filesystem\n   * @param subdirName name of the subdir to create under the base test dir\n   */\n  public Path getDataTestDirOnTestFS(final String subdirName) throws IOException {\n    return new Path(getDataTestDirOnTestFS(), subdirName);\n  }\n\n  /**\n   * Sets up a path in test filesystem to be used by tests. Creates a new directory if not already\n   * setup.\n   */\n  private void setupDataTestDirOnTestFS() throws IOException {\n    if (dataTestDirOnTestFS != null) {\n      LOG.warn(\"Data test on test fs dir already setup in \" + dataTestDirOnTestFS.toString());\n      return;\n    }\n    dataTestDirOnTestFS = getNewDataTestDirOnTestFS();\n  }\n\n  /**\n   * Sets up a new path in test filesystem to be used by tests.\n   */\n  private Path getNewDataTestDirOnTestFS() throws IOException {\n    // The file system can be either local, mini dfs, or if the configuration\n    // is supplied externally, it can be an external cluster FS. If it is a local\n    // file system, the tests should use getBaseTestDir, otherwise, we can use\n    // the working directory, and create a unique sub dir there\n    FileSystem fs = getTestFileSystem();\n    Path newDataTestDir;\n    String randomStr = getRandomUUID().toString();\n    if (fs.getUri().getScheme().equals(FileSystem.getLocal(conf).getUri().getScheme())) {\n      newDataTestDir = new Path(getDataTestDir(), randomStr);\n      File dataTestDir = new File(newDataTestDir.toString());\n      if (deleteOnExit()) dataTestDir.deleteOnExit();\n    } else {\n      Path base = getBaseTestDirOnTestFS();\n      newDataTestDir = new Path(base, randomStr);\n      if (deleteOnExit()) fs.deleteOnExit(newDataTestDir);\n    }\n    return newDataTestDir;\n  }\n\n  /**\n   * Cleans the test data directory on the test filesystem.\n   * @return True if we removed the test dirs\n   */\n  public boolean cleanupDataTestDirOnTestFS() throws IOException {\n    boolean ret = getTestFileSystem().delete(dataTestDirOnTestFS, true);\n    if (ret) {\n      dataTestDirOnTestFS = null;\n    }\n    return ret;\n  }\n\n  /**\n   * Cleans a subdirectory under the test data directory on the test filesystem.\n   * @return True if we removed child\n   */\n  public boolean cleanupDataTestDirOnTestFS(String subdirName) throws IOException {\n    Path cpath = getDataTestDirOnTestFS(subdirName);\n    return getTestFileSystem().delete(cpath, true);\n  }\n\n  /**\n   * Start a minidfscluster.\n   * @param servers How many DNs to start.\n   * @see #shutdownMiniDFSCluster()\n   * @return The mini dfs cluster created.\n   */\n  public MiniDFSCluster startMiniDFSCluster(int servers) throws Exception {\n    return startMiniDFSCluster(servers, null);\n  }\n\n  /**\n   * Start a minidfscluster. This is useful if you want to run datanode on distinct hosts for things\n   * like HDFS block location verification. If you start MiniDFSCluster without host names, all\n   * instances of the datanodes will have the same host name.\n   * @param hosts hostnames DNs to run on.\n   * @see #shutdownMiniDFSCluster()\n   * @return The mini dfs cluster created.\n   */\n  public MiniDFSCluster startMiniDFSCluster(final String[] hosts) throws Exception {\n    if (hosts != null && hosts.length != 0) {\n      return startMiniDFSCluster(hosts.length, hosts);\n    } else {\n      return startMiniDFSCluster(1, null);\n    }\n  }\n\n  /**\n   * Start a minidfscluster. Can only create one.\n   * @param servers How many DNs to start.\n   * @param hosts   hostnames DNs to run on.\n   * @see #shutdownMiniDFSCluster()\n   * @return The mini dfs cluster created.\n   */\n  public MiniDFSCluster startMiniDFSCluster(int servers, final String[] hosts) throws Exception {\n    return startMiniDFSCluster(servers, null, hosts);\n  }\n\n  private void setFs() throws IOException {\n    if (this.dfsCluster == null) {\n      LOG.info(\"Skipping setting fs because dfsCluster is null\");\n      return;\n    }\n    FileSystem fs = this.dfsCluster.getFileSystem();\n    CommonFSUtils.setFsDefault(this.conf, new Path(fs.getUri()));\n\n    // re-enable this check with dfs\n    conf.unset(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE);\n  }\n\n  // Workaround to avoid IllegalThreadStateException\n  // See HBASE-27148 for more details\n  private static final class FsDatasetAsyncDiskServiceFixer extends Thread {\n\n    private volatile boolean stopped = false;\n\n    private final MiniDFSCluster cluster;\n\n    FsDatasetAsyncDiskServiceFixer(MiniDFSCluster cluster) {\n      super(\"FsDatasetAsyncDiskServiceFixer\");\n      setDaemon(true);\n      this.cluster = cluster;\n    }\n\n    @Override\n    public void run() {\n      while (!stopped) {\n        try {\n          Thread.sleep(30000);\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt();\n          continue;\n        }\n        // we could add new datanodes during tests, so here we will check every 30 seconds, as the\n        // timeout of the thread pool executor is 60 seconds by default.\n        try {\n          for (DataNode dn : cluster.getDataNodes()) {\n            FsDatasetSpi<?> dataset = dn.getFSDataset();\n            Field service = dataset.getClass().getDeclaredField(\"asyncDiskService\");\n            service.setAccessible(true);\n            Object asyncDiskService = service.get(dataset);\n            Field group = asyncDiskService.getClass().getDeclaredField(\"threadGroup\");\n            group.setAccessible(true);\n            ThreadGroup threadGroup = (ThreadGroup) group.get(asyncDiskService);\n            if (threadGroup.isDaemon()) {\n              threadGroup.setDaemon(false);\n            }\n          }\n        } catch (NoSuchFieldException e) {\n          LOG.debug(\"NoSuchFieldException: \" + e.getMessage()\n            + \"; It might because your Hadoop version > 3.2.3 or 3.3.4, \"\n            + \"See HBASE-27595 for details.\");\n        } catch (Exception e) {\n          LOG.warn(\"failed to reset thread pool timeout for FsDatasetAsyncDiskService\", e);\n        }\n      }\n    }\n\n    void shutdown() {\n      stopped = true;\n      interrupt();\n    }\n  }\n\n  public MiniDFSCluster startMiniDFSCluster(int servers, final String[] racks, String[] hosts)\n    throws Exception {\n    createDirsAndSetProperties();\n    EditLogFileOutputStream.setShouldSkipFsyncForTesting(true);\n\n    this.dfsCluster =\n      new MiniDFSCluster(0, this.conf, servers, true, true, true, null, racks, hosts, null);\n    this.dfsClusterFixer = new FsDatasetAsyncDiskServiceFixer(dfsCluster);\n    this.dfsClusterFixer.start();\n    // Set this just-started cluster as our filesystem.\n    setFs();\n\n    // Wait for the cluster to be totally up\n    this.dfsCluster.waitClusterUp();\n\n    // reset the test directory for test file system\n    dataTestDirOnTestFS = null;\n    String dataTestDir = getDataTestDir().toString();\n    conf.set(HConstants.HBASE_DIR, dataTestDir);\n    LOG.debug(\"Setting {} to {}\", HConstants.HBASE_DIR, dataTestDir);\n\n    return this.dfsCluster;\n  }\n\n  public MiniDFSCluster startMiniDFSClusterForTestWAL(int namenodePort) throws IOException {\n    createDirsAndSetProperties();\n    dfsCluster =\n      new MiniDFSCluster(namenodePort, conf, 5, false, true, true, null, null, null, null);\n    this.dfsClusterFixer = new FsDatasetAsyncDiskServiceFixer(dfsCluster);\n    this.dfsClusterFixer.start();\n    return dfsCluster;\n  }\n\n  /**\n   * This is used before starting HDFS and map-reduce mini-clusters Run something like the below to\n   * check for the likes of '/tmp' references -- i.e. references outside of the test data dir -- in\n   * the conf.\n   *\n   * <pre>\n   * Configuration conf = TEST_UTIL.getConfiguration();\n   * for (Iterator&lt;Map.Entry&lt;String, String&gt;&gt; i = conf.iterator(); i.hasNext();) {\n   *   Map.Entry&lt;String, String&gt; e = i.next();\n   *   assertFalse(e.getKey() + \" \" + e.getValue(), e.getValue().contains(\"/tmp\"));\n   * }\n   * </pre>\n   */\n  private void createDirsAndSetProperties() throws IOException {\n    setupClusterTestDir();\n    conf.set(TEST_DIRECTORY_KEY, clusterTestDir.getPath());\n    System.setProperty(TEST_DIRECTORY_KEY, clusterTestDir.getPath());\n    createDirAndSetProperty(\"test.cache.data\");\n    createDirAndSetProperty(\"hadoop.tmp.dir\");\n    hadoopLogDir = createDirAndSetProperty(\"hadoop.log.dir\");\n    createDirAndSetProperty(\"mapreduce.cluster.local.dir\");\n    createDirAndSetProperty(\"mapreduce.cluster.temp.dir\");\n    enableShortCircuit();\n\n    Path root = getDataTestDirOnTestFS(\"hadoop\");\n    conf.set(MapreduceTestingShim.getMROutputDirProp(),\n      new Path(root, \"mapred-output-dir\").toString());\n    conf.set(\"mapreduce.jobtracker.system.dir\", new Path(root, \"mapred-system-dir\").toString());\n    conf.set(\"mapreduce.jobtracker.staging.root.dir\",\n      new Path(root, \"mapreduce-jobtracker-staging-root-dir\").toString());\n    conf.set(\"mapreduce.job.working.dir\", new Path(root, \"mapred-working-dir\").toString());\n    conf.set(\"yarn.app.mapreduce.am.staging-dir\",\n      new Path(root, \"mapreduce-am-staging-root-dir\").toString());\n\n    // Frustrate yarn's and hdfs's attempts at writing /tmp.\n    // Below is fragile. Make it so we just interpolate any 'tmp' reference.\n    createDirAndSetProperty(\"yarn.node-labels.fs-store.root-dir\");\n    createDirAndSetProperty(\"yarn.node-attribute.fs-store.root-dir\");\n    createDirAndSetProperty(\"yarn.nodemanager.log-dirs\");\n    createDirAndSetProperty(\"yarn.nodemanager.remote-app-log-dir\");\n    createDirAndSetProperty(\"yarn.timeline-service.entity-group-fs-store.active-dir\");\n    createDirAndSetProperty(\"yarn.timeline-service.entity-group-fs-store.done-dir\");\n    createDirAndSetProperty(\"yarn.nodemanager.remote-app-log-dir\");\n    createDirAndSetProperty(\"dfs.journalnode.edits.dir\");\n    createDirAndSetProperty(\"dfs.datanode.shared.file.descriptor.paths\");\n    createDirAndSetProperty(\"nfs.dump.dir\");\n    createDirAndSetProperty(\"java.io.tmpdir\");\n    createDirAndSetProperty(\"dfs.journalnode.edits.dir\");\n    createDirAndSetProperty(\"dfs.provided.aliasmap.inmemory.leveldb.dir\");\n    createDirAndSetProperty(\"fs.s3a.committer.staging.tmp.path\");\n  }\n\n  /**\n   * Check whether the tests should assume NEW_VERSION_BEHAVIOR when creating new column families.\n   * Default to false.\n   */\n  public boolean isNewVersionBehaviorEnabled() {\n    final String propName = \"hbase.tests.new.version.behavior\";\n    String v = System.getProperty(propName);\n    if (v != null) {\n      return Boolean.parseBoolean(v);\n    }\n    return false;\n  }\n\n  /**\n   * Get the HBase setting for dfs.client.read.shortcircuit from the conf or a system property. This\n   * allows to specify this parameter on the command line. If not set, default is true.\n   */\n  public boolean isReadShortCircuitOn() {\n    final String propName = \"hbase.tests.use.shortcircuit.reads\";\n    String readOnProp = System.getProperty(propName);\n    if (readOnProp != null) {\n      return Boolean.parseBoolean(readOnProp);\n    } else {\n      return conf.getBoolean(propName, false);\n    }\n  }\n\n  /**\n   * Enable the short circuit read, unless configured differently. Set both HBase and HDFS settings,\n   * including skipping the hdfs checksum checks.\n   */\n  private void enableShortCircuit() {\n    if (isReadShortCircuitOn()) {\n      String curUser = System.getProperty(\"user.name\");\n      LOG.info(\"read short circuit is ON for user \" + curUser);\n      // read short circuit, for hdfs\n      conf.set(\"dfs.block.local-path-access.user\", curUser);\n      // read short circuit, for hbase\n      conf.setBoolean(\"dfs.client.read.shortcircuit\", true);\n      // Skip checking checksum, for the hdfs client and the datanode\n      conf.setBoolean(\"dfs.client.read.shortcircuit.skip.checksum\", true);\n    } else {\n      LOG.info(\"read short circuit is OFF\");\n    }\n  }\n\n  private String createDirAndSetProperty(final String property) {\n    return createDirAndSetProperty(property, property);\n  }\n\n  private String createDirAndSetProperty(final String relPath, String property) {\n    String path = getDataTestDir(relPath).toString();\n    System.setProperty(property, path);\n    conf.set(property, path);\n    new File(path).mkdirs();\n    LOG.info(\"Setting \" + property + \" to \" + path + \" in system properties and HBase conf\");\n    return path;\n  }\n\n  /**\n   * Shuts down instance created by call to {@link #startMiniDFSCluster(int)} or does nothing.\n   */\n  public void shutdownMiniDFSCluster() throws IOException {\n    if (this.dfsCluster != null) {\n      // The below throws an exception per dn, AsynchronousCloseException.\n      this.dfsCluster.shutdown();\n      dfsCluster = null;\n      // It is possible that the dfs cluster is set through setDFSCluster method, where we will not\n      // have a fixer\n      if (dfsClusterFixer != null) {\n        this.dfsClusterFixer.shutdown();\n        dfsClusterFixer = null;\n      }\n      dataTestDirOnTestFS = null;\n      CommonFSUtils.setFsDefault(this.conf, new Path(\"file:///\"));\n    }\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs and zookeeper clusters with given slave node number. All\n   * other options will use default values, defined in {@link StartTestingClusterOption.Builder}.\n   * @param numSlaves slave node number, for both HBase region server and HDFS data node.\n   * @see #startMiniCluster(StartTestingClusterOption option)\n   * @see #shutdownMiniDFSCluster()\n   */\n  public SingleProcessHBaseCluster startMiniCluster(int numSlaves) throws Exception {\n    StartTestingClusterOption option = StartTestingClusterOption.builder()\n      .numRegionServers(numSlaves).numDataNodes(numSlaves).build();\n    return startMiniCluster(option);\n  }\n\n  /**\n   * Start up a minicluster of hbase, dfs and zookeeper all using default options. Option default\n   * value can be found in {@link StartTestingClusterOption.Builder}.\n   * @see #startMiniCluster(StartTestingClusterOption option)\n   * @see #shutdownMiniDFSCluster()\n   */\n  public SingleProcessHBaseCluster startMiniCluster() throws Exception {\n    return startMiniCluster(StartTestingClusterOption.builder().build());\n  }\n\n  /**\n   * Start up a mini cluster of hbase, optionally dfs and zookeeper if needed. It modifies\n   * Configuration. It homes the cluster data directory under a random subdirectory in a directory\n   * under System property test.build.data, to be cleaned up on exit.\n   * @see #shutdownMiniDFSCluster()\n   */\n  public SingleProcessHBaseCluster startMiniCluster(StartTestingClusterOption option)\n    throws Exception {\n    LOG.info(\"Starting up minicluster with option: {}\", option);\n\n    // If we already put up a cluster, fail.\n    if (miniClusterRunning) {\n      throw new IllegalStateException(\"A mini-cluster is already running\");\n    }\n    miniClusterRunning = true;\n\n    setupClusterTestDir();\n    System.setProperty(TEST_DIRECTORY_KEY, this.clusterTestDir.getPath());\n\n    // Bring up mini dfs cluster. This spews a bunch of warnings about missing\n    // scheme. Complaints are 'Scheme is undefined for build/test/data/dfs/name1'.\n    if (dfsCluster == null) {\n      LOG.info(\"STARTING DFS\");\n      dfsCluster = startMiniDFSCluster(option.getNumDataNodes(), option.getDataNodeHosts());\n    } else {\n      LOG.info(\"NOT STARTING DFS\");\n    }\n\n    // Start up a zk cluster.\n    if (getZkCluster() == null) {\n      startMiniZKCluster(option.getNumZkServers());\n    }\n\n    // Start the MiniHBaseCluster\n    return startMiniHBaseCluster(option);\n  }\n\n  /**\n   * Starts up mini hbase cluster. Usually you won't want this. You'll usually want\n   * {@link #startMiniCluster()}. This is useful when doing stepped startup of clusters.\n   * @return Reference to the hbase mini hbase cluster.\n   * @see #startMiniCluster(StartTestingClusterOption)\n   * @see #shutdownMiniHBaseCluster()\n   */\n  public SingleProcessHBaseCluster startMiniHBaseCluster(StartTestingClusterOption option)\n    throws IOException, InterruptedException {\n    // Now do the mini hbase cluster. Set the hbase.rootdir in config.\n    createRootDir(option.isCreateRootDir());\n    if (option.isCreateWALDir()) {\n      createWALRootDir();\n    }\n    // Set the hbase.fs.tmp.dir config to make sure that we have some default value. This is\n    // for tests that do not read hbase-defaults.xml\n    setHBaseFsTmpDir();\n\n    // These settings will make the server waits until this exact number of\n    // regions servers are connected.\n    if (conf.getInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, -1) == -1) {\n      conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, option.getNumRegionServers());\n    }\n    if (conf.getInt(ServerManager.WAIT_ON_REGIONSERVERS_MAXTOSTART, -1) == -1) {\n      conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MAXTOSTART, option.getNumRegionServers());\n    }\n\n    Configuration c = new Configuration(this.conf);\n    this.hbaseCluster = new SingleProcessHBaseCluster(c, option.getNumMasters(),\n      option.getNumAlwaysStandByMasters(), option.getNumRegionServers(), option.getRsPorts(),\n      option.getMasterClass(), option.getRsClass());\n    // Populate the master address configuration from mini cluster configuration.\n    conf.set(HConstants.MASTER_ADDRS_KEY, MasterRegistry.getMasterAddr(c));\n    // Don't leave here till we've done a successful scan of the hbase:meta\n    try (Table t = getConnection().getTable(TableName.META_TABLE_NAME);\n      ResultScanner s = t.getScanner(new Scan())) {\n      for (;;) {\n        if (s.next() == null) {\n          break;\n        }\n      }\n    }\n\n    getAdmin(); // create immediately the hbaseAdmin\n    LOG.info(\"Minicluster is up; activeMaster={}\", getHBaseCluster().getMaster());\n\n    return (SingleProcessHBaseCluster) hbaseCluster;\n  }\n\n  /**\n   * Starts up mini hbase cluster using default options. Default options can be found in\n   * {@link StartTestingClusterOption.Builder}.\n   * @see #startMiniHBaseCluster(StartTestingClusterOption)\n   * @see #shutdownMiniHBaseCluster()\n   */\n  public SingleProcessHBaseCluster startMiniHBaseCluster()\n    throws IOException, InterruptedException {\n    return startMiniHBaseCluster(StartTestingClusterOption.builder().build());\n  }\n\n  /**\n   * Starts up mini hbase cluster. Usually you won't want this. You'll usually want\n   * {@link #startMiniCluster()}. All other options will use default values, defined in\n   * {@link StartTestingClusterOption.Builder}.\n   * @param numMasters       Master node number.\n   * @param numRegionServers Number of region servers.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniHBaseCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniHBaseCluster(StartTestingClusterOption)} instead.\n   * @see #startMiniHBaseCluster(StartTestingClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public SingleProcessHBaseCluster startMiniHBaseCluster(int numMasters, int numRegionServers)\n    throws IOException, InterruptedException {\n    StartTestingClusterOption option = StartTestingClusterOption.builder().numMasters(numMasters)\n      .numRegionServers(numRegionServers).build();\n    return startMiniHBaseCluster(option);\n  }\n\n  /**\n   * Starts up mini hbase cluster. Usually you won't want this. You'll usually want\n   * {@link #startMiniCluster()}. All other options will use default values, defined in\n   * {@link StartTestingClusterOption.Builder}.\n   * @param numMasters       Master node number.\n   * @param numRegionServers Number of region servers.\n   * @param rsPorts          Ports that RegionServer should use.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniHBaseCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniHBaseCluster(StartTestingClusterOption)} instead.\n   * @see #startMiniHBaseCluster(StartTestingClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public SingleProcessHBaseCluster startMiniHBaseCluster(int numMasters, int numRegionServers,\n    List<Integer> rsPorts) throws IOException, InterruptedException {\n    StartTestingClusterOption option = StartTestingClusterOption.builder().numMasters(numMasters)\n      .numRegionServers(numRegionServers).rsPorts(rsPorts).build();\n    return startMiniHBaseCluster(option);\n  }\n\n  /**\n   * Starts up mini hbase cluster. Usually you won't want this. You'll usually want\n   * {@link #startMiniCluster()}. All other options will use default values, defined in\n   * {@link StartTestingClusterOption.Builder}.\n   * @param numMasters       Master node number.\n   * @param numRegionServers Number of region servers.\n   * @param rsPorts          Ports that RegionServer should use.\n   * @param masterClass      The class to use as HMaster, or null for default.\n   * @param rsClass          The class to use as HRegionServer, or null for default.\n   * @param createRootDir    Whether to create a new root or data directory path.\n   * @param createWALDir     Whether to create a new WAL directory.\n   * @return The mini HBase cluster created.\n   * @see #shutdownMiniHBaseCluster()\n   * @deprecated since 2.2.0 and will be removed in 4.0.0. Use\n   *             {@link #startMiniHBaseCluster(StartTestingClusterOption)} instead.\n   * @see #startMiniHBaseCluster(StartTestingClusterOption)\n   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-21071\">HBASE-21071</a>\n   */\n  @Deprecated\n  public SingleProcessHBaseCluster startMiniHBaseCluster(int numMasters, int numRegionServers,\n    List<Integer> rsPorts, Class<? extends HMaster> masterClass,\n    Class<? extends SingleProcessHBaseCluster.MiniHBaseClusterRegionServer> rsClass,\n    boolean createRootDir, boolean createWALDir) throws IOException, InterruptedException {\n    StartTestingClusterOption option = StartTestingClusterOption.builder().numMasters(numMasters)\n      .masterClass(masterClass).numRegionServers(numRegionServers).rsClass(rsClass).rsPorts(rsPorts)\n      .createRootDir(createRootDir).createWALDir(createWALDir).build();\n    return startMiniHBaseCluster(option);\n  }\n\n  /**\n   * Starts the hbase cluster up again after shutting it down previously in a test. Use this if you\n   * want to keep dfs/zk up and just stop/start hbase.\n   * @param servers number of region servers\n   */\n  public void restartHBaseCluster(int servers) throws IOException, InterruptedException {\n    this.restartHBaseCluster(servers, null);\n  }\n\n  public void restartHBaseCluster(int servers, List<Integer> ports)\n    throws IOException, InterruptedException {\n    StartTestingClusterOption option =\n      StartTestingClusterOption.builder().numRegionServers(servers).rsPorts(ports).build();\n    restartHBaseCluster(option);\n    invalidateConnection();\n  }\n\n  public void restartHBaseCluster(StartTestingClusterOption option)\n    throws IOException, InterruptedException {\n    closeConnection();\n    this.hbaseCluster = new SingleProcessHBaseCluster(this.conf, option.getNumMasters(),\n      option.getNumAlwaysStandByMasters(), option.getNumRegionServers(), option.getRsPorts(),\n      option.getMasterClass(), option.getRsClass());\n    // Don't leave here till we've done a successful scan of the hbase:meta\n    Connection conn = ConnectionFactory.createConnection(this.conf);\n    Table t = conn.getTable(TableName.META_TABLE_NAME);\n    ResultScanner s = t.getScanner(new Scan());\n    while (s.next() != null) {\n      // do nothing\n    }\n    LOG.info(\"HBase has been restarted\");\n    s.close();\n    t.close();\n    conn.close();\n  }\n\n  /**\n   * Returns current mini hbase cluster. Only has something in it after a call to\n   * {@link #startMiniCluster()}.\n   * @see #startMiniCluster()\n   */\n  public SingleProcessHBaseCluster getMiniHBaseCluster() {\n    if (this.hbaseCluster == null || this.hbaseCluster instanceof SingleProcessHBaseCluster) {\n      return (SingleProcessHBaseCluster) this.hbaseCluster;\n    }\n    throw new RuntimeException(\n      hbaseCluster + \" not an instance of \" + SingleProcessHBaseCluster.class.getName());\n  }\n\n  /**\n   * Stops mini hbase, zk, and hdfs clusters.\n   * @see #startMiniCluster(int)\n   */\n  public void shutdownMiniCluster() throws IOException {\n    LOG.info(\"Shutting down minicluster\");\n    shutdownMiniHBaseCluster();\n    shutdownMiniDFSCluster();\n    shutdownMiniZKCluster();\n\n    cleanupTestDir();\n    miniClusterRunning = false;\n    LOG.info(\"Minicluster is down\");\n  }\n\n  /**\n   * Shutdown HBase mini cluster.Does not shutdown zk or dfs if running.\n   * @throws java.io.IOException in case command is unsuccessful\n   */\n  public void shutdownMiniHBaseCluster() throws IOException {\n    cleanup();\n    if (this.hbaseCluster != null) {\n      this.hbaseCluster.shutdown();\n      // Wait till hbase is down before going on to shutdown zk.\n      this.hbaseCluster.waitUntilShutDown();\n      this.hbaseCluster = null;\n    }\n    if (zooKeeperWatcher != null) {\n      zooKeeperWatcher.close();\n      zooKeeperWatcher = null;\n    }\n  }\n\n  /**\n   * Abruptly Shutdown HBase mini cluster. Does not shutdown zk or dfs if running.\n   * @throws java.io.IOException throws in case command is unsuccessful\n   */\n  public void killMiniHBaseCluster() throws IOException {\n    cleanup();\n    if (this.hbaseCluster != null) {\n      getMiniHBaseCluster().killAll();\n      this.hbaseCluster = null;\n    }\n    if (zooKeeperWatcher != null) {\n      zooKeeperWatcher.close();\n      zooKeeperWatcher = null;\n    }\n  }\n\n  // close hbase admin, close current connection and reset MIN MAX configs for RS.\n  private void cleanup() throws IOException {\n    closeConnection();\n    // unset the configuration for MIN and MAX RS to start\n    conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, -1);\n    conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MAXTOSTART, -1);\n  }\n\n  /**\n   * Returns the path to the default root dir the minicluster uses. If <code>create</code> is true,\n   * a new root directory path is fetched irrespective of whether it has been fetched before or not.\n   * If false, previous path is used. Note: this does not cause the root dir to be created.\n   * @return Fully qualified path for the default hbase root dir\n   */\n  public Path getDefaultRootDirPath(boolean create) throws IOException {\n    if (!create) {\n      return getDataTestDirOnTestFS();\n    } else {\n      return getNewDataTestDirOnTestFS();\n    }\n  }\n\n  /**\n   * Same as {{@link HBaseTestingUtil#getDefaultRootDirPath(boolean create)} except that\n   * <code>create</code> flag is false. Note: this does not cause the root dir to be created.\n   * @return Fully qualified path for the default hbase root dir\n   */\n  public Path getDefaultRootDirPath() throws IOException {\n    return getDefaultRootDirPath(false);\n  }\n\n  /**\n   * Creates an hbase rootdir in user home directory. Also creates hbase version file. Normally you\n   * won't make use of this method. Root hbasedir is created for you as part of mini cluster\n   * startup. You'd only use this method if you were doing manual operation.\n   * @param create This flag decides whether to get a new root or data directory path or not, if it\n   *               has been fetched already. Note : Directory will be made irrespective of whether\n   *               path has been fetched or not. If directory already exists, it will be overwritten\n   * @return Fully qualified path to hbase root dir\n   */\n  public Path createRootDir(boolean create) throws IOException {\n    FileSystem fs = FileSystem.get(this.conf);\n    Path hbaseRootdir = getDefaultRootDirPath(create);\n    CommonFSUtils.setRootDir(this.conf, hbaseRootdir);\n    fs.mkdirs(hbaseRootdir);\n    FSUtils.setVersion(fs, hbaseRootdir);\n    return hbaseRootdir;\n  }\n\n  /**\n   * Same as {@link HBaseTestingUtil#createRootDir(boolean create)} except that <code>create</code>\n   * flag is false.\n   * @return Fully qualified path to hbase root dir\n   */\n  public Path createRootDir() throws IOException {\n    return createRootDir(false);\n  }\n\n  /**\n   * Creates a hbase walDir in the user's home directory. Normally you won't make use of this\n   * method. Root hbaseWALDir is created for you as part of mini cluster startup. You'd only use\n   * this method if you were doing manual operation.\n   * @return Fully qualified path to hbase root dir\n   */\n  public Path createWALRootDir() throws IOException {\n    FileSystem fs = FileSystem.get(this.conf);\n    Path walDir = getNewDataTestDirOnTestFS();\n    CommonFSUtils.setWALRootDir(this.conf, walDir);\n    fs.mkdirs(walDir);\n    return walDir;\n  }\n\n  private void setHBaseFsTmpDir() throws IOException {\n    String hbaseFsTmpDirInString = this.conf.get(\"hbase.fs.tmp.dir\");\n    if (hbaseFsTmpDirInString == null) {\n      this.conf.set(\"hbase.fs.tmp.dir\", getDataTestDirOnTestFS(\"hbase-staging\").toString());\n      LOG.info(\"Setting hbase.fs.tmp.dir to \" + this.conf.get(\"hbase.fs.tmp.dir\"));\n    } else {\n      LOG.info(\"The hbase.fs.tmp.dir is set to \" + hbaseFsTmpDirInString);\n    }\n  }\n\n  /**\n   * Flushes all caches in the mini hbase cluster\n   */\n  public void flush() throws IOException {\n    getMiniHBaseCluster().flushcache();\n  }\n\n  /**\n   * Flushes all caches in the mini hbase cluster\n   */\n  public void flush(TableName tableName) throws IOException {\n    getMiniHBaseCluster().flushcache(tableName);\n  }\n\n  /**\n   * Compact all regions in the mini hbase cluster\n   */\n  public void compact(boolean major) throws IOException {\n    getMiniHBaseCluster().compact(major);\n  }\n\n  /**\n   * Compact all of a table's reagion in the mini hbase cluster\n   */\n  public void compact(TableName tableName, boolean major) throws IOException {\n    getMiniHBaseCluster().compact(tableName, major);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, String family) throws IOException {\n    return createTable(tableName, new String[] { family });\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, String[] families) throws IOException {\n    List<byte[]> fams = new ArrayList<>(families.length);\n    for (String family : families) {\n      fams.add(Bytes.toBytes(family));\n    }\n    return createTable(tableName, fams.toArray(new byte[0][]));\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[] family) throws IOException {\n    return createTable(tableName, new byte[][] { family });\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, byte[] family, int numRegions)\n    throws IOException {\n    if (numRegions < 3) throw new IOException(\"Must create at least 3 regions\");\n    byte[] startKey = Bytes.toBytes(\"aaaaa\");\n    byte[] endKey = Bytes.toBytes(\"zzzzz\");\n    byte[][] splitKeys = Bytes.split(startKey, endKey, numRegions - 3);\n\n    return createTable(tableName, new byte[][] { family }, splitKeys);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families) throws IOException {\n    return createTable(tableName, families, (byte[][]) null);\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, byte[][] families) throws IOException {\n    return createTable(tableName, families, KEYS_FOR_HBA_CREATE_TABLE);\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @param replicaCount replica count.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, int replicaCount, byte[][] families)\n    throws IOException {\n    return createTable(tableName, families, KEYS_FOR_HBA_CREATE_TABLE, replicaCount);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, byte[][] splitKeys)\n    throws IOException {\n    return createTable(tableName, families, splitKeys, 1, new Configuration(getConfiguration()));\n  }\n\n  /**\n   * Create a table.\n   * @param tableName    the table name\n   * @param families     the families\n   * @param splitKeys    the splitkeys\n   * @param replicaCount the region replica count\n   * @return A Table instance for the created table.\n   * @throws IOException throws IOException\n   */\n  public Table createTable(TableName tableName, byte[][] families, byte[][] splitKeys,\n    int replicaCount) throws IOException {\n    return createTable(tableName, families, splitKeys, replicaCount,\n      new Configuration(getConfiguration()));\n  }\n\n  public Table createTable(TableName tableName, byte[][] families, int numVersions, byte[] startKey,\n    byte[] endKey, int numRegions) throws IOException {\n    TableDescriptor desc = createTableDescriptor(tableName, families, numVersions);\n\n    getAdmin().createTable(desc, startKey, endKey, numRegions);\n    // HBaseAdmin only waits for regions to appear in hbase:meta we\n    // should wait until they are assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table.\n   * @param c Configuration to use\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableDescriptor htd, byte[][] families, Configuration c)\n    throws IOException {\n    return createTable(htd, families, null, c);\n  }\n\n  /**\n   * Create a table.\n   * @param htd       table descriptor\n   * @param families  array of column families\n   * @param splitKeys array of split keys\n   * @param c         Configuration to use\n   * @return A Table instance for the created table.\n   * @throws IOException if getAdmin or createTable fails\n   */\n  public Table createTable(TableDescriptor htd, byte[][] families, byte[][] splitKeys,\n    Configuration c) throws IOException {\n    // Disable blooms (they are on by default as of 0.95) but we disable them here because\n    // tests have hard coded counts of what to expect in block cache, etc., and blooms being\n    // on is interfering.\n    return createTable(htd, families, splitKeys, BloomType.NONE, HConstants.DEFAULT_BLOCKSIZE, c);\n  }\n\n  /**\n   * Create a table.\n   * @param htd       table descriptor\n   * @param families  array of column families\n   * @param splitKeys array of split keys\n   * @param type      Bloom type\n   * @param blockSize block size\n   * @param c         Configuration to use\n   * @return A Table instance for the created table.\n   * @throws IOException if getAdmin or createTable fails\n   */\n\n  public Table createTable(TableDescriptor htd, byte[][] families, byte[][] splitKeys,\n    BloomType type, int blockSize, Configuration c) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(htd);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfdb = ColumnFamilyDescriptorBuilder.newBuilder(family)\n        .setBloomFilterType(type).setBlocksize(blockSize);\n      if (isNewVersionBehaviorEnabled()) {\n        cfdb.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfdb.build());\n    }\n    TableDescriptor td = builder.build();\n    if (splitKeys != null) {\n      getAdmin().createTable(td, splitKeys);\n    } else {\n      getAdmin().createTable(td);\n    }\n    // HBaseAdmin only waits for regions to appear in hbase:meta\n    // we should wait until they are assigned\n    waitUntilAllRegionsAssigned(td.getTableName());\n    return getConnection().getTable(td.getTableName());\n  }\n\n  /**\n   * Create a table.\n   * @param htd       table descriptor\n   * @param splitRows array of split keys\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableDescriptor htd, byte[][] splitRows) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(htd);\n    if (isNewVersionBehaviorEnabled()) {\n      for (ColumnFamilyDescriptor family : htd.getColumnFamilies()) {\n        builder.setColumnFamily(\n          ColumnFamilyDescriptorBuilder.newBuilder(family).setNewVersionBehavior(true).build());\n      }\n    }\n    if (splitRows != null) {\n      getAdmin().createTable(builder.build(), splitRows);\n    } else {\n      getAdmin().createTable(builder.build());\n    }\n    // HBaseAdmin only waits for regions to appear in hbase:meta\n    // we should wait until they are assigned\n    waitUntilAllRegionsAssigned(htd.getTableName());\n    return getConnection().getTable(htd.getTableName());\n  }\n\n  /**\n   * Create a table.\n   * @param tableName    the table name\n   * @param families     the families\n   * @param splitKeys    the split keys\n   * @param replicaCount the replica count\n   * @param c            Configuration to use\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, byte[][] splitKeys,\n    int replicaCount, final Configuration c) throws IOException {\n    TableDescriptor htd =\n      TableDescriptorBuilder.newBuilder(tableName).setRegionReplication(replicaCount).build();\n    return createTable(htd, families, splitKeys, c);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[] family, int numVersions) throws IOException {\n    return createTable(tableName, new byte[][] { family }, numVersions);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, int numVersions)\n    throws IOException {\n    return createTable(tableName, families, numVersions, (byte[][]) null);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, int numVersions,\n    byte[][] splitKeys) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder =\n        ColumnFamilyDescriptorBuilder.newBuilder(family).setMaxVersions(numVersions);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    if (splitKeys != null) {\n      getAdmin().createTable(builder.build(), splitKeys);\n    } else {\n      getAdmin().createTable(builder.build());\n    }\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, byte[][] families, int numVersions)\n    throws IOException {\n    return createTable(tableName, families, numVersions, KEYS_FOR_HBA_CREATE_TABLE);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, int numVersions, int blockSize)\n    throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(family)\n        .setMaxVersions(numVersions).setBlocksize(blockSize);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    getAdmin().createTable(builder.build());\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  public Table createTable(TableName tableName, byte[][] families, int numVersions, int blockSize,\n    String cpName) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(family)\n        .setMaxVersions(numVersions).setBlocksize(blockSize);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    if (cpName != null) {\n      builder.setCoprocessor(cpName);\n    }\n    getAdmin().createTable(builder.build());\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[][] families, int[] numVersions)\n    throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    int i = 0;\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder =\n        ColumnFamilyDescriptorBuilder.newBuilder(family).setMaxVersions(numVersions[i]);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n      i++;\n    }\n    getAdmin().createTable(builder.build());\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table.\n   * @return A Table instance for the created table.\n   */\n  public Table createTable(TableName tableName, byte[] family, byte[][] splitRows)\n    throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(family);\n    if (isNewVersionBehaviorEnabled()) {\n      cfBuilder.setNewVersionBehavior(true);\n    }\n    builder.setColumnFamily(cfBuilder.build());\n    getAdmin().createTable(builder.build(), splitRows);\n    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are\n    // assigned\n    waitUntilAllRegionsAssigned(tableName);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Create a table with multiple regions.\n   * @return A Table instance for the created table.\n   */\n  public Table createMultiRegionTable(TableName tableName, byte[] family) throws IOException {\n    return createTable(tableName, family, KEYS_FOR_HBA_CREATE_TABLE);\n  }\n\n  /**\n   * Set the number of Region replicas.\n   */\n  public static void setReplicas(Admin admin, TableName table, int replicaCount)\n    throws IOException, InterruptedException {\n    TableDescriptor desc = TableDescriptorBuilder.newBuilder(admin.getDescriptor(table))\n      .setRegionReplication(replicaCount).build();\n    admin.modifyTable(desc);\n  }\n\n  /**\n   * Set the number of Region replicas.\n   */\n  public static void setReplicas(AsyncAdmin admin, TableName table, int replicaCount)\n    throws ExecutionException, IOException, InterruptedException {\n    TableDescriptor desc = TableDescriptorBuilder.newBuilder(admin.getDescriptor(table).get())\n      .setRegionReplication(replicaCount).build();\n    admin.modifyTable(desc).get();\n  }\n\n  /**\n   * Drop an existing table\n   * @param tableName existing table\n   */\n  public void deleteTable(TableName tableName) throws IOException {\n    try {\n      getAdmin().disableTable(tableName);\n    } catch (TableNotEnabledException e) {\n      LOG.debug(\"Table: \" + tableName + \" already disabled, so just deleting it.\");\n    }\n    getAdmin().deleteTable(tableName);\n  }\n\n  /**\n   * Drop an existing table\n   * @param tableName existing table\n   */\n  public void deleteTableIfAny(TableName tableName) throws IOException {\n    try {\n      deleteTable(tableName);\n    } catch (TableNotFoundException e) {\n      // ignore\n    }\n  }\n\n  // ==========================================================================\n  // Canned table and table descriptor creation\n\n  public final static byte[] fam1 = Bytes.toBytes(\"colfamily11\");\n  public final static byte[] fam2 = Bytes.toBytes(\"colfamily21\");\n  public final static byte[] fam3 = Bytes.toBytes(\"colfamily31\");\n  public static final byte[][] COLUMNS = { fam1, fam2, fam3 };\n  private static final int MAXVERSIONS = 3;\n\n  public static final char FIRST_CHAR = 'a';\n  public static final char LAST_CHAR = 'z';\n  public static final byte[] START_KEY_BYTES = { FIRST_CHAR, FIRST_CHAR, FIRST_CHAR };\n  public static final String START_KEY = new String(START_KEY_BYTES, HConstants.UTF8_CHARSET);\n\n  public TableDescriptorBuilder createModifyableTableDescriptor(final String name) {\n    return createModifyableTableDescriptor(TableName.valueOf(name),\n      ColumnFamilyDescriptorBuilder.DEFAULT_MIN_VERSIONS, MAXVERSIONS, HConstants.FOREVER,\n      ColumnFamilyDescriptorBuilder.DEFAULT_KEEP_DELETED);\n  }\n\n  public TableDescriptor createTableDescriptor(final TableName name, final int minVersions,\n    final int versions, final int ttl, KeepDeletedCells keepDeleted) {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(name);\n    for (byte[] cfName : new byte[][] { fam1, fam2, fam3 }) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(cfName)\n        .setMinVersions(minVersions).setMaxVersions(versions).setKeepDeletedCells(keepDeleted)\n        .setBlockCacheEnabled(false).setTimeToLive(ttl);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    return builder.build();\n  }\n\n  public TableDescriptorBuilder createModifyableTableDescriptor(final TableName name,\n    final int minVersions, final int versions, final int ttl, KeepDeletedCells keepDeleted) {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(name);\n    for (byte[] cfName : new byte[][] { fam1, fam2, fam3 }) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(cfName)\n        .setMinVersions(minVersions).setMaxVersions(versions).setKeepDeletedCells(keepDeleted)\n        .setBlockCacheEnabled(false).setTimeToLive(ttl);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    return builder;\n  }\n\n  /**\n   * Create a table of name <code>name</code>.\n   * @param name Name to give table.\n   * @return Column descriptor.\n   */\n  public TableDescriptor createTableDescriptor(final TableName name) {\n    return createTableDescriptor(name, ColumnFamilyDescriptorBuilder.DEFAULT_MIN_VERSIONS,\n      MAXVERSIONS, HConstants.FOREVER, ColumnFamilyDescriptorBuilder.DEFAULT_KEEP_DELETED);\n  }\n\n  public TableDescriptor createTableDescriptor(final TableName tableName, byte[] family) {\n    return createTableDescriptor(tableName, new byte[][] { family }, 1);\n  }\n\n  public TableDescriptor createTableDescriptor(final TableName tableName, byte[][] families,\n    int maxVersions) {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder =\n        ColumnFamilyDescriptorBuilder.newBuilder(family).setMaxVersions(maxVersions);\n      if (isNewVersionBehaviorEnabled()) {\n        cfBuilder.setNewVersionBehavior(true);\n      }\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    return builder.build();\n  }\n\n  /**\n   * Create an HRegion that writes to the local tmp dirs\n   * @param desc     a table descriptor indicating which table the region belongs to\n   * @param startKey the start boundary of the region\n   * @param endKey   the end boundary of the region\n   * @return a region that writes to local dir for testing\n   */\n  public HRegion createLocalHRegion(TableDescriptor desc, byte[] startKey, byte[] endKey)\n    throws IOException {\n    RegionInfo hri = RegionInfoBuilder.newBuilder(desc.getTableName()).setStartKey(startKey)\n      .setEndKey(endKey).build();\n    return createLocalHRegion(hri, desc);\n  }\n\n  /**\n   * Create an HRegion that writes to the local tmp dirs. Creates the WAL for you. Be sure to call\n   * {@link HBaseTestingUtil#closeRegionAndWAL(HRegion)} when you're finished with it.\n   */\n  public HRegion createLocalHRegion(RegionInfo info, TableDescriptor desc) throws IOException {\n    return createRegionAndWAL(info, getDataTestDir(), getConfiguration(), desc);\n  }\n\n  /**\n   * Create an HRegion that writes to the local tmp dirs with specified wal\n   * @param info regioninfo\n   * @param conf configuration\n   * @param desc table descriptor\n   * @param wal  wal for this region.\n   * @return created hregion\n   */\n  public HRegion createLocalHRegion(RegionInfo info, Configuration conf, TableDescriptor desc,\n    WAL wal) throws IOException {\n    return HRegion.createHRegion(info, getDataTestDir(), conf, desc, wal);\n  }\n\n  /**\n   * @return A region on which you must call {@link HBaseTestingUtil#closeRegionAndWAL(HRegion)}\n   *         when done.\n   */\n  public HRegion createLocalHRegion(TableName tableName, byte[] startKey, byte[] stopKey,\n    Configuration conf, boolean isReadOnly, Durability durability, WAL wal, byte[]... families)\n    throws IOException {\n    return createLocalHRegionWithInMemoryFlags(tableName, startKey, stopKey, conf, isReadOnly,\n      durability, wal, null, families);\n  }\n\n  public HRegion createLocalHRegionWithInMemoryFlags(TableName tableName, byte[] startKey,\n    byte[] stopKey, Configuration conf, boolean isReadOnly, Durability durability, WAL wal,\n    boolean[] compactedMemStore, byte[]... families) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    builder.setReadOnly(isReadOnly);\n    int i = 0;\n    for (byte[] family : families) {\n      ColumnFamilyDescriptorBuilder cfBuilder = ColumnFamilyDescriptorBuilder.newBuilder(family);\n      if (compactedMemStore != null && i < compactedMemStore.length) {\n        cfBuilder.setInMemoryCompaction(MemoryCompactionPolicy.BASIC);\n      } else {\n        cfBuilder.setInMemoryCompaction(MemoryCompactionPolicy.NONE);\n\n      }\n      i++;\n      // Set default to be three versions.\n      cfBuilder.setMaxVersions(Integer.MAX_VALUE);\n      builder.setColumnFamily(cfBuilder.build());\n    }\n    builder.setDurability(durability);\n    RegionInfo info =\n      RegionInfoBuilder.newBuilder(tableName).setStartKey(startKey).setEndKey(stopKey).build();\n    return createLocalHRegion(info, conf, builder.build(), wal);\n  }\n\n  //\n  // ==========================================================================\n\n  /**\n   * Provide an existing table name to truncate. Scans the table and issues a delete for each row\n   * read.\n   * @param tableName existing table\n   * @return HTable to that new table\n   */\n  public Table deleteTableData(TableName tableName) throws IOException {\n    Table table = getConnection().getTable(tableName);\n    Scan scan = new Scan();\n    ResultScanner resScan = table.getScanner(scan);\n    for (Result res : resScan) {\n      Delete del = new Delete(res.getRow());\n      table.delete(del);\n    }\n    resScan = table.getScanner(scan);\n    resScan.close();\n    return table;\n  }\n\n  /**\n   * Truncate a table using the admin command. Effectively disables, deletes, and recreates the\n   * table.\n   * @param tableName       table which must exist.\n   * @param preserveRegions keep the existing split points\n   * @return HTable for the new table\n   */\n  public Table truncateTable(final TableName tableName, final boolean preserveRegions)\n    throws IOException {\n    Admin admin = getAdmin();\n    if (!admin.isTableDisabled(tableName)) {\n      admin.disableTable(tableName);\n    }\n    admin.truncateTable(tableName, preserveRegions);\n    return getConnection().getTable(tableName);\n  }\n\n  /**\n   * Truncate a table using the admin command. Effectively disables, deletes, and recreates the\n   * table. For previous behavior of issuing row deletes, see deleteTableData. Expressly does not\n   * preserve regions of existing table.\n   * @param tableName table which must exist.\n   * @return HTable for the new table\n   */\n  public Table truncateTable(final TableName tableName) throws IOException {\n    return truncateTable(tableName, false);\n  }\n\n  /**\n   * Load table with rows from 'aaa' to 'zzz'.\n   * @param t Table\n   * @param f Family\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[] f) throws IOException {\n    return loadTable(t, new byte[][] { f });\n  }\n\n  /**\n   * Load table with rows from 'aaa' to 'zzz'.\n   * @param t Table\n   * @param f Family\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[] f, boolean writeToWAL) throws IOException {\n    return loadTable(t, new byte[][] { f }, null, writeToWAL);\n  }\n\n  /**\n   * Load table of multiple column families with rows from 'aaa' to 'zzz'.\n   * @param t Table\n   * @param f Array of Families to load\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[][] f) throws IOException {\n    return loadTable(t, f, null);\n  }\n\n  /**\n   * Load table of multiple column families with rows from 'aaa' to 'zzz'.\n   * @param t     Table\n   * @param f     Array of Families to load\n   * @param value the values of the cells. If null is passed, the row key is used as value\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[][] f, byte[] value) throws IOException {\n    return loadTable(t, f, value, true);\n  }\n\n  /**\n   * Load table of multiple column families with rows from 'aaa' to 'zzz'.\n   * @param t     Table\n   * @param f     Array of Families to load\n   * @param value the values of the cells. If null is passed, the row key is used as value\n   * @return Count of rows loaded.\n   */\n  public int loadTable(final Table t, final byte[][] f, byte[] value, boolean writeToWAL)\n    throws IOException {\n    List<Put> puts = new ArrayList<>();\n    for (byte[] row : HBaseTestingUtil.ROWS) {\n      Put put = new Put(row);\n      put.setDurability(writeToWAL ? Durability.USE_DEFAULT : Durability.SKIP_WAL);\n      for (int i = 0; i < f.length; i++) {\n        byte[] value1 = value != null ? value : row;\n        put.addColumn(f[i], f[i], value1);\n      }\n      puts.add(put);\n    }\n    t.put(puts);\n    return puts.size();\n  }\n\n  /**\n   * A tracker for tracking and validating table rows generated with\n   * {@link HBaseTestingUtil#loadTable(Table, byte[])}\n   */\n  public static class SeenRowTracker {\n    int dim = 'z' - 'a' + 1;\n    int[][][] seenRows = new int[dim][dim][dim]; // count of how many times the row is seen\n    byte[] startRow;\n    byte[] stopRow;\n\n    public SeenRowTracker(byte[] startRow, byte[] stopRow) {\n      this.startRow = startRow;\n      this.stopRow = stopRow;\n    }\n\n    void reset() {\n      for (byte[] row : ROWS) {\n        seenRows[i(row[0])][i(row[1])][i(row[2])] = 0;\n      }\n    }\n\n    int i(byte b) {\n      return b - 'a';\n    }\n\n    public void addRow(byte[] row) {\n      seenRows[i(row[0])][i(row[1])][i(row[2])]++;\n    }\n\n    /**\n     * Validate that all the rows between startRow and stopRow are seen exactly once, and all other\n     * rows none\n     */\n    public void validate() {\n      for (byte b1 = 'a'; b1 <= 'z'; b1++) {\n        for (byte b2 = 'a'; b2 <= 'z'; b2++) {\n          for (byte b3 = 'a'; b3 <= 'z'; b3++) {\n            int count = seenRows[i(b1)][i(b2)][i(b3)];\n            int expectedCount = 0;\n            if (\n              Bytes.compareTo(new byte[] { b1, b2, b3 }, startRow) >= 0\n                && Bytes.compareTo(new byte[] { b1, b2, b3 }, stopRow) < 0\n            ) {\n              expectedCount = 1;\n            }\n            if (count != expectedCount) {\n              String row = new String(new byte[] { b1, b2, b3 }, StandardCharsets.UTF_8);\n              throw new RuntimeException(\"Row:\" + row + \" has a seen count of \" + count + \" \"\n                + \"instead of \" + expectedCount);\n            }\n          }\n        }\n      }\n    }\n  }\n\n  public int loadRegion(final HRegion r, final byte[] f) throws IOException {\n    return loadRegion(r, f, false);\n  }\n\n  public int loadRegion(final Region r, final byte[] f) throws IOException {\n    return loadRegion((HRegion) r, f);\n  }\n\n  /**\n   * Load region with rows from 'aaa' to 'zzz'.\n   * @param r     Region\n   * @param f     Family\n   * @param flush flush the cache if true\n   * @return Count of rows loaded.\n   */\n  public int loadRegion(final HRegion r, final byte[] f, final boolean flush) throws IOException {\n    byte[] k = new byte[3];\n    int rowCount = 0;\n    for (byte b1 = 'a'; b1 <= 'z'; b1++) {\n      for (byte b2 = 'a'; b2 <= 'z'; b2++) {\n        for (byte b3 = 'a'; b3 <= 'z'; b3++) {\n          k[0] = b1;\n          k[1] = b2;\n          k[2] = b3;\n          Put put = new Put(k);\n          put.setDurability(Durability.SKIP_WAL);\n          put.addColumn(f, null, k);\n          if (r.getWAL() == null) {\n            put.setDurability(Durability.SKIP_WAL);\n          }\n          int preRowCount = rowCount;\n          int pause = 10;\n          int maxPause = 1000;\n          while (rowCount == preRowCount) {\n            try {\n              r.put(put);\n              rowCount++;\n            } catch (RegionTooBusyException e) {\n              pause = (pause * 2 >= maxPause) ? maxPause : pause * 2;\n              Threads.sleep(pause);\n            }\n          }\n        }\n      }\n      if (flush) {\n        r.flush(true);\n      }\n    }\n    return rowCount;\n  }\n\n  public void loadNumericRows(final Table t, final byte[] f, int startRow, int endRow)\n    throws IOException {\n    for (int i = startRow; i < endRow; i++) {\n      byte[] data = Bytes.toBytes(String.valueOf(i));\n      Put put = new Put(data);\n      put.addColumn(f, null, data);\n      t.put(put);\n    }\n  }\n\n  public void loadRandomRows(final Table t, final byte[] f, int rowSize, int totalRows)\n    throws IOException {\n    for (int i = 0; i < totalRows; i++) {\n      byte[] row = new byte[rowSize];\n      Bytes.random(row);\n      Put put = new Put(row);\n      put.addColumn(f, new byte[] { 0 }, new byte[] { 0 });\n      t.put(put);\n    }\n  }\n\n  public void verifyNumericRows(Table table, final byte[] f, int startRow, int endRow,\n    int replicaId) throws IOException {\n    for (int i = startRow; i < endRow; i++) {\n      String failMsg = \"Failed verification of row :\" + i;\n      byte[] data = Bytes.toBytes(String.valueOf(i));\n      Get get = new Get(data);\n      get.setReplicaId(replicaId);\n      get.setConsistency(Consistency.TIMELINE);\n      Result result = table.get(get);\n      assertTrue(failMsg, result.containsColumn(f, null));\n      assertEquals(failMsg, 1, result.getColumnCells(f, null).size());\n      Cell cell = result.getColumnLatestCell(f, null);\n      assertTrue(failMsg, Bytes.equals(data, 0, data.length, cell.getValueArray(),\n        cell.getValueOffset(), cell.getValueLength()));\n    }\n  }\n\n  public void verifyNumericRows(Region region, final byte[] f, int startRow, int endRow)\n    throws IOException {\n    verifyNumericRows((HRegion) region, f, startRow, endRow);\n  }\n\n  public void verifyNumericRows(HRegion region, final byte[] f, int startRow, int endRow)\n    throws IOException {\n    verifyNumericRows(region, f, startRow, endRow, true);\n  }\n\n  public void verifyNumericRows(Region region, final byte[] f, int startRow, int endRow,\n    final boolean present) throws IOException {\n    verifyNumericRows((HRegion) region, f, startRow, endRow, present);\n  }\n\n  public void verifyNumericRows(HRegion region, final byte[] f, int startRow, int endRow,\n    final boolean present) throws IOException {\n    for (int i = startRow; i < endRow; i++) {\n      String failMsg = \"Failed verification of row :\" + i;\n      byte[] data = Bytes.toBytes(String.valueOf(i));\n      Result result = region.get(new Get(data));\n\n      boolean hasResult = result != null && !result.isEmpty();\n      assertEquals(failMsg + result, present, hasResult);\n      if (!present) continue;\n\n      assertTrue(failMsg, result.containsColumn(f, null));\n      assertEquals(failMsg, 1, result.getColumnCells(f, null).size());\n      Cell cell = result.getColumnLatestCell(f, null);\n      assertTrue(failMsg, Bytes.equals(data, 0, data.length, cell.getValueArray(),\n        cell.getValueOffset(), cell.getValueLength()));\n    }\n  }\n\n  public void deleteNumericRows(final Table t, final byte[] f, int startRow, int endRow)\n    throws IOException {\n    for (int i = startRow; i < endRow; i++) {\n      byte[] data = Bytes.toBytes(String.valueOf(i));\n      Delete delete = new Delete(data);\n      delete.addFamily(f);\n      t.delete(delete);\n    }\n  }\n\n  /**\n   * Return the number of rows in the given table.\n   * @param table to count rows\n   * @return count of rows\n   */\n  public static int countRows(final Table table) throws IOException {\n    return countRows(table, new Scan());\n  }\n\n  public static int countRows(final Table table, final Scan scan) throws IOException {\n    try (ResultScanner results = table.getScanner(scan)) {\n      int count = 0;\n      while (results.next() != null) {\n        count++;\n      }\n      return count;\n    }\n  }\n\n  public static int countRows(final Table table, final byte[]... families) throws IOException {\n    Scan scan = new Scan();\n    for (byte[] family : families) {\n      scan.addFamily(family);\n    }\n    return countRows(table, scan);\n  }\n\n  /**\n   * Return the number of rows in the given table.\n   */\n  public int countRows(final TableName tableName) throws IOException {\n    try (Table table = getConnection().getTable(tableName)) {\n      return countRows(table);\n    }\n  }\n\n  public static int countRows(final Region region) throws IOException {\n    return countRows(region, new Scan());\n  }\n\n  public static int countRows(final Region region, final Scan scan) throws IOException {\n    try (InternalScanner scanner = region.getScanner(scan)) {\n      return countRows(scanner);\n    }\n  }\n\n  public static int countRows(final InternalScanner scanner) throws IOException {\n    int scannedCount = 0;\n    List<Cell> results = new ArrayList<>();\n    boolean hasMore = true;\n    while (hasMore) {\n      hasMore = scanner.next(results);\n      scannedCount += results.size();\n      results.clear();\n    }\n    return scannedCount;\n  }\n\n  /**\n   * Return an md5 digest of the entire contents of a table.\n   */\n  public String checksumRows(final Table table) throws Exception {\n    MessageDigest digest = MessageDigest.getInstance(\"MD5\");\n    try (ResultScanner results = table.getScanner(new Scan())) {\n      for (Result res : results) {\n        digest.update(res.getRow());\n      }\n    }\n    return digest.toString();\n  }\n\n  /** All the row values for the data loaded by {@link #loadTable(Table, byte[])} */\n  public static final byte[][] ROWS = new byte[(int) Math.pow('z' - 'a' + 1, 3)][3]; // ~52KB\n  static {\n    int i = 0;\n    for (byte b1 = 'a'; b1 <= 'z'; b1++) {\n      for (byte b2 = 'a'; b2 <= 'z'; b2++) {\n        for (byte b3 = 'a'; b3 <= 'z'; b3++) {\n          ROWS[i][0] = b1;\n          ROWS[i][1] = b2;\n          ROWS[i][2] = b3;\n          i++;\n        }\n      }\n    }\n  }\n\n  public static final byte[][] KEYS = { HConstants.EMPTY_BYTE_ARRAY, Bytes.toBytes(\"bbb\"),\n    Bytes.toBytes(\"ccc\"), Bytes.toBytes(\"ddd\"), Bytes.toBytes(\"eee\"), Bytes.toBytes(\"fff\"),\n    Bytes.toBytes(\"ggg\"), Bytes.toBytes(\"hhh\"), Bytes.toBytes(\"iii\"), Bytes.toBytes(\"jjj\"),\n    Bytes.toBytes(\"kkk\"), Bytes.toBytes(\"lll\"), Bytes.toBytes(\"mmm\"), Bytes.toBytes(\"nnn\"),\n    Bytes.toBytes(\"ooo\"), Bytes.toBytes(\"ppp\"), Bytes.toBytes(\"qqq\"), Bytes.toBytes(\"rrr\"),\n    Bytes.toBytes(\"sss\"), Bytes.toBytes(\"ttt\"), Bytes.toBytes(\"uuu\"), Bytes.toBytes(\"vvv\"),\n    Bytes.toBytes(\"www\"), Bytes.toBytes(\"xxx\"), Bytes.toBytes(\"yyy\") };\n\n  public static final byte[][] KEYS_FOR_HBA_CREATE_TABLE = { Bytes.toBytes(\"bbb\"),\n    Bytes.toBytes(\"ccc\"), Bytes.toBytes(\"ddd\"), Bytes.toBytes(\"eee\"), Bytes.toBytes(\"fff\"),\n    Bytes.toBytes(\"ggg\"), Bytes.toBytes(\"hhh\"), Bytes.toBytes(\"iii\"), Bytes.toBytes(\"jjj\"),\n    Bytes.toBytes(\"kkk\"), Bytes.toBytes(\"lll\"), Bytes.toBytes(\"mmm\"), Bytes.toBytes(\"nnn\"),\n    Bytes.toBytes(\"ooo\"), Bytes.toBytes(\"ppp\"), Bytes.toBytes(\"qqq\"), Bytes.toBytes(\"rrr\"),\n    Bytes.toBytes(\"sss\"), Bytes.toBytes(\"ttt\"), Bytes.toBytes(\"uuu\"), Bytes.toBytes(\"vvv\"),\n    Bytes.toBytes(\"www\"), Bytes.toBytes(\"xxx\"), Bytes.toBytes(\"yyy\"), Bytes.toBytes(\"zzz\") };\n\n  /**\n   * Create rows in hbase:meta for regions of the specified table with the specified start keys. The\n   * first startKey should be a 0 length byte array if you want to form a proper range of regions.\n   * @return list of region info for regions added to meta\n   */\n  public List<RegionInfo> createMultiRegionsInMeta(final Configuration conf,\n    final TableDescriptor htd, byte[][] startKeys) throws IOException {\n    try (Table meta = getConnection().getTable(TableName.META_TABLE_NAME)) {\n      Arrays.sort(startKeys, Bytes.BYTES_COMPARATOR);\n      List<RegionInfo> newRegions = new ArrayList<>(startKeys.length);\n      MetaTableAccessor.updateTableState(getConnection(), htd.getTableName(),\n        TableState.State.ENABLED);\n      // add custom ones\n      for (int i = 0; i < startKeys.length; i++) {\n        int j = (i + 1) % startKeys.length;\n        RegionInfo hri = RegionInfoBuilder.newBuilder(htd.getTableName()).setStartKey(startKeys[i])\n          .setEndKey(startKeys[j]).build();\n        MetaTableAccessor.addRegionsToMeta(getConnection(), Collections.singletonList(hri), 1);\n        newRegions.add(hri);\n      }\n      return newRegions;\n    }\n  }\n\n  /**\n   * Create an unmanaged WAL. Be sure to close it when you're through.\n   */\n  public static WAL createWal(final Configuration conf, final Path rootDir, final RegionInfo hri)\n    throws IOException {\n    // The WAL subsystem will use the default rootDir rather than the passed in rootDir\n    // unless I pass along via the conf.\n    Configuration confForWAL = new Configuration(conf);\n    confForWAL.set(HConstants.HBASE_DIR, rootDir.toString());\n    return new WALFactory(confForWAL, \"hregion-\" + RandomStringUtils.randomNumeric(8)).getWAL(hri);\n  }\n\n  /**\n   * Create a region with it's own WAL. Be sure to call\n   * {@link HBaseTestingUtil#closeRegionAndWAL(HRegion)} to clean up all resources.\n   */\n  public static HRegion createRegionAndWAL(final RegionInfo info, final Path rootDir,\n    final Configuration conf, final TableDescriptor htd) throws IOException {\n    return createRegionAndWAL(info, rootDir, conf, htd, true);\n  }\n\n  /**\n   * Create a region with it's own WAL. Be sure to call\n   * {@link HBaseTestingUtil#closeRegionAndWAL(HRegion)} to clean up all resources.\n   */\n  public static HRegion createRegionAndWAL(final RegionInfo info, final Path rootDir,\n    final Configuration conf, final TableDescriptor htd, BlockCache blockCache) throws IOException {\n    HRegion region = createRegionAndWAL(info, rootDir, conf, htd, false);\n    region.setBlockCache(blockCache);\n    region.initialize();\n    return region;\n  }\n\n  /**\n   * Create a region with it's own WAL. Be sure to call\n   * {@link HBaseTestingUtil#closeRegionAndWAL(HRegion)} to clean up all resources.\n   */\n  public static HRegion createRegionAndWAL(final RegionInfo info, final Path rootDir,\n    final Configuration conf, final TableDescriptor htd, MobFileCache mobFileCache)\n    throws IOException {\n    HRegion region = createRegionAndWAL(info, rootDir, conf, htd, false);\n    region.setMobFileCache(mobFileCache);\n    region.initialize();\n    return region;\n  }\n\n  /**\n   * Create a region with it's own WAL. Be sure to call\n   * {@link HBaseTestingUtil#closeRegionAndWAL(HRegion)} to clean up all resources.\n   */\n  public static HRegion createRegionAndWAL(final RegionInfo info, final Path rootDir,\n    final Configuration conf, final TableDescriptor htd, boolean initialize) throws IOException {\n    ChunkCreator.initialize(MemStoreLAB.CHUNK_SIZE_DEFAULT, false, 0, 0, 0, null,\n      MemStoreLAB.INDEX_CHUNK_SIZE_PERCENTAGE_DEFAULT);\n    WAL wal = createWal(conf, rootDir, info);\n    return HRegion.createHRegion(info, rootDir, conf, htd, wal, initialize);\n  }\n\n  /**\n   * Find any other region server which is different from the one identified by parameter\n   * @return another region server\n   */\n  public HRegionServer getOtherRegionServer(HRegionServer rs) {\n    for (JVMClusterUtil.RegionServerThread rst : getMiniHBaseCluster().getRegionServerThreads()) {\n      if (!(rst.getRegionServer() == rs)) {\n        return rst.getRegionServer();\n      }\n    }\n    return null;\n  }\n\n  /**\n   * Tool to get the reference to the region server object that holds the region of the specified\n   * user table.\n   * @param tableName user table to lookup in hbase:meta\n   * @return region server that holds it, null if the row doesn't exist\n   */\n  public HRegionServer getRSForFirstRegionInTable(TableName tableName)\n    throws IOException, InterruptedException {\n    List<RegionInfo> regions = getAdmin().getRegions(tableName);\n    if (regions == null || regions.isEmpty()) {\n      return null;\n    }\n    LOG.debug(\"Found \" + regions.size() + \" regions for table \" + tableName);\n\n    byte[] firstRegionName =\n      regions.stream().filter(r -> !r.isOffline()).map(RegionInfo::getRegionName).findFirst()\n        .orElseThrow(() -> new IOException(\"online regions not found in table \" + tableName));\n\n    LOG.debug(\"firstRegionName=\" + Bytes.toString(firstRegionName));\n    long pause = getConfiguration().getLong(HConstants.HBASE_CLIENT_PAUSE,\n      HConstants.DEFAULT_HBASE_CLIENT_PAUSE);\n    int numRetries = getConfiguration().getInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER,\n      HConstants.DEFAULT_HBASE_CLIENT_RETRIES_NUMBER);\n    RetryCounter retrier = new RetryCounter(numRetries + 1, (int) pause, TimeUnit.MICROSECONDS);\n    while (retrier.shouldRetry()) {\n      int index = getMiniHBaseCluster().getServerWith(firstRegionName);\n      if (index != -1) {\n        return getMiniHBaseCluster().getRegionServerThreads().get(index).getRegionServer();\n      }\n      // Came back -1. Region may not be online yet. Sleep a while.\n      retrier.sleepUntilNextRetry();\n    }\n    return null;\n  }\n\n  /**\n   * Starts a <code>MiniMRCluster</code> with a default number of <code>TaskTracker</code>'s.\n   * @throws IOException When starting the cluster fails.\n   */\n  public MiniMRCluster startMiniMapReduceCluster() throws IOException {\n    // Set a very high max-disk-utilization percentage to avoid the NodeManagers from failing.\n    conf.setIfUnset(\"yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage\",\n      \"99.0\");\n    startMiniMapReduceCluster(2);\n    return mrCluster;\n  }\n\n  /**\n   * Tasktracker has a bug where changing the hadoop.log.dir system property will not change its\n   * internal static LOG_DIR variable.\n   */\n  private void forceChangeTaskLogDir() {\n    Field logDirField;\n    try {\n      logDirField = TaskLog.class.getDeclaredField(\"LOG_DIR\");\n      logDirField.setAccessible(true);\n\n      Field modifiersField = ReflectionUtils.getModifiersField();\n      modifiersField.setAccessible(true);\n      modifiersField.setInt(logDirField, logDirField.getModifiers() & ~Modifier.FINAL);\n\n      logDirField.set(null, new File(hadoopLogDir, \"userlogs\"));\n    } catch (SecurityException e) {\n      throw new RuntimeException(e);\n    } catch (NoSuchFieldException e) {\n      throw new RuntimeException(e);\n    } catch (IllegalArgumentException e) {\n      throw new RuntimeException(e);\n    } catch (IllegalAccessException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  /**\n   * Starts a <code>MiniMRCluster</code>. Call {@link #setFileSystemURI(String)} to use a different\n   * filesystem.\n   * @param servers The number of <code>TaskTracker</code>'s to start.\n   * @throws IOException When starting the cluster fails.\n   */\n  private void startMiniMapReduceCluster(final int servers) throws IOException {\n    if (mrCluster != null) {\n      throw new IllegalStateException(\"MiniMRCluster is already running\");\n    }\n    LOG.info(\"Starting mini mapreduce cluster...\");\n    setupClusterTestDir();\n    createDirsAndSetProperties();\n\n    forceChangeTaskLogDir();\n\n    //// hadoop2 specific settings\n    // Tests were failing because this process used 6GB of virtual memory and was getting killed.\n    // we up the VM usable so that processes don't get killed.\n    conf.setFloat(\"yarn.nodemanager.vmem-pmem-ratio\", 8.0f);\n\n    // Tests were failing due to MAPREDUCE-4880 / MAPREDUCE-4607 against hadoop 2.0.2-alpha and\n    // this avoids the problem by disabling speculative task execution in tests.\n    conf.setBoolean(\"mapreduce.map.speculative\", false);\n    conf.setBoolean(\"mapreduce.reduce.speculative\", false);\n    ////\n\n    // Yarn container runs in independent JVM. We need to pass the argument manually here if the\n    // JDK version >= 17. Otherwise, the MiniMRCluster will fail.\n    if (JVM.getJVMSpecVersion() >= 17) {\n      String jvmOpts = conf.get(\"yarn.app.mapreduce.am.command-opts\", \"\");\n      conf.set(\"yarn.app.mapreduce.am.command-opts\",\n        jvmOpts + \" --add-opens java.base/java.lang=ALL-UNNAMED\");\n    }\n\n    // Allow the user to override FS URI for this map-reduce cluster to use.\n    mrCluster =\n      new MiniMRCluster(servers, FS_URI != null ? FS_URI : FileSystem.get(conf).getUri().toString(),\n        1, null, null, new JobConf(this.conf));\n    JobConf jobConf = MapreduceTestingShim.getJobConf(mrCluster);\n    if (jobConf == null) {\n      jobConf = mrCluster.createJobConf();\n    }\n\n    // Hadoop MiniMR overwrites this while it should not\n    jobConf.set(\"mapreduce.cluster.local.dir\", conf.get(\"mapreduce.cluster.local.dir\"));\n    LOG.info(\"Mini mapreduce cluster started\");\n\n    // In hadoop2, YARN/MR2 starts a mini cluster with its own conf instance and updates settings.\n    // Our HBase MR jobs need several of these settings in order to properly run. So we copy the\n    // necessary config properties here. YARN-129 required adding a few properties.\n    conf.set(\"mapreduce.jobtracker.address\", jobConf.get(\"mapreduce.jobtracker.address\"));\n    // this for mrv2 support; mr1 ignores this\n    conf.set(\"mapreduce.framework.name\", \"yarn\");\n    conf.setBoolean(\"yarn.is.minicluster\", true);\n    String rmAddress = jobConf.get(\"yarn.resourcemanager.address\");\n    if (rmAddress != null) {\n      conf.set(\"yarn.resourcemanager.address\", rmAddress);\n    }\n    String historyAddress = jobConf.get(\"mapreduce.jobhistory.address\");\n    if (historyAddress != null) {\n      conf.set(\"mapreduce.jobhistory.address\", historyAddress);\n    }\n    String schedulerAddress = jobConf.get(\"yarn.resourcemanager.scheduler.address\");\n    if (schedulerAddress != null) {\n      conf.set(\"yarn.resourcemanager.scheduler.address\", schedulerAddress);\n    }\n    String mrJobHistoryWebappAddress = jobConf.get(\"mapreduce.jobhistory.webapp.address\");\n    if (mrJobHistoryWebappAddress != null) {\n      conf.set(\"mapreduce.jobhistory.webapp.address\", mrJobHistoryWebappAddress);\n    }\n    String yarnRMWebappAddress = jobConf.get(\"yarn.resourcemanager.webapp.address\");\n    if (yarnRMWebappAddress != null) {\n      conf.set(\"yarn.resourcemanager.webapp.address\", yarnRMWebappAddress);\n    }\n  }\n\n  /**\n   * Stops the previously started <code>MiniMRCluster</code>.\n   */\n  public void shutdownMiniMapReduceCluster() {\n    if (mrCluster != null) {\n      LOG.info(\"Stopping mini mapreduce cluster...\");\n      mrCluster.shutdown();\n      mrCluster = null;\n      LOG.info(\"Mini mapreduce cluster stopped\");\n    }\n    // Restore configuration to point to local jobtracker\n    conf.set(\"mapreduce.jobtracker.address\", \"local\");\n  }\n\n  /**\n   * Create a stubbed out RegionServerService, mainly for getting FS.\n   */\n  public RegionServerServices createMockRegionServerService() throws IOException {\n    return createMockRegionServerService((ServerName) null);\n  }\n\n  /**\n   * Create a stubbed out RegionServerService, mainly for getting FS. This version is used by\n   * TestTokenAuthentication\n   */\n  public RegionServerServices createMockRegionServerService(RpcServerInterface rpc)\n    throws IOException {\n    final MockRegionServerServices rss = new MockRegionServerServices(getZooKeeperWatcher());\n    rss.setFileSystem(getTestFileSystem());\n    rss.setRpcServer(rpc);\n    return rss;\n  }\n\n  /**\n   * Create a stubbed out RegionServerService, mainly for getting FS. This version is used by\n   * TestOpenRegionHandler\n   */\n  public RegionServerServices createMockRegionServerService(ServerName name) throws IOException {\n    final MockRegionServerServices rss = new MockRegionServerServices(getZooKeeperWatcher(), name);\n    rss.setFileSystem(getTestFileSystem());\n    return rss;\n  }\n\n  /**\n   * Expire the Master's session\n   */\n  public void expireMasterSession() throws Exception {\n    HMaster master = getMiniHBaseCluster().getMaster();\n    expireSession(master.getZooKeeper(), false);\n  }\n\n  /**\n   * Expire a region server's session\n   * @param index which RS\n   */\n  public void expireRegionServerSession(int index) throws Exception {\n    HRegionServer rs = getMiniHBaseCluster().getRegionServer(index);\n    expireSession(rs.getZooKeeper(), false);\n    decrementMinRegionServerCount();\n  }\n\n  private void decrementMinRegionServerCount() {\n    // decrement the count for this.conf, for newly spwaned master\n    // this.hbaseCluster shares this configuration too\n    decrementMinRegionServerCount(getConfiguration());\n\n    // each master thread keeps a copy of configuration\n    for (MasterThread master : getHBaseCluster().getMasterThreads()) {\n      decrementMinRegionServerCount(master.getMaster().getConfiguration());\n    }\n  }\n\n  private void decrementMinRegionServerCount(Configuration conf) {\n    int currentCount = conf.getInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, -1);\n    if (currentCount != -1) {\n      conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, Math.max(currentCount - 1, 1));\n    }\n  }\n\n  public void expireSession(ZKWatcher nodeZK) throws Exception {\n    expireSession(nodeZK, false);\n  }\n\n  /**\n   * Expire a ZooKeeper session as recommended in ZooKeeper documentation\n   * http://hbase.apache.org/book.html#trouble.zookeeper\n   * <p/>\n   * There are issues when doing this:\n   * <ol>\n   * <li>http://www.mail-archive.com/dev@zookeeper.apache.org/msg01942.html</li>\n   * <li>https://issues.apache.org/jira/browse/ZOOKEEPER-1105</li>\n   * </ol>\n   * @param nodeZK      - the ZK watcher to expire\n   * @param checkStatus - true to check if we can create a Table with the current configuration.\n   */\n  public void expireSession(ZKWatcher nodeZK, boolean checkStatus) throws Exception {\n    Configuration c = new Configuration(this.conf);\n    String quorumServers = ZKConfig.getZKQuorumServersString(c);\n    ZooKeeper zk = nodeZK.getRecoverableZooKeeper().getZooKeeper();\n    byte[] password = zk.getSessionPasswd();\n    long sessionID = zk.getSessionId();\n\n    // Expiry seems to be asynchronous (see comment from P. Hunt in [1]),\n    // so we create a first watcher to be sure that the\n    // event was sent. We expect that if our watcher receives the event\n    // other watchers on the same machine will get is as well.\n    // When we ask to close the connection, ZK does not close it before\n    // we receive all the events, so don't have to capture the event, just\n    // closing the connection should be enough.\n    ZooKeeper monitor = new ZooKeeper(quorumServers, 1000, new org.apache.zookeeper.Watcher() {\n      @Override\n      public void process(WatchedEvent watchedEvent) {\n        LOG.info(\"Monitor ZKW received event=\" + watchedEvent);\n      }\n    }, sessionID, password);\n\n    // Making it expire\n    ZooKeeper newZK =\n      new ZooKeeper(quorumServers, 1000, EmptyWatcher.instance, sessionID, password);\n\n    // ensure that we have connection to the server before closing down, otherwise\n    // the close session event will be eaten out before we start CONNECTING state\n    long start = EnvironmentEdgeManager.currentTime();\n    while (\n      newZK.getState() != States.CONNECTED && EnvironmentEdgeManager.currentTime() - start < 1000\n    ) {\n      Thread.sleep(1);\n    }\n    newZK.close();\n    LOG.info(\"ZK Closed Session 0x\" + Long.toHexString(sessionID));\n\n    // Now closing & waiting to be sure that the clients get it.\n    monitor.close();\n\n    if (checkStatus) {\n      getConnection().getTable(TableName.META_TABLE_NAME).close();\n    }\n  }\n\n  /**\n   * Get the Mini HBase cluster.\n   * @return hbase cluster\n   * @see #getHBaseClusterInterface()\n   */\n  public SingleProcessHBaseCluster getHBaseCluster() {\n    return getMiniHBaseCluster();\n  }\n\n  /**\n   * Returns the HBaseCluster instance.\n   * <p>\n   * Returned object can be any of the subclasses of HBaseCluster, and the tests referring this\n   * should not assume that the cluster is a mini cluster or a distributed one. If the test only\n   * works on a mini cluster, then specific method {@link #getMiniHBaseCluster()} can be used\n   * instead w/o the need to type-cast.\n   */\n  public HBaseClusterInterface getHBaseClusterInterface() {\n    // implementation note: we should rename this method as #getHBaseCluster(),\n    // but this would require refactoring 90+ calls.\n    return hbaseCluster;\n  }\n\n  /**\n   * Resets the connections so that the next time getConnection() is called, a new connection is\n   * created. This is needed in cases where the entire cluster / all the masters are shutdown and\n   * the connection is not valid anymore.\n   * <p/>\n   * TODO: There should be a more coherent way of doing this. Unfortunately the way tests are\n   * written, not all start() stop() calls go through this class. Most tests directly operate on the\n   * underlying mini/local hbase cluster. That makes it difficult for this wrapper class to maintain\n   * the connection state automatically. Cleaning this is a much bigger refactor.\n   */\n  public void invalidateConnection() throws IOException {\n    closeConnection();\n    // Update the master addresses if they changed.\n    final String masterConfigBefore = conf.get(HConstants.MASTER_ADDRS_KEY);\n    final String masterConfAfter = getMiniHBaseCluster().getConf().get(HConstants.MASTER_ADDRS_KEY);\n    LOG.info(\"Invalidated connection. Updating master addresses before: {} after: {}\",\n      masterConfigBefore, masterConfAfter);\n    conf.set(HConstants.MASTER_ADDRS_KEY,\n      getMiniHBaseCluster().getConf().get(HConstants.MASTER_ADDRS_KEY));\n  }\n\n  /**\n   * Get a shared Connection to the cluster. this method is thread safe.\n   * @return A Connection that can be shared. Don't close. Will be closed on shutdown of cluster.\n   */\n  public Connection getConnection() throws IOException {\n    return getAsyncConnection().toConnection();\n  }\n\n  /**\n   * Get a assigned Connection to the cluster. this method is thread safe.\n   * @param user assigned user\n   * @return A Connection with assigned user.\n   */\n  public Connection getConnection(User user) throws IOException {\n    return getAsyncConnection(user).toConnection();\n  }\n\n  /**\n   * Get a shared AsyncClusterConnection to the cluster. this method is thread safe.\n   * @return An AsyncClusterConnection that can be shared. Don't close. Will be closed on shutdown\n   *         of cluster.\n   */\n  public AsyncClusterConnection getAsyncConnection() throws IOException {\n    try {\n      return asyncConnection.updateAndGet(connection -> {\n        if (connection == null) {\n          try {\n            User user = UserProvider.instantiate(conf).getCurrent();\n            connection = getAsyncConnection(user);\n          } catch (IOException ioe) {\n            throw new UncheckedIOException(\"Failed to create connection\", ioe);\n          }\n        }\n        return connection;\n      });\n    } catch (UncheckedIOException exception) {\n      throw exception.getCause();\n    }\n  }\n\n  /**\n   * Get a assigned AsyncClusterConnection to the cluster. this method is thread safe.\n   * @param user assigned user\n   * @return An AsyncClusterConnection with assigned user.\n   */\n  public AsyncClusterConnection getAsyncConnection(User user) throws IOException {\n    return ClusterConnectionFactory.createAsyncClusterConnection(conf, null, user);\n  }\n\n  public void closeConnection() throws IOException {\n    if (hbaseAdmin != null) {\n      Closeables.close(hbaseAdmin, true);\n      hbaseAdmin = null;\n    }\n    AsyncClusterConnection asyncConnection = this.asyncConnection.getAndSet(null);\n    if (asyncConnection != null) {\n      Closeables.close(asyncConnection, true);\n    }\n  }\n\n  /**\n   * Returns an Admin instance which is shared between HBaseTestingUtility instance users. Closing\n   * it has no effect, it will be closed automatically when the cluster shutdowns\n   */\n  public Admin getAdmin() throws IOException {\n    if (hbaseAdmin == null) {\n      this.hbaseAdmin = getConnection().getAdmin();\n    }\n    return hbaseAdmin;\n  }\n\n  private Admin hbaseAdmin = null;\n\n  /**\n   * Returns an {@link Hbck} instance. Needs be closed when done.\n   */\n  public Hbck getHbck() throws IOException {\n    return getConnection().getHbck();\n  }\n\n  /**\n   * Unassign the named region.\n   * @param regionName The region to unassign.\n   */\n  public void unassignRegion(String regionName) throws IOException {\n    unassignRegion(Bytes.toBytes(regionName));\n  }\n\n  /**\n   * Unassign the named region.\n   * @param regionName The region to unassign.\n   */\n  public void unassignRegion(byte[] regionName) throws IOException {\n    getAdmin().unassign(regionName);\n  }\n\n  /**\n   * Closes the region containing the given row.\n   * @param row   The row to find the containing region.\n   * @param table The table to find the region.\n   */\n  public void unassignRegionByRow(String row, RegionLocator table) throws IOException {\n    unassignRegionByRow(Bytes.toBytes(row), table);\n  }\n\n  /**\n   * Closes the region containing the given row.\n   * @param row   The row to find the containing region.\n   * @param table The table to find the region.\n   */\n  public void unassignRegionByRow(byte[] row, RegionLocator table) throws IOException {\n    HRegionLocation hrl = table.getRegionLocation(row);\n    unassignRegion(hrl.getRegion().getRegionName());\n  }\n\n  /**\n   * Retrieves a splittable region randomly from tableName\n   * @param tableName   name of table\n   * @param maxAttempts maximum number of attempts, unlimited for value of -1\n   * @return the HRegion chosen, null if none was found within limit of maxAttempts\n   */\n  public HRegion getSplittableRegion(TableName tableName, int maxAttempts) {\n    List<HRegion> regions = getHBaseCluster().getRegions(tableName);\n    int regCount = regions.size();\n    Set<Integer> attempted = new HashSet<>();\n    int idx;\n    int attempts = 0;\n    do {\n      regions = getHBaseCluster().getRegions(tableName);\n      if (regCount != regions.size()) {\n        // if there was region movement, clear attempted Set\n        attempted.clear();\n      }\n      regCount = regions.size();\n      // There are chances that before we get the region for the table from an RS the region may\n      // be going for CLOSE. This may be because online schema change is enabled\n      if (regCount > 0) {\n        idx = ThreadLocalRandom.current().nextInt(regCount);\n        // if we have just tried this region, there is no need to try again\n        if (attempted.contains(idx)) {\n          continue;\n        }\n        HRegion region = regions.get(idx);\n        if (region.checkSplit().isPresent()) {\n          return region;\n        }\n        attempted.add(idx);\n      }\n      attempts++;\n    } while (maxAttempts == -1 || attempts < maxAttempts);\n    return null;\n  }\n\n  public MiniDFSCluster getDFSCluster() {\n    return dfsCluster;\n  }\n\n  public void setDFSCluster(MiniDFSCluster cluster) throws IllegalStateException, IOException {\n    setDFSCluster(cluster, true);\n  }\n\n  /**\n   * Set the MiniDFSCluster\n   * @param cluster     cluster to use\n   * @param requireDown require the that cluster not be \"up\" (MiniDFSCluster#isClusterUp) before it\n   *                    is set.\n   * @throws IllegalStateException if the passed cluster is up when it is required to be down\n   * @throws IOException           if the FileSystem could not be set from the passed dfs cluster\n   */\n  public void setDFSCluster(MiniDFSCluster cluster, boolean requireDown)\n    throws IllegalStateException, IOException {\n    if (dfsCluster != null && requireDown && dfsCluster.isClusterUp()) {\n      throw new IllegalStateException(\"DFSCluster is already running! Shut it down first.\");\n    }\n    this.dfsCluster = cluster;\n    this.setFs();\n  }\n\n  public FileSystem getTestFileSystem() throws IOException {\n    return HFileSystem.get(conf);\n  }\n\n  /**\n   * Wait until all regions in a table have been assigned. Waits default timeout before giving up\n   * (30 seconds).\n   * @param table Table to wait on.\n   */\n  public void waitTableAvailable(TableName table) throws InterruptedException, IOException {\n    waitTableAvailable(table.getName(), 30000);\n  }\n\n  public void waitTableAvailable(TableName table, long timeoutMillis)\n    throws InterruptedException, IOException {\n    waitFor(timeoutMillis, predicateTableAvailable(table));\n  }\n\n  /**\n   * Wait until all regions in a table have been assigned\n   * @param table         Table to wait on.\n   * @param timeoutMillis Timeout.\n   */\n  public void waitTableAvailable(byte[] table, long timeoutMillis)\n    throws InterruptedException, IOException {\n    waitFor(timeoutMillis, predicateTableAvailable(TableName.valueOf(table)));\n  }\n\n  public String explainTableAvailability(TableName tableName) throws IOException {\n    StringBuilder msg =\n      new StringBuilder(explainTableState(tableName, TableState.State.ENABLED)).append(\", \");\n    if (getHBaseCluster().getMaster().isAlive()) {\n      Map<RegionInfo, ServerName> assignments = getHBaseCluster().getMaster().getAssignmentManager()\n        .getRegionStates().getRegionAssignments();\n      final List<Pair<RegionInfo, ServerName>> metaLocations =\n        MetaTableAccessor.getTableRegionsAndLocations(getConnection(), tableName);\n      for (Pair<RegionInfo, ServerName> metaLocation : metaLocations) {\n        RegionInfo hri = metaLocation.getFirst();\n        ServerName sn = metaLocation.getSecond();\n        if (!assignments.containsKey(hri)) {\n          msg.append(\", region \").append(hri)\n            .append(\" not assigned, but found in meta, it expected to be on \").append(sn);\n        } else if (sn == null) {\n          msg.append(\",  region \").append(hri).append(\" assigned,  but has no server in meta\");\n        } else if (!sn.equals(assignments.get(hri))) {\n          msg.append(\",  region \").append(hri)\n            .append(\" assigned,  but has different servers in meta and AM ( \").append(sn)\n            .append(\" <> \").append(assignments.get(hri));\n        }\n      }\n    }\n    return msg.toString();\n  }\n\n  public String explainTableState(final TableName table, TableState.State state)\n    throws IOException {\n    TableState tableState = MetaTableAccessor.getTableState(getConnection(), table);\n    if (tableState == null) {\n      return \"TableState in META: No table state in META for table \" + table\n        + \" last state in meta (including deleted is \" + findLastTableState(table) + \")\";\n    } else if (!tableState.inStates(state)) {\n      return \"TableState in META: Not \" + state + \" state, but \" + tableState;\n    } else {\n      return \"TableState in META: OK\";\n    }\n  }\n\n  @Nullable\n  public TableState findLastTableState(final TableName table) throws IOException {\n    final AtomicReference<TableState> lastTableState = new AtomicReference<>(null);\n    ClientMetaTableAccessor.Visitor visitor = new ClientMetaTableAccessor.Visitor() {\n      @Override\n      public boolean visit(Result r) throws IOException {\n        if (!Arrays.equals(r.getRow(), table.getName())) {\n          return false;\n        }\n        TableState state = CatalogFamilyFormat.getTableState(r);\n        if (state != null) {\n          lastTableState.set(state);\n        }\n        return true;\n      }\n    };\n    MetaTableAccessor.scanMeta(getConnection(), null, null, ClientMetaTableAccessor.QueryType.TABLE,\n      Integer.MAX_VALUE, visitor);\n    return lastTableState.get();\n  }\n\n  /**\n   * Waits for a table to be 'enabled'. Enabled means that table is set as 'enabled' and the regions\n   * have been all assigned. Will timeout after default period (30 seconds) Tolerates nonexistent\n   * table.\n   * @param table the table to wait on.\n   * @throws InterruptedException if interrupted while waiting\n   * @throws IOException          if an IO problem is encountered\n   */\n  public void waitTableEnabled(TableName table) throws InterruptedException, IOException {\n    waitTableEnabled(table, 30000);\n  }\n\n  /**\n   * Waits for a table to be 'enabled'. Enabled means that table is set as 'enabled' and the regions\n   * have been all assigned.\n   * @see #waitTableEnabled(TableName, long)\n   * @param table         Table to wait on.\n   * @param timeoutMillis Time to wait on it being marked enabled.\n   */\n  public void waitTableEnabled(byte[] table, long timeoutMillis)\n    throws InterruptedException, IOException {\n    waitTableEnabled(TableName.valueOf(table), timeoutMillis);\n  }\n\n  public void waitTableEnabled(TableName table, long timeoutMillis) throws IOException {\n    waitFor(timeoutMillis, predicateTableEnabled(table));\n  }\n\n  /**\n   * Waits for a table to be 'disabled'. Disabled means that table is set as 'disabled' Will timeout\n   * after default period (30 seconds)\n   * @param table Table to wait on.\n   */\n  public void waitTableDisabled(byte[] table) throws InterruptedException, IOException {\n    waitTableDisabled(table, 30000);\n  }\n\n  public void waitTableDisabled(TableName table, long millisTimeout)\n    throws InterruptedException, IOException {\n    waitFor(millisTimeout, predicateTableDisabled(table));\n  }\n\n  /**\n   * Waits for a table to be 'disabled'. Disabled means that table is set as 'disabled'\n   * @param table         Table to wait on.\n   * @param timeoutMillis Time to wait on it being marked disabled.\n   */\n  public void waitTableDisabled(byte[] table, long timeoutMillis)\n    throws InterruptedException, IOException {\n    waitTableDisabled(TableName.valueOf(table), timeoutMillis);\n  }\n\n  /**\n   * Make sure that at least the specified number of region servers are running\n   * @param num minimum number of region servers that should be running\n   * @return true if we started some servers\n   */\n  public boolean ensureSomeRegionServersAvailable(final int num) throws IOException {\n    boolean startedServer = false;\n    SingleProcessHBaseCluster hbaseCluster = getMiniHBaseCluster();\n    for (int i = hbaseCluster.getLiveRegionServerThreads().size(); i < num; ++i) {\n      LOG.info(\"Started new server=\" + hbaseCluster.startRegionServer());\n      startedServer = true;\n    }\n\n    return startedServer;\n  }\n\n  /**\n   * Make sure that at least the specified number of region servers are running. We don't count the\n   * ones that are currently stopping or are stopped.\n   * @param num minimum number of region servers that should be running\n   * @return true if we started some servers\n   */\n  public boolean ensureSomeNonStoppedRegionServersAvailable(final int num) throws IOException {\n    boolean startedServer = ensureSomeRegionServersAvailable(num);\n\n    int nonStoppedServers = 0;\n    for (JVMClusterUtil.RegionServerThread rst : getMiniHBaseCluster().getRegionServerThreads()) {\n\n      HRegionServer hrs = rst.getRegionServer();\n      if (hrs.isStopping() || hrs.isStopped()) {\n        LOG.info(\"A region server is stopped or stopping:\" + hrs);\n      } else {\n        nonStoppedServers++;\n      }\n    }\n    for (int i = nonStoppedServers; i < num; ++i) {\n      LOG.info(\"Started new server=\" + getMiniHBaseCluster().startRegionServer());\n      startedServer = true;\n    }\n    return startedServer;\n  }\n\n  /**\n   * This method clones the passed <code>c</code> configuration setting a new user into the clone.\n   * Use it getting new instances of FileSystem. Only works for DistributedFileSystem w/o Kerberos.\n   * @param c                     Initial configuration\n   * @param differentiatingSuffix Suffix to differentiate this user from others.\n   * @return A new configuration instance with a different user set into it.\n   */\n  public static User getDifferentUser(final Configuration c, final String differentiatingSuffix)\n    throws IOException {\n    FileSystem currentfs = FileSystem.get(c);\n    if (!(currentfs instanceof DistributedFileSystem) || User.isHBaseSecurityEnabled(c)) {\n      return User.getCurrent();\n    }\n    // Else distributed filesystem. Make a new instance per daemon. Below\n    // code is taken from the AppendTestUtil over in hdfs.\n    String username = User.getCurrent().getName() + differentiatingSuffix;\n    User user = User.createUserForTesting(c, username, new String[] { \"supergroup\" });\n    return user;\n  }\n\n  public static NavigableSet<String> getAllOnlineRegions(SingleProcessHBaseCluster cluster)\n    throws IOException {\n    NavigableSet<String> online = new TreeSet<>();\n    for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {\n      try {\n        for (RegionInfo region : ProtobufUtil\n          .getOnlineRegions(rst.getRegionServer().getRSRpcServices())) {\n          online.add(region.getRegionNameAsString());\n        }\n      } catch (RegionServerStoppedException e) {\n        // That's fine.\n      }\n    }\n    return online;\n  }\n\n  /**\n   * Set maxRecoveryErrorCount in DFSClient. In 0.20 pre-append its hard-coded to 5 and makes tests\n   * linger. Here is the exception you'll see:\n   *\n   * <pre>\n   * 2010-06-15 11:52:28,511 WARN  [DataStreamer for file /hbase/.logs/wal.1276627923013 block\n   * blk_928005470262850423_1021] hdfs.DFSClient$DFSOutputStream(2657): Error Recovery for block\n   * blk_928005470262850423_1021 failed  because recovery from primary datanode 127.0.0.1:53683\n   * failed 4 times.  Pipeline was 127.0.0.1:53687, 127.0.0.1:53683. Will retry...\n   * </pre>\n   *\n   * @param stream A DFSClient.DFSOutputStream.\n   */\n  public static void setMaxRecoveryErrorCount(final OutputStream stream, final int max) {\n    try {\n      Class<?>[] clazzes = DFSClient.class.getDeclaredClasses();\n      for (Class<?> clazz : clazzes) {\n        String className = clazz.getSimpleName();\n        if (className.equals(\"DFSOutputStream\")) {\n          if (clazz.isInstance(stream)) {\n            Field maxRecoveryErrorCountField =\n              stream.getClass().getDeclaredField(\"maxRecoveryErrorCount\");\n            maxRecoveryErrorCountField.setAccessible(true);\n            maxRecoveryErrorCountField.setInt(stream, max);\n            break;\n          }\n        }\n      }\n    } catch (Exception e) {\n      LOG.info(\"Could not set max recovery field\", e);\n    }\n  }\n\n  /**\n   * Uses directly the assignment manager to assign the region. and waits until the specified region\n   * has completed assignment.\n   * @return true if the region is assigned false otherwise.\n   */\n  public boolean assignRegion(final RegionInfo regionInfo)\n    throws IOException, InterruptedException {\n    final AssignmentManager am = getHBaseCluster().getMaster().getAssignmentManager();\n    am.assign(regionInfo);\n    return AssignmentTestingUtil.waitForAssignment(am, regionInfo);\n  }\n\n  /**\n   * Move region to destination server and wait till region is completely moved and online\n   * @param destRegion region to move\n   * @param destServer destination server of the region\n   */\n  public void moveRegionAndWait(RegionInfo destRegion, ServerName destServer)\n    throws InterruptedException, IOException {\n    HMaster master = getMiniHBaseCluster().getMaster();\n    // TODO: Here we start the move. The move can take a while.\n    getAdmin().move(destRegion.getEncodedNameAsBytes(), destServer);\n    while (true) {\n      ServerName serverName =\n        master.getAssignmentManager().getRegionStates().getRegionServerOfRegion(destRegion);\n      if (serverName != null && serverName.equals(destServer)) {\n        assertRegionOnServer(destRegion, serverName, 2000);\n        break;\n      }\n      Thread.sleep(10);\n    }\n  }\n\n  /**\n   * Wait until all regions for a table in hbase:meta have a non-empty info:server, up to a\n   * configuable timeout value (default is 60 seconds) This means all regions have been deployed,\n   * master has been informed and updated hbase:meta with the regions deployed server.\n   * @param tableName the table name\n   */\n  public void waitUntilAllRegionsAssigned(final TableName tableName) throws IOException {\n    waitUntilAllRegionsAssigned(tableName,\n      this.conf.getLong(\"hbase.client.sync.wait.timeout.msec\", 60000));\n  }\n\n  /**\n   * Waith until all system table's regions get assigned\n   */\n  public void waitUntilAllSystemRegionsAssigned() throws IOException {\n    waitUntilAllRegionsAssigned(TableName.META_TABLE_NAME);\n  }\n\n  /**\n   * Wait until all regions for a table in hbase:meta have a non-empty info:server, or until\n   * timeout. This means all regions have been deployed, master has been informed and updated\n   * hbase:meta with the regions deployed server.\n   * @param tableName the table name\n   * @param timeout   timeout, in milliseconds\n   */\n  public void waitUntilAllRegionsAssigned(final TableName tableName, final long timeout)\n    throws IOException {\n    if (!TableName.isMetaTableName(tableName)) {\n      try (final Table meta = getConnection().getTable(TableName.META_TABLE_NAME)) {\n        LOG.debug(\"Waiting until all regions of table \" + tableName + \" get assigned. Timeout = \"\n          + timeout + \"ms\");\n        waitFor(timeout, 200, true, new ExplainingPredicate<IOException>() {\n          @Override\n          public String explainFailure() throws IOException {\n            return explainTableAvailability(tableName);\n          }\n\n          @Override\n          public boolean evaluate() throws IOException {\n            Scan scan = new Scan();\n            scan.addFamily(HConstants.CATALOG_FAMILY);\n            boolean tableFound = false;\n            try (ResultScanner s = meta.getScanner(scan)) {\n              for (Result r; (r = s.next()) != null;) {\n                byte[] b = r.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);\n                RegionInfo info = RegionInfo.parseFromOrNull(b);\n                if (info != null && info.getTable().equals(tableName)) {\n                  // Get server hosting this region from catalog family. Return false if no server\n                  // hosting this region, or if the server hosting this region was recently killed\n                  // (for fault tolerance testing).\n                  tableFound = true;\n                  byte[] server =\n                    r.getValue(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER);\n                  if (server == null) {\n                    return false;\n                  } else {\n                    byte[] startCode =\n                      r.getValue(HConstants.CATALOG_FAMILY, HConstants.STARTCODE_QUALIFIER);\n                    ServerName serverName =\n                      ServerName.valueOf(Bytes.toString(server).replaceFirst(\":\", \",\") + \",\"\n                        + Bytes.toLong(startCode));\n                    if (\n                      !getHBaseClusterInterface().isDistributedCluster()\n                        && getHBaseCluster().isKilledRS(serverName)\n                    ) {\n                      return false;\n                    }\n                  }\n                  if (RegionStateStore.getRegionState(r, info) != RegionState.State.OPEN) {\n                    return false;\n                  }\n                }\n              }\n            }\n            if (!tableFound) {\n              LOG.warn(\n                \"Didn't find the entries for table \" + tableName + \" in meta, already deleted?\");\n            }\n            return tableFound;\n          }\n        });\n      }\n    }\n    LOG.info(\"All regions for table \" + tableName + \" assigned to meta. Checking AM states.\");\n    // check from the master state if we are using a mini cluster\n    if (!getHBaseClusterInterface().isDistributedCluster()) {\n      // So, all regions are in the meta table but make sure master knows of the assignments before\n      // returning -- sometimes this can lag.\n      HMaster master = getHBaseCluster().getMaster();\n      final RegionStates states = master.getAssignmentManager().getRegionStates();\n      waitFor(timeout, 200, new ExplainingPredicate<IOException>() {\n        @Override\n        public String explainFailure() throws IOException {\n          return explainTableAvailability(tableName);\n        }\n\n        @Override\n        public boolean evaluate() throws IOException {\n          List<RegionInfo> hris = states.getRegionsOfTable(tableName);\n          return hris != null && !hris.isEmpty();\n        }\n      });\n    }\n    LOG.info(\"All regions for table \" + tableName + \" assigned.\");\n  }\n\n  /**\n   * Do a small get/scan against one store. This is required because store has no actual methods of\n   * querying itself, and relies on StoreScanner.\n   */\n  public static List<Cell> getFromStoreFile(HStore store, Get get) throws IOException {\n    Scan scan = new Scan(get);\n    InternalScanner scanner = (InternalScanner) store.getScanner(scan,\n      scan.getFamilyMap().get(store.getColumnFamilyDescriptor().getName()),\n      // originally MultiVersionConcurrencyControl.resetThreadReadPoint() was called to set\n      // readpoint 0.\n      0);\n\n    List<Cell> result = new ArrayList<>();\n    scanner.next(result);\n    if (!result.isEmpty()) {\n      // verify that we are on the row we want:\n      Cell kv = result.get(0);\n      if (!CellUtil.matchingRows(kv, get.getRow())) {\n        result.clear();\n      }\n    }\n    scanner.close();\n    return result;\n  }\n\n  /**\n   * Create region split keys between startkey and endKey\n   * @param numRegions the number of regions to be created. it has to be greater than 3.\n   * @return resulting split keys\n   */\n  public byte[][] getRegionSplitStartKeys(byte[] startKey, byte[] endKey, int numRegions) {\n    assertTrue(numRegions > 3);\n    byte[][] tmpSplitKeys = Bytes.split(startKey, endKey, numRegions - 3);\n    byte[][] result = new byte[tmpSplitKeys.length + 1][];\n    System.arraycopy(tmpSplitKeys, 0, result, 1, tmpSplitKeys.length);\n    result[0] = HConstants.EMPTY_BYTE_ARRAY;\n    return result;\n  }\n\n  /**\n   * Do a small get/scan against one store. This is required because store has no actual methods of\n   * querying itself, and relies on StoreScanner.\n   */\n  public static List<Cell> getFromStoreFile(HStore store, byte[] row, NavigableSet<byte[]> columns)\n    throws IOException {\n    Get get = new Get(row);\n    Map<byte[], NavigableSet<byte[]>> s = get.getFamilyMap();\n    s.put(store.getColumnFamilyDescriptor().getName(), columns);\n\n    return getFromStoreFile(store, get);\n  }\n\n  public static void assertKVListsEqual(String additionalMsg, final List<? extends Cell> expected,\n    final List<? extends Cell> actual) {\n    final int eLen = expected.size();\n    final int aLen = actual.size();\n    final int minLen = Math.min(eLen, aLen);\n\n    int i = 0;\n    while (\n      i < minLen && CellComparator.getInstance().compare(expected.get(i), actual.get(i)) == 0\n    ) {\n      i++;\n    }\n\n    if (additionalMsg == null) {\n      additionalMsg = \"\";\n    }\n    if (!additionalMsg.isEmpty()) {\n      additionalMsg = \". \" + additionalMsg;\n    }\n\n    if (eLen != aLen || i != minLen) {\n      throw new AssertionError(\"Expected and actual KV arrays differ at position \" + i + \": \"\n        + safeGetAsStr(expected, i) + \" (length \" + eLen + \") vs. \" + safeGetAsStr(actual, i)\n        + \" (length \" + aLen + \")\" + additionalMsg);\n    }\n  }\n\n  public static <T> String safeGetAsStr(List<T> lst, int i) {\n    if (0 <= i && i < lst.size()) {\n      return lst.get(i).toString();\n    } else {\n      return \"<out_of_range>\";\n    }\n  }\n\n  public String getClusterKey() {\n    return conf.get(HConstants.ZOOKEEPER_QUORUM) + \":\" + conf.get(HConstants.ZOOKEEPER_CLIENT_PORT)\n      + \":\"\n      + conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT, HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT);\n  }\n\n  /**\n   * Creates a random table with the given parameters\n   */\n  public Table createRandomTable(TableName tableName, final Collection<String> families,\n    final int maxVersions, final int numColsPerRow, final int numFlushes, final int numRegions,\n    final int numRowsPerFlush) throws IOException, InterruptedException {\n    LOG.info(\"\\n\\nCreating random table \" + tableName + \" with \" + numRegions + \" regions, \"\n      + numFlushes + \" storefiles per region, \" + numRowsPerFlush + \" rows per flush, maxVersions=\"\n      + maxVersions + \"\\n\");\n\n    final Random rand = new Random(tableName.hashCode() * 17L + 12938197137L);\n    final int numCF = families.size();\n    final byte[][] cfBytes = new byte[numCF][];\n    {\n      int cfIndex = 0;\n      for (String cf : families) {\n        cfBytes[cfIndex++] = Bytes.toBytes(cf);\n      }\n    }\n\n    final int actualStartKey = 0;\n    final int actualEndKey = Integer.MAX_VALUE;\n    final int keysPerRegion = (actualEndKey - actualStartKey) / numRegions;\n    final int splitStartKey = actualStartKey + keysPerRegion;\n    final int splitEndKey = actualEndKey - keysPerRegion;\n    final String keyFormat = \"%08x\";\n    final Table table = createTable(tableName, cfBytes, maxVersions,\n      Bytes.toBytes(String.format(keyFormat, splitStartKey)),\n      Bytes.toBytes(String.format(keyFormat, splitEndKey)), numRegions);\n\n    if (hbaseCluster != null) {\n      getMiniHBaseCluster().flushcache(TableName.META_TABLE_NAME);\n    }\n\n    BufferedMutator mutator = getConnection().getBufferedMutator(tableName);\n\n    for (int iFlush = 0; iFlush < numFlushes; ++iFlush) {\n      for (int iRow = 0; iRow < numRowsPerFlush; ++iRow) {\n        final byte[] row = Bytes.toBytes(\n          String.format(keyFormat, actualStartKey + rand.nextInt(actualEndKey - actualStartKey)));\n\n        Put put = new Put(row);\n        Delete del = new Delete(row);\n        for (int iCol = 0; iCol < numColsPerRow; ++iCol) {\n          final byte[] cf = cfBytes[rand.nextInt(numCF)];\n          final long ts = rand.nextInt();\n          final byte[] qual = Bytes.toBytes(\"col\" + iCol);\n          if (rand.nextBoolean()) {\n            final byte[] value =\n              Bytes.toBytes(\"value_for_row_\" + iRow + \"_cf_\" + Bytes.toStringBinary(cf) + \"_col_\"\n                + iCol + \"_ts_\" + ts + \"_random_\" + rand.nextLong());\n            put.addColumn(cf, qual, ts, value);\n          } else if (rand.nextDouble() < 0.8) {\n            del.addColumn(cf, qual, ts);\n          } else {\n            del.addColumns(cf, qual, ts);\n          }\n        }\n\n        if (!put.isEmpty()) {\n          mutator.mutate(put);\n        }\n\n        if (!del.isEmpty()) {\n          mutator.mutate(del);\n        }\n      }\n      LOG.info(\"Initiating flush #\" + iFlush + \" for table \" + tableName);\n      mutator.flush();\n      if (hbaseCluster != null) {\n        getMiniHBaseCluster().flushcache(table.getName());\n      }\n    }\n    mutator.close();\n\n    return table;\n  }\n\n  public static int randomFreePort() {\n    return HBaseCommonTestingUtil.randomFreePort();\n  }\n\n  public static String randomMultiCastAddress() {\n    return \"226.1.1.\" + ThreadLocalRandom.current().nextInt(254);\n  }\n\n  public static void waitForHostPort(String host, int port) throws IOException {\n    final int maxTimeMs = 10000;\n    final int maxNumAttempts = maxTimeMs / HConstants.SOCKET_RETRY_WAIT_MS;\n    IOException savedException = null;\n    LOG.info(\"Waiting for server at \" + host + \":\" + port);\n    for (int attempt = 0; attempt < maxNumAttempts; ++attempt) {\n      try {\n        Socket sock = new Socket(InetAddress.getByName(host), port);\n        sock.close();\n        savedException = null;\n        LOG.info(\"Server at \" + host + \":\" + port + \" is available\");\n        break;\n      } catch (UnknownHostException e) {\n        throw new IOException(\"Failed to look up \" + host, e);\n      } catch (IOException e) {\n        savedException = e;\n      }\n      Threads.sleepWithoutInterrupt(HConstants.SOCKET_RETRY_WAIT_MS);\n    }\n\n    if (savedException != null) {\n      throw savedException;\n    }\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableName tableName,\n    byte[] columnFamily, Algorithm compression, DataBlockEncoding dataBlockEncoding)\n    throws IOException {\n    return createPreSplitLoadTestTable(conf, tableName, columnFamily, compression,\n      dataBlockEncoding, DEFAULT_REGIONS_PER_SERVER, 1, Durability.USE_DEFAULT);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableName tableName,\n    byte[] columnFamily, Algorithm compression, DataBlockEncoding dataBlockEncoding,\n    int numRegionsPerServer, int regionReplication, Durability durability) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    builder.setDurability(durability);\n    builder.setRegionReplication(regionReplication);\n    ColumnFamilyDescriptorBuilder cfBuilder =\n      ColumnFamilyDescriptorBuilder.newBuilder(columnFamily);\n    cfBuilder.setDataBlockEncoding(dataBlockEncoding);\n    cfBuilder.setCompressionType(compression);\n    return createPreSplitLoadTestTable(conf, builder.build(), cfBuilder.build(),\n      numRegionsPerServer);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableName tableName,\n    byte[][] columnFamilies, Algorithm compression, DataBlockEncoding dataBlockEncoding,\n    int numRegionsPerServer, int regionReplication, Durability durability) throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(tableName);\n    builder.setDurability(durability);\n    builder.setRegionReplication(regionReplication);\n    ColumnFamilyDescriptor[] hcds = new ColumnFamilyDescriptor[columnFamilies.length];\n    for (int i = 0; i < columnFamilies.length; i++) {\n      ColumnFamilyDescriptorBuilder cfBuilder =\n        ColumnFamilyDescriptorBuilder.newBuilder(columnFamilies[i]);\n      cfBuilder.setDataBlockEncoding(dataBlockEncoding);\n      cfBuilder.setCompressionType(compression);\n      hcds[i] = cfBuilder.build();\n    }\n    return createPreSplitLoadTestTable(conf, builder.build(), hcds, numRegionsPerServer);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableDescriptor desc,\n    ColumnFamilyDescriptor hcd) throws IOException {\n    return createPreSplitLoadTestTable(conf, desc, hcd, DEFAULT_REGIONS_PER_SERVER);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableDescriptor desc,\n    ColumnFamilyDescriptor hcd, int numRegionsPerServer) throws IOException {\n    return createPreSplitLoadTestTable(conf, desc, new ColumnFamilyDescriptor[] { hcd },\n      numRegionsPerServer);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableDescriptor desc,\n    ColumnFamilyDescriptor[] hcds, int numRegionsPerServer) throws IOException {\n    return createPreSplitLoadTestTable(conf, desc, hcds, new RegionSplitter.HexStringSplit(),\n      numRegionsPerServer);\n  }\n\n  /**\n   * Creates a pre-split table for load testing. If the table already exists, logs a warning and\n   * continues.\n   * @return the number of regions the table was split into\n   */\n  public static int createPreSplitLoadTestTable(Configuration conf, TableDescriptor td,\n    ColumnFamilyDescriptor[] cds, SplitAlgorithm splitter, int numRegionsPerServer)\n    throws IOException {\n    TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(td);\n    for (ColumnFamilyDescriptor cd : cds) {\n      if (!td.hasColumnFamily(cd.getName())) {\n        builder.setColumnFamily(cd);\n      }\n    }\n    td = builder.build();\n    int totalNumberOfRegions = 0;\n    Connection unmanagedConnection = ConnectionFactory.createConnection(conf);\n    Admin admin = unmanagedConnection.getAdmin();\n\n    try {\n      // create a table a pre-splits regions.\n      // The number of splits is set as:\n      // region servers * regions per region server).\n      int numberOfServers = admin.getRegionServers().size();\n      if (numberOfServers == 0) {\n        throw new IllegalStateException(\"No live regionservers\");\n      }\n\n      totalNumberOfRegions = numberOfServers * numRegionsPerServer;\n      LOG.info(\"Number of live regionservers: \" + numberOfServers + \", \"\n        + \"pre-splitting table into \" + totalNumberOfRegions + \" regions \" + \"(regions per server: \"\n        + numRegionsPerServer + \")\");\n\n      byte[][] splits = splitter.split(totalNumberOfRegions);\n\n      admin.createTable(td, splits);\n    } catch (MasterNotRunningException e) {\n      LOG.error(\"Master not running\", e);\n      throw new IOException(e);\n    } catch (TableExistsException e) {\n      LOG.warn(\"Table \" + td.getTableName() + \" already exists, continuing\");\n    } finally {\n      admin.close();\n      unmanagedConnection.close();\n    }\n    return totalNumberOfRegions;\n  }\n\n  public static int getMetaRSPort(Connection connection) throws IOException {\n    try (RegionLocator locator = connection.getRegionLocator(TableName.META_TABLE_NAME)) {\n      return locator.getRegionLocation(Bytes.toBytes(\"\")).getPort();\n    }\n  }\n\n  /**\n   * Due to async racing issue, a region may not be in the online region list of a region server\n   * yet, after the assignment znode is deleted and the new assignment is recorded in master.\n   */\n  public void assertRegionOnServer(final RegionInfo hri, final ServerName server,\n    final long timeout) throws IOException, InterruptedException {\n    long timeoutTime = EnvironmentEdgeManager.currentTime() + timeout;\n    while (true) {\n      List<RegionInfo> regions = getAdmin().getRegions(server);\n      if (regions.stream().anyMatch(r -> RegionInfo.COMPARATOR.compare(r, hri) == 0)) return;\n      long now = EnvironmentEdgeManager.currentTime();\n      if (now > timeoutTime) break;\n      Thread.sleep(10);\n    }\n    fail(\"Could not find region \" + hri.getRegionNameAsString() + \" on server \" + server);\n  }\n\n  /**\n   * Check to make sure the region is open on the specified region server, but not on any other one.\n   */\n  public void assertRegionOnlyOnServer(final RegionInfo hri, final ServerName server,\n    final long timeout) throws IOException, InterruptedException {\n    long timeoutTime = EnvironmentEdgeManager.currentTime() + timeout;\n    while (true) {\n      List<RegionInfo> regions = getAdmin().getRegions(server);\n      if (regions.stream().anyMatch(r -> RegionInfo.COMPARATOR.compare(r, hri) == 0)) {\n        List<JVMClusterUtil.RegionServerThread> rsThreads =\n          getHBaseCluster().getLiveRegionServerThreads();\n        for (JVMClusterUtil.RegionServerThread rsThread : rsThreads) {\n          HRegionServer rs = rsThread.getRegionServer();\n          if (server.equals(rs.getServerName())) {\n            continue;\n          }\n          Collection<HRegion> hrs = rs.getOnlineRegionsLocalContext();\n          for (HRegion r : hrs) {\n            assertTrue(\"Region should not be double assigned\",\n              r.getRegionInfo().getRegionId() != hri.getRegionId());\n          }\n        }\n        return; // good, we are happy\n      }\n      long now = EnvironmentEdgeManager.currentTime();\n      if (now > timeoutTime) break;\n      Thread.sleep(10);\n    }\n    fail(\"Could not find region \" + hri.getRegionNameAsString() + \" on server \" + server);\n  }\n\n  public HRegion createTestRegion(String tableName, ColumnFamilyDescriptor cd) throws IOException {\n    TableDescriptor td =\n      TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName)).setColumnFamily(cd).build();\n    RegionInfo info = RegionInfoBuilder.newBuilder(TableName.valueOf(tableName)).build();\n    return createRegionAndWAL(info, getDataTestDir(), getConfiguration(), td);\n  }\n\n  public HRegion createTestRegion(String tableName, ColumnFamilyDescriptor cd,\n    BlockCache blockCache) throws IOException {\n    TableDescriptor td =\n      TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName)).setColumnFamily(cd).build();\n    RegionInfo info = RegionInfoBuilder.newBuilder(TableName.valueOf(tableName)).build();\n    return createRegionAndWAL(info, getDataTestDir(), getConfiguration(), td, blockCache);\n  }\n\n  public static void setFileSystemURI(String fsURI) {\n    FS_URI = fsURI;\n  }\n\n  /**\n   * Returns a {@link Predicate} for checking that there are no regions in transition in master\n   */\n  public ExplainingPredicate<IOException> predicateNoRegionsInTransition() {\n    return new ExplainingPredicate<IOException>() {\n      @Override\n      public String explainFailure() throws IOException {\n        final RegionStates regionStates =\n          getMiniHBaseCluster().getMaster().getAssignmentManager().getRegionStates();\n        return \"found in transition: \" + regionStates.getRegionsInTransition().toString();\n      }\n\n      @Override\n      public boolean evaluate() throws IOException {\n        HMaster master = getMiniHBaseCluster().getMaster();\n        if (master == null) return false;\n        AssignmentManager am = master.getAssignmentManager();\n        if (am == null) return false;\n        return !am.hasRegionsInTransition();\n      }\n    };\n  }\n\n  /**\n   * Returns a {@link Predicate} for checking that table is enabled\n   */\n  public Waiter.Predicate<IOException> predicateTableEnabled(final TableName tableName) {\n    return new ExplainingPredicate<IOException>() {\n      @Override\n      public String explainFailure() throws IOException {\n        return explainTableState(tableName, TableState.State.ENABLED);\n      }\n\n      @Override\n      public boolean evaluate() throws IOException {\n        return getAdmin().tableExists(tableName) && getAdmin().isTableEnabled(tableName);\n      }\n    };\n  }\n\n  /**\n   * Returns a {@link Predicate} for checking that table is enabled\n   */\n  public Waiter.Predicate<IOException> predicateTableDisabled(final TableName tableName) {\n    return new ExplainingPredicate<IOException>() {\n      @Override\n      public String explainFailure() throws IOException {\n        return explainTableState(tableName, TableState.State.DISABLED);\n      }\n\n      @Override\n      public boolean evaluate() throws IOException {\n        return getAdmin().isTableDisabled(tableName);\n      }\n    };\n  }\n\n  /**\n   * Returns a {@link Predicate} for checking that table is enabled\n   */\n  public Waiter.Predicate<IOException> predicateTableAvailable(final TableName tableName) {\n    return new ExplainingPredicate<IOException>() {\n      @Override\n      public String explainFailure() throws IOException {\n        return explainTableAvailability(tableName);\n      }\n\n      @Override\n      public boolean evaluate() throws IOException {\n        boolean tableAvailable = getAdmin().isTableAvailable(tableName);\n        if (tableAvailable) {\n          try (Table table = getConnection().getTable(tableName)) {\n            TableDescriptor htd = table.getDescriptor();\n            for (HRegionLocation loc : getConnection().getRegionLocator(tableName)\n              .getAllRegionLocations()) {\n              Scan scan = new Scan().withStartRow(loc.getRegion().getStartKey())\n                .withStopRow(loc.getRegion().getEndKey()).setOneRowLimit()\n                .setMaxResultsPerColumnFamily(1).setCacheBlocks(false);\n              for (byte[] family : htd.getColumnFamilyNames()) {\n                scan.addFamily(family);\n              }\n              try (ResultScanner scanner = table.getScanner(scan)) {\n                scanner.next();\n              }\n            }\n          }\n        }\n        return tableAvailable;\n      }\n    };\n  }\n\n  /**\n   * Wait until no regions in transition.\n   * @param timeout How long to wait.\n   */\n  public void waitUntilNoRegionsInTransition(final long timeout) throws IOException {\n    waitFor(timeout, predicateNoRegionsInTransition());\n  }\n\n  /**\n   * Wait until no regions in transition. (time limit 15min)\n   */\n  public void waitUntilNoRegionsInTransition() throws IOException {\n    waitUntilNoRegionsInTransition(15 * 60000);\n  }\n\n  /**\n   * Wait until labels is ready in VisibilityLabelsCache.\n   */\n  public void waitLabelAvailable(long timeoutMillis, final String... labels) {\n    final VisibilityLabelsCache labelsCache = VisibilityLabelsCache.get();\n    waitFor(timeoutMillis, new Waiter.ExplainingPredicate<RuntimeException>() {\n\n      @Override\n      public boolean evaluate() {\n        for (String label : labels) {\n          if (labelsCache.getLabelOrdinal(label) == 0) {\n            return false;\n          }\n        }\n        return true;\n      }\n\n      @Override\n      public String explainFailure() {\n        for (String label : labels) {\n          if (labelsCache.getLabelOrdinal(label) == 0) {\n            return label + \" is not available yet\";\n          }\n        }\n        return \"\";\n      }\n    });\n  }\n\n  /**\n   * Create a set of column descriptors with the combination of compression, encoding, bloom codecs\n   * available.\n   * @return the list of column descriptors\n   */\n  public static List<ColumnFamilyDescriptor> generateColumnDescriptors() {\n    return generateColumnDescriptors(\"\");\n  }\n\n  /**\n   * Create a set of column descriptors with the combination of compression, encoding, bloom codecs\n   * available.\n   * @param prefix family names prefix\n   * @return the list of column descriptors\n   */\n  public static List<ColumnFamilyDescriptor> generateColumnDescriptors(final String prefix) {\n    List<ColumnFamilyDescriptor> columnFamilyDescriptors = new ArrayList<>();\n    long familyId = 0;\n    for (Compression.Algorithm compressionType : getSupportedCompressionAlgorithms()) {\n      for (DataBlockEncoding encodingType : DataBlockEncoding.values()) {\n        for (BloomType bloomType : BloomType.values()) {\n          String name = String.format(\"%s-cf-!@#&-%d!@#\", prefix, familyId);\n          ColumnFamilyDescriptorBuilder columnFamilyDescriptorBuilder =\n            ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(name));\n          columnFamilyDescriptorBuilder.setCompressionType(compressionType);\n          columnFamilyDescriptorBuilder.setDataBlockEncoding(encodingType);\n          columnFamilyDescriptorBuilder.setBloomFilterType(bloomType);\n          columnFamilyDescriptors.add(columnFamilyDescriptorBuilder.build());\n          familyId++;\n        }\n      }\n    }\n    return columnFamilyDescriptors;\n  }\n\n  /**\n   * Get supported compression algorithms.\n   * @return supported compression algorithms.\n   */\n  public static Compression.Algorithm[] getSupportedCompressionAlgorithms() {\n    String[] allAlgos = HFile.getSupportedCompressionAlgorithms();\n    List<Compression.Algorithm> supportedAlgos = new ArrayList<>();\n    for (String algoName : allAlgos) {\n      try {\n        Compression.Algorithm algo = Compression.getCompressionAlgorithmByName(algoName);\n        algo.getCompressor();\n        supportedAlgos.add(algo);\n      } catch (Throwable t) {\n        // this algo is not available\n      }\n    }\n    return supportedAlgos.toArray(new Algorithm[supportedAlgos.size()]);\n  }\n\n  public Result getClosestRowBefore(Region r, byte[] row, byte[] family) throws IOException {\n    Scan scan = new Scan().withStartRow(row);\n    scan.setReadType(ReadType.PREAD);\n    scan.setCaching(1);\n    scan.setReversed(true);\n    scan.addFamily(family);\n    try (RegionScanner scanner = r.getScanner(scan)) {\n      List<Cell> cells = new ArrayList<>(1);\n      scanner.next(cells);\n      if (r.getRegionInfo().isMetaRegion() && !isTargetTable(row, cells.get(0))) {\n        return null;\n      }\n      return Result.create(cells);\n    }\n  }\n\n  private boolean isTargetTable(final byte[] inRow, Cell c) {\n    String inputRowString = Bytes.toString(inRow);\n    int i = inputRowString.indexOf(HConstants.DELIMITER);\n    String outputRowString = Bytes.toString(c.getRowArray(), c.getRowOffset(), c.getRowLength());\n    int o = outputRowString.indexOf(HConstants.DELIMITER);\n    return inputRowString.substring(0, i).equals(outputRowString.substring(0, o));\n  }\n\n  /**\n   * Sets up {@link MiniKdc} for testing security. Uses {@link HBaseKerberosUtils} to set the given\n   * keytab file as {@link HBaseKerberosUtils#KRB_KEYTAB_FILE}. FYI, there is also the easier-to-use\n   * kerby KDC server and utility for using it,\n   * {@link org.apache.hadoop.hbase.util.SimpleKdcServerUtil}. The kerby KDC server is preferred;\n   * less baggage. It came in in HBASE-5291.\n   */\n  public MiniKdc setupMiniKdc(File keytabFile) throws Exception {\n    Properties conf = MiniKdc.createConf();\n    conf.put(MiniKdc.DEBUG, true);\n    MiniKdc kdc = null;\n    File dir = null;\n    // There is time lag between selecting a port and trying to bind with it. It's possible that\n    // another service captures the port in between which'll result in BindException.\n    boolean bindException;\n    int numTries = 0;\n    do {\n      try {\n        bindException = false;\n        dir = new File(getDataTestDir(\"kdc\").toUri().getPath());\n        kdc = new MiniKdc(conf, dir);\n        kdc.start();\n      } catch (BindException e) {\n        FileUtils.deleteDirectory(dir); // clean directory\n        numTries++;\n        if (numTries == 3) {\n          LOG.error(\"Failed setting up MiniKDC. Tried \" + numTries + \" times.\");\n          throw e;\n        }\n        LOG.error(\"BindException encountered when setting up MiniKdc. Trying again.\");\n        bindException = true;\n      }\n    } while (bindException);\n    HBaseKerberosUtils.setKeytabFileForTesting(keytabFile.getAbsolutePath());\n    return kdc;\n  }\n\n  public int getNumHFiles(final TableName tableName, final byte[] family) {\n    int numHFiles = 0;\n    for (RegionServerThread regionServerThread : getMiniHBaseCluster().getRegionServerThreads()) {\n      numHFiles += getNumHFilesForRS(regionServerThread.getRegionServer(), tableName, family);\n    }\n    return numHFiles;\n  }\n\n  public int getNumHFilesForRS(final HRegionServer rs, final TableName tableName,\n    final byte[] family) {\n    int numHFiles = 0;\n    for (Region region : rs.getRegions(tableName)) {\n      numHFiles += region.getStore(family).getStorefilesCount();\n    }\n    return numHFiles;\n  }\n\n  public void verifyTableDescriptorIgnoreTableName(TableDescriptor ltd, TableDescriptor rtd) {\n    assertEquals(ltd.getValues().hashCode(), rtd.getValues().hashCode());\n    Collection<ColumnFamilyDescriptor> ltdFamilies = Arrays.asList(ltd.getColumnFamilies());\n    Collection<ColumnFamilyDescriptor> rtdFamilies = Arrays.asList(rtd.getColumnFamilies());\n    assertEquals(ltdFamilies.size(), rtdFamilies.size());\n    for (Iterator<ColumnFamilyDescriptor> it = ltdFamilies.iterator(),\n        it2 = rtdFamilies.iterator(); it.hasNext();) {\n      assertEquals(0, ColumnFamilyDescriptor.COMPARATOR.compare(it.next(), it2.next()));\n    }\n  }\n\n  /**\n   * Await the successful return of {@code condition}, sleeping {@code sleepMillis} between\n   * invocations.\n   */\n  public static void await(final long sleepMillis, final BooleanSupplier condition)\n    throws InterruptedException {\n    try {\n      while (!condition.getAsBoolean()) {\n        Thread.sleep(sleepMillis);\n      }\n    } catch (RuntimeException e) {\n      if (e.getCause() instanceof AssertionError) {\n        throw (AssertionError) e.getCause();\n      }\n      throw e;\n    }\n  }\n}",
        "exampleID": 4,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/HBaseTestingUtil.java#L2106"
    },
    {
        "url": "dummy",
        "rawCode": "public class AliyunOSSMockLocalStore {\n  private static final Logger LOG = LoggerFactory.getLogger(AliyunOSSMockLocalStore.class);\n\n  private static final String DATA_FILE = \".DATA\";\n  private static final String META_FILE = \".META\";\n\n  private final File root;\n\n  private final ObjectMapper objectMapper = new ObjectMapper();\n\n  public AliyunOSSMockLocalStore(\n      @Value(\"${\" + AliyunOSSMockApp.PROP_ROOT_DIR + \":}\") String rootDir) {\n    Preconditions.checkNotNull(rootDir, \"Root directory cannot be null\");\n    this.root = new File(rootDir);\n\n    root.deleteOnExit();\n    root.mkdirs();\n\n    LOG.info(\"Root directory of local OSS store is {}\", root);\n  }\n\n  static String md5sum(String filepath) throws IOException {\n    try (InputStream is = new FileInputStream(filepath)) {\n      return md5sum(is);\n    }\n  }\n\n  static String md5sum(InputStream is) throws IOException {\n    MessageDigest md;\n    try {\n      md = MessageDigest.getInstance(\"MD5\");\n      md.reset();\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(e);\n    }\n    byte[] bytes = new byte[1024];\n    int numBytes;\n\n    while ((numBytes = is.read(bytes)) != -1) {\n      md.update(bytes, 0, numBytes);\n    }\n    return new String(Hex.encodeHex(md.digest())).toUpperCase(Locale.ROOT);\n  }\n\n  private static void inputStreamToFile(InputStream inputStream, File targetFile)\n      throws IOException {\n    try (OutputStream outputStream = new FileOutputStream(targetFile)) {\n      ByteStreams.copy(inputStream, outputStream);\n    }\n  }\n\n  void createBucket(String bucketName) throws IOException {\n    File newBucket = new File(root, bucketName);\n    FileUtils.forceMkdir(newBucket);\n  }\n\n  Bucket getBucket(String bucketName) {\n    List<Bucket> buckets =\n        findBucketsByFilter(\n            file -> Files.isDirectory(file) && file.getFileName().endsWith(bucketName));\n\n    return buckets.size() > 0 ? buckets.get(0) : null;\n  }\n\n  void deleteBucket(String bucketName) throws IOException {\n    Bucket bucket = getBucket(bucketName);\n    Preconditions.checkNotNull(bucket, \"Bucket %s shouldn't be null.\", bucketName);\n\n    File dir = new File(root, bucket.getName());\n    if (Files.walk(dir.toPath()).anyMatch(p -> p.toFile().isFile())) {\n      throw new AliyunOSSMockLocalController.OssException(\n          409, OSSErrorCode.BUCKET_NOT_EMPTY, \"The bucket you tried to delete is not empty. \");\n    }\n\n    FileUtils.deleteDirectory(dir);\n  }\n\n  ObjectMetadata putObject(\n      String bucketName,\n      String fileName,\n      InputStream dataStream,\n      String contentType,\n      String contentEncoding,\n      Map<String, String> userMetaData)\n      throws IOException {\n    File bucketDir = new File(root, bucketName);\n    assert bucketDir.exists() || bucketDir.mkdirs();\n\n    File dataFile = new File(bucketDir, fileName + DATA_FILE);\n    File metaFile = new File(bucketDir, fileName + META_FILE);\n    if (!dataFile.exists()) {\n      dataFile.getParentFile().mkdirs();\n      dataFile.createNewFile();\n    }\n\n    inputStreamToFile(dataStream, dataFile);\n\n    ObjectMetadata metadata = new ObjectMetadata();\n    metadata.setContentLength(dataFile.length());\n    metadata.setContentMD5(md5sum(dataFile.getAbsolutePath()));\n    metadata.setContentType(\n        contentType != null ? contentType : MediaType.APPLICATION_OCTET_STREAM_VALUE);\n    metadata.setContentEncoding(contentEncoding);\n    metadata.setDataFile(dataFile.getAbsolutePath());\n    metadata.setMetaFile(metaFile.getAbsolutePath());\n\n    BasicFileAttributes attributes =\n        Files.readAttributes(dataFile.toPath(), BasicFileAttributes.class);\n    metadata.setLastModificationDate(attributes.lastModifiedTime().toMillis());\n\n    metadata.setUserMetaData(userMetaData);\n\n    objectMapper.writeValue(metaFile, metadata);\n\n    return metadata;\n  }\n\n  void deleteObject(String bucketName, String filename) {\n    File bucketDir = new File(root, bucketName);\n    assert bucketDir.exists();\n\n    File dataFile = new File(bucketDir, filename + DATA_FILE);\n    File metaFile = new File(bucketDir, filename + META_FILE);\n    assert !dataFile.exists() || dataFile.delete();\n    assert !metaFile.exists() || metaFile.delete();\n  }\n\n  ObjectMetadata getObjectMetadata(String bucketName, String filename) throws IOException {\n    File bucketDir = new File(root, bucketName);\n    assert bucketDir.exists();\n\n    File dataFile = new File(bucketDir, filename + DATA_FILE);\n    if (!dataFile.exists()) {\n      return null;\n    }\n\n    File metaFile = new File(bucketDir, filename + META_FILE);\n    return objectMapper.readValue(metaFile, ObjectMetadata.class);\n  }\n\n  private List<Bucket> findBucketsByFilter(final DirectoryStream.Filter<Path> filter) {\n    List<Bucket> buckets = Lists.newArrayList();\n\n    try (DirectoryStream<Path> stream = Files.newDirectoryStream(root.toPath(), filter)) {\n      for (final Path path : stream) {\n        buckets.add(new Bucket(path.getFileName().toString()));\n      }\n    } catch (final IOException e) {\n      LOG.error(\"Could not iterate over Bucket-Folders\", e);\n    }\n\n    return buckets;\n  }\n}",
        "exampleID": 2,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/AliyunOSSMockLocalStore.java#L81"
    },
    {
        "url": "dummy",
        "rawCode": "public class FakeGoServer implements ExtensionContext.Store.CloseableResource {\n    public enum TestResource {\n        TEST_AGENT(Resource.newClassPathResource(\"testdata/gen/test-agent.jar\")),\n        TEST_AGENT_LAUNCHER(Resource.newClassPathResource(\"testdata/gen/agent-launcher.jar\")),\n        TEST_AGENT_PLUGINS(Resource.newClassPathResource(\"testdata/agent-plugins.zip\")),\n        TEST_TFS_IMPL(Resource.newClassPathResource(\"testdata/gen/tfs-impl-14.jar\")),;\n\n        private final Resource source;\n\n        TestResource(Resource source) {\n            this.source = source;\n        }\n\n        public String getMd5() {\n            try (InputStream input = source.getInputStream()) {\n                MessageDigest digester = MessageDigest.getInstance(\"MD5\");\n                try (DigestInputStream digest = new DigestInputStream(input, digester)) {\n                    IOUtils.copy(digest, new NullOutputStream());\n                }\n                return Hexadecimals.toHexString(digester.digest()).toLowerCase();\n            } catch (Exception e) {\n                throw new RuntimeException(e);\n            }\n        }\n\n        public void copyTo(OutputStream outputStream) throws IOException {\n            try (InputStream in = source.getInputStream()) {\n                IOUtils.copy(in, outputStream);\n            }\n        }\n\n        // Because the resource can be a jarresource, which extracts to dir instead of a simple copy.\n        public void copyTo(File output) throws IOException {\n            try (InputStream in = source.getInputStream()) {\n                FileUtils.copyToFile(in, output);\n            }\n        }\n    }\n\n    private Server server;\n    private int port;\n    private int securePort;\n    private String extraPropertiesHeaderValue;\n\n    FakeGoServer() {\n    }\n\n    public int getPort() {\n        return port;\n    }\n\n    public int getSecurePort() {\n        return securePort;\n    }\n\n    @Override\n    public void close() throws Throwable {\n        stop();\n    }\n\n    private void stop() throws Exception {\n        if (server != null) {\n            server.stop();\n            server.join();\n        }\n    }\n\n    void start() throws Exception {\n        server = new Server();\n        ServerConnector connector = new ServerConnector(server);\n        server.addConnector(connector);\n\n        SslContextFactory sslContextFactory = new SslContextFactory.Server();\n        sslContextFactory.setCertAlias(\"cruise\");\n        sslContextFactory.setKeyStoreResource(Resource.newClassPathResource(\"testdata/fake-server-keystore\"));\n        sslContextFactory.setKeyStorePassword(\"serverKeystorepa55w0rd\");\n\n        ServerConnector secureConnnector = new ServerConnector(server,\n                new SslConnectionFactory(sslContextFactory, HttpVersion.HTTP_1_1.asString()),\n                new HttpConnectionFactory(new HttpConfiguration())\n        );\n        server.addConnector(secureConnnector);\n\n        WebAppContext wac = new WebAppContext(\".\", \"/go\");\n        ServletHolder holder = new ServletHolder();\n        holder.setServlet(new HttpServlet() {\n            @Override\n            protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws IOException {\n                resp.getOutputStream().println(\"Hello\");\n            }\n        });\n        wac.addServlet(holder, \"/hello\");\n        addFakeAgentBinaryServlet(wac, \"/admin/agent\", TEST_AGENT, this);\n        addFakeAgentBinaryServlet(wac, \"/admin/agent-launcher.jar\", TEST_AGENT_LAUNCHER, this);\n        addFakeAgentBinaryServlet(wac, \"/admin/agent-plugins.zip\", TEST_AGENT_PLUGINS, this);\n        addFakeAgentBinaryServlet(wac, \"/admin/tfs-impl.jar\", TEST_TFS_IMPL, this);\n        addlatestAgentStatusCall(wac);\n        addDefaultServlet(wac);\n        server.setHandler(wac);\n        server.setStopAtShutdown(true);\n        server.start();\n\n        port = connector.getLocalPort();\n        securePort = secureConnnector.getLocalPort();\n    }\n\n    private static final class AgentStatusApi extends HttpServlet {\n        public static String status = \"disabled\";\n        static Properties pluginProps = new Properties();\n\n        @Override\n        protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws IOException {\n            resp.setHeader(\"Plugins-Status\", status);\n            pluginProps.setProperty(\"Active Mock Bundle 1\", \"1.1.1\");\n            pluginProps.setProperty(\"Active Mock Bundle 2\", \"2.2.2\");\n            ByteArrayOutputStream baos = new ByteArrayOutputStream();\n            pluginProps.store(baos, \"Go Plugins for Testing\");\n            resp.getOutputStream().write(baos.toByteArray());\n            baos.close();\n        }\n    }\n\n    public FakeGoServer setExtraPropertiesHeaderValue(String value) {\n        extraPropertiesHeaderValue = value;\n        return this;\n    }\n\n    String getExtraPropertiesHeaderValue() {\n        return extraPropertiesHeaderValue;\n    }\n\n    private void addlatestAgentStatusCall(WebAppContext wac) {\n        wac.addServlet(AgentStatusApi.class, \"/admin/latest-agent.status\");\n    }\n\n    public static final class BreakpointFriendlyFilter implements Filter {\n        @Override\n        public void init(FilterConfig filterConfig) {\n\n        }\n\n        @Override\n        public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {\n            filterChain.doFilter(servletRequest, servletResponse);\n        }\n\n        @Override\n        public void destroy() {\n\n        }\n    }\n\n    private void addDefaultServlet(WebAppContext wac) {\n        wac.addFilter(BreakpointFriendlyFilter.class, \"*\", EnumSet.of(DispatcherType.REQUEST));\n    }\n\n    private static void addFakeAgentBinaryServlet(WebAppContext wac, final String pathSpec, final TestResource resource, FakeGoServer fakeGoServer) {\n        ServletHolder holder = new ServletHolder();\n        holder.setServlet(new AgentBinariesServlet(resource, fakeGoServer));\n        wac.addServlet(holder, pathSpec);\n    }\n\n}",
        "exampleID": 1008,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/FakeGoServer.java#L57"
    },
    {
        "url": "dummy",
        "rawCode": "private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n  private static final String digestAlgorithm = \"MD5\";\n\n  private static volatile ExecutorService executorService;\n\n  private final StackTraceElement[] createStack;\n  private final S3Client s3;\n  private final S3URI location;\n  private final S3FileIOProperties s3FileIOProperties;\n  private final Set<Tag> writeTags;\n\n  private CountingOutputStream stream;\n  private final List<FileAndDigest> stagingFiles = Lists.newArrayList();\n  private final File stagingDirectory;\n  private File currentStagingFile;\n  private String multipartUploadId;\n  private final Map<File, CompletableFuture<CompletedPart>> multiPartMap = Maps.newHashMap();\n  private final int multiPartSize;\n  private final int multiPartThresholdSize;\n  private final boolean isChecksumEnabled;\n  private final MessageDigest completeMessageDigest;\n  private MessageDigest currentPartMessageDigest;\n\n  private final Counter writeBytes;\n  private final Counter writeOperations;\n\n  private long pos = 0;\n  private boolean closed = false;\n\n  @SuppressWarnings(\"StaticAssignmentInConstructor\")\n  S3OutputStream(\n      S3Client s3, S3URI location, S3FileIOProperties s3FileIOProperties, MetricsContext metrics)\n      throws IOException {\n    if (executorService == null) {\n      synchronized (S3OutputStream.class) {\n        if (executorService == null) {\n          executorService =\n              MoreExecutors.getExitingExecutorService(\n                  (ThreadPoolExecutor)\n                      Executors.newFixedThreadPool(\n                          s3FileIOProperties.multipartUploadThreads(),\n                          new ThreadFactoryBuilder()\n                              .setDaemon(true)\n                              .setNameFormat(\"iceberg-s3fileio-upload-%d\")\n                              .build()));\n        }\n      }\n    }\n\n    this.s3 = s3;\n    this.location = location;\n    this.s3FileIOProperties = s3FileIOProperties;\n    this.writeTags = s3FileIOProperties.writeTags();\n\n    this.createStack = Thread.currentThread().getStackTrace();\n\n    this.multiPartSize = s3FileIOProperties.multiPartSize();\n    this.multiPartThresholdSize =\n        (int) (multiPartSize * s3FileIOProperties.multipartThresholdFactor());\n    this.stagingDirectory = new File(s3FileIOProperties.stagingDirectory());\n    this.isChecksumEnabled = s3FileIOProperties.isChecksumEnabled();\n    try {\n      this.completeMessageDigest =\n          isChecksumEnabled ? MessageDigest.getInstance(digestAlgorithm) : null;\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(\n          \"Failed to create message digest needed for s3 checksum checks\", e);\n    }\n\n    this.writeBytes = metrics.counter(FileIOMetricsContext.WRITE_BYTES, Unit.BYTES);\n    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS);\n\n    newStream();\n  }\n\n  @Override\n  public long getPos() {\n    return pos;\n  }\n\n  @Override\n  public void flush() throws IOException {\n    stream.flush();\n  }\n\n  @Override\n  public void write(int b) throws IOException {\n    if (stream.getCount() >= multiPartSize) {\n      newStream();\n      uploadParts();\n    }\n\n    stream.write(b);\n    pos += 1;\n    writeBytes.increment();\n    writeOperations.increment();\n\n    // switch to multipart upload\n    if (multipartUploadId == null && pos >= multiPartThresholdSize) {\n      initializeMultiPartUpload();\n      uploadParts();\n    }\n  }\n\n  @Override\n  public void write(byte[] b, int off, int len) throws IOException {\n    int remaining = len;\n    int relativeOffset = off;\n\n    // Write the remainder of the part size to the staging file\n    // and continue to write new staging files if the write is\n    // larger than the part size.\n    while (stream.getCount() + remaining > multiPartSize) {\n      int writeSize = multiPartSize - (int) stream.getCount();\n\n      stream.write(b, relativeOffset, writeSize);\n      remaining -= writeSize;\n      relativeOffset += writeSize;\n\n      newStream();\n      uploadParts();\n    }\n\n    stream.write(b, relativeOffset, remaining);\n    pos += len;\n    writeBytes.increment(len);\n    writeOperations.increment();\n\n    // switch to multipart upload\n    if (multipartUploadId == null && pos >= multiPartThresholdSize) {\n      initializeMultiPartUpload();\n      uploadParts();\n    }\n  }\n\n  private void newStream() throws IOException {\n    if (stream != null) {\n      stream.close();\n    }\n\n    createStagingDirectoryIfNotExists();\n    currentStagingFile = File.createTempFile(\"s3fileio-\", \".tmp\", stagingDirectory);\n    currentStagingFile.deleteOnExit();\n    try {\n      currentPartMessageDigest =\n          isChecksumEnabled ? MessageDigest.getInstance(digestAlgorithm) : null;\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(\n          \"Failed to create message digest needed for s3 checksum checks.\", e);\n    }\n\n    stagingFiles.add(new FileAndDigest(currentStagingFile, currentPartMessageDigest));\n\n    if (isChecksumEnabled) {\n      DigestOutputStream digestOutputStream;\n\n      // if switched over to multipart threshold already, no need to update complete message digest\n      if (multipartUploadId != null) {\n        digestOutputStream =\n            new DigestOutputStream(\n                new BufferedOutputStream(new FileOutputStream(currentStagingFile)),\n                currentPartMessageDigest);\n      } else {\n        digestOutputStream =\n            new DigestOutputStream(\n                new DigestOutputStream(\n                    new BufferedOutputStream(new FileOutputStream(currentStagingFile)),\n                    currentPartMessageDigest),\n                completeMessageDigest);\n      }\n\n      stream = new CountingOutputStream(digestOutputStream);\n    } else {\n      stream =\n          new CountingOutputStream(\n              new BufferedOutputStream(new FileOutputStream(currentStagingFile)));\n    }\n  }\n\n  @Override\n  public void close() throws IOException {\n    if (closed) {\n      return;\n    }\n\n    super.close();\n    closed = true;\n\n    try {\n      stream.close();\n      completeUploads();\n    } finally {\n      cleanUpStagingFiles();\n    }\n  }\n\n  private void initializeMultiPartUpload() {\n    CreateMultipartUploadRequest.Builder requestBuilder =\n        CreateMultipartUploadRequest.builder().bucket(location.bucket()).key(location.key());\n    if (writeTags != null && !writeTags.isEmpty()) {\n      requestBuilder.tagging(Tagging.builder().tagSet(writeTags).build());\n    }\n\n    S3RequestUtil.configureEncryption(s3FileIOProperties, requestBuilder);\n    S3RequestUtil.configurePermission(s3FileIOProperties, requestBuilder);\n\n    multipartUploadId = s3.createMultipartUpload(requestBuilder.build()).uploadId();\n  }\n\n  @SuppressWarnings(\"checkstyle:LocalVariableName\")\n  private void uploadParts() {\n    // exit if multipart has not been initiated\n    if (multipartUploadId == null) {\n      return;\n    }\n\n    stagingFiles.stream()\n        // do not upload the file currently being written\n        .filter(f -> closed || !f.file().equals(currentStagingFile))\n        // do not upload any files that have already been processed\n        .filter(Predicates.not(f -> multiPartMap.containsKey(f.file())))\n        .forEach(\n            fileAndDigest -> {\n              File f = fileAndDigest.file();\n              UploadPartRequest.Builder requestBuilder =\n                  UploadPartRequest.builder()\n                      .bucket(location.bucket())\n                      .key(location.key())\n                      .uploadId(multipartUploadId)\n                      .partNumber(stagingFiles.indexOf(fileAndDigest) + 1)\n                      .contentLength(f.length());\n\n              if (fileAndDigest.hasDigest()) {\n                requestBuilder.contentMD5(BinaryUtils.toBase64(fileAndDigest.digest()));\n              }\n\n              S3RequestUtil.configureEncryption(s3FileIOProperties, requestBuilder);\n\n              UploadPartRequest uploadRequest = requestBuilder.build();\n\n              CompletableFuture<CompletedPart> future =\n                  CompletableFuture.supplyAsync(\n                          () -> {\n                            UploadPartResponse response =\n                                s3.uploadPart(uploadRequest, RequestBody.fromFile(f));\n                            return CompletedPart.builder()\n                                .eTag(response.eTag())\n                                .partNumber(uploadRequest.partNumber())\n                                .build();\n                          },\n                          executorService)\n                      .whenComplete(\n                          (result, thrown) -> {\n                            try {\n                              Files.deleteIfExists(f.toPath());\n                            } catch (IOException e) {\n                              LOG.warn(\"Failed to delete staging file: {}\", f, e);\n                            }\n\n                            if (thrown != null) {\n                              // Exception observed here will be thrown as part of\n                              // CompletionException\n                              // when we will join completable futures.\n                              LOG.error(\"Failed to upload part: {}\", uploadRequest, thrown);\n                            }\n                          });\n\n              multiPartMap.put(f, future);\n            });\n  }\n\n  private void completeMultiPartUpload() {\n    Preconditions.checkState(closed, \"Complete upload called on open stream: \" + location);\n\n    List<CompletedPart> completedParts;\n    try {\n      completedParts =\n          multiPartMap.values().stream()\n              .map(CompletableFuture::join)\n              .sorted(Comparator.comparing(CompletedPart::partNumber))\n              .collect(Collectors.toList());\n    } catch (CompletionException ce) {\n      // cancel the remaining futures.\n      multiPartMap.values().forEach(c -> c.cancel(true));\n      abortUpload();\n      throw ce;\n    }\n\n    CompleteMultipartUploadRequest request =\n        CompleteMultipartUploadRequest.builder()\n            .bucket(location.bucket())\n            .key(location.key())\n            .uploadId(multipartUploadId)\n            .multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build())\n            .build();\n\n    Tasks.foreach(request)\n        .noRetry()\n        .onFailure(\n            (r, thrown) -> {\n              LOG.error(\"Failed to complete multipart upload request: {}\", r, thrown);\n              abortUpload();\n            })\n        .throwFailureWhenFinished()\n        .run(s3::completeMultipartUpload);\n  }\n\n  private void abortUpload() {\n    if (multipartUploadId != null) {\n      try {\n        s3.abortMultipartUpload(\n            AbortMultipartUploadRequest.builder()\n                .bucket(location.bucket())\n                .key(location.key())\n                .uploadId(multipartUploadId)\n                .build());\n      } finally {\n        cleanUpStagingFiles();\n      }\n    }\n  }\n\n  private void cleanUpStagingFiles() {\n    Tasks.foreach(stagingFiles.stream().map(FileAndDigest::file))\n        .suppressFailureWhenFinished()\n        .onFailure((file, thrown) -> LOG.warn(\"Failed to delete staging file: {}\", file, thrown))\n        .run(File::delete);\n  }\n\n  private void completeUploads() {\n    if (multipartUploadId == null) {\n      long contentLength =\n          stagingFiles.stream().map(FileAndDigest::file).mapToLong(File::length).sum();\n      ContentStreamProvider contentProvider =\n          () ->\n              new BufferedInputStream(\n                  stagingFiles.stream()\n                      .map(FileAndDigest::file)\n                      .map(S3OutputStream::uncheckedInputStream)\n                      .reduce(SequenceInputStream::new)\n                      .orElseGet(() -> new ByteArrayInputStream(new byte[0])));\n\n      PutObjectRequest.Builder requestBuilder =\n          PutObjectRequest.builder().bucket(location.bucket()).key(location.key());\n\n      if (writeTags != null && !writeTags.isEmpty()) {\n        requestBuilder.tagging(Tagging.builder().tagSet(writeTags).build());\n      }\n\n      if (isChecksumEnabled) {\n        requestBuilder.contentMD5(BinaryUtils.toBase64(completeMessageDigest.digest()));\n      }\n\n      S3RequestUtil.configureEncryption(s3FileIOProperties, requestBuilder);\n      S3RequestUtil.configurePermission(s3FileIOProperties, requestBuilder);\n\n      s3.putObject(\n          requestBuilder.build(),\n          RequestBody.fromContentProvider(\n              contentProvider, contentLength, Mimetype.MIMETYPE_OCTET_STREAM));\n    } else {\n      uploadParts();\n      completeMultiPartUpload();\n    }\n  }\n\n  private static InputStream uncheckedInputStream(File file) {\n    try {\n      return new FileInputStream(file);\n    } catch (IOException e) {\n      throw new UncheckedIOException(e);\n    }\n  }\n\n  private void createStagingDirectoryIfNotExists() throws IOException, SecurityException {\n    if (!stagingDirectory.exists()) {\n      LOG.info(\n          \"Staging directory does not exist, trying to create one: {}\",\n          stagingDirectory.getAbsolutePath());\n      boolean createdStagingDirectory = stagingDirectory.mkdirs();\n      if (createdStagingDirectory) {\n        LOG.info(\"Successfully created staging directory: {}\", stagingDirectory.getAbsolutePath());\n      } else {\n        if (stagingDirectory.exists()) {\n          LOG.info(\n              \"Successfully created staging directory by another process: {}\",\n              stagingDirectory.getAbsolutePath());\n        } else {\n          throw new IOException(\n              \"Failed to create staging directory due to some unknown reason: \"\n                  + stagingDirectory.getAbsolutePath());\n        }\n      }\n    }\n  }\n\n  @SuppressWarnings(\"checkstyle:NoFinalizer\")\n  @Override\n  protected void finalize() throws Throwable {\n    super.finalize();\n    if (!closed) {\n      close(); // releasing resources is more important than printing the warning\n      String trace = Joiner.on(\"\\n\\t\").join(Arrays.copyOfRange(createStack, 1, createStack.length));\n      LOG.warn(\"Unclosed output stream created by:\\n\\t{}\", trace);\n    }\n  }\n\n  private static class FileAndDigest {\n    private final File file;\n    private final MessageDigest digest;\n\n    FileAndDigest(File file, MessageDigest digest) {\n      this.file = file;\n      this.digest = digest;\n    }\n\n    File file() {\n      return file;\n    }\n\n    byte[] digest() {\n      return digest.digest();\n    }\n\n    public boolean hasDigest() {\n      return digest != null;\n    }\n  }\n}",
        "exampleID": 1009,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/S3OutputStream.java#L142"
    },
    {
        "url": "dummy",
        "rawCode": "public class HashCrypt {\n\n    private static final String MODULE = HashCrypt.class.getName();\n\n    private static final String PBKDF2_SHA1 = \"PBKDF2-SHA1\";\n    private static final String PBKDF2_SHA256 = \"PBKDF2-SHA256\";\n    private static final String PBKDF2_SHA384 = \"PBKDF2-SHA384\";\n    private static final String PBKDF2_SHA512 = \"PBKDF2-SHA512\";\n    private static final int PBKDF2_ITERATIONS = UtilProperties.getPropertyAsInteger(\"security.properties\",\n            \"password.encrypt.pbkdf2.iterations\", 10000);\n\n    private static final SecureRandom SECURE_RANDOM = new SecureRandom();\n\n    private static final String CRYPT_CHAR_SET = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789./\";\n\n    private static MessageDigest getMessageDigest(String type) {\n        try {\n            return MessageDigest.getInstance(type);\n        } catch (NoSuchAlgorithmException e) {\n            throw new GeneralRuntimeException(\"Could not load digestor(\" + type + \")\", e);\n        }\n    }\n\n    public static boolean comparePassword(String crypted, String defaultCrypt, String password) {\n        if (crypted.startsWith(\"{PBKDF2\")) {\n            return doComparePbkdf2(crypted, password);\n        } else if (crypted.startsWith(\"{\")) {\n            return doCompareTypePrefix(crypted, defaultCrypt, password.getBytes(StandardCharsets.UTF_8));\n        } else if (crypted.startsWith(\"$\")) {\n            return doComparePosix(crypted, defaultCrypt, password.getBytes(StandardCharsets.UTF_8));\n        } else {\n            return doCompareBare(crypted, defaultCrypt, password.getBytes(StandardCharsets.UTF_8));\n        }\n    }\n\n    private static boolean doCompareTypePrefix(String crypted, String defaultCrypt, byte[] bytes) {\n        int typeEnd = crypted.indexOf(\"}\");\n        String hashType = crypted.substring(1, typeEnd);\n        String hashed = crypted.substring(typeEnd + 1);\n        MessageDigest messagedigest = getMessageDigest(hashType);\n        messagedigest.update(bytes);\n        byte[] digestBytes = messagedigest.digest();\n        char[] digestChars = Hex.encodeHex(digestBytes);\n        String checkCrypted = new String(digestChars);\n        if (hashed.equals(checkCrypted)) {\n            return true;\n        }\n        // This next block should be removed when all {prefix}oldFunnyHex are fixed.\n        if (hashed.equals(oldFunnyHex(digestBytes))) {\n            Debug.logWarning(\"Warning: detected oldFunnyHex password prefixed with a hashType; this is not valid, please update the value \"\n                    + \"in the database with ({%s}%s)\", MODULE, hashType, checkCrypted);\n            return true;\n        }\n        return false;\n    }\n\n    private static boolean doComparePosix(String crypted, String defaultCrypt, byte[] bytes) {\n        int typeEnd = crypted.indexOf(\"$\", 1);\n        int saltEnd = crypted.indexOf(\"$\", typeEnd + 1);\n        String hashType = crypted.substring(1, typeEnd);\n        String salt = crypted.substring(typeEnd + 1, saltEnd);\n        String hashed = crypted.substring(saltEnd + 1);\n        return hashed.equals(getCryptedBytes(hashType, salt, bytes));\n    }\n\n    private static boolean doCompareBare(String crypted, String defaultCrypt, byte[] bytes) {\n        String hashType = defaultCrypt;\n        String hashed = crypted;\n        MessageDigest messagedigest = getMessageDigest(hashType);\n        messagedigest.update(bytes);\n        return hashed.equals(oldFunnyHex(messagedigest.digest()));\n    }\n\n    /*\n     * @deprecated use cryptBytes(hashType, salt, password); eventually, use\n     * cryptUTF8(hashType, salt, password) after all existing installs are\n     * salt-based.  If the call-site of cryptPassword is just used to create a *new*\n     * value, then you can switch to cryptUTF8 directly.\n     */\n    @Deprecated\n    public static String cryptPassword(String hashType, String salt, String password) {\n        if (hashType.startsWith(\"PBKDF2\")) {\n            return password != null ? pbkdf2HashCrypt(hashType, salt, password) : null;\n        }\n        return password != null ? cryptBytes(hashType, salt, password.getBytes(StandardCharsets.UTF_8)) : null;\n    }\n\n    public static String cryptUTF8(String hashType, String salt, String value) {\n        if (hashType.startsWith(\"PBKDF2\")) {\n            return value != null ? pbkdf2HashCrypt(hashType, salt, value) : null;\n        }\n        return value != null ? cryptBytes(hashType, salt, value.getBytes(StandardCharsets.UTF_8)) : null;\n    }\n\n    public static String cryptValue(String hashType, String salt, String value) {\n        if (hashType.startsWith(\"PBKDF2\")) {\n            return value != null ? pbkdf2HashCrypt(hashType, salt, value) : null;\n        }\n        return value != null ? cryptBytes(hashType, salt, value.getBytes(StandardCharsets.UTF_8)) : null;\n    }\n\n    public static String cryptBytes(String hashType, String salt, byte[] bytes) {\n        if (hashType == null) {\n            hashType = \"SHA\";\n        }\n        if (salt == null) {\n            salt = RandomStringUtils.random(SECURE_RANDOM.nextInt(15) + 1, CRYPT_CHAR_SET);\n        }\n        StringBuilder sb = new StringBuilder();\n        sb.append(\"$\").append(hashType).append(\"$\").append(salt).append(\"$\");\n        sb.append(getCryptedBytes(hashType, salt, bytes));\n        return sb.toString();\n    }\n\n    private static String getCryptedBytes(String hashType, String salt, byte[] bytes) {\n        try {\n            MessageDigest messagedigest = MessageDigest.getInstance(hashType);\n            messagedigest.update(salt.getBytes(StandardCharsets.UTF_8));\n            messagedigest.update(bytes);\n            return Base64.encodeBase64URLSafeString(messagedigest.digest()).replace('+', '.');\n        } catch (NoSuchAlgorithmException e) {\n            throw new GeneralRuntimeException(\"Error while comparing password\", e);\n        }\n    }\n\n    public static String pbkdf2HashCrypt(String hashType, String salt, String value) {\n        char[] chars = value.toCharArray();\n        if (UtilValidate.isEmpty(salt)) {\n            salt = getSalt();\n        }\n        try {\n            PBEKeySpec spec = new PBEKeySpec(chars, salt.getBytes(StandardCharsets.UTF_8), PBKDF2_ITERATIONS, 64 * 4);\n            SecretKeyFactory skf = SecretKeyFactory.getInstance(hashType);\n            byte[] hash = Base64.encodeBase64(skf.generateSecret(spec).getEncoded());\n            String pbkdf2Type = null;\n            switch (hashType) {\n            case \"PBKDF2WithHmacSHA1\":\n                pbkdf2Type = PBKDF2_SHA1;\n                break;\n            case \"PBKDF2WithHmacSHA256\":\n                pbkdf2Type = PBKDF2_SHA256;\n                break;\n            case \"PBKDF2WithHmacSHA384\":\n                pbkdf2Type = PBKDF2_SHA384;\n                break;\n            case \"PBKDF2WithHmacSHA512\":\n                pbkdf2Type = PBKDF2_SHA512;\n                break;\n            default:\n                pbkdf2Type = PBKDF2_SHA1;\n            }\n            StringBuilder sb = new StringBuilder();\n            sb.append(\"{\").append(pbkdf2Type).append(\"}\");\n            sb.append(PBKDF2_ITERATIONS).append(\"$\");\n            sb.append(java.util.Base64.getMimeEncoder().encodeToString(salt.getBytes(StandardCharsets.UTF_8))).append(\"$\");\n            sb.append(new String(hash));\n            return sb.toString();\n        } catch (InvalidKeySpecException e) {\n            throw new GeneralRuntimeException(\"Error while creating SecretKey\", e);\n        } catch (NoSuchAlgorithmException e) {\n            throw new GeneralRuntimeException(\"Error while computing SecretKeyFactory\", e);\n        }\n    }\n\n    public static boolean doComparePbkdf2(String crypted, String password) {\n        try {\n            int typeEnd = crypted.indexOf(\"}\");\n            String hashType = crypted.substring(1, typeEnd);\n            String[] parts = crypted.split(\"\\\\$\");\n            int iterations = Integer.parseInt(parts[0].substring(typeEnd + 1));\n            byte[] salt = Arrays.toString(java.util.Base64.getMimeDecoder().decode(parts[1].getBytes(StandardCharsets.UTF_8)))\n                    .getBytes(StandardCharsets.UTF_8);\n            byte[] hash = Base64.decodeBase64(parts[2].getBytes(StandardCharsets.UTF_8));\n\n            PBEKeySpec spec = new PBEKeySpec(password.toCharArray(), salt, iterations, hash.length * 8);\n            switch (hashType.substring(hashType.indexOf(\"-\") + 1)) {\n            case \"SHA256\":\n                hashType = \"PBKDF2WithHmacSHA256\";\n                break;\n            case \"SHA384\":\n                hashType = \"PBKDF2WithHmacSHA384\";\n                break;\n            case \"SHA512\":\n                hashType = \"PBKDF2WithHmacSHA512\";\n                break;\n            default:\n                hashType = \"PBKDF2WithHmacSHA1\";\n            }\n            SecretKeyFactory skf = SecretKeyFactory.getInstance(hashType);\n            byte[] testHash = skf.generateSecret(spec).getEncoded();\n            int diff = hash.length ^ testHash.length;\n\n            for (int i = 0; i < hash.length && i < testHash.length; i++) {\n                diff |= hash[i] ^ testHash[i];\n            }\n\n            return diff == 0;\n        } catch (NoSuchAlgorithmException e) {\n            throw new GeneralRuntimeException(\"Error while computing SecretKeyFactory\", e);\n        } catch (InvalidKeySpecException e) {\n            throw new GeneralRuntimeException(\"Error while creating SecretKey\", e);\n        }\n    }\n\n    private static String getSalt() {\n        try {\n            SecureRandom sr = SecureRandom.getInstance(\"SHA1PRNG\");\n            byte[] salt = new byte[16];\n            sr.nextBytes(salt);\n            return Arrays.toString(salt);\n        } catch (NoSuchAlgorithmException e) {\n            throw new GeneralRuntimeException(\"Error while creating salt\", e);\n        }\n    }\n\n    public static String digestHash(String hashType, String code, String str) {\n        if (str == null) {\n            return null;\n        }\n        byte[] codeBytes;\n        try {\n            if (code == null) {\n                codeBytes = str.getBytes(StandardCharsets.UTF_8);\n            } else {\n                codeBytes = str.getBytes(code);\n            }\n        } catch (UnsupportedEncodingException e) {\n            throw new GeneralRuntimeException(\"Error while computing hash of type \" + hashType, e);\n        }\n        return digestHash(hashType, codeBytes);\n    }\n\n    public static String digestHash(String hashType, byte[] bytes) {\n        try {\n            MessageDigest messagedigest = MessageDigest.getInstance(hashType);\n            messagedigest.update(bytes);\n            byte[] digestBytes = messagedigest.digest();\n            char[] digestChars = Hex.encodeHex(digestBytes);\n\n            StringBuilder sb = new StringBuilder();\n            sb.append(\"{\").append(hashType).append(\"}\");\n            sb.append(digestChars, 0, digestChars.length);\n            return sb.toString();\n        } catch (NoSuchAlgorithmException e) {\n            throw new GeneralRuntimeException(\"Error while computing hash of type \" + hashType, e);\n        }\n    }\n\n    public static String digestHash64(String hashType, byte[] bytes) {\n        if (hashType == null) {\n            hashType = \"SHA\";\n        }\n        try {\n            MessageDigest messagedigest = MessageDigest.getInstance(hashType);\n            messagedigest.update(bytes);\n            byte[] digestBytes = messagedigest.digest();\n\n            StringBuilder sb = new StringBuilder();\n            sb.append(\"{\").append(hashType).append(\"}\");\n            sb.append(Base64.encodeBase64URLSafeString(digestBytes).replace('+', '.'));\n            return sb.toString();\n        } catch (NoSuchAlgorithmException e) {\n            throw new GeneralRuntimeException(\"Error while computing hash of type \" + hashType, e);\n        }\n    }\n\n    /**\n     * @deprecated use cryptPassword\n     */\n    @Deprecated\n    public static String getHashTypeFromPrefix(String hashString) {\n        if (UtilValidate.isEmpty(hashString) || hashString.charAt(0) != '{') {\n            return null;\n        }\n\n        return hashString.substring(1, hashString.indexOf('}'));\n    }\n\n    /**\n     * @deprecated use cryptPassword\n     */\n    @Deprecated\n    public static String removeHashTypePrefix(String hashString) {\n        if (UtilValidate.isEmpty(hashString) || hashString.charAt(0) != '{') {\n            return hashString;\n        }\n\n        return hashString.substring(hashString.indexOf('}') + 1);\n    }\n\n    /**\n     * @deprecated use digestHashOldFunnyHex(hashType, str)\n     */\n    @Deprecated\n    public static String getDigestHashOldFunnyHexEncode(String str, String hashType) {\n        return digestHashOldFunnyHex(hashType, str);\n    }\n\n    public static String digestHashOldFunnyHex(String hashType, String str) {\n        if (UtilValidate.isEmpty(hashType)) {\n            hashType = \"SHA\";\n        }\n        if (str == null) {\n            return null;\n        }\n        try {\n            MessageDigest messagedigest = MessageDigest.getInstance(hashType);\n            byte[] strBytes = str.getBytes(StandardCharsets.UTF_8);\n\n            messagedigest.update(strBytes);\n            return oldFunnyHex(messagedigest.digest());\n        } catch (Exception e) {\n            Debug.logError(e, \"Error while computing hash of type \" + hashType, MODULE);\n        }\n        return str;\n    }\n\n    // This next block should be removed when all {prefix}oldFunnyHex are fixed.\n    private static String oldFunnyHex(byte[] bytes) {\n        int k = 0;\n        char[] digestChars = new char[bytes.length * 2];\n        for (byte b : bytes) {\n            int i1 = b;\n\n            if (i1 < 0) {\n                i1 = 127 + i1 * -1;\n            }\n            StringUtil.encodeInt(i1, k, digestChars);\n            k += 2;\n        }\n        return new String(digestChars);\n    }\n}",
        "exampleID": 10,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/HashCrypt.java#L233"
    },
    {
        "url": "dummy",
        "rawCode": "public final class Encryption {\n\n  private static final Logger LOG = LoggerFactory.getLogger(Encryption.class);\n\n  /**\n   * Configuration key for globally enable / disable column family encryption\n   */\n  public static final String CRYPTO_ENABLED_CONF_KEY = \"hbase.crypto.enabled\";\n\n  /**\n   * Default value for globally enable / disable column family encryption (set to \"true\" for\n   * backward compatibility)\n   */\n  public static final boolean CRYPTO_ENABLED_CONF_DEFAULT = true;\n\n  /**\n   * Configuration key for the hash algorithm used for generating key hash in encrypted HFiles. This\n   * is a MessageDigest algorithm identifier string, like \"MD5\", \"SHA-256\" or \"SHA-384\". (default:\n   * \"MD5\" for backward compatibility reasons)\n   */\n  public static final String CRYPTO_KEY_HASH_ALGORITHM_CONF_KEY = \"hbase.crypto.key.hash.algorithm\";\n\n  /**\n   * Default hash algorithm used for generating key hash in encrypted HFiles. (we use \"MD5\" for\n   * backward compatibility reasons)\n   */\n  public static final String CRYPTO_KEY_HASH_ALGORITHM_CONF_DEFAULT = \"MD5\";\n\n  /**\n   * Configuration key for specifying the behaviour if the configured hash algorithm differs from\n   * the one used for generating key hash in encrypted HFiles currently being read. - \"false\"\n   * (default): we won't fail but use the hash algorithm stored in the HFile - \"true\": we throw an\n   * exception (this can be useful if regulations are enforcing the usage of certain algorithms,\n   * e.g. on FIPS compliant clusters)\n   */\n  public static final String CRYPTO_KEY_FAIL_ON_ALGORITHM_MISMATCH_CONF_KEY =\n    \"hbase.crypto.key.hash.algorithm.failOnMismatch\";\n\n  /**\n   * Default behaviour is not to fail if the hash algorithm configured differs from the one used in\n   * the HFile. (this is the more fail-safe approach, allowing us to read encrypted HFiles written\n   * using a different encryption key hash algorithm)\n   */\n  public static final boolean CRYPTO_KEY_FAIL_ON_ALGORITHM_MISMATCH_CONF_DEFAULT = false;\n\n  /**\n   * Crypto context\n   */\n  @InterfaceAudience.Public\n  public static class Context extends org.apache.hadoop.hbase.io.crypto.Context {\n\n    /** The null crypto context */\n    public static final Context NONE = new Context();\n\n    private Context() {\n      super();\n    }\n\n    private Context(Configuration conf) {\n      super(conf);\n    }\n\n    @Override\n    public Context setCipher(Cipher cipher) {\n      super.setCipher(cipher);\n      return this;\n    }\n\n    @Override\n    public Context setKey(Key key) {\n      super.setKey(key);\n      return this;\n    }\n\n    public Context setKey(byte[] key) {\n      super.setKey(new SecretKeySpec(key, getCipher().getName()));\n      return this;\n    }\n  }\n\n  public static Context newContext() {\n    return new Context();\n  }\n\n  public static Context newContext(Configuration conf) {\n    return new Context(conf);\n  }\n\n  // Prevent instantiation\n  private Encryption() {\n    super();\n  }\n\n  /**\n   * Returns true if the column family encryption feature is enabled globally.\n   */\n  public static boolean isEncryptionEnabled(Configuration conf) {\n    return conf.getBoolean(CRYPTO_ENABLED_CONF_KEY, CRYPTO_ENABLED_CONF_DEFAULT);\n  }\n\n  /**\n   * Get an cipher given a name\n   * @param name the cipher name\n   * @return the cipher, or null if a suitable one could not be found\n   */\n  public static Cipher getCipher(Configuration conf, String name) {\n    return getCipherProvider(conf).getCipher(name);\n  }\n\n  /**\n   * Get names of supported encryption algorithms\n   * @return Array of strings, each represents a supported encryption algorithm\n   */\n  public static String[] getSupportedCiphers() {\n    return getSupportedCiphers(HBaseConfiguration.create());\n  }\n\n  /**\n   * Get names of supported encryption algorithms\n   * @return Array of strings, each represents a supported encryption algorithm\n   */\n  public static String[] getSupportedCiphers(Configuration conf) {\n    return getCipherProvider(conf).getSupportedCiphers();\n  }\n\n  /**\n   * Returns the Hash Algorithm defined in the crypto configuration.\n   */\n  public static String getConfiguredHashAlgorithm(Configuration conf) {\n    return conf.getTrimmed(CRYPTO_KEY_HASH_ALGORITHM_CONF_KEY,\n      CRYPTO_KEY_HASH_ALGORITHM_CONF_DEFAULT);\n  }\n\n  /**\n   * Returns the Hash Algorithm mismatch behaviour defined in the crypto configuration.\n   */\n  public static boolean failOnHashAlgorithmMismatch(Configuration conf) {\n    return conf.getBoolean(CRYPTO_KEY_FAIL_ON_ALGORITHM_MISMATCH_CONF_KEY,\n      CRYPTO_KEY_FAIL_ON_ALGORITHM_MISMATCH_CONF_DEFAULT);\n  }\n\n  /**\n   * Returns the hash of the supplied argument, using the hash algorithm specified in the given\n   * config.\n   */\n  public static byte[] computeCryptoKeyHash(Configuration conf, byte[] arg) {\n    String algorithm = getConfiguredHashAlgorithm(conf);\n    try {\n      return hashWithAlg(algorithm, arg);\n    } catch (RuntimeException e) {\n      String message = format(\n        \"Error in computeCryptoKeyHash (please check your configuration \"\n          + \"parameter %s and the security provider configuration of the JVM)\",\n        CRYPTO_KEY_HASH_ALGORITHM_CONF_KEY);\n      throw new RuntimeException(message, e);\n    }\n  }\n\n  /**\n   * Return the MD5 digest of the concatenation of the supplied arguments.\n   */\n  public static byte[] hash128(String... args) {\n    return hashWithAlg(\"MD5\", Bytes.toByteArrays(args));\n  }\n\n  /**\n   * Return the MD5 digest of the concatenation of the supplied arguments.\n   */\n  public static byte[] hash128(byte[]... args) {\n    return hashWithAlg(\"MD5\", args);\n  }\n\n  /**\n   * Return the SHA-256 digest of the concatenation of the supplied arguments.\n   */\n  public static byte[] hash256(String... args) {\n    return hashWithAlg(\"SHA-256\", Bytes.toByteArrays(args));\n  }\n\n  /**\n   * Return the SHA-256 digest of the concatenation of the supplied arguments.\n   */\n  public static byte[] hash256(byte[]... args) {\n    return hashWithAlg(\"SHA-256\", args);\n  }\n\n  /**\n   * Return a 128 bit key derived from the concatenation of the supplied arguments using\n   * PBKDF2WithHmacSHA1 at 10,000 iterations.\n   */\n  public static byte[] pbkdf128(String... args) {\n    StringBuilder sb = new StringBuilder();\n    for (String s : args) {\n      sb.append(s);\n    }\n    return generateSecretKey(\"PBKDF2WithHmacSHA1\", AES.KEY_LENGTH, sb.toString().toCharArray());\n  }\n\n  /**\n   * Return a 128 bit key derived from the concatenation of the supplied arguments using\n   * PBKDF2WithHmacSHA1 at 10,000 iterations.\n   */\n  public static byte[] pbkdf128(byte[]... args) {\n    StringBuilder sb = new StringBuilder();\n    for (byte[] b : args) {\n      sb.append(Arrays.toString(b));\n    }\n    return generateSecretKey(\"PBKDF2WithHmacSHA1\", AES.KEY_LENGTH, sb.toString().toCharArray());\n  }\n\n  /**\n   * Return a key derived from the concatenation of the supplied arguments using\n   * PBKDF2WithHmacSHA384 key derivation algorithm at 10,000 iterations. The length of the returned\n   * key is determined based on the need of the cypher algorithm. E.g. for the default \"AES\" we will\n   * need a 128 bit long key, while if the user is using a custom cipher, we might generate keys\n   * with other length. This key generation method is used currently e.g. in the HBase Shell\n   * (admin.rb) to generate a column family data encryption key, if the user provided an\n   * ENCRYPTION_KEY parameter.\n   */\n  public static byte[] generateSecretKey(Configuration conf, String cypherAlg, String... args) {\n    StringBuilder sb = new StringBuilder();\n    for (String s : args) {\n      sb.append(s);\n    }\n    int keyLengthBytes = Encryption.getCipher(conf, cypherAlg).getKeyLength();\n    return generateSecretKey(\"PBKDF2WithHmacSHA384\", keyLengthBytes, sb.toString().toCharArray());\n  }\n\n  /**\n   * Return a key derived from the concatenation of the supplied arguments using\n   * PBKDF2WithHmacSHA384 key derivation algorithm at 10,000 iterations. The length of the returned\n   * key is determined based on the need of the cypher algorithm. E.g. for the default \"AES\" we will\n   * need a 128 bit long key, while if the user is using a custom cipher, we might generate keys\n   * with other length. This key generation method is used currently e.g. in the HBase Shell\n   * (admin.rb) to generate a column family data encryption key, if the user provided an\n   * ENCRYPTION_KEY parameter.\n   */\n  public static byte[] generateSecretKey(Configuration conf, String cypherAlg, byte[]... args) {\n    StringBuilder sb = new StringBuilder();\n    for (byte[] b : args) {\n      sb.append(Arrays.toString(b));\n    }\n    int keyLength = Encryption.getCipher(conf, cypherAlg).getKeyLength();\n    return generateSecretKey(\"PBKDF2WithHmacSHA384\", keyLength, sb.toString().toCharArray());\n  }\n\n  /**\n   * Return a key (byte array) derived from the supplied password argument using the given algorithm\n   * with a random salt at 10,000 iterations.\n   * @param algorithm      the secret key generation algorithm to use\n   * @param keyLengthBytes the length of the key to be derived (in bytes, not in bits)\n   * @param password       char array to use as password for the key generation algorithm\n   * @return secret key encoded as a byte array\n   */\n  private static byte[] generateSecretKey(String algorithm, int keyLengthBytes, char[] password) {\n    byte[] salt = new byte[keyLengthBytes];\n    Bytes.secureRandom(salt);\n    PBEKeySpec spec = new PBEKeySpec(password, salt, 10000, keyLengthBytes * 8);\n    try {\n      return SecretKeyFactory.getInstance(algorithm).generateSecret(spec).getEncoded();\n    } catch (NoSuchAlgorithmException | InvalidKeySpecException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  /**\n   * Encrypt a block of plaintext\n   * <p>\n   * The encryptor's state will be finalized. It should be reinitialized or returned to the pool.\n   * @param out ciphertext\n   * @param src plaintext\n   */\n  public static void encrypt(OutputStream out, byte[] src, int offset, int length, Encryptor e)\n    throws IOException {\n    OutputStream cout = e.createEncryptionStream(out);\n    try {\n      cout.write(src, offset, length);\n    } finally {\n      cout.close();\n    }\n  }\n\n  /**\n   * Encrypt a block of plaintext\n   * @param out ciphertext\n   * @param src plaintext\n   */\n  public static void encrypt(OutputStream out, byte[] src, int offset, int length, Context context,\n    byte[] iv) throws IOException {\n    Encryptor e = context.getCipher().getEncryptor();\n    e.setKey(context.getKey());\n    e.setIv(iv); // can be null\n    e.reset();\n    encrypt(out, src, offset, length, e);\n  }\n\n  /**\n   * Encrypt a stream of plaintext given an encryptor\n   * <p>\n   * The encryptor's state will be finalized. It should be reinitialized or returned to the pool.\n   * @param out ciphertext\n   * @param in  plaintext\n   */\n  public static void encrypt(OutputStream out, InputStream in, Encryptor e) throws IOException {\n    OutputStream cout = e.createEncryptionStream(out);\n    try {\n      IOUtils.copy(in, cout);\n    } finally {\n      cout.close();\n    }\n  }\n\n  /**\n   * Encrypt a stream of plaintext given a context and IV\n   * @param out ciphertext\n   * @param in  plaintet\n   */\n  public static void encrypt(OutputStream out, InputStream in, Context context, byte[] iv)\n    throws IOException {\n    Encryptor e = context.getCipher().getEncryptor();\n    e.setKey(context.getKey());\n    e.setIv(iv); // can be null\n    e.reset();\n    encrypt(out, in, e);\n  }\n\n  /**\n   * Decrypt a block of ciphertext read in from a stream with the given cipher and context\n   * <p>\n   * The decryptor's state will be finalized. It should be reinitialized or returned to the pool.\n   */\n  public static void decrypt(byte[] dest, int destOffset, InputStream in, int destSize, Decryptor d)\n    throws IOException {\n    InputStream cin = d.createDecryptionStream(in);\n    try {\n      IOUtils.readFully(cin, dest, destOffset, destSize);\n    } finally {\n      cin.close();\n    }\n  }\n\n  /**\n   * Decrypt a block of ciphertext from a stream given a context and IV\n   */\n  public static void decrypt(byte[] dest, int destOffset, InputStream in, int destSize,\n    Context context, byte[] iv) throws IOException {\n    Decryptor d = context.getCipher().getDecryptor();\n    d.setKey(context.getKey());\n    d.setIv(iv); // can be null\n    decrypt(dest, destOffset, in, destSize, d);\n  }\n\n  /**\n   * Decrypt a stream of ciphertext given a decryptor\n   */\n  public static void decrypt(OutputStream out, InputStream in, int outLen, Decryptor d)\n    throws IOException {\n    InputStream cin = d.createDecryptionStream(in);\n    byte buf[] = new byte[8 * 1024];\n    long remaining = outLen;\n    try {\n      while (remaining > 0) {\n        int toRead = (int) (remaining < buf.length ? remaining : buf.length);\n        int read = cin.read(buf, 0, toRead);\n        if (read < 0) {\n          break;\n        }\n        out.write(buf, 0, read);\n        remaining -= read;\n      }\n    } finally {\n      cin.close();\n    }\n  }\n\n  /**\n   * Decrypt a stream of ciphertext given a context and IV\n   */\n  public static void decrypt(OutputStream out, InputStream in, int outLen, Context context,\n    byte[] iv) throws IOException {\n    Decryptor d = context.getCipher().getDecryptor();\n    d.setKey(context.getKey());\n    d.setIv(iv); // can be null\n    decrypt(out, in, outLen, d);\n  }\n\n  /**\n   * Resolves a key for the given subject\n   * @return a key for the given subject\n   * @throws IOException if the key is not found\n   */\n  public static Key getSecretKeyForSubject(String subject, Configuration conf) throws IOException {\n    KeyProvider provider = getKeyProvider(conf);\n    if (provider != null) {\n      try {\n        Key[] keys = provider.getKeys(new String[] { subject });\n        if (keys != null && keys.length > 0) {\n          return keys[0];\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n    throw new IOException(\"No key found for subject '\" + subject + \"'\");\n  }\n\n  /**\n   * Encrypts a block of plaintext with the symmetric key resolved for the given subject\n   * @param out    ciphertext\n   * @param in     plaintext\n   * @param conf   configuration\n   * @param cipher the encryption algorithm\n   * @param iv     the initialization vector, can be null\n   */\n  public static void encryptWithSubjectKey(OutputStream out, InputStream in, String subject,\n    Configuration conf, Cipher cipher, byte[] iv) throws IOException {\n    Key key = getSecretKeyForSubject(subject, conf);\n    if (key == null) {\n      throw new IOException(\"No key found for subject '\" + subject + \"'\");\n    }\n    Encryptor e = cipher.getEncryptor();\n    e.setKey(key);\n    e.setIv(iv); // can be null\n    encrypt(out, in, e);\n  }\n\n  /**\n   * Decrypts a block of ciphertext with the symmetric key resolved for the given subject\n   * @param out     plaintext\n   * @param in      ciphertext\n   * @param outLen  the expected plaintext length\n   * @param subject the subject's key alias\n   * @param conf    configuration\n   * @param cipher  the encryption algorithm\n   * @param iv      the initialization vector, can be null\n   */\n  public static void decryptWithSubjectKey(OutputStream out, InputStream in, int outLen,\n    String subject, Configuration conf, Cipher cipher, byte[] iv) throws IOException {\n    Key key = getSecretKeyForSubject(subject, conf);\n    if (key == null) {\n      throw new IOException(\"No key found for subject '\" + subject + \"'\");\n    }\n    Decryptor d = cipher.getDecryptor();\n    d.setKey(key);\n    d.setIv(iv); // can be null\n    try {\n      decrypt(out, in, outLen, d);\n    } catch (IOException e) {\n      // If the current cipher algorithm fails to unwrap, try the alternate cipher algorithm, if one\n      // is configured\n      String alternateAlgorithm = conf.get(HConstants.CRYPTO_ALTERNATE_KEY_ALGORITHM_CONF_KEY);\n      if (alternateAlgorithm != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unable to decrypt data with current cipher algorithm '\"\n            + conf.get(HConstants.CRYPTO_KEY_ALGORITHM_CONF_KEY, HConstants.CIPHER_AES)\n            + \"'. Trying with the alternate cipher algorithm '\" + alternateAlgorithm\n            + \"' configured.\");\n        }\n        Cipher alterCipher = Encryption.getCipher(conf, alternateAlgorithm);\n        if (alterCipher == null) {\n          throw new RuntimeException(\"Cipher '\" + alternateAlgorithm + \"' not available\");\n        }\n        d = alterCipher.getDecryptor();\n        d.setKey(key);\n        d.setIv(iv); // can be null\n        decrypt(out, in, outLen, d);\n      } else {\n        throw new IOException(e);\n      }\n    }\n  }\n\n  private static ClassLoader getClassLoaderForClass(Class<?> c) {\n    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n    if (cl == null) {\n      cl = c.getClassLoader();\n    }\n    if (cl == null) {\n      cl = ClassLoader.getSystemClassLoader();\n    }\n    if (cl == null) {\n      throw new RuntimeException(\"A ClassLoader to load the Cipher could not be determined\");\n    }\n    return cl;\n  }\n\n  public static CipherProvider getCipherProvider(Configuration conf) {\n    String providerClassName =\n      conf.get(HConstants.CRYPTO_CIPHERPROVIDER_CONF_KEY, DefaultCipherProvider.class.getName());\n    try {\n      CipherProvider provider = (CipherProvider) ReflectionUtils.newInstance(\n        getClassLoaderForClass(CipherProvider.class).loadClass(providerClassName), conf);\n      return provider;\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  static final Map<Pair<String, String>, KeyProvider> keyProviderCache = new ConcurrentHashMap<>();\n\n  public static KeyProvider getKeyProvider(Configuration conf) {\n    String providerClassName =\n      conf.get(HConstants.CRYPTO_KEYPROVIDER_CONF_KEY, KeyStoreKeyProvider.class.getName());\n    String providerParameters = conf.get(HConstants.CRYPTO_KEYPROVIDER_PARAMETERS_KEY, \"\");\n    try {\n      Pair<String, String> providerCacheKey = new Pair<>(providerClassName, providerParameters);\n      KeyProvider provider = keyProviderCache.get(providerCacheKey);\n      if (provider != null) {\n        return provider;\n      }\n      provider = (KeyProvider) ReflectionUtils\n        .newInstance(getClassLoaderForClass(KeyProvider.class).loadClass(providerClassName), conf);\n      provider.init(providerParameters);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Installed \" + providerClassName + \" into key provider cache\");\n      }\n      keyProviderCache.put(providerCacheKey, provider);\n      return provider;\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  public static void incrementIv(byte[] iv) {\n    incrementIv(iv, 1);\n  }\n\n  public static void incrementIv(byte[] iv, int v) {\n    // v should be > 0\n    int length = iv.length;\n    int sum = 0;\n    for (int i = 0; i < length; i++) {\n      if (v <= 0) {\n        break;\n      }\n      sum = v + (iv[i] & 0xFF);\n      v = sum / 256;\n      iv[i] = (byte) (sum % 256);\n    }\n  }\n\n  /**\n   * Return the hash of the concatenation of the supplied arguments, using the hash algorithm\n   * provided.\n   */\n  public static byte[] hashWithAlg(String algorithm, byte[]... args) {\n    try {\n      MessageDigest md = MessageDigest.getInstance(algorithm);\n      for (byte[] arg : args) {\n        md.update(arg);\n      }\n      return md.digest();\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(\"unable to use hash algorithm: \" + algorithm, e);\n    }\n  }\n\n}",
        "exampleID": 5,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/Encryption.java#L310"
    },
    {
        "url": "dummy",
        "rawCode": "public class PathAssembler {\n    public static final String GRADLE_USER_HOME_STRING = \"GRADLE_USER_HOME\";\n    public static final String PROJECT_STRING = \"PROJECT\";\n\n    private final File gradleUserHome;\n    private final File projectDirectory;\n\n    public PathAssembler(File gradleUserHome, File projectDirectory) {\n        this.gradleUserHome = gradleUserHome;\n        this.projectDirectory = projectDirectory;\n    }\n\n    /**\n     * Determines the local locations for the distribution to use given the supplied configuration.\n     */\n    public LocalDistribution getDistribution(WrapperConfiguration configuration) {\n        String baseName = getDistName(configuration.getDistribution());\n        String distName = removeExtension(baseName);\n        String rootDirName = rootDirName(distName, configuration);\n        File distDir = new File(getBaseDir(configuration.getDistributionBase()), configuration.getDistributionPath() + \"/\" + rootDirName);\n        File distZip = new File(getBaseDir(configuration.getZipBase()), configuration.getZipPath() + \"/\" + rootDirName + \"/\" + baseName);\n        return new LocalDistribution(distDir, distZip);\n    }\n\n    private String rootDirName(String distName, WrapperConfiguration configuration) {\n        String urlHash = getHash(Download.safeUri(configuration.getDistribution()).toASCIIString());\n        return distName + \"/\" + urlHash;\n    }\n\n    /**\n     * This method computes a hash of the provided {@code string}.\n     * <p>\n     * The algorithm in use by this method is as follows:\n     * <ol>\n     *    <li>Compute the MD5 value of {@code string}.</li>\n     *    <li>Truncate leading zeros (i.e., treat the MD5 value as a number).</li>\n     *    <li>Convert to base 36 (the characters {@code 0-9a-z}).</li>\n     * </ol>\n     */\n    private String getHash(String string) {\n        try {\n            MessageDigest messageDigest = MessageDigest.getInstance(\"MD5\");\n            byte[] bytes = string.getBytes();\n            messageDigest.update(bytes);\n            return new BigInteger(1, messageDigest.digest()).toString(36);\n        } catch (Exception e) {\n            throw new RuntimeException(\"Could not hash input string.\", e);\n        }\n    }\n\n    private String removeExtension(String name) {\n        int p = name.lastIndexOf(\".\");\n        if (p < 0) {\n            return name;\n        }\n        return name.substring(0, p);\n    }\n\n    private String getDistName(URI distUrl) {\n        String path = distUrl.getPath();\n        int p = path.lastIndexOf(\"/\");\n        if (p < 0) {\n            return path;\n        }\n        return path.substring(p + 1);\n    }\n\n    private File getBaseDir(String base) {\n        if (base.equals(GRADLE_USER_HOME_STRING)) {\n            return gradleUserHome;\n        } else if (base.equals(PROJECT_STRING)) {\n            return projectDirectory;\n        } else {\n            throw new RuntimeException(\"Base: \" + base + \" is unknown\");\n        }\n    }\n\n    public static class LocalDistribution {\n        private final File distZip;\n        private final File distDir;\n\n        public LocalDistribution(File distDir, File distZip) {\n            this.distDir = distDir;\n            this.distZip = distZip;\n        }\n\n        /**\n         * Returns the location to install the distribution into.\n         */\n        public File getDistributionDir() {\n            return distDir;\n        }\n\n        /**\n         * Returns the location to install the distribution ZIP file to.\n         */\n        public File getZipFile() {\n            return distZip;\n        }\n    }\n}",
        "exampleID": 1012,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/PathAssembler.java#L64"
    },
    {
        "url": "dummy",
        "rawCode": "private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n  private static final String digestAlgorithm = \"MD5\";\n\n  private static volatile ExecutorService executorService;\n\n  private final StackTraceElement[] createStack;\n  private final S3Client s3;\n  private final S3URI location;\n  private final S3FileIOProperties s3FileIOProperties;\n  private final Set<Tag> writeTags;\n\n  private CountingOutputStream stream;\n  private final List<FileAndDigest> stagingFiles = Lists.newArrayList();\n  private final File stagingDirectory;\n  private File currentStagingFile;\n  private String multipartUploadId;\n  private final Map<File, CompletableFuture<CompletedPart>> multiPartMap = Maps.newHashMap();\n  private final int multiPartSize;\n  private final int multiPartThresholdSize;\n  private final boolean isChecksumEnabled;\n  private final MessageDigest completeMessageDigest;\n  private MessageDigest currentPartMessageDigest;\n\n  private final Counter writeBytes;\n  private final Counter writeOperations;\n\n  private long pos = 0;\n  private boolean closed = false;\n\n  @SuppressWarnings(\"StaticAssignmentInConstructor\")\n  S3OutputStream(\n      S3Client s3, S3URI location, S3FileIOProperties s3FileIOProperties, MetricsContext metrics)\n      throws IOException {\n    if (executorService == null) {\n      synchronized (S3OutputStream.class) {\n        if (executorService == null) {\n          executorService =\n              MoreExecutors.getExitingExecutorService(\n                  (ThreadPoolExecutor)\n                      Executors.newFixedThreadPool(\n                          s3FileIOProperties.multipartUploadThreads(),\n                          new ThreadFactoryBuilder()\n                              .setDaemon(true)\n                              .setNameFormat(\"iceberg-s3fileio-upload-%d\")\n                              .build()));\n        }\n      }\n    }\n\n    this.s3 = s3;\n    this.location = location;\n    this.s3FileIOProperties = s3FileIOProperties;\n    this.writeTags = s3FileIOProperties.writeTags();\n\n    this.createStack = Thread.currentThread().getStackTrace();\n\n    this.multiPartSize = s3FileIOProperties.multiPartSize();\n    this.multiPartThresholdSize =\n        (int) (multiPartSize * s3FileIOProperties.multipartThresholdFactor());\n    this.stagingDirectory = new File(s3FileIOProperties.stagingDirectory());\n    this.isChecksumEnabled = s3FileIOProperties.isChecksumEnabled();\n    try {\n      this.completeMessageDigest =\n          isChecksumEnabled ? MessageDigest.getInstance(digestAlgorithm) : null;\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(\n          \"Failed to create message digest needed for s3 checksum checks\", e);\n    }\n\n    this.writeBytes = metrics.counter(FileIOMetricsContext.WRITE_BYTES, Unit.BYTES);\n    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS);\n\n    newStream();\n  }\n\n  @Override\n  public long getPos() {\n    return pos;\n  }\n\n  @Override\n  public void flush() throws IOException {\n    stream.flush();\n  }\n\n  @Override\n  public void write(int b) throws IOException {\n    if (stream.getCount() >= multiPartSize) {\n      newStream();\n      uploadParts();\n    }\n\n    stream.write(b);\n    pos += 1;\n    writeBytes.increment();\n    writeOperations.increment();\n\n    // switch to multipart upload\n    if (multipartUploadId == null && pos >= multiPartThresholdSize) {\n      initializeMultiPartUpload();\n      uploadParts();\n    }\n  }\n\n  @Override\n  public void write(byte[] b, int off, int len) throws IOException {\n    int remaining = len;\n    int relativeOffset = off;\n\n    // Write the remainder of the part size to the staging file\n    // and continue to write new staging files if the write is\n    // larger than the part size.\n    while (stream.getCount() + remaining > multiPartSize) {\n      int writeSize = multiPartSize - (int) stream.getCount();\n\n      stream.write(b, relativeOffset, writeSize);\n      remaining -= writeSize;\n      relativeOffset += writeSize;\n\n      newStream();\n      uploadParts();\n    }\n\n    stream.write(b, relativeOffset, remaining);\n    pos += len;\n    writeBytes.increment(len);\n    writeOperations.increment();\n\n    // switch to multipart upload\n    if (multipartUploadId == null && pos >= multiPartThresholdSize) {\n      initializeMultiPartUpload();\n      uploadParts();\n    }\n  }\n\n  private void newStream() throws IOException {\n    if (stream != null) {\n      stream.close();\n    }\n\n    createStagingDirectoryIfNotExists();\n    currentStagingFile = File.createTempFile(\"s3fileio-\", \".tmp\", stagingDirectory);\n    currentStagingFile.deleteOnExit();\n    try {\n      currentPartMessageDigest =\n          isChecksumEnabled ? MessageDigest.getInstance(digestAlgorithm) : null;\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(\n          \"Failed to create message digest needed for s3 checksum checks.\", e);\n    }\n\n    stagingFiles.add(new FileAndDigest(currentStagingFile, currentPartMessageDigest));\n\n    if (isChecksumEnabled) {\n      DigestOutputStream digestOutputStream;\n\n      // if switched over to multipart threshold already, no need to update complete message digest\n      if (multipartUploadId != null) {\n        digestOutputStream =\n            new DigestOutputStream(\n                new BufferedOutputStream(new FileOutputStream(currentStagingFile)),\n                currentPartMessageDigest);\n      } else {\n        digestOutputStream =\n            new DigestOutputStream(\n                new DigestOutputStream(\n                    new BufferedOutputStream(new FileOutputStream(currentStagingFile)),\n                    currentPartMessageDigest),\n                completeMessageDigest);\n      }\n\n      stream = new CountingOutputStream(digestOutputStream);\n    } else {\n      stream =\n          new CountingOutputStream(\n              new BufferedOutputStream(new FileOutputStream(currentStagingFile)));\n    }\n  }\n\n  @Override\n  public void close() throws IOException {\n    if (closed) {\n      return;\n    }\n\n    super.close();\n    closed = true;\n\n    try {\n      stream.close();\n      completeUploads();\n    } finally {\n      cleanUpStagingFiles();\n    }\n  }\n\n  private void initializeMultiPartUpload() {\n    CreateMultipartUploadRequest.Builder requestBuilder =\n        CreateMultipartUploadRequest.builder().bucket(location.bucket()).key(location.key());\n    if (writeTags != null && !writeTags.isEmpty()) {\n      requestBuilder.tagging(Tagging.builder().tagSet(writeTags).build());\n    }\n\n    S3RequestUtil.configureEncryption(s3FileIOProperties, requestBuilder);\n    S3RequestUtil.configurePermission(s3FileIOProperties, requestBuilder);\n\n    multipartUploadId = s3.createMultipartUpload(requestBuilder.build()).uploadId();\n  }\n\n  @SuppressWarnings(\"checkstyle:LocalVariableName\")\n  private void uploadParts() {\n    // exit if multipart has not been initiated\n    if (multipartUploadId == null) {\n      return;\n    }\n\n    stagingFiles.stream()\n        // do not upload the file currently being written\n        .filter(f -> closed || !f.file().equals(currentStagingFile))\n        // do not upload any files that have already been processed\n        .filter(Predicates.not(f -> multiPartMap.containsKey(f.file())))\n        .forEach(\n            fileAndDigest -> {\n              File f = fileAndDigest.file();\n              UploadPartRequest.Builder requestBuilder =\n                  UploadPartRequest.builder()\n                      .bucket(location.bucket())\n                      .key(location.key())\n                      .uploadId(multipartUploadId)\n                      .partNumber(stagingFiles.indexOf(fileAndDigest) + 1)\n                      .contentLength(f.length());\n\n              if (fileAndDigest.hasDigest()) {\n                requestBuilder.contentMD5(BinaryUtils.toBase64(fileAndDigest.digest()));\n              }\n\n              S3RequestUtil.configureEncryption(s3FileIOProperties, requestBuilder);\n\n              UploadPartRequest uploadRequest = requestBuilder.build();\n\n              CompletableFuture<CompletedPart> future =\n                  CompletableFuture.supplyAsync(\n                          () -> {\n                            UploadPartResponse response =\n                                s3.uploadPart(uploadRequest, RequestBody.fromFile(f));\n                            return CompletedPart.builder()\n                                .eTag(response.eTag())\n                                .partNumber(uploadRequest.partNumber())\n                                .build();\n                          },\n                          executorService)\n                      .whenComplete(\n                          (result, thrown) -> {\n                            try {\n                              Files.deleteIfExists(f.toPath());\n                            } catch (IOException e) {\n                              LOG.warn(\"Failed to delete staging file: {}\", f, e);\n                            }\n\n                            if (thrown != null) {\n                              // Exception observed here will be thrown as part of\n                              // CompletionException\n                              // when we will join completable futures.\n                              LOG.error(\"Failed to upload part: {}\", uploadRequest, thrown);\n                            }\n                          });\n\n              multiPartMap.put(f, future);\n            });\n  }\n\n  private void completeMultiPartUpload() {\n    Preconditions.checkState(closed, \"Complete upload called on open stream: \" + location);\n\n    List<CompletedPart> completedParts;\n    try {\n      completedParts =\n          multiPartMap.values().stream()\n              .map(CompletableFuture::join)\n              .sorted(Comparator.comparing(CompletedPart::partNumber))\n              .collect(Collectors.toList());\n    } catch (CompletionException ce) {\n      // cancel the remaining futures.\n      multiPartMap.values().forEach(c -> c.cancel(true));\n      abortUpload();\n      throw ce;\n    }\n\n    CompleteMultipartUploadRequest request =\n        CompleteMultipartUploadRequest.builder()\n            .bucket(location.bucket())\n            .key(location.key())\n            .uploadId(multipartUploadId)\n            .multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build())\n            .build();\n\n    Tasks.foreach(request)\n        .noRetry()\n        .onFailure(\n            (r, thrown) -> {\n              LOG.error(\"Failed to complete multipart upload request: {}\", r, thrown);\n              abortUpload();\n            })\n        .throwFailureWhenFinished()\n        .run(s3::completeMultipartUpload);\n  }\n\n  private void abortUpload() {\n    if (multipartUploadId != null) {\n      try {\n        s3.abortMultipartUpload(\n            AbortMultipartUploadRequest.builder()\n                .bucket(location.bucket())\n                .key(location.key())\n                .uploadId(multipartUploadId)\n                .build());\n      } finally {\n        cleanUpStagingFiles();\n      }\n    }\n  }\n\n  private void cleanUpStagingFiles() {\n    Tasks.foreach(stagingFiles.stream().map(FileAndDigest::file))\n        .suppressFailureWhenFinished()\n        .onFailure((file, thrown) -> LOG.warn(\"Failed to delete staging file: {}\", file, thrown))\n        .run(File::delete);\n  }\n\n  private void completeUploads() {\n    if (multipartUploadId == null) {\n      long contentLength =\n          stagingFiles.stream().map(FileAndDigest::file).mapToLong(File::length).sum();\n      ContentStreamProvider contentProvider =\n          () ->\n              new BufferedInputStream(\n                  stagingFiles.stream()\n                      .map(FileAndDigest::file)\n                      .map(S3OutputStream::uncheckedInputStream)\n                      .reduce(SequenceInputStream::new)\n                      .orElseGet(() -> new ByteArrayInputStream(new byte[0])));\n\n      PutObjectRequest.Builder requestBuilder =\n          PutObjectRequest.builder().bucket(location.bucket()).key(location.key());\n\n      if (writeTags != null && !writeTags.isEmpty()) {\n        requestBuilder.tagging(Tagging.builder().tagSet(writeTags).build());\n      }\n\n      if (isChecksumEnabled) {\n        requestBuilder.contentMD5(BinaryUtils.toBase64(completeMessageDigest.digest()));\n      }\n\n      S3RequestUtil.configureEncryption(s3FileIOProperties, requestBuilder);\n      S3RequestUtil.configurePermission(s3FileIOProperties, requestBuilder);\n\n      s3.putObject(\n          requestBuilder.build(),\n          RequestBody.fromContentProvider(\n              contentProvider, contentLength, Mimetype.MIMETYPE_OCTET_STREAM));\n    } else {\n      uploadParts();\n      completeMultiPartUpload();\n    }\n  }\n\n  private static InputStream uncheckedInputStream(File file) {\n    try {\n      return new FileInputStream(file);\n    } catch (IOException e) {\n      throw new UncheckedIOException(e);\n    }\n  }\n\n  private void createStagingDirectoryIfNotExists() throws IOException, SecurityException {\n    if (!stagingDirectory.exists()) {\n      LOG.info(\n          \"Staging directory does not exist, trying to create one: {}\",\n          stagingDirectory.getAbsolutePath());\n      boolean createdStagingDirectory = stagingDirectory.mkdirs();\n      if (createdStagingDirectory) {\n        LOG.info(\"Successfully created staging directory: {}\", stagingDirectory.getAbsolutePath());\n      } else {\n        if (stagingDirectory.exists()) {\n          LOG.info(\n              \"Successfully created staging directory by another process: {}\",\n              stagingDirectory.getAbsolutePath());\n        } else {\n          throw new IOException(\n              \"Failed to create staging directory due to some unknown reason: \"\n                  + stagingDirectory.getAbsolutePath());\n        }\n      }\n    }\n  }\n\n  @SuppressWarnings(\"checkstyle:NoFinalizer\")\n  @Override\n  protected void finalize() throws Throwable {\n    super.finalize();\n    if (!closed) {\n      close(); // releasing resources is more important than printing the warning\n      String trace = Joiner.on(\"\\n\\t\").join(Arrays.copyOfRange(createStack, 1, createStack.length));\n      LOG.warn(\"Unclosed output stream created by:\\n\\t{}\", trace);\n    }\n  }\n\n  private static class FileAndDigest {\n    private final File file;\n    private final MessageDigest digest;\n\n    FileAndDigest(File file, MessageDigest digest) {\n      this.file = file;\n      this.digest = digest;\n    }\n\n    File file() {\n      return file;\n    }\n\n    byte[] digest() {\n      return digest.digest();\n    }\n\n    public boolean hasDigest() {\n      return digest != null;\n    }\n  }\n}",
        "exampleID": 9,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/S3OutputStream.java#L224"
    },
    {
        "url": "dummy",
        "rawCode": "public class MD5Hash {\n\n  /**\n   * Given a byte array, returns in MD5 hash as a hex string.\n   * @return SHA1 hash as a 32 character hex string.\n   */\n  public static String getMD5AsHex(byte[] key) {\n    return getMD5AsHex(key, 0, key.length);\n  }\n\n  /**\n   * Given a byte array, returns its MD5 hash as a hex string. Only \"length\" number of bytes\n   * starting at \"offset\" within the byte array are used.\n   * @param key the key to hash (variable length byte array)\n   * @return MD5 hash as a 32 character hex string.\n   */\n  public static String getMD5AsHex(byte[] key, int offset, int length) {\n    try {\n      MessageDigest md = MessageDigest.getInstance(\"MD5\");\n      md.update(key, offset, length);\n      byte[] digest = md.digest();\n      return new String(Hex.encodeHex(digest));\n    } catch (NoSuchAlgorithmException e) {\n      // this should never happen unless the JDK is messed up.\n      throw new RuntimeException(\"Error computing MD5 hash\", e);\n    }\n  }\n}",
        "exampleID": 1,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/MD5Hash.java#L47"
    },
    {
        "url": "dummy",
        "rawCode": "public final class Encryption {\n\n  private static final Logger LOG = LoggerFactory.getLogger(Encryption.class);\n\n  /**\n   * Configuration key for globally enable / disable column family encryption\n   */\n  public static final String CRYPTO_ENABLED_CONF_KEY = \"hbase.crypto.enabled\";\n\n  /**\n   * Default value for globally enable / disable column family encryption (set to \"true\" for\n   * backward compatibility)\n   */\n  public static final boolean CRYPTO_ENABLED_CONF_DEFAULT = true;\n\n  /**\n   * Configuration key for the hash algorithm used for generating key hash in encrypted HFiles. This\n   * is a MessageDigest algorithm identifier string, like \"MD5\", \"SHA-256\" or \"SHA-384\". (default:\n   * \"MD5\" for backward compatibility reasons)\n   */\n  public static final String CRYPTO_KEY_HASH_ALGORITHM_CONF_KEY = \"hbase.crypto.key.hash.algorithm\";\n\n  /**\n   * Default hash algorithm used for generating key hash in encrypted HFiles. (we use \"MD5\" for\n   * backward compatibility reasons)\n   */\n  public static final String CRYPTO_KEY_HASH_ALGORITHM_CONF_DEFAULT = \"MD5\";\n\n  /**\n   * Configuration key for specifying the behaviour if the configured hash algorithm differs from\n   * the one used for generating key hash in encrypted HFiles currently being read. - \"false\"\n   * (default): we won't fail but use the hash algorithm stored in the HFile - \"true\": we throw an\n   * exception (this can be useful if regulations are enforcing the usage of certain algorithms,\n   * e.g. on FIPS compliant clusters)\n   */\n  public static final String CRYPTO_KEY_FAIL_ON_ALGORITHM_MISMATCH_CONF_KEY =\n    \"hbase.crypto.key.hash.algorithm.failOnMismatch\";\n\n  /**\n   * Default behaviour is not to fail if the hash algorithm configured differs from the one used in\n   * the HFile. (this is the more fail-safe approach, allowing us to read encrypted HFiles written\n   * using a different encryption key hash algorithm)\n   */\n  public static final boolean CRYPTO_KEY_FAIL_ON_ALGORITHM_MISMATCH_CONF_DEFAULT = false;\n\n  /**\n   * Crypto context\n   */\n  @InterfaceAudience.Public\n  public static class Context extends org.apache.hadoop.hbase.io.crypto.Context {\n\n    /** The null crypto context */\n    public static final Context NONE = new Context();\n\n    private Context() {\n      super();\n    }\n\n    private Context(Configuration conf) {\n      super(conf);\n    }\n\n    @Override\n    public Context setCipher(Cipher cipher) {\n      super.setCipher(cipher);\n      return this;\n    }\n\n    @Override\n    public Context setKey(Key key) {\n      super.setKey(key);\n      return this;\n    }\n\n    public Context setKey(byte[] key) {\n      super.setKey(new SecretKeySpec(key, getCipher().getName()));\n      return this;\n    }\n  }\n\n  public static Context newContext() {\n    return new Context();\n  }\n\n  public static Context newContext(Configuration conf) {\n    return new Context(conf);\n  }\n\n  // Prevent instantiation\n  private Encryption() {\n    super();\n  }\n\n  /**\n   * Returns true if the column family encryption feature is enabled globally.\n   */\n  public static boolean isEncryptionEnabled(Configuration conf) {\n    return conf.getBoolean(CRYPTO_ENABLED_CONF_KEY, CRYPTO_ENABLED_CONF_DEFAULT);\n  }\n\n  /**\n   * Get an cipher given a name\n   * @param name the cipher name\n   * @return the cipher, or null if a suitable one could not be found\n   */\n  public static Cipher getCipher(Configuration conf, String name) {\n    return getCipherProvider(conf).getCipher(name);\n  }\n\n  /**\n   * Get names of supported encryption algorithms\n   * @return Array of strings, each represents a supported encryption algorithm\n   */\n  public static String[] getSupportedCiphers() {\n    return getSupportedCiphers(HBaseConfiguration.create());\n  }\n\n  /**\n   * Get names of supported encryption algorithms\n   * @return Array of strings, each represents a supported encryption algorithm\n   */\n  public static String[] getSupportedCiphers(Configuration conf) {\n    return getCipherProvider(conf).getSupportedCiphers();\n  }\n\n  /**\n   * Returns the Hash Algorithm defined in the crypto configuration.\n   */\n  public static String getConfiguredHashAlgorithm(Configuration conf) {\n    return conf.getTrimmed(CRYPTO_KEY_HASH_ALGORITHM_CONF_KEY,\n      CRYPTO_KEY_HASH_ALGORITHM_CONF_DEFAULT);\n  }\n\n  /**\n   * Returns the Hash Algorithm mismatch behaviour defined in the crypto configuration.\n   */\n  public static boolean failOnHashAlgorithmMismatch(Configuration conf) {\n    return conf.getBoolean(CRYPTO_KEY_FAIL_ON_ALGORITHM_MISMATCH_CONF_KEY,\n      CRYPTO_KEY_FAIL_ON_ALGORITHM_MISMATCH_CONF_DEFAULT);\n  }\n\n  /**\n   * Returns the hash of the supplied argument, using the hash algorithm specified in the given\n   * config.\n   */\n  public static byte[] computeCryptoKeyHash(Configuration conf, byte[] arg) {\n    String algorithm = getConfiguredHashAlgorithm(conf);\n    try {\n      return hashWithAlg(algorithm, arg);\n    } catch (RuntimeException e) {\n      String message = format(\n        \"Error in computeCryptoKeyHash (please check your configuration \"\n          + \"parameter %s and the security provider configuration of the JVM)\",\n        CRYPTO_KEY_HASH_ALGORITHM_CONF_KEY);\n      throw new RuntimeException(message, e);\n    }\n  }\n\n  /**\n   * Return the MD5 digest of the concatenation of the supplied arguments.\n   */\n  public static byte[] hash128(String... args) {\n    return hashWithAlg(\"MD5\", Bytes.toByteArrays(args));\n  }\n\n  /**\n   * Return the MD5 digest of the concatenation of the supplied arguments.\n   */\n  public static byte[] hash128(byte[]... args) {\n    return hashWithAlg(\"MD5\", args);\n  }\n\n  /**\n   * Return the SHA-256 digest of the concatenation of the supplied arguments.\n   */\n  public static byte[] hash256(String... args) {\n    return hashWithAlg(\"SHA-256\", Bytes.toByteArrays(args));\n  }\n\n  /**\n   * Return the SHA-256 digest of the concatenation of the supplied arguments.\n   */\n  public static byte[] hash256(byte[]... args) {\n    return hashWithAlg(\"SHA-256\", args);\n  }\n\n  /**\n   * Return a 128 bit key derived from the concatenation of the supplied arguments using\n   * PBKDF2WithHmacSHA1 at 10,000 iterations.\n   */\n  public static byte[] pbkdf128(String... args) {\n    StringBuilder sb = new StringBuilder();\n    for (String s : args) {\n      sb.append(s);\n    }\n    return generateSecretKey(\"PBKDF2WithHmacSHA1\", AES.KEY_LENGTH, sb.toString().toCharArray());\n  }\n\n  /**\n   * Return a 128 bit key derived from the concatenation of the supplied arguments using\n   * PBKDF2WithHmacSHA1 at 10,000 iterations.\n   */\n  public static byte[] pbkdf128(byte[]... args) {\n    StringBuilder sb = new StringBuilder();\n    for (byte[] b : args) {\n      sb.append(Arrays.toString(b));\n    }\n    return generateSecretKey(\"PBKDF2WithHmacSHA1\", AES.KEY_LENGTH, sb.toString().toCharArray());\n  }\n\n  /**\n   * Return a key derived from the concatenation of the supplied arguments using\n   * PBKDF2WithHmacSHA384 key derivation algorithm at 10,000 iterations. The length of the returned\n   * key is determined based on the need of the cypher algorithm. E.g. for the default \"AES\" we will\n   * need a 128 bit long key, while if the user is using a custom cipher, we might generate keys\n   * with other length. This key generation method is used currently e.g. in the HBase Shell\n   * (admin.rb) to generate a column family data encryption key, if the user provided an\n   * ENCRYPTION_KEY parameter.\n   */\n  public static byte[] generateSecretKey(Configuration conf, String cypherAlg, String... args) {\n    StringBuilder sb = new StringBuilder();\n    for (String s : args) {\n      sb.append(s);\n    }\n    int keyLengthBytes = Encryption.getCipher(conf, cypherAlg).getKeyLength();\n    return generateSecretKey(\"PBKDF2WithHmacSHA384\", keyLengthBytes, sb.toString().toCharArray());\n  }\n\n  /**\n   * Return a key derived from the concatenation of the supplied arguments using\n   * PBKDF2WithHmacSHA384 key derivation algorithm at 10,000 iterations. The length of the returned\n   * key is determined based on the need of the cypher algorithm. E.g. for the default \"AES\" we will\n   * need a 128 bit long key, while if the user is using a custom cipher, we might generate keys\n   * with other length. This key generation method is used currently e.g. in the HBase Shell\n   * (admin.rb) to generate a column family data encryption key, if the user provided an\n   * ENCRYPTION_KEY parameter.\n   */\n  public static byte[] generateSecretKey(Configuration conf, String cypherAlg, byte[]... args) {\n    StringBuilder sb = new StringBuilder();\n    for (byte[] b : args) {\n      sb.append(Arrays.toString(b));\n    }\n    int keyLength = Encryption.getCipher(conf, cypherAlg).getKeyLength();\n    return generateSecretKey(\"PBKDF2WithHmacSHA384\", keyLength, sb.toString().toCharArray());\n  }\n\n  /**\n   * Return a key (byte array) derived from the supplied password argument using the given algorithm\n   * with a random salt at 10,000 iterations.\n   * @param algorithm      the secret key generation algorithm to use\n   * @param keyLengthBytes the length of the key to be derived (in bytes, not in bits)\n   * @param password       char array to use as password for the key generation algorithm\n   * @return secret key encoded as a byte array\n   */\n  private static byte[] generateSecretKey(String algorithm, int keyLengthBytes, char[] password) {\n    byte[] salt = new byte[keyLengthBytes];\n    Bytes.secureRandom(salt);\n    PBEKeySpec spec = new PBEKeySpec(password, salt, 10000, keyLengthBytes * 8);\n    try {\n      return SecretKeyFactory.getInstance(algorithm).generateSecret(spec).getEncoded();\n    } catch (NoSuchAlgorithmException | InvalidKeySpecException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  /**\n   * Encrypt a block of plaintext\n   * <p>\n   * The encryptor's state will be finalized. It should be reinitialized or returned to the pool.\n   * @param out ciphertext\n   * @param src plaintext\n   */\n  public static void encrypt(OutputStream out, byte[] src, int offset, int length, Encryptor e)\n    throws IOException {\n    OutputStream cout = e.createEncryptionStream(out);\n    try {\n      cout.write(src, offset, length);\n    } finally {\n      cout.close();\n    }\n  }\n\n  /**\n   * Encrypt a block of plaintext\n   * @param out ciphertext\n   * @param src plaintext\n   */\n  public static void encrypt(OutputStream out, byte[] src, int offset, int length, Context context,\n    byte[] iv) throws IOException {\n    Encryptor e = context.getCipher().getEncryptor();\n    e.setKey(context.getKey());\n    e.setIv(iv); // can be null\n    e.reset();\n    encrypt(out, src, offset, length, e);\n  }\n\n  /**\n   * Encrypt a stream of plaintext given an encryptor\n   * <p>\n   * The encryptor's state will be finalized. It should be reinitialized or returned to the pool.\n   * @param out ciphertext\n   * @param in  plaintext\n   */\n  public static void encrypt(OutputStream out, InputStream in, Encryptor e) throws IOException {\n    OutputStream cout = e.createEncryptionStream(out);\n    try {\n      IOUtils.copy(in, cout);\n    } finally {\n      cout.close();\n    }\n  }\n\n  /**\n   * Encrypt a stream of plaintext given a context and IV\n   * @param out ciphertext\n   * @param in  plaintet\n   */\n  public static void encrypt(OutputStream out, InputStream in, Context context, byte[] iv)\n    throws IOException {\n    Encryptor e = context.getCipher().getEncryptor();\n    e.setKey(context.getKey());\n    e.setIv(iv); // can be null\n    e.reset();\n    encrypt(out, in, e);\n  }\n\n  /**\n   * Decrypt a block of ciphertext read in from a stream with the given cipher and context\n   * <p>\n   * The decryptor's state will be finalized. It should be reinitialized or returned to the pool.\n   */\n  public static void decrypt(byte[] dest, int destOffset, InputStream in, int destSize, Decryptor d)\n    throws IOException {\n    InputStream cin = d.createDecryptionStream(in);\n    try {\n      IOUtils.readFully(cin, dest, destOffset, destSize);\n    } finally {\n      cin.close();\n    }\n  }\n\n  /**\n   * Decrypt a block of ciphertext from a stream given a context and IV\n   */\n  public static void decrypt(byte[] dest, int destOffset, InputStream in, int destSize,\n    Context context, byte[] iv) throws IOException {\n    Decryptor d = context.getCipher().getDecryptor();\n    d.setKey(context.getKey());\n    d.setIv(iv); // can be null\n    decrypt(dest, destOffset, in, destSize, d);\n  }\n\n  /**\n   * Decrypt a stream of ciphertext given a decryptor\n   */\n  public static void decrypt(OutputStream out, InputStream in, int outLen, Decryptor d)\n    throws IOException {\n    InputStream cin = d.createDecryptionStream(in);\n    byte buf[] = new byte[8 * 1024];\n    long remaining = outLen;\n    try {\n      while (remaining > 0) {\n        int toRead = (int) (remaining < buf.length ? remaining : buf.length);\n        int read = cin.read(buf, 0, toRead);\n        if (read < 0) {\n          break;\n        }\n        out.write(buf, 0, read);\n        remaining -= read;\n      }\n    } finally {\n      cin.close();\n    }\n  }\n\n  /**\n   * Decrypt a stream of ciphertext given a context and IV\n   */\n  public static void decrypt(OutputStream out, InputStream in, int outLen, Context context,\n    byte[] iv) throws IOException {\n    Decryptor d = context.getCipher().getDecryptor();\n    d.setKey(context.getKey());\n    d.setIv(iv); // can be null\n    decrypt(out, in, outLen, d);\n  }\n\n  /**\n   * Resolves a key for the given subject\n   * @return a key for the given subject\n   * @throws IOException if the key is not found\n   */\n  public static Key getSecretKeyForSubject(String subject, Configuration conf) throws IOException {\n    KeyProvider provider = getKeyProvider(conf);\n    if (provider != null) {\n      try {\n        Key[] keys = provider.getKeys(new String[] { subject });\n        if (keys != null && keys.length > 0) {\n          return keys[0];\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n    throw new IOException(\"No key found for subject '\" + subject + \"'\");\n  }\n\n  /**\n   * Encrypts a block of plaintext with the symmetric key resolved for the given subject\n   * @param out    ciphertext\n   * @param in     plaintext\n   * @param conf   configuration\n   * @param cipher the encryption algorithm\n   * @param iv     the initialization vector, can be null\n   */\n  public static void encryptWithSubjectKey(OutputStream out, InputStream in, String subject,\n    Configuration conf, Cipher cipher, byte[] iv) throws IOException {\n    Key key = getSecretKeyForSubject(subject, conf);\n    if (key == null) {\n      throw new IOException(\"No key found for subject '\" + subject + \"'\");\n    }\n    Encryptor e = cipher.getEncryptor();\n    e.setKey(key);\n    e.setIv(iv); // can be null\n    encrypt(out, in, e);\n  }\n\n  /**\n   * Decrypts a block of ciphertext with the symmetric key resolved for the given subject\n   * @param out     plaintext\n   * @param in      ciphertext\n   * @param outLen  the expected plaintext length\n   * @param subject the subject's key alias\n   * @param conf    configuration\n   * @param cipher  the encryption algorithm\n   * @param iv      the initialization vector, can be null\n   */\n  public static void decryptWithSubjectKey(OutputStream out, InputStream in, int outLen,\n    String subject, Configuration conf, Cipher cipher, byte[] iv) throws IOException {\n    Key key = getSecretKeyForSubject(subject, conf);\n    if (key == null) {\n      throw new IOException(\"No key found for subject '\" + subject + \"'\");\n    }\n    Decryptor d = cipher.getDecryptor();\n    d.setKey(key);\n    d.setIv(iv); // can be null\n    try {\n      decrypt(out, in, outLen, d);\n    } catch (IOException e) {\n      // If the current cipher algorithm fails to unwrap, try the alternate cipher algorithm, if one\n      // is configured\n      String alternateAlgorithm = conf.get(HConstants.CRYPTO_ALTERNATE_KEY_ALGORITHM_CONF_KEY);\n      if (alternateAlgorithm != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unable to decrypt data with current cipher algorithm '\"\n            + conf.get(HConstants.CRYPTO_KEY_ALGORITHM_CONF_KEY, HConstants.CIPHER_AES)\n            + \"'. Trying with the alternate cipher algorithm '\" + alternateAlgorithm\n            + \"' configured.\");\n        }\n        Cipher alterCipher = Encryption.getCipher(conf, alternateAlgorithm);\n        if (alterCipher == null) {\n          throw new RuntimeException(\"Cipher '\" + alternateAlgorithm + \"' not available\");\n        }\n        d = alterCipher.getDecryptor();\n        d.setKey(key);\n        d.setIv(iv); // can be null\n        decrypt(out, in, outLen, d);\n      } else {\n        throw new IOException(e);\n      }\n    }\n  }\n\n  private static ClassLoader getClassLoaderForClass(Class<?> c) {\n    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n    if (cl == null) {\n      cl = c.getClassLoader();\n    }\n    if (cl == null) {\n      cl = ClassLoader.getSystemClassLoader();\n    }\n    if (cl == null) {\n      throw new RuntimeException(\"A ClassLoader to load the Cipher could not be determined\");\n    }\n    return cl;\n  }\n\n  public static CipherProvider getCipherProvider(Configuration conf) {\n    String providerClassName =\n      conf.get(HConstants.CRYPTO_CIPHERPROVIDER_CONF_KEY, DefaultCipherProvider.class.getName());\n    try {\n      CipherProvider provider = (CipherProvider) ReflectionUtils.newInstance(\n        getClassLoaderForClass(CipherProvider.class).loadClass(providerClassName), conf);\n      return provider;\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  static final Map<Pair<String, String>, KeyProvider> keyProviderCache = new ConcurrentHashMap<>();\n\n  public static KeyProvider getKeyProvider(Configuration conf) {\n    String providerClassName =\n      conf.get(HConstants.CRYPTO_KEYPROVIDER_CONF_KEY, KeyStoreKeyProvider.class.getName());\n    String providerParameters = conf.get(HConstants.CRYPTO_KEYPROVIDER_PARAMETERS_KEY, \"\");\n    try {\n      Pair<String, String> providerCacheKey = new Pair<>(providerClassName, providerParameters);\n      KeyProvider provider = keyProviderCache.get(providerCacheKey);\n      if (provider != null) {\n        return provider;\n      }\n      provider = (KeyProvider) ReflectionUtils\n        .newInstance(getClassLoaderForClass(KeyProvider.class).loadClass(providerClassName), conf);\n      provider.init(providerParameters);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Installed \" + providerClassName + \" into key provider cache\");\n      }\n      keyProviderCache.put(providerCacheKey, provider);\n      return provider;\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  public static void incrementIv(byte[] iv) {\n    incrementIv(iv, 1);\n  }\n\n  public static void incrementIv(byte[] iv, int v) {\n    // v should be > 0\n    int length = iv.length;\n    int sum = 0;\n    for (int i = 0; i < length; i++) {\n      if (v <= 0) {\n        break;\n      }\n      sum = v + (iv[i] & 0xFF);\n      v = sum / 256;\n      iv[i] = (byte) (sum % 256);\n    }\n  }\n\n  /**\n   * Return the hash of the concatenation of the supplied arguments, using the hash algorithm\n   * provided.\n   */\n  public static byte[] hashWithAlg(String algorithm, byte[]... args) {\n    try {\n      MessageDigest md = MessageDigest.getInstance(algorithm);\n      for (byte[] arg : args) {\n        md.update(arg);\n      }\n      return md.digest();\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(\"unable to use hash algorithm: \" + algorithm, e);\n    }\n  }\n\n}",
        "exampleID": 1015,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/Encryption.java#L598"
    },
    {
        "url": "dummy",
        "rawCode": "public abstract class PersistentIOEngine implements IOEngine {\n  private static final Logger LOG = LoggerFactory.getLogger(PersistentIOEngine.class);\n  private static final DuFileCommand DU = new DuFileCommand(new String[] { \"du\", \"\" });\n  protected final String[] filePaths;\n\n  public PersistentIOEngine(String... filePaths) {\n    this.filePaths = filePaths;\n  }\n\n  /**\n   * Verify cache files's integrity\n   * @param algorithm the backingMap persistence path\n   */\n  protected void verifyFileIntegrity(byte[] persistentChecksum, String algorithm)\n    throws IOException {\n    byte[] calculateChecksum = calculateChecksum(algorithm);\n    if (!Bytes.equals(persistentChecksum, calculateChecksum)) {\n      throw new IOException(\n        \"Mismatch of checksum! The persistent checksum is \" + Bytes.toString(persistentChecksum)\n          + \", but the calculate checksum is \" + Bytes.toString(calculateChecksum));\n    }\n  }\n\n  /**\n   * Using an encryption algorithm to calculate a checksum, the default encryption algorithm is MD5\n   * @return the checksum which is convert to HexString\n   * @throws IOException              something happened like file not exists\n   * @throws NoSuchAlgorithmException no such algorithm\n   */\n  protected byte[] calculateChecksum(String algorithm) {\n    try {\n      StringBuilder sb = new StringBuilder();\n      for (String filePath : filePaths) {\n        File file = new File(filePath);\n        sb.append(filePath);\n        sb.append(getFileSize(filePath));\n        sb.append(file.lastModified());\n      }\n      MessageDigest messageDigest = MessageDigest.getInstance(algorithm);\n      messageDigest.update(Bytes.toBytes(sb.toString()));\n      return messageDigest.digest();\n    } catch (IOException ioex) {\n      LOG.error(\"Calculating checksum failed, because of \", ioex);\n      return new byte[0];\n    } catch (NoSuchAlgorithmException e) {\n      LOG.error(\"No such algorithm : \" + algorithm + \"!\");\n      return new byte[0];\n    }\n  }\n\n  /**\n   * Using Linux command du to get file's real size\n   * @param filePath the file\n   * @return file's real size\n   * @throws IOException something happened like file not exists\n   */\n  private static long getFileSize(String filePath) throws IOException {\n    DU.setExecCommand(filePath);\n    DU.execute();\n    return Long.parseLong(DU.getOutput().split(\"\\t\")[0]);\n  }\n\n  private static class DuFileCommand extends Shell.ShellCommandExecutor {\n    private String[] execCommand;\n\n    DuFileCommand(String[] execString) {\n      super(execString);\n      execCommand = execString;\n    }\n\n    void setExecCommand(String filePath) {\n      this.execCommand[1] = filePath;\n    }\n\n    @Override\n    public String[] getExecString() {\n      return this.execCommand;\n    }\n  }\n}",
        "exampleID": 6,
        "dataset": "crypto",
        "filepath": "/Users/xxx/repos/suppression_interface//GithubExamples/crypto/PersistentIOEngine.java#L73"
    }
]