[
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/analysis/stempel/src/java/org/apache/lucene/analysis/stempel/StempelStemmer.java",
        "rawCode": "/**\n * Copyright 2004 The Apache Software Foundation\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n * \n * http://www.apache.org/licenses/LICENSE-2.0\n * \n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n */\n\npackage org.apache.lucene.analysis.stempel;\n\nimport java.io.BufferedInputStream;\nimport java.io.DataInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.Locale;\n\nimport org.egothor.stemmer.Diff;\nimport org.egothor.stemmer.Trie;\n\n/**\n * <p>\n * Stemmer class is a convenient facade for other stemmer-related classes. The\n * core stemming algorithm and its implementation is taken verbatim from the\n * Egothor project ( <a href=\"http://www.egothor.org\">www.egothor.org </a>).\n * </p>\n * <p>\n * Even though the stemmer tables supplied in the distribution package are built\n * for Polish language, there is nothing language-specific here.\n * </p>\n */\npublic class StempelStemmer {\n  private Trie stemmer = null;\n  private StringBuilder buffer = new StringBuilder();\n\n  /**\n   * Create a Stemmer using selected stemmer table\n   * \n   * @param stemmerTable stemmer table.\n   */\n  public StempelStemmer(InputStream stemmerTable) throws IOException {\n    this(load(stemmerTable));\n  }\n\n  /**\n   * Create a Stemmer using pre-loaded stemmer table\n   * \n   * @param stemmer pre-loaded stemmer table\n   */\n  public StempelStemmer(Trie stemmer) {\n    this.stemmer = stemmer;\n  }\n  \n  /**\n   * Load a stemmer table from an inputstream.\n   */\n  public static Trie load(InputStream stemmerTable) throws IOException {\n    DataInputStream in = null;\n    try {\n      in = new DataInputStream(new BufferedInputStream(stemmerTable));\n      String method = in.readUTF().toUpperCase(Locale.ROOT);\n      if (method.indexOf('M') < 0) {\n        return new org.egothor.stemmer.Trie(in);\n      } else {\n        return new org.egothor.stemmer.MultiTrie2(in);\n      }\n    } finally {\n      in.close();\n    }\n  }\n\n  /**\n   * Stem a word. \n   * \n   * @param word input word to be stemmed.\n   * @return stemmed word, or null if the stem could not be generated.\n   */\n  public StringBuilder stem(CharSequence word) {\n    CharSequence cmd = stemmer.getLastOnPath(word);\n    \n    if (cmd == null)\n        return null;\n    \n    buffer.setLength(0);\n    buffer.append(word);\n\n    Diff.apply(buffer, cmd);\n    \n    if (buffer.length() > 0)\n      return buffer;\n    else\n      return null;\n  }\n}\n",
        "methodName": "load",
        "exampleID": 0,
        "dataset": "spotbugs",
        "filepath": "/lucene/analysis/stempel/src/java/org/apache/lucene/analysis/stempel/StempelStemmer.java",
        "line": "67",
        "source": "in",
        "sourceLine": "75",
        "qualifier": "Possible null pointer dereference of $$in/$",
        "steps": [
            {
                "exampleID": 1
            }
        ],
        "line_number": "75"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractReuters.java",
        "rawCode": "package org.apache.lucene.benchmark.utils;\n/**\n * Copyright 2005 The Apache Software Foundation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileFilter;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.FileReader;\nimport java.io.FileWriter;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.io.OutputStreamWriter;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport org.apache.lucene.util.IOUtils;\n\n\n/**\n * Split the Reuters SGML documents into Simple Text files containing: Title, Date, Dateline, Body\n */\npublic class ExtractReuters {\n  private File reutersDir;\n  private File outputDir;\n  private static final String LINE_SEPARATOR = System.getProperty(\"line.separator\");\n\n  public ExtractReuters(File reutersDir, File outputDir) {\n    this.reutersDir = reutersDir;\n    this.outputDir = outputDir;\n    System.out.println(\"Deleting all files in \" + outputDir);\n    for (File f : outputDir.listFiles()) {\n      f.delete();\n    }\n  }\n\n  public void extract() {\n    File[] sgmFiles = reutersDir.listFiles(new FileFilter() {\n      @Override\n      public boolean accept(File file) {\n        return file.getName().endsWith(\".sgm\");\n      }\n    });\n    if (sgmFiles != null && sgmFiles.length > 0) {\n      for (File sgmFile : sgmFiles) {\n        extractFile(sgmFile);\n      }\n    } else {\n      System.err.println(\"No .sgm files in \" + reutersDir);\n    }\n  }\n\n  Pattern EXTRACTION_PATTERN = Pattern\n      .compile(\"<TITLE>(.*?)</TITLE>|<DATE>(.*?)</DATE>|<BODY>(.*?)</BODY>\");\n\n  private static String[] META_CHARS = { \"&\", \"<\", \">\", \"\\\"\", \"'\" };\n\n  private static String[] META_CHARS_SERIALIZATIONS = { \"&amp;\", \"&lt;\",\n      \"&gt;\", \"&quot;\", \"&apos;\" };\n\n  /**\n   * Override if you wish to change what is extracted\n   */\n  protected void extractFile(File sgmFile) {\n    try {\n      BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(sgmFile), IOUtils.CHARSET_UTF_8));\n\n      StringBuilder buffer = new StringBuilder(1024);\n      StringBuilder outBuffer = new StringBuilder(1024);\n\n      String line = null;\n      int docNumber = 0;\n      while ((line = reader.readLine()) != null) {\n        // when we see a closing reuters tag, flush the file\n\n        if (line.indexOf(\"</REUTERS\") == -1) {\n          // Replace the SGM escape sequences\n\n          buffer.append(line).append(' ');// accumulate the strings for now,\n                                          // then apply regular expression to\n                                          // get the pieces,\n        } else {\n          // Extract the relevant pieces and write to a file in the output dir\n          Matcher matcher = EXTRACTION_PATTERN.matcher(buffer);\n          while (matcher.find()) {\n            for (int i = 1; i <= matcher.groupCount(); i++) {\n              if (matcher.group(i) != null) {\n                outBuffer.append(matcher.group(i));\n              }\n            }\n            outBuffer.append(LINE_SEPARATOR).append(LINE_SEPARATOR);\n          }\n          String out = outBuffer.toString();\n          for (int i = 0; i < META_CHARS_SERIALIZATIONS.length; i++) {\n            out = out.replaceAll(META_CHARS_SERIALIZATIONS[i], META_CHARS[i]);\n          }\n          File outFile = new File(outputDir, sgmFile.getName() + \"-\"\n              + (docNumber++) + \".txt\");\n          // System.out.println(\"Writing \" + outFile);\n          OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(outFile), IOUtils.CHARSET_UTF_8);\n          writer.write(out);\n          writer.close();\n          outBuffer.setLength(0);\n          buffer.setLength(0);\n        }\n      }\n      reader.close();\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  public static void main(String[] args) {\n    if (args.length != 2) {\n      usage(\"Wrong number of arguments (\"+args.length+\")\");\n      return;\n    }\n    File reutersDir = new File(args[0]);\n    if (!reutersDir.exists()) {\n      usage(\"Cannot find Path to Reuters SGM files (\"+reutersDir+\")\");\n      return;\n    }\n    \n    // First, extract to a tmp directory and only if everything succeeds, rename\n    // to output directory.\n    File outputDir = new File(args[1]);\n    outputDir = new File(outputDir.getAbsolutePath() + \"-tmp\");\n    outputDir.mkdirs();\n    ExtractReuters extractor = new ExtractReuters(reutersDir, outputDir);\n    extractor.extract();\n    // Now rename to requested output dir\n    outputDir.renameTo(new File(args[1]));\n  }\n\n  private static void usage(String msg) {\n    System.err.println(\"Usage: \"+msg+\" :: java -cp <...> org.apache.lucene.benchmark.utils.ExtractReuters <Path to Reuters SGM files> <Output Path>\");\n  }\n  \n}\n",
        "methodName": "<init>",
        "exampleID": 2,
        "dataset": "spotbugs",
        "filepath": "/lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractReuters.java",
        "line": "47",
        "source": "?",
        "sourceLine": "47",
        "qualifier": "Possible null pointer dereference of $$?/$",
        "steps": [
            {
                "exampleID": 3
            }
        ],
        "line_number": "47"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractWikipedia.java",
        "rawCode": "package org.apache.lucene.benchmark.utils;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.OutputStreamWriter;\nimport java.io.Writer;\nimport java.util.Properties;\n\nimport org.apache.lucene.benchmark.byTask.feeds.ContentSource;\nimport org.apache.lucene.benchmark.byTask.feeds.DocMaker;\nimport org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource;\nimport org.apache.lucene.benchmark.byTask.feeds.NoMoreDataException;\nimport org.apache.lucene.benchmark.byTask.utils.Config;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.util.IOUtils;\n\n/**\n * Extract the downloaded Wikipedia dump into separate files for indexing.\n */\npublic class ExtractWikipedia {\n\n  private File outputDir;\n\n  static public int count = 0;\n\n  static final int BASE = 10;\n  protected DocMaker docMaker;\n\n  public ExtractWikipedia(DocMaker docMaker, File outputDir) {\n    this.outputDir = outputDir;\n    this.docMaker = docMaker;\n    System.out.println(\"Deleting all files in \" + outputDir);\n    File[] files = outputDir.listFiles();\n    for (int i = 0; i < files.length; i++) {\n      files[i].delete();\n    }\n  }\n\n  public File directory(int count, File directory) {\n    if (directory == null) {\n      directory = outputDir;\n    }\n    int base = BASE;\n    while (base <= count) {\n      base *= BASE;\n    }\n    if (count < BASE) {\n      return directory;\n    }\n    directory = new File(directory, (Integer.toString(base / BASE)));\n    directory = new File(directory, (Integer.toString(count / (base / BASE))));\n    return directory(count % (base / BASE), directory);\n  }\n\n  public void create(String id, String title, String time, String body) {\n\n    File d = directory(count++, null);\n    d.mkdirs();\n    File f = new File(d, id + \".txt\");\n\n    StringBuilder contents = new StringBuilder();\n\n    contents.append(time);\n    contents.append(\"\\n\\n\");\n    contents.append(title);\n    contents.append(\"\\n\\n\");\n    contents.append(body);\n    contents.append(\"\\n\");\n\n    try {\n      Writer writer = new OutputStreamWriter(new FileOutputStream(f), IOUtils.CHARSET_UTF_8);\n      writer.write(contents.toString());\n      writer.close();\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n\n  }\n\n  public void extract() throws Exception {\n    Document doc = null;\n    System.out.println(\"Starting Extraction\");\n    long start = System.currentTimeMillis();\n    try {\n      while ((doc = docMaker.makeDocument()) != null) {\n        create(doc.get(DocMaker.ID_FIELD), doc.get(DocMaker.TITLE_FIELD), doc\n            .get(DocMaker.DATE_FIELD), doc.get(DocMaker.BODY_FIELD));\n      }\n    } catch (NoMoreDataException e) {\n      //continue\n    }\n    long finish = System.currentTimeMillis();\n    System.out.println(\"Extraction took \" + (finish - start) + \" ms\");\n  }\n\n  public static void main(String[] args) throws Exception {\n\n    File wikipedia = null;\n    File outputDir = new File(\"./enwiki\");\n    boolean keepImageOnlyDocs = true;\n    for (int i = 0; i < args.length; i++) {\n      String arg = args[i];\n      if (arg.equals(\"--input\") || arg.equals(\"-i\")) {\n        wikipedia = new File(args[i + 1]);\n        i++;\n      } else if (arg.equals(\"--output\") || arg.equals(\"-o\")) {\n        outputDir = new File(args[i + 1]);\n        i++;\n      } else if (arg.equals(\"--discardImageOnlyDocs\") || arg.equals(\"-d\")) {\n        keepImageOnlyDocs = false;\n      }\n    }\n    \n    Properties properties = new Properties();\n    properties.setProperty(\"docs.file\", wikipedia.getAbsolutePath());\n    properties.setProperty(\"content.source.forever\", \"false\");\n    properties.setProperty(\"keep.image.only.docs\", String.valueOf(keepImageOnlyDocs));\n    Config config = new Config(properties);\n\n    ContentSource source = new EnwikiContentSource();\n    source.setConfig(config);\n    \n    DocMaker docMaker = new DocMaker();\n    docMaker.setConfig(config, source);\n    docMaker.resetInputs();\n    if (wikipedia.exists()) {\n      System.out.println(\"Extracting Wikipedia to: \" + outputDir + \" using EnwikiContentSource\");\n      outputDir.mkdirs();\n      ExtractWikipedia extractor = new ExtractWikipedia(docMaker, outputDir);\n      extractor.extract();\n    } else {\n      printUsage();\n    }\n  }\n\n  private static void printUsage() {\n    System.err.println(\"Usage: java -cp <...> org.apache.lucene.benchmark.utils.ExtractWikipedia --input|-i <Path to Wikipedia XML file> \" +\n            \"[--output|-o <Output Path>] [--discardImageOnlyDocs|-d]\");\n    System.err.println(\"--discardImageOnlyDocs tells the extractor to skip Wiki docs that contain only images\");\n  }\n\n}",
        "methodName": "<init>",
        "exampleID": 4,
        "dataset": "spotbugs",
        "filepath": "/lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/ExtractWikipedia.java",
        "line": "51",
        "source": "files",
        "sourceLine": "52",
        "qualifier": "Possible null pointer dereference of $$files/$",
        "steps": [
            {
                "exampleID": 5
            }
        ],
        "line_number": "52"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/facet/src/java/org/apache/lucene/facet/collections/ArrayHashMap.java",
        "rawCode": "package org.apache.lucene.facet.collections;\n\nimport java.util.Arrays;\nimport java.util.Iterator;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n/**\n * An Array-based hashtable which maps, similar to Java's HashMap, only\n * performance tests showed it performs better.\n * <p>\n * The hashtable is constructed with a given capacity, or 16 as a default. In\n * case there's not enough room for new pairs, the hashtable grows. Capacity is\n * adjusted to a power of 2, and there are 2 * capacity entries for the hash.\n * The pre allocated arrays (for keys, values) are at length of capacity + 1,\n * where index 0 is used as 'Ground' or 'NULL'.\n * <p>\n * The arrays are allocated ahead of hash operations, and form an 'empty space'\n * list, to which the &lt;key,value&gt; pair is allocated.\n * \n * @lucene.experimental\n */\npublic class ArrayHashMap<K,V> implements Iterable<V> {\n\n  /** Implements an IntIterator which iterates over all the allocated indexes. */\n  private final class IndexIterator implements IntIterator {\n    /**\n     * The last used baseHashIndex. Needed for \"jumping\" from one hash entry\n     * to another.\n     */\n    private int baseHashIndex = 0;\n\n    /** The next not-yet-visited index. */\n    private int index = 0;\n\n    /** Index of the last visited pair. Used in {@link #remove()}. */\n    private int lastIndex = 0;\n\n    /**\n     * Create the Iterator, make <code>index</code> point to the \"first\"\n     * index which is not empty. If such does not exist (eg. the map is\n     * empty) it would be zero.\n     */\n    public IndexIterator() {\n      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {\n        index = baseHash[baseHashIndex];\n        if (index != 0) {\n          break;\n        }\n      }\n    }\n\n    @Override\n    public boolean hasNext() {\n      return index != 0;\n    }\n\n    @Override\n    public int next() {\n      // Save the last index visited\n      lastIndex = index;\n\n      // next the index\n      index = next[index];\n\n      // if the next index points to the 'Ground' it means we're done with\n      // the current hash entry and we need to jump to the next one. This\n      // is done until all the hash entries had been visited.\n      while (index == 0 && ++baseHashIndex < baseHash.length) {\n        index = baseHash[baseHashIndex];\n      }\n\n      return lastIndex;\n    }\n\n    @Override\n    @SuppressWarnings(\"unchecked\")\n    public void remove() {\n      ArrayHashMap.this.remove((K) keys[lastIndex]);\n    }\n\n  }\n\n  /** Implements an Iterator, used for iteration over the map's keys. */\n  private final class KeyIterator implements Iterator<K> {\n    private IntIterator iterator = new IndexIterator();\n\n    KeyIterator() { }\n\n    @Override\n    public boolean hasNext() {\n      return iterator.hasNext();\n    }\n\n    @Override\n    @SuppressWarnings(\"unchecked\")\n    public K next() {\n      return (K) keys[iterator.next()];\n    }\n\n    @Override\n    public void remove() {\n      iterator.remove();\n    }\n  }\n\n  /** Implements an Iterator, used for iteration over the map's values. */\n  private final class ValueIterator implements Iterator<V> {\n    private IntIterator iterator = new IndexIterator();\n\n    ValueIterator() { }\n\n    @Override\n    public boolean hasNext() {\n      return iterator.hasNext();\n    }\n\n    @Override\n    @SuppressWarnings(\"unchecked\")\n    public V next() {\n      return (V) values[iterator.next()];\n    }\n\n    @Override\n    public void remove() {\n      iterator.remove();\n    }\n  }\n\n  /** Default capacity - in case no capacity was specified in the constructor */\n  private static final int DEFAULT_CAPACITY = 16;\n\n  /**\n   * Holds the base hash entries. if the capacity is 2^N, than the base hash\n   * holds 2^(N+1).\n   */\n  int[] baseHash;\n\n  /**\n   * The current capacity of the map. Always 2^N and never less than 16. We\n   * never use the zero index. It is needed to improve performance and is also\n   * used as \"ground\".\n   */\n  private int capacity;\n\n  /**\n   * All objects are being allocated at map creation. Those objects are \"free\"\n   * or empty. Whenever a new pair comes along, a pair is being \"allocated\" or\n   * taken from the free-linked list. as this is just a free list.\n   */\n  private int firstEmpty;\n\n  /** hashFactor is always (2^(N+1)) - 1. Used for faster hashing. */\n  private int hashFactor;\n\n  /** Holds the unique keys. */\n  Object[] keys;\n\n  /**\n   * In case of collisions, we implement a double linked list of the colliding\n   * hash's with the following next[] and prev[]. Those are also used to store\n   * the \"empty\" list.\n   */\n  int[] next;\n\n  private int prev;\n\n  /** Number of currently stored objects in the map. */\n  private int size;\n\n  /** Holds the values. */\n  Object[] values;\n\n  /** Constructs a map with default capacity. */\n  public ArrayHashMap() {\n    this(DEFAULT_CAPACITY);\n  }\n\n  /**\n   * Constructs a map with given capacity. Capacity is adjusted to a native\n   * power of 2, with minimum of 16.\n   * \n   * @param capacity minimum capacity for the map.\n   */\n  public ArrayHashMap(int capacity) {\n    this.capacity = 16;\n    while (this.capacity < capacity) {\n      // Multiply by 2 as long as we're still under the requested capacity\n      this.capacity <<= 1;\n    }\n\n    // As mentioned, we use the first index (0) as 'Ground', so we need the\n    // length of the arrays to be one more than the capacity\n    int arrayLength = this.capacity + 1;\n\n    values = new Object[arrayLength];\n    keys = new Object[arrayLength];\n    next = new int[arrayLength];\n\n    // Hash entries are twice as big as the capacity.\n    int baseHashSize = this.capacity << 1;\n\n    baseHash = new int[baseHashSize];\n\n    // The has factor is 2^M - 1 which is used as an \"AND\" hashing operator.\n    // {@link #calcBaseHash()}\n    hashFactor = baseHashSize - 1;\n\n    size = 0;\n\n    clear();\n  }\n\n  /**\n   * Adds a pair to the map. Takes the first empty position from the\n   * empty-linked-list's head - {@link #firstEmpty}. New pairs are always\n   * inserted to baseHash, and are followed by the old colliding pair.\n   */\n  private void prvt_put(K key, V value) {\n    // Hash entry to which the new pair would be inserted\n    int hashIndex = calcBaseHashIndex(key);\n\n    // 'Allocating' a pair from the \"Empty\" list.\n    int objectIndex = firstEmpty;\n\n    // Setting data\n    firstEmpty = next[firstEmpty];\n    values[objectIndex] = value;\n    keys[objectIndex] = key;\n\n    // Inserting the new pair as the first node in the specific hash entry\n    next[objectIndex] = baseHash[hashIndex];\n    baseHash[hashIndex] = objectIndex;\n\n    // Announcing a new pair was added!\n    ++size;\n  }\n\n  /** Calculating the baseHash index using the internal internal <code>hashFactor</code>. */\n  protected int calcBaseHashIndex(K key) {\n    return key.hashCode() & hashFactor;\n  }\n\n  /** Empties the map. Generates the \"Empty\" space list for later allocation. */\n  public void clear() {\n    // Clears the hash entries\n    Arrays.fill(baseHash, 0);\n\n    // Set size to zero\n    size = 0;\n\n    // Mark all array entries as empty. This is done with\n    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is\n    // used as 'Ground').\n    firstEmpty = 1;\n\n    // And setting all the <code>next[i]</code> to point at\n    // <code>i+1</code>.\n    for (int i = 1; i < capacity;) {\n      next[i] = ++i;\n    }\n\n    // Surly, the last one should point to the 'Ground'.\n    next[capacity] = 0;\n  }\n\n  /** Returns true iff the key exists in the map. */\n  public boolean containsKey(K key) {\n    return find(key) != 0;\n  }\n\n  /** Returns true iff the object exists in the map. */\n  public boolean containsValue(Object o) {\n    for (Iterator<V> iterator = iterator(); iterator.hasNext();) {\n      V object = iterator.next();\n      if (object.equals(o)) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /** Returns the index of the given key, or zero if the key wasn't found. */\n  protected int find(K key) {\n    // Calculate the hash entry.\n    int baseHashIndex = calcBaseHashIndex(key);\n\n    // Start from the hash entry.\n    int localIndex = baseHash[baseHashIndex];\n\n    // while the index does not point to the 'Ground'\n    while (localIndex != 0) {\n      // returns the index found in case of of a matching key.\n      if (keys[localIndex].equals(key)) {\n        return localIndex;\n      }\n\n      // next the local index\n      localIndex = next[localIndex];\n    }\n\n    // If we got this far, it could only mean we did not find the key we\n    // were asked for. return 'Ground' index.\n    return 0;\n  }\n\n  /**\n   * Finds the actual index of a given key with it's baseHashIndex. Some methods\n   * use the baseHashIndex. If those call {@link #find} there's no need to\n   * re-calculate that hash.\n   * \n   * @return the index of the given key, or 0 if the key wasn't found.\n   */\n  private int findForRemove(K key, int baseHashIndex) {\n    // Start from the hash entry.\n    prev = 0;\n    int index = baseHash[baseHashIndex];\n\n    // while the index does not point to the 'Ground'\n    while (index != 0) {\n      // returns the index found in case of of a matching key.\n      if (keys[index].equals(key)) {\n        return index;\n      }\n\n      // next the local index\n      prev = index;\n      index = next[index];\n    }\n\n    // If we got thus far, it could only mean we did not find the key we\n    // were asked for. return 'Ground' index.\n    return prev = 0;\n  }\n\n  /** Returns the object mapped with the given key, or null if the key wasn't found. */\n  @SuppressWarnings(\"unchecked\")\n  public V get(K key) {\n    return (V) values[find(key)];\n  }\n\n  /**\n   * Allocates a new map of double the capacity, and fast-insert the old\n   * key-value pairs.\n   */\n  @SuppressWarnings(\"unchecked\")\n  protected void grow() {\n    ArrayHashMap<K,V> newmap = new ArrayHashMap<K,V>(capacity * 2);\n\n    // Iterates fast over the collection. Any valid pair is put into the new\n    // map without checking for duplicates or if there's enough space for\n    // it.\n    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {\n      int index = iterator.next();\n      newmap.prvt_put((K) keys[index], (V) values[index]);\n    }\n\n    // Copy that's data into this.\n    capacity = newmap.capacity;\n    size = newmap.size;\n    firstEmpty = newmap.firstEmpty;\n    values = newmap.values;\n    keys = newmap.keys;\n    next = newmap.next;\n    baseHash = newmap.baseHash;\n    hashFactor = newmap.hashFactor;\n  }\n\n  /** Returns true iff the map is empty. */\n  public boolean isEmpty() {\n    return size == 0;\n  }\n\n  /** Returns an iterator on the mapped objects. */\n  @Override\n  public Iterator<V> iterator() {\n    return new ValueIterator();\n  }\n\n  /** Returns an iterator on the map keys. */\n  public Iterator<K> keyIterator() {\n    return new KeyIterator();\n  }\n\n  /** Prints the baseHash array, used for debugging purposes. */\n  @SuppressWarnings(\"unused\")\n  private String getBaseHashAsString() {\n    return Arrays.toString(this.baseHash);\n  }\n\n  /**\n   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,\n   * this method updates the mapped value to the given one, returning the old\n   * mapped value.\n   * \n   * @return the old mapped value, or null if the key didn't exist.\n   */\n  @SuppressWarnings(\"unchecked\")\n  public V put(K key, V e) {\n    // Does key exists?\n    int index = find(key);\n\n    // Yes!\n    if (index != 0) {\n      // Set new data and exit.\n      V old = (V) values[index];\n      values[index] = e;\n      return old;\n    }\n\n    // Is there enough room for a new pair?\n    if (size == capacity) {\n      // No? Than grow up!\n      grow();\n    }\n\n    // Now that everything is set, the pair can be just put inside with no\n    // worries.\n    prvt_put(key, e);\n\n    return null;\n  }\n\n  /**\n   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,\n   * or null if the none existed.\n   * \n   * @param key used to find the value to remove\n   * @return the removed value or null if none existed.\n   */\n  @SuppressWarnings(\"unchecked\")\n  public V remove(K key) {\n    int baseHashIndex = calcBaseHashIndex(key);\n    int index = findForRemove(key, baseHashIndex);\n    if (index != 0) {\n      // If it is the first in the collision list, we should promote its\n      // next colliding element.\n      if (prev == 0) {\n        baseHash[baseHashIndex] = next[index];\n      }\n\n      next[prev] = next[index];\n      next[index] = firstEmpty;\n      firstEmpty = index;\n      --size;\n      return (V) values[index];\n    }\n\n    return null;\n  }\n\n  /** Returns number of pairs currently in the map. */\n  public int size() {\n    return this.size;\n  }\n\n  /**\n   * Translates the mapped pairs' values into an array of Objects\n   * \n   * @return an object array of all the values currently in the map.\n   */\n  public Object[] toArray() {\n    int j = -1;\n    Object[] array = new Object[size];\n\n    // Iterates over the values, adding them to the array.\n    for (Iterator<V> iterator = iterator(); iterator.hasNext();) {\n      array[++j] = iterator.next();\n    }\n    return array;\n  }\n\n  /**\n   * Translates the mapped pairs' values into an array of V\n   * \n   * @param a the array into which the elements of the list are to be stored, if\n   *        it is big enough; otherwise, use as much space as it can.\n   * @return an array containing the elements of the list\n   */\n  public V[] toArray(V[] a) {\n    int j = 0;\n    // Iterates over the values, adding them to the array.\n    for (Iterator<V> iterator = iterator(); j < a.length\n    && iterator.hasNext(); ++j) {\n      a[j] = iterator.next();\n    }\n    if (j < a.length) {\n      a[j] = null;\n    }\n\n    return a;\n  }\n\n  @Override\n  public String toString() {\n    StringBuffer sb = new StringBuffer();\n    sb.append('{');\n    Iterator<K> keyIterator = keyIterator();\n    while (keyIterator.hasNext()) {\n      K key = keyIterator.next();\n      sb.append(key);\n      sb.append('=');\n      sb.append(get(key));\n      if (keyIterator.hasNext()) {\n        sb.append(',');\n        sb.append(' ');\n      }\n    }\n    sb.append('}');\n    return sb.toString();\n  }\n\n  @Override\n  public int hashCode() {\n    return getClass().hashCode() ^ size();\n  }\n\n  @SuppressWarnings(\"unchecked\")\n  @Override\n  public boolean equals(Object o) {\n    ArrayHashMap<K, V> that = (ArrayHashMap<K,V>)o;\n    if (that.size() != this.size()) {\n      return false;\n    }\n\n    Iterator<K> it = keyIterator();\n    while (it.hasNext()) {\n      K key = it.next();\n      V v1 = this.get(key);\n      V v2 = that.get(key);\n      if ((v1 == null && v2 != null) ||\n          (v1 != null && v2 == null) ||\n          (!v1.equals(v2))) {\n        return false;\n      }\n    }\n    return true;\n  }\n}",
        "methodName": "equals",
        "exampleID": 6,
        "dataset": "spotbugs",
        "filepath": "/lucene/facet/src/java/org/apache/lucene/facet/collections/ArrayHashMap.java",
        "line": "546",
        "source": "v1",
        "sourceLine": "548",
        "qualifier": "Possible null pointer dereference of $$v1/$",
        "steps": [
            {
                "exampleID": 7
            }
        ],
        "line_number": "548"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/facet/src/java/org/apache/lucene/facet/collections/FloatToObjectMap.java",
        "rawCode": "package org.apache.lucene.facet.collections;\n\nimport java.util.Arrays;\nimport java.util.Iterator;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n/**\n\n * An Array-based hashtable which maps primitive float to Objects of generic type\n * T.<br>\n * The hashtable is constracted with a given capacity, or 16 as a default. In\n * case there's not enough room for new pairs, the hashtable grows. <br>\n * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for\n * the hash.\n * \n * The pre allocated arrays (for keys, values) are at length of capacity + 1,\n * when index 0 is used as 'Ground' or 'NULL'.<br>\n * \n * The arrays are allocated ahead of hash operations, and form an 'empty space'\n * list, to which the key,value pair is allocated.\n * \n * @lucene.experimental\n */\npublic class FloatToObjectMap<T> implements Iterable<T> {\n\n  /**\n   * Implements an IntIterator which iterates over all the allocated indexes.\n   */\n  private final class IndexIterator implements IntIterator {\n    /**\n     * The last used baseHashIndex. Needed for \"jumping\" from one hash entry\n     * to another.\n     */\n    private int baseHashIndex = 0;\n\n    /**\n     * The next not-yet-visited index.\n     */\n    private int index = 0;\n\n    /**\n     * Index of the last visited pair. Used in {@link #remove()}.\n     */\n    private int lastIndex = 0;\n\n    /**\n     * Create the Iterator, make <code>index</code> point to the \"first\"\n     * index which is not empty. If such does not exist (eg. the map is\n     * empty) it would be zero.\n     */\n    public IndexIterator() {\n      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {\n        index = baseHash[baseHashIndex];\n        if (index != 0) {\n          break;\n        }\n      }\n    }\n\n    @Override\n    public boolean hasNext() {\n      return (index != 0);\n    }\n\n    @Override\n    public int next() {\n      // Save the last index visited\n      lastIndex = index;\n\n      // next the index\n      index = next[index];\n\n      // if the next index points to the 'Ground' it means we're done with\n      // the current hash entry and we need to jump to the next one. This\n      // is done until all the hash entries had been visited.\n      while (index == 0 && ++baseHashIndex < baseHash.length) {\n        index = baseHash[baseHashIndex];\n      }\n\n      return lastIndex;\n    }\n\n    @Override\n    public void remove() {\n      FloatToObjectMap.this.remove(keys[lastIndex]);\n    }\n\n  }\n\n  /**\n   * Implements an IntIterator, used for iteration over the map's keys.\n   */\n  private final class KeyIterator implements FloatIterator {\n    private IntIterator iterator = new IndexIterator();\n\n    KeyIterator() { }\n\n    @Override\n    public boolean hasNext() {\n      return iterator.hasNext();\n    }\n\n    @Override\n    public float next() {\n      return keys[iterator.next()];\n    }\n\n    @Override\n    public void remove() {\n      iterator.remove();\n    }\n  }\n\n  /**\n   * Implements an Iterator of a generic type T used for iteration over the\n   * map's values.\n   */\n  private final class ValueIterator implements Iterator<T> {\n    private IntIterator iterator = new IndexIterator();\n\n    ValueIterator() { }\n\n    @Override\n    public boolean hasNext() {\n      return iterator.hasNext();\n    }\n\n    @Override\n    @SuppressWarnings(\"unchecked\")\n    public T next() {\n      return (T) values[iterator.next()];\n    }\n\n    @Override\n    public void remove() {\n      iterator.remove();\n    }\n  }\n\n  /**\n   * Default capacity - in case no capacity was specified in the constructor\n   */\n  private static int defaultCapacity = 16;\n\n  /**\n   * Holds the base hash entries. if the capacity is 2^N, than the base hash\n   * holds 2^(N+1). It can hold\n   */\n  int[] baseHash;\n\n  /**\n   * The current capacity of the map. Always 2^N and never less than 16. We\n   * never use the zero index. It is needed to improve performance and is also\n   * used as \"ground\".\n   */\n  private int capacity;\n  /**\n   * All objects are being allocated at map creation. Those objects are \"free\"\n   * or empty. Whenever a new pair comes along, a pair is being \"allocated\" or\n   * taken from the free-linked list. as this is just a free list.\n   */\n  private int firstEmpty;\n\n  /**\n   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.\n   */\n  private int hashFactor;\n\n  /**\n   * This array holds the unique keys\n   */\n  float[] keys;\n\n  /**\n   * In case of collisions, we implement a double linked list of the colliding\n   * hash's with the following next[] and prev[]. Those are also used to store\n   * the \"empty\" list.\n   */\n  int[] next;\n\n  private int prev;\n\n  /**\n   * Number of currently objects in the map.\n   */\n  private int size;\n\n  /**\n   * This array holds the values\n   */\n  Object[] values;\n\n  /**\n   * Constructs a map with default capacity.\n   */\n  public FloatToObjectMap() {\n    this(defaultCapacity);\n  }\n\n  /**\n   * Constructs a map with given capacity. Capacity is adjusted to a native\n   * power of 2, with minimum of 16.\n   * \n   * @param capacity\n   *            minimum capacity for the map.\n   */\n  public FloatToObjectMap(int capacity) {\n    this.capacity = 16;\n    // Minimum capacity is 16..\n    while (this.capacity < capacity) {\n      // Multiply by 2 as long as we're still under the requested capacity\n      this.capacity <<= 1;\n    }\n\n    // As mentioned, we use the first index (0) as 'Ground', so we need the\n    // length of the arrays to be one more than the capacity\n    int arrayLength = this.capacity + 1;\n\n    this.values = new Object[arrayLength];\n    this.keys = new float[arrayLength];\n    this.next = new int[arrayLength];\n\n    // Hash entries are twice as big as the capacity.\n    int baseHashSize = this.capacity << 1;\n\n    this.baseHash = new int[baseHashSize];\n\n    // The has factor is 2^M - 1 which is used as an \"AND\" hashing operator.\n    // {@link #calcBaseHash()}\n    this.hashFactor = baseHashSize - 1;\n\n    this.size = 0;\n\n    clear();\n  }\n\n  /**\n   * Adds a pair to the map. Takes the first empty position from the\n   * empty-linked-list's head - {@link #firstEmpty}.\n   * \n   * New pairs are always inserted to baseHash, and are followed by the old\n   * colliding pair.\n   * \n   * @param key\n   *            integer which maps the given Object\n   * @param e\n   *            element which is being mapped using the given key\n   */\n  private void prvt_put(float key, T e) {\n    // Hash entry to which the new pair would be inserted\n    int hashIndex = calcBaseHashIndex(key);\n\n    // 'Allocating' a pair from the \"Empty\" list.\n    int objectIndex = firstEmpty;\n\n    // Setting data\n    firstEmpty = next[firstEmpty];\n    values[objectIndex] = e;\n    keys[objectIndex] = key;\n\n    // Inserting the new pair as the first node in the specific hash entry\n    next[objectIndex] = baseHash[hashIndex];\n    baseHash[hashIndex] = objectIndex;\n\n    // Announcing a new pair was added!\n    ++size;\n  }\n\n  /**\n   * Calculating the baseHash index using the internal <code>hashFactor</code>.\n   */\n  protected int calcBaseHashIndex(float key) {\n    return Float.floatToIntBits(key) & hashFactor;\n  }\n\n  /**\n   * Empties the map. Generates the \"Empty\" space list for later allocation.\n   */\n  public void clear() {\n    // Clears the hash entries\n    Arrays.fill(this.baseHash, 0);\n\n    // Set size to zero\n    size = 0;\n\n    // Mark all array entries as empty. This is done with\n    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is\n    // used as 'Ground').\n    firstEmpty = 1;\n\n    // And setting all the <code>next[i]</code> to point at\n    // <code>i+1</code>.\n    for (int i = 1; i < this.capacity;) {\n      next[i] = ++i;\n    }\n\n    // Surly, the last one should point to the 'Ground'.\n    next[this.capacity] = 0;\n  }\n\n  /**\n   * Checks if a given key exists in the map.\n   * \n   * @param key\n   *            that is checked against the map data.\n   * @return true if the key exists in the map. false otherwise.\n   */\n  public boolean containsKey(float key) {\n    return find(key) != 0;\n  }\n\n  /**\n   * Checks if the given object exists in the map.<br>\n   * This method iterates over the collection, trying to find an equal object.\n   * \n   * @param o\n   *            object that is checked against the map data.\n   * @return true if the object exists in the map (in .equals() meaning).\n   *         false otherwise.\n   */\n  public boolean containsValue(Object o) {\n    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {\n      T object = iterator.next();\n      if (object.equals(o)) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Find the actual index of a given key.\n   * \n   * @return index of the key. zero if the key wasn't found.\n   */\n  protected int find(float key) {\n    // Calculate the hash entry.\n    int baseHashIndex = calcBaseHashIndex(key);\n\n    // Start from the hash entry.\n    int localIndex = baseHash[baseHashIndex];\n\n    // while the index does not point to the 'Ground'\n    while (localIndex != 0) {\n      // returns the index found in case of of a matching key.\n      if (keys[localIndex] == key) {\n        return localIndex;\n      }\n\n      // next the local index\n      localIndex = next[localIndex];\n    }\n\n    // If we got this far, it could only mean we did not find the key we\n    // were asked for. return 'Ground' index.\n    return 0;\n  }\n\n  /**\n   * Find the actual index of a given key with it's baseHashIndex.<br>\n   * Some methods use the baseHashIndex. If those call {@link #find} there's\n   * no need to re-calculate that hash.\n   * \n   * @return the index of the given key, or 0 as 'Ground' if the key wasn't\n   *         found.\n   */\n  private int findForRemove(float key, int baseHashIndex) {\n    // Start from the hash entry.\n    this.prev = 0;\n    int index = baseHash[baseHashIndex];\n\n    // while the index does not point to the 'Ground'\n    while (index != 0) {\n      // returns the index found in case of of a matching key.\n      if (keys[index] == key) {\n        return index;\n      }\n\n      // next the local index\n      prev = index;\n      index = next[index];\n    }\n\n    // If we got this far, it could only mean we did not find the key we\n    // were asked for. return 'Ground' index.\n    this.prev = 0;\n    return 0;\n  }\n\n  /**\n   * Returns the object mapped with the given key.\n   * \n   * @param key\n   *            int who's mapped object we're interested in.\n   * @return an object mapped by the given key. null if the key wasn't found.\n   */\n  @SuppressWarnings(\"unchecked\")\n  public T get(float key) {\n    return (T) values[find(key)];\n  }\n\n  /**\n   * Grows the map. Allocates a new map of double the capacity, and\n   * fast-insert the old key-value pairs.\n   */\n  @SuppressWarnings(\"unchecked\")\n  protected void grow() {\n    FloatToObjectMap<T> that = new FloatToObjectMap<T>(\n        this.capacity * 2);\n\n    // Iterates fast over the collection. Any valid pair is put into the new\n    // map without checking for duplicates or if there's enough space for\n    // it.\n    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {\n      int index = iterator.next();\n      that.prvt_put(this.keys[index], (T) this.values[index]);\n    }\n\n    // Copy that's data into this.\n    this.capacity = that.capacity;\n    this.size = that.size;\n    this.firstEmpty = that.firstEmpty;\n    this.values = that.values;\n    this.keys = that.keys;\n    this.next = that.next;\n    this.baseHash = that.baseHash;\n    this.hashFactor = that.hashFactor;\n  }\n\n  /**\n   * \n   * @return true if the map is empty. false otherwise.\n   */\n  public boolean isEmpty() {\n    return size == 0;\n  }\n\n  /**\n   * Returns a new iterator for the mapped objects.\n   */\n  @Override\n  public Iterator<T> iterator() {\n    return new ValueIterator();\n  }\n\n  /** Returns an iterator on the map keys. */\n  public FloatIterator keyIterator() {\n    return new KeyIterator();\n  }\n\n  /**\n   * Prints the baseHash array, used for DEBUG purposes.\n   */\n  @SuppressWarnings(\"unused\")\n  private String getBaseHashAsString() {\n    return Arrays.toString(this.baseHash);\n  }\n\n  /**\n   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,\n   * this method updates the mapped value to the given one, returning the old\n   * mapped value.\n   * \n   * @return the old mapped value, or null if the key didn't exist.\n   */\n  @SuppressWarnings(\"unchecked\")\n  public T put(float key, T e) {\n    // Does key exists?\n    int index = find(key);\n\n    // Yes!\n    if (index != 0) {\n      // Set new data and exit.\n      T old = (T) values[index];\n      values[index] = e;\n      return old;\n    }\n\n    // Is there enough room for a new pair?\n    if (size == capacity) {\n      // No? Than grow up!\n      grow();\n    }\n\n    // Now that everything is set, the pair can be just put inside with no\n    // worries.\n    prvt_put(key, e);\n\n    return null;\n  }\n\n  /**\n   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,\n   * or null if the none existed.\n   * \n   * @param key used to find the value to remove\n   * @return the removed value or null if none existed.\n   */\n  @SuppressWarnings(\"unchecked\")\n  public T remove(float key) {\n    int baseHashIndex = calcBaseHashIndex(key);\n    int index = findForRemove(key, baseHashIndex);\n    if (index != 0) {\n      // If it is the first in the collision list, we should promote its\n      // next colliding element.\n      if (prev == 0) {\n        baseHash[baseHashIndex] = next[index];\n      }\n\n      next[prev] = next[index];\n      next[index] = firstEmpty;\n      firstEmpty = index;\n      --size;\n      return (T) values[index];\n    }\n\n    return null;\n  }\n\n  /**\n   * @return number of pairs currently in the map\n   */\n  public int size() {\n    return this.size;\n  }\n\n  /**\n   * Translates the mapped pairs' values into an array of Objects\n   * \n   * @return an object array of all the values currently in the map.\n   */\n  public Object[] toArray() {\n    int j = -1;\n    Object[] array = new Object[size];\n\n    // Iterates over the values, adding them to the array.\n    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {\n      array[++j] = iterator.next();\n    }\n    return array;\n  }\n\n  /**\n   * Translates the mapped pairs' values into an array of T\n   * \n   * @param a\n   *            the array into which the elements of the list are to be\n   *            stored, if it is big enough; otherwise, use whatever space we\n   *            have, setting the one after the true data as null.\n   * \n   * @return an array containing the elements of the list\n   * \n   */\n  public T[] toArray(T[] a) {\n    int j = 0;\n    // Iterates over the values, adding them to the array.\n    for (Iterator<T> iterator = iterator(); j < a.length\n    && iterator.hasNext(); ++j) {\n      a[j] = iterator.next();\n    }\n\n    if (j < a.length) {\n      a[j] = null;\n    }\n\n    return a;\n  }\n\n  @Override\n  public String toString() {\n    StringBuffer sb = new StringBuffer();\n    sb.append('{');\n    FloatIterator keyIterator = keyIterator();\n    while (keyIterator.hasNext()) {\n      float key = keyIterator.next();\n      sb.append(key);\n      sb.append('=');\n      sb.append(get(key));\n      if (keyIterator.hasNext()) {\n        sb.append(',');\n        sb.append(' ');\n      }\n    }\n    sb.append('}');\n    return sb.toString();\n  }\n\n  @Override\n  public int hashCode() {\n    return getClass().hashCode() ^ size();\n  }\n\n  @SuppressWarnings(\"unchecked\")\n  @Override\n  public boolean equals(Object o) {\n    FloatToObjectMap<T> that = (FloatToObjectMap<T>)o;\n    if (that.size() != this.size()) {\n      return false;\n    }\n\n    FloatIterator it = keyIterator();\n    while (it.hasNext()) {\n      float key = it.next();\n      if (!that.containsKey(key)) {\n        return false;\n      }\n\n      T v1 = this.get(key);\n      T v2 = that.get(key);\n      if ((v1 == null && v2 != null) ||\n          (v1 != null && v2 == null) ||\n          (!v1.equals(v2))) {\n        return false;\n      }\n    }\n    return true;\n  }\n}",
        "methodName": "equals",
        "exampleID": 8,
        "dataset": "spotbugs",
        "filepath": "/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatToObjectMap.java",
        "line": "626",
        "source": "v1",
        "sourceLine": "628",
        "qualifier": "Possible null pointer dereference of $$v1/$",
        "steps": [
            {
                "exampleID": 9
            }
        ],
        "line_number": "628"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/facet/src/java/org/apache/lucene/facet/collections/IntToObjectMap.java",
        "rawCode": "package org.apache.lucene.facet.collections;\n\nimport java.util.Arrays;\nimport java.util.Iterator;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n/**\n * An Array-based hashtable which maps primitive int to Objects of generic type\n * T.<br>\n * The hashtable is constracted with a given capacity, or 16 as a default. In\n * case there's not enough room for new pairs, the hashtable grows. <br>\n * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for\n * the hash.\n * \n * The pre allocated arrays (for keys, values) are at length of capacity + 1,\n * when index 0 is used as 'Ground' or 'NULL'.<br>\n * \n * The arrays are allocated ahead of hash operations, and form an 'empty space'\n * list, to which the key,value pair is allocated.\n * \n * @lucene.experimental\n */\npublic class IntToObjectMap<T> implements Iterable<T> {\n\n  /**\n   * Implements an IntIterator which iterates over all the allocated indexes.\n   */\n  private final class IndexIterator implements IntIterator {\n    /**\n     * The last used baseHashIndex. Needed for \"jumping\" from one hash entry\n     * to another.\n     */\n    private int baseHashIndex = 0;\n\n    /**\n     * The next not-yet-visited index.\n     */\n    private int index = 0;\n\n    /**\n     * Index of the last visited pair. Used in {@link #remove()}.\n     */\n    private int lastIndex = 0;\n\n    /**\n     * Create the Iterator, make <code>index</code> point to the \"first\"\n     * index which is not empty. If such does not exist (eg. the map is\n     * empty) it would be zero.\n     */\n    public IndexIterator() {\n      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {\n        index = baseHash[baseHashIndex];\n        if (index != 0) {\n          break;\n        }\n      }\n    }\n\n    @Override\n    public boolean hasNext() {\n      return (index != 0);\n    }\n\n    @Override\n    public int next() {\n      // Save the last index visited\n      lastIndex = index;\n\n      // next the index\n      index = next[index];\n\n      // if the next index points to the 'Ground' it means we're done with\n      // the current hash entry and we need to jump to the next one. This\n      // is done until all the hash entries had been visited.\n      while (index == 0 && ++baseHashIndex < baseHash.length) {\n        index = baseHash[baseHashIndex];\n      }\n\n      return lastIndex;\n    }\n\n    @Override\n    public void remove() {\n      IntToObjectMap.this.remove(keys[lastIndex]);\n    }\n\n  }\n\n  /**\n   * Implements an IntIterator, used for iteration over the map's keys.\n   */\n  private final class KeyIterator implements IntIterator {\n    private IntIterator iterator = new IndexIterator();\n\n    KeyIterator() { }\n    \n    @Override\n    public boolean hasNext() {\n      return iterator.hasNext();\n    }\n\n    @Override\n    public int next() {\n      return keys[iterator.next()];\n    }\n\n    @Override\n    public void remove() {\n      iterator.remove();\n    }\n  }\n\n  /**\n   * Implements an Iterator of a generic type T used for iteration over the\n   * map's values.\n   */\n  private final class ValueIterator implements Iterator<T> {\n    private IntIterator iterator = new IndexIterator();\n\n    ValueIterator() { }\n    \n    @Override\n    public boolean hasNext() {\n      return iterator.hasNext();\n    }\n\n    @Override\n    @SuppressWarnings(\"unchecked\")\n    public T next() {\n      return (T) values[iterator.next()];\n    }\n\n    @Override\n    public void remove() {\n      iterator.remove();\n    }\n  }\n\n  /**\n   * Default capacity - in case no capacity was specified in the constructor\n   */\n  private static int defaultCapacity = 16;\n\n  /**\n   * Holds the base hash entries. if the capacity is 2^N, than the base hash\n   * holds 2^(N+1). It can hold\n   */\n  int[] baseHash;\n\n  /**\n   * The current capacity of the map. Always 2^N and never less than 16. We\n   * never use the zero index. It is needed to improve performance and is also\n   * used as \"ground\".\n   */\n  private int capacity;\n  /**\n   * All objects are being allocated at map creation. Those objects are \"free\"\n   * or empty. Whenever a new pair comes along, a pair is being \"allocated\" or\n   * taken from the free-linked list. as this is just a free list.\n   */\n  private int firstEmpty;\n\n  /**\n   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.\n   */\n  private int hashFactor;\n\n  /**\n   * This array holds the unique keys\n   */\n  int[] keys;\n\n  /**\n   * In case of collisions, we implement a double linked list of the colliding\n   * hash's with the following next[] and prev[]. Those are also used to store\n   * the \"empty\" list.\n   */\n  int[] next;\n\n  private int prev;\n\n  /**\n   * Number of currently objects in the map.\n   */\n  private int size;\n\n  /**\n   * This array holds the values\n   */\n  Object[] values;\n\n  /**\n   * Constructs a map with default capacity.\n   */\n  public IntToObjectMap() {\n    this(defaultCapacity);\n  }\n\n  /**\n   * Constructs a map with given capacity. Capacity is adjusted to a native\n   * power of 2, with minimum of 16.\n   * \n   * @param capacity\n   *            minimum capacity for the map.\n   */\n  public IntToObjectMap(int capacity) {\n    this.capacity = 16;\n    // Minimum capacity is 16..\n    while (this.capacity < capacity) {\n      // Multiply by 2 as long as we're still under the requested capacity\n      this.capacity <<= 1;\n    }\n\n    // As mentioned, we use the first index (0) as 'Ground', so we need the\n    // length of the arrays to be one more than the capacity\n    int arrayLength = this.capacity + 1;\n\n    this.values = new Object[arrayLength];\n    this.keys = new int[arrayLength];\n    this.next = new int[arrayLength];\n\n    // Hash entries are twice as big as the capacity.\n    int baseHashSize = this.capacity << 1;\n\n    this.baseHash = new int[baseHashSize];\n\n    // The has factor is 2^M - 1 which is used as an \"AND\" hashing operator.\n    // {@link #calcBaseHash()}\n    this.hashFactor = baseHashSize - 1;\n\n    this.size = 0;\n\n    clear();\n  }\n\n  /**\n   * Adds a pair to the map. Takes the first empty position from the\n   * empty-linked-list's head - {@link #firstEmpty}.\n   * \n   * New pairs are always inserted to baseHash, and are followed by the old\n   * colliding pair.\n   * \n   * @param key\n   *            integer which maps the given Object\n   * @param e\n   *            element which is being mapped using the given key\n   */\n  private void prvt_put(int key, T e) {\n    // Hash entry to which the new pair would be inserted\n    int hashIndex = calcBaseHashIndex(key);\n\n    // 'Allocating' a pair from the \"Empty\" list.\n    int objectIndex = firstEmpty;\n\n    // Setting data\n    firstEmpty = next[firstEmpty];\n    values[objectIndex] = e;\n    keys[objectIndex] = key;\n\n    // Inserting the new pair as the first node in the specific hash entry\n    next[objectIndex] = baseHash[hashIndex];\n    baseHash[hashIndex] = objectIndex;\n\n    // Announcing a new pair was added!\n    ++size;\n  }\n\n  /**\n   * Calculating the baseHash index using the internal <code>hashFactor</code>.\n   * \n   */\n  protected int calcBaseHashIndex(int key) {\n    return key & hashFactor;\n  }\n\n  /**\n   * Empties the map. Generates the \"Empty\" space list for later allocation.\n   */\n  public void clear() {\n    // Clears the hash entries\n    Arrays.fill(this.baseHash, 0);\n\n    // Set size to zero\n    size = 0;\n\n    // Mark all array entries as empty. This is done with\n    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is\n    // used as 'Ground').\n    firstEmpty = 1;\n\n    // And setting all the <code>next[i]</code> to point at\n    // <code>i+1</code>.\n    for (int i = 1; i < this.capacity;) {\n      next[i] = ++i;\n    }\n\n    // Surly, the last one should point to the 'Ground'.\n    next[this.capacity] = 0;\n  }\n\n  /**\n   * Checks if a given key exists in the map.\n   * \n   * @param key\n   *            that is checked against the map data.\n   * @return true if the key exists in the map. false otherwise.\n   */\n  public boolean containsKey(int key) {\n    return find(key) != 0;\n  }\n\n  /**\n   * Checks if the given object exists in the map.<br>\n   * This method iterates over the collection, trying to find an equal object.\n   * \n   * @param o\n   *            object that is checked against the map data.\n   * @return true if the object exists in the map (in .equals() meaning).\n   *         false otherwise.\n   */\n  public boolean containsValue(Object o) {\n    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {\n      T object = iterator.next();\n      if (object.equals(o)) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Find the actual index of a given key.\n   * \n   * @return index of the key. zero if the key wasn't found.\n   */\n  protected int find(int key) {\n    // Calculate the hash entry.\n    int baseHashIndex = calcBaseHashIndex(key);\n\n    // Start from the hash entry.\n    int localIndex = baseHash[baseHashIndex];\n\n    // while the index does not point to the 'Ground'\n    while (localIndex != 0) {\n      // returns the index found in case of of a matching key.\n      if (keys[localIndex] == key) {\n        return localIndex;\n      }\n\n      // next the local index\n      localIndex = next[localIndex];\n    }\n\n    // If we got this far, it could only mean we did not find the key we\n    // were asked for. return 'Ground' index.\n    return 0;\n  }\n\n  /**\n   * Find the actual index of a given key with it's baseHashIndex.<br>\n   * Some methods use the baseHashIndex. If those call {@link #find} there's\n   * no need to re-calculate that hash.\n   * \n   * @return the index of the given key, or 0 as 'Ground' if the key wasn't\n   *         found.\n   */\n  private int findForRemove(int key, int baseHashIndex) {\n    // Start from the hash entry.\n    this.prev = 0;\n    int index = baseHash[baseHashIndex];\n\n    // while the index does not point to the 'Ground'\n    while (index != 0) {\n      // returns the index found in case of of a matching key.\n      if (keys[index] == key) {\n        return index;\n      }\n\n      // next the local index\n      prev = index;\n      index = next[index];\n    }\n\n    // If we got this far, it could only mean we did not find the key we\n    // were asked for. return 'Ground' index.\n    this.prev = 0;\n    return 0;\n  }\n\n  /**\n   * Returns the object mapped with the given key.\n   * \n   * @param key\n   *            int who's mapped object we're interested in.\n   * @return an object mapped by the given key. null if the key wasn't found.\n   */\n  @SuppressWarnings(\"unchecked\")\n  public T get(int key) {\n    return (T) values[find(key)];\n  }\n\n  /**\n   * Grows the map. Allocates a new map of double the capacity, and\n   * fast-insert the old key-value pairs.\n   */\n  @SuppressWarnings(\"unchecked\")\n  protected void grow() {\n    IntToObjectMap<T> that = new IntToObjectMap<T>(\n        this.capacity * 2);\n\n    // Iterates fast over the collection. Any valid pair is put into the new\n    // map without checking for duplicates or if there's enough space for\n    // it.\n    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {\n      int index = iterator.next();\n      that.prvt_put(this.keys[index], (T) this.values[index]);\n    }\n\n    // Copy that's data into this.\n    this.capacity = that.capacity;\n    this.size = that.size;\n    this.firstEmpty = that.firstEmpty;\n    this.values = that.values;\n    this.keys = that.keys;\n    this.next = that.next;\n    this.baseHash = that.baseHash;\n    this.hashFactor = that.hashFactor;\n  }\n\n  /**\n   * \n   * @return true if the map is empty. false otherwise.\n   */\n  public boolean isEmpty() {\n    return size == 0;\n  }\n\n  /**\n   * Returns a new iterator for the mapped objects.\n   */\n  @Override\n  public Iterator<T> iterator() {\n    return new ValueIterator();\n  }\n\n  /** Returns an iterator on the map keys. */\n  public IntIterator keyIterator() {\n    return new KeyIterator();\n  }\n\n  /**\n   * Prints the baseHash array, used for debug purposes.\n   */\n  @SuppressWarnings(\"unused\")\n  private String getBaseHashAsString() {\n    return Arrays.toString(baseHash);\n  }\n\n  /**\n   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,\n   * this method updates the mapped value to the given one, returning the old\n   * mapped value.\n   * \n   * @return the old mapped value, or null if the key didn't exist.\n   */\n  @SuppressWarnings(\"unchecked\")\n  public T put(int key, T e) {\n    // Does key exists?\n    int index = find(key);\n\n    // Yes!\n    if (index != 0) {\n      // Set new data and exit.\n      T old = (T) values[index];\n      values[index] = e;\n      return old;\n    }\n\n    // Is there enough room for a new pair?\n    if (size == capacity) {\n      // No? Than grow up!\n      grow();\n    }\n\n    // Now that everything is set, the pair can be just put inside with no\n    // worries.\n    prvt_put(key, e);\n\n    return null;\n  }\n\n  /**\n   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,\n   * or null if the none existed.\n   * \n   * @param key used to find the value to remove\n   * @return the removed value or null if none existed.\n   */\n  @SuppressWarnings(\"unchecked\")\n  public T remove(int key) {\n    int baseHashIndex = calcBaseHashIndex(key);\n    int index = findForRemove(key, baseHashIndex);\n    if (index != 0) {\n      // If it is the first in the collision list, we should promote its\n      // next colliding element.\n      if (prev == 0) {\n        baseHash[baseHashIndex] = next[index];\n      }\n\n      next[prev] = next[index];\n      next[index] = firstEmpty;\n      firstEmpty = index;\n      --size;\n      return (T) values[index];\n    }\n\n    return null;\n  }\n\n  /**\n   * @return number of pairs currently in the map\n   */\n  public int size() {\n    return this.size;\n  }\n\n  /**\n   * Translates the mapped pairs' values into an array of Objects\n   * \n   * @return an object array of all the values currently in the map.\n   */\n  public Object[] toArray() {\n    int j = -1;\n    Object[] array = new Object[size];\n\n    // Iterates over the values, adding them to the array.\n    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {\n      array[++j] = iterator.next();\n    }\n    return array;\n  }\n\n  /**\n   * Translates the mapped pairs' values into an array of T\n   * \n   * @param a\n   *            the array into which the elements of the list are to be\n   *            stored, if it is big enough; otherwise, use whatever space we\n   *            have, setting the one after the true data as null.\n   * \n   * @return an array containing the elements of the list\n   * \n   */\n  public T[] toArray(T[] a) {\n    int j = 0;\n    // Iterates over the values, adding them to the array.\n    for (Iterator<T> iterator = iterator(); j < a.length\n    && iterator.hasNext(); ++j) {\n      a[j] = iterator.next();\n    }\n\n    if (j < a.length) {\n      a[j] = null;\n    }\n\n    return a;\n  }\n\n  @Override\n  public String toString() {\n    StringBuffer sb = new StringBuffer();\n    sb.append('{');\n    IntIterator keyIterator = keyIterator();\n    while (keyIterator.hasNext()) {\n      int key = keyIterator.next();\n      sb.append(key);\n      sb.append('=');\n      sb.append(get(key));\n      if (keyIterator.hasNext()) {\n        sb.append(',');\n        sb.append(' ');\n      }\n    }\n    sb.append('}');\n    return sb.toString();\n  }\n  \n  @Override\n  public int hashCode() {\n    return getClass().hashCode() ^ size();\n  }\n  \n  @SuppressWarnings(\"unchecked\")\n  @Override\n  public boolean equals(Object o) {\n    IntToObjectMap<T> that = (IntToObjectMap<T>)o;\n    if (that.size() != this.size()) {\n      return false;\n    }\n    \n    IntIterator it = keyIterator();\n    while (it.hasNext()) {\n      int key = it.next();\n      if (!that.containsKey(key)) {\n        return false;\n      }\n\n      T v1 = this.get(key);\n      T v2 = that.get(key);\n      if ((v1 == null && v2 != null) ||\n          (v1 != null && v2 == null) ||\n          (!v1.equals(v2))) {\n        return false;\n      }\n    }\n    return true;\n  }\n}",
        "methodName": "equals",
        "exampleID": 10,
        "dataset": "spotbugs",
        "filepath": "/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToObjectMap.java",
        "line": "626",
        "source": "v1",
        "sourceLine": "628",
        "qualifier": "Possible null pointer dereference of $$v1/$",
        "steps": [
            {
                "exampleID": 11
            }
        ],
        "line_number": "628"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java",
        "rawCode": "package org.apache.lucene.index;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.*;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.concurrent.atomic.AtomicInteger;\n\nimport org.apache.lucene.analysis.MockAnalyzer;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.PhraseQuery;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.ScoreDoc;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.TermQuery;\nimport org.apache.lucene.search.TopDocs;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.MockDirectoryWrapper;\nimport org.apache.lucene.util.Bits;\nimport org.apache.lucene.util.BytesRef;\nimport org.apache.lucene.util.FailOnNonBulkMergesInfoStream;\nimport org.apache.lucene.util.LineFileDocs;\nimport org.apache.lucene.util.LuceneTestCase;\nimport org.apache.lucene.util.NamedThreadFactory;\nimport org.apache.lucene.util.PrintStreamInfoStream;\nimport org.apache.lucene.util._TestUtil;\n\n// TODO\n//   - mix in forceMerge, addIndexes\n//   - randomly mix in non-congruent docs\n\n/** Utility class that spawns multiple indexing and\n *  searching threads. */\npublic abstract class ThreadedIndexingAndSearchingTestCase extends LuceneTestCase {\n\n  protected final AtomicBoolean failed = new AtomicBoolean();\n  protected final AtomicInteger addCount = new AtomicInteger();\n  protected final AtomicInteger delCount = new AtomicInteger();\n  protected final AtomicInteger packCount = new AtomicInteger();\n\n  protected MockDirectoryWrapper dir;\n  protected IndexWriter writer;\n\n  private static class SubDocs {\n    public final String packID;\n    public final List<String> subIDs;\n    public boolean deleted;\n\n    public SubDocs(String packID, List<String> subIDs) {\n      this.packID = packID;\n      this.subIDs = subIDs;\n    }\n  }\n\n  // Called per-search\n  protected abstract IndexSearcher getCurrentSearcher() throws Exception;\n\n  protected abstract IndexSearcher getFinalSearcher() throws Exception;\n\n  protected void releaseSearcher(IndexSearcher s) throws Exception {\n  }\n\n  // Called once to run searching\n  protected abstract void doSearching(ExecutorService es, long stopTime) throws Exception;\n\n  protected Directory getDirectory(Directory in) {\n    return in;\n  }\n\n  protected void updateDocuments(Term id, List<? extends IndexDocument> docs) throws Exception {\n    writer.updateDocuments(id, docs);\n  }\n\n  protected void addDocuments(Term id, List<? extends IndexDocument> docs) throws Exception {\n    writer.addDocuments(docs);\n  }\n\n  protected void addDocument(Term id, IndexDocument doc) throws Exception {\n    writer.addDocument(doc);\n  }\n\n  protected void updateDocument(Term term, IndexDocument doc) throws Exception {\n    writer.updateDocument(term, doc);\n  }\n\n  protected void deleteDocuments(Term term) throws Exception {\n    writer.deleteDocuments(term);\n  }\n\n  protected void doAfterIndexingThreadDone() {\n  }\n\n  private Thread[] launchIndexingThreads(final LineFileDocs docs,\n                                         int numThreads,\n                                         final long stopTime,\n                                         final Set<String> delIDs,\n                                         final Set<String> delPackIDs,\n                                         final List<SubDocs> allSubDocs) {\n    final Thread[] threads = new Thread[numThreads];\n    for(int thread=0;thread<numThreads;thread++) {\n      threads[thread] = new Thread() {\n          @Override\n          public void run() {\n            // TODO: would be better if this were cross thread, so that we make sure one thread deleting anothers added docs works:\n            final List<String> toDeleteIDs = new ArrayList<String>();\n            final List<SubDocs> toDeleteSubDocs = new ArrayList<SubDocs>();\n            while(System.currentTimeMillis() < stopTime && !failed.get()) {\n              try {\n\n                // Occasional longish pause if running\n                // nightly\n                if (LuceneTestCase.TEST_NIGHTLY && random().nextInt(6) == 3) {\n                  if (VERBOSE) {\n                    System.out.println(Thread.currentThread().getName() + \": now long sleep\");\n                  }\n                  Thread.sleep(_TestUtil.nextInt(random(), 50, 500));\n                }\n\n                // Rate limit ingest rate:\n                if (random().nextInt(7) == 5) {\n                  Thread.sleep(_TestUtil.nextInt(random(), 1, 10));\n                  if (VERBOSE) {\n                    System.out.println(Thread.currentThread().getName() + \": done sleep\");\n                  }\n                }\n\n                Document doc = docs.nextDoc();\n                if (doc == null) {\n                  break;\n                }\n\n                // Maybe add randomly named field\n                final String addedField;\n                if (random().nextBoolean()) {\n                  addedField = \"extra\" + random().nextInt(40);\n                  doc.add(newTextField(addedField, \"a random field\", Field.Store.YES));\n                } else {\n                  addedField = null;\n                }\n\n                if (random().nextBoolean()) {\n\n                  if (random().nextBoolean()) {\n                    // Add/update doc block:\n                    final String packID;\n                    final SubDocs delSubDocs;\n                    if (toDeleteSubDocs.size() > 0 && random().nextBoolean()) {\n                      delSubDocs = toDeleteSubDocs.get(random().nextInt(toDeleteSubDocs.size()));\n                      assert !delSubDocs.deleted;\n                      toDeleteSubDocs.remove(delSubDocs);\n                      // Update doc block, replacing prior packID\n                      packID = delSubDocs.packID;\n                    } else {\n                      delSubDocs = null;\n                      // Add doc block, using new packID\n                      packID = packCount.getAndIncrement() + \"\";\n                    }\n\n                    final Field packIDField = newStringField(\"packID\", packID, Field.Store.YES);\n                    final List<String> docIDs = new ArrayList<String>();\n                    final SubDocs subDocs = new SubDocs(packID, docIDs);\n                    final List<Document> docsList = new ArrayList<Document>();\n\n                    allSubDocs.add(subDocs);\n                    doc.add(packIDField);\n                    docsList.add(_TestUtil.cloneDocument(doc));\n                    docIDs.add(doc.get(\"docid\"));\n\n                    final int maxDocCount = _TestUtil.nextInt(random(), 1, 10);\n                    while(docsList.size() < maxDocCount) {\n                      doc = docs.nextDoc();\n                      if (doc == null) {\n                        break;\n                      }\n                      docsList.add(_TestUtil.cloneDocument(doc));\n                      docIDs.add(doc.get(\"docid\"));\n                    }\n                    addCount.addAndGet(docsList.size());\n\n                    final Term packIDTerm = new Term(\"packID\", packID);\n\n                    if (delSubDocs != null) {\n                      delSubDocs.deleted = true;\n                      delIDs.addAll(delSubDocs.subIDs);\n                      delCount.addAndGet(delSubDocs.subIDs.size());\n                      if (VERBOSE) {\n                        System.out.println(Thread.currentThread().getName() + \": update pack packID=\" + delSubDocs.packID + \" count=\" + docsList.size() + \" docs=\" + docIDs);\n                      }\n                      updateDocuments(packIDTerm, docsList);\n                    } else {\n                      if (VERBOSE) {\n                        System.out.println(Thread.currentThread().getName() + \": add pack packID=\" + packID + \" count=\" + docsList.size() + \" docs=\" + docIDs);\n                      }\n                      addDocuments(packIDTerm, docsList);\n                    }\n                    doc.removeField(\"packID\");\n\n                    if (random().nextInt(5) == 2) {\n                      if (VERBOSE) {\n                        System.out.println(Thread.currentThread().getName() + \": buffer del id:\" + packID);\n                      }\n                      toDeleteSubDocs.add(subDocs);\n                    }\n\n                  } else {\n                    // Add single doc\n                    final String docid = doc.get(\"docid\");\n                    if (VERBOSE) {\n                      System.out.println(Thread.currentThread().getName() + \": add doc docid:\" + docid);\n                    }\n                    addDocument(new Term(\"docid\", docid), doc);\n                    addCount.getAndIncrement();\n\n                    if (random().nextInt(5) == 3) {\n                      if (VERBOSE) {\n                        System.out.println(Thread.currentThread().getName() + \": buffer del id:\" + doc.get(\"docid\"));\n                      }\n                      toDeleteIDs.add(docid);\n                    }\n                  }\n                } else {\n\n                  // Update single doc, but we never re-use\n                  // and ID so the delete will never\n                  // actually happen:\n                  if (VERBOSE) {\n                    System.out.println(Thread.currentThread().getName() + \": update doc id:\" + doc.get(\"docid\"));\n                  }\n                  final String docid = doc.get(\"docid\");\n                  updateDocument(new Term(\"docid\", docid), doc);\n                  addCount.getAndIncrement();\n\n                  if (random().nextInt(5) == 3) {\n                    if (VERBOSE) {\n                      System.out.println(Thread.currentThread().getName() + \": buffer del id:\" + doc.get(\"docid\"));\n                    }\n                    toDeleteIDs.add(docid);\n                  }\n                }\n\n                if (random().nextInt(30) == 17) {\n                  if (VERBOSE) {\n                    System.out.println(Thread.currentThread().getName() + \": apply \" + toDeleteIDs.size() + \" deletes\");\n                  }\n                  for(String id : toDeleteIDs) {\n                    if (VERBOSE) {\n                      System.out.println(Thread.currentThread().getName() + \": del term=id:\" + id);\n                    }\n                    deleteDocuments(new Term(\"docid\", id));\n                  }\n                  final int count = delCount.addAndGet(toDeleteIDs.size());\n                  if (VERBOSE) {\n                    System.out.println(Thread.currentThread().getName() + \": tot \" + count + \" deletes\");\n                  }\n                  delIDs.addAll(toDeleteIDs);\n                  toDeleteIDs.clear();\n\n                  for(SubDocs subDocs : toDeleteSubDocs) {\n                    assert !subDocs.deleted;\n                    delPackIDs.add(subDocs.packID);\n                    deleteDocuments(new Term(\"packID\", subDocs.packID));\n                    subDocs.deleted = true;\n                    if (VERBOSE) {\n                      System.out.println(Thread.currentThread().getName() + \": del subs: \" + subDocs.subIDs + \" packID=\" + subDocs.packID);\n                    }\n                    delIDs.addAll(subDocs.subIDs);\n                    delCount.addAndGet(subDocs.subIDs.size());\n                  }\n                  toDeleteSubDocs.clear();\n                }\n                if (addedField != null) {\n                  doc.removeField(addedField);\n                }\n              } catch (Throwable t) {\n                System.out.println(Thread.currentThread().getName() + \": hit exc\");\n                t.printStackTrace();\n                failed.set(true);\n                throw new RuntimeException(t);\n              }\n            }\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": indexing done\");\n            }\n\n            doAfterIndexingThreadDone();\n          }\n        };\n      threads[thread].setDaemon(true);\n      threads[thread].start();\n    }\n\n    return threads;\n  }\n\n  protected void runSearchThreads(final long stopTimeMS) throws Exception {\n    final int numThreads = _TestUtil.nextInt(random(), 1, 5);\n    final Thread[] searchThreads = new Thread[numThreads];\n    final AtomicInteger totHits = new AtomicInteger();\n\n    // silly starting guess:\n    final AtomicInteger totTermCount = new AtomicInteger(100);\n\n    // TODO: we should enrich this to do more interesting searches\n    for(int thread=0;thread<searchThreads.length;thread++) {\n      searchThreads[thread] = new Thread() {\n          @Override\n          public void run() {\n            if (VERBOSE) {\n              System.out.println(Thread.currentThread().getName() + \": launch search thread\");\n            }\n            while (System.currentTimeMillis() < stopTimeMS) {\n              try {\n                final IndexSearcher s = getCurrentSearcher();\n                try {\n                  // Verify 1) IW is correctly setting\n                  // diagnostics, and 2) segment warming for\n                  // merged segments is actually happening:\n                  for(final AtomicReaderContext sub : s.getIndexReader().leaves()) {\n                    SegmentReader segReader = (SegmentReader) sub.reader();\n                    Map<String,String> diagnostics = segReader.getSegmentInfo().info.getDiagnostics();\n                    assertNotNull(diagnostics);\n                    String source = diagnostics.get(\"source\");\n                    assertNotNull(source);\n                    if (source.equals(\"merge\")) {\n                      assertTrue(\"sub reader \" + sub + \" wasn't warmed: warmed=\" + warmed + \" diagnostics=\" + diagnostics + \" si=\" + segReader.getSegmentInfo(),\n                                 !assertMergedSegmentsWarmed || warmed.containsKey(segReader.core));\n                    }\n                  }\n                  if (s.getIndexReader().numDocs() > 0) {\n                    smokeTestSearcher(s);\n                    Fields fields = MultiFields.getFields(s.getIndexReader());\n                    if (fields == null) {\n                      continue;\n                    }\n                    Terms terms = fields.terms(\"body\");\n                    if (terms == null) {\n                      continue;\n                    }\n                    TermsEnum termsEnum = terms.iterator(null);\n                    int seenTermCount = 0;\n                    int shift;\n                    int trigger; \n                    if (totTermCount.get() < 30) {\n                      shift = 0;\n                      trigger = 1;\n                    } else {\n                      trigger = totTermCount.get()/30;\n                      shift = random().nextInt(trigger);\n                    }\n                    while (System.currentTimeMillis() < stopTimeMS) {\n                      BytesRef term = termsEnum.next();\n                      if (term == null) {\n                        totTermCount.set(seenTermCount);\n                        break;\n                      }\n                      seenTermCount++;\n                      // search 30 terms\n                      if ((seenTermCount + shift) % trigger == 0) {\n                        //if (VERBOSE) {\n                        //System.out.println(Thread.currentThread().getName() + \" now search body:\" + term.utf8ToString());\n                        //}\n                        totHits.addAndGet(runQuery(s, new TermQuery(new Term(\"body\", term))));\n                      }\n                    }\n                    //if (VERBOSE) {\n                    //System.out.println(Thread.currentThread().getName() + \": search done\");\n                    //}\n                  }\n                } finally {\n                  releaseSearcher(s);\n                }\n              } catch (Throwable t) {\n                System.out.println(Thread.currentThread().getName() + \": hit exc\");\n                failed.set(true);\n                t.printStackTrace(System.out);\n                throw new RuntimeException(t);\n              }\n            }\n          }\n        };\n      searchThreads[thread].setDaemon(true);\n      searchThreads[thread].start();\n    }\n\n    for(int thread=0;thread<searchThreads.length;thread++) {\n      searchThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE search: totHits=\" + totHits);\n    }\n  }\n\n  protected void doAfterWriter(ExecutorService es) throws Exception {\n  }\n\n  protected void doClose() throws Exception {\n  }\n\n  protected boolean assertMergedSegmentsWarmed = true;\n\n  private final Map<SegmentCoreReaders,Boolean> warmed = Collections.synchronizedMap(new WeakHashMap<SegmentCoreReaders,Boolean>());\n\n  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newMockFSDirectory(tempDir); // some subclasses rely on this being MDW\n    dir.setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n  private int runQuery(IndexSearcher s, Query q) throws Exception {\n    s.search(q, 10);\n    int hitCount = s.search(q, null, 10, new Sort(new SortField(\"title\", SortField.Type.STRING))).totalHits;\n    final Sort dvSort = new Sort(new SortField(\"title\", SortField.Type.STRING));\n    int hitCount2 = s.search(q, null, 10, dvSort).totalHits;\n    assertEquals(hitCount, hitCount2);\n    return hitCount;\n  }\n\n  protected void smokeTestSearcher(IndexSearcher s) throws Exception {\n    runQuery(s, new TermQuery(new Term(\"body\", \"united\")));\n    runQuery(s, new TermQuery(new Term(\"titleTokenized\", \"states\")));\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"united\"));\n    pq.add(new Term(\"body\", \"states\"));\n    runQuery(s, pq);\n  }\n}\n",
        "methodName": "run",
        "exampleID": 12,
        "dataset": "spotbugs",
        "filepath": "/lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java",
        "line": "196",
        "source": "doc",
        "sourceLine": "219",
        "qualifier": "Possible null pointer dereference of $$doc/$",
        "steps": [
            {
                "exampleID": 13
            }
        ],
        "line_number": "219"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java",
        "rawCode": "package org.apache.lucene.misc;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport org.apache.lucene.index.DirectoryReader;\nimport org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.index.MultiFields;\nimport org.apache.lucene.index.Fields;\nimport org.apache.lucene.index.TermsEnum;\nimport org.apache.lucene.index.Terms;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.FSDirectory;\nimport org.apache.lucene.util.PriorityQueue;\nimport org.apache.lucene.util.BytesRef;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.Comparator;\n\n/**\n * <code>HighFreqTerms</code> class extracts the top n most frequent terms\n * (by document frequency) from an existing Lucene index and reports their\n * document frequency.\n * <p>\n * If the -t flag is given, both document frequency and total tf (total\n * number of occurrences) are reported, ordered by descending total tf.\n *\n */\npublic class HighFreqTerms {\n  \n  // The top numTerms will be displayed\n  public static final int DEFAULT_NUMTERMS = 100;\n  \n  public static void main(String[] args) throws Exception {\n    String field = null;\n    int numTerms = DEFAULT_NUMTERMS;\n   \n    if (args.length == 0 || args.length > 4) {\n      usage();\n      System.exit(1);\n    }     \n\n    Directory dir = FSDirectory.open(new File(args[0]));\n    \n    Comparator<TermStats> comparator = new DocFreqComparator();\n   \n    for (int i = 1; i < args.length; i++) {\n      if (args[i].equals(\"-t\")) {\n        comparator = new TotalTermFreqComparator();\n      }\n      else{\n        try {\n          numTerms = Integer.parseInt(args[i]);\n        } catch (NumberFormatException e) {\n          field=args[i];\n        }\n      }\n    }\n    \n    IndexReader reader = DirectoryReader.open(dir);\n    TermStats[] terms = getHighFreqTerms(reader, numTerms, field, comparator);\n\n    for (int i = 0; i < terms.length; i++) {\n      System.out.printf(\"%s:%s \\t totalTF = %,d \\t docFreq = %,d \\n\",\n            terms[i].field, terms[i].termtext.utf8ToString(), terms[i].totalTermFreq, terms[i].docFreq);\n    }\n    reader.close();\n  }\n  \n  private static void usage() {\n    System.out\n        .println(\"\\n\\n\"\n            + \"java org.apache.lucene.misc.HighFreqTerms <index dir> [-t] [number_terms] [field]\\n\\t -t: order by totalTermFreq\\n\\n\");\n  }\n  \n  /**\n   * Returns TermStats[] ordered by the specified comparator\n   */\n  public static TermStats[] getHighFreqTerms(IndexReader reader, int numTerms, String field, Comparator<TermStats> comparator) throws Exception {\n    TermStatsQueue tiq = null;\n    \n    if (field != null) {\n      Fields fields = MultiFields.getFields(reader);\n      if (fields == null) {\n        throw new RuntimeException(\"field \" + field + \" not found\");\n      }\n      Terms terms = fields.terms(field);\n      if (terms != null) {\n        TermsEnum termsEnum = terms.iterator(null);\n        tiq = new TermStatsQueue(numTerms, comparator);\n        tiq.fill(field, termsEnum);\n      }\n    } else {\n      Fields fields = MultiFields.getFields(reader);\n      if (fields == null) {\n        throw new RuntimeException(\"no fields found for this index\");\n      }\n      tiq = new TermStatsQueue(numTerms, comparator);\n      for (String fieldName : fields) {\n        Terms terms = fields.terms(fieldName);\n        if (terms != null) {\n          tiq.fill(fieldName, terms.iterator(null));\n        }\n      }\n    }\n    \n    TermStats[] result = new TermStats[tiq.size()];\n    // we want highest first so we read the queue and populate the array\n    // starting at the end and work backwards\n    int count = tiq.size() - 1;\n    while (tiq.size() != 0) {\n      result[count] = tiq.pop();\n      count--;\n    }\n    return result;\n  }\n  \n  /**\n   * Compares terms by docTermFreq\n   */\n  public static final class DocFreqComparator implements Comparator<TermStats> {\n    \n    @Override\n    public int compare(TermStats a, TermStats b) {\n      int res = Long.compare(a.docFreq, b.docFreq);\n      if (res == 0) {\n        res = a.field.compareTo(b.field);\n        if (res == 0) {\n          res = a.termtext.compareTo(b.termtext);\n        }\n      }\n      return res;\n    }\n  }\n\n  /**\n   * Compares terms by totalTermFreq\n   */\n  public static final class TotalTermFreqComparator implements Comparator<TermStats> {\n    \n    @Override\n    public int compare(TermStats a, TermStats b) {\n      int res = Long.compare(a.totalTermFreq, b.totalTermFreq);\n      if (res == 0) {\n        res = a.field.compareTo(b.field);\n        if (res == 0) {\n          res = a.termtext.compareTo(b.termtext);\n        }\n      }\n      return res;\n    }\n  }\n  \n  /**\n   * Priority queue for TermStats objects\n   **/\n  static final class TermStatsQueue extends PriorityQueue<TermStats> {\n    final Comparator<TermStats> comparator;\n    \n    TermStatsQueue(int size, Comparator<TermStats> comparator) {\n      super(size);\n      this.comparator = comparator;\n    }\n    \n    @Override\n    protected boolean lessThan(TermStats termInfoA, TermStats termInfoB) {\n      return comparator.compare(termInfoA, termInfoB) < 0;\n    }\n    \n    protected void fill(String field, TermsEnum termsEnum) throws IOException {\n      BytesRef term = null;\n      while ((term = termsEnum.next()) != null) {\n        insertWithOverflow(new TermStats(field, term, termsEnum.docFreq(), termsEnum.totalTermFreq()));\n      }\n    }\n  }\n}\n",
        "methodName": "getHighFreqTerms",
        "exampleID": 14,
        "dataset": "spotbugs",
        "filepath": "/lucene/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java",
        "line": "103",
        "source": "tiq",
        "sourceLine": "122",
        "qualifier": "Possible null pointer dereference of $$tiq/$",
        "steps": [
            {
                "exampleID": 15
            }
        ],
        "line_number": "122"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java",
        "rawCode": "/*\n * Created on 25-Jan-2006\n */\npackage org.apache.lucene.queryparser.xml.builders;\n\nimport java.io.IOException;\nimport java.util.HashSet;\nimport java.util.Set;\n\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\nimport org.apache.lucene.queries.mlt.MoreLikeThisQuery;\nimport org.apache.lucene.queryparser.xml.QueryBuilder;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.queryparser.xml.DOMUtils;\nimport org.apache.lucene.queryparser.xml.ParserException;\nimport org.w3c.dom.Element;\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n/**\n * Builder for {@link MoreLikeThisQuery}\n */\npublic class LikeThisQueryBuilder implements QueryBuilder {\n\n  private static final int DEFAULT_MAX_QUERY_TERMS = 20;\n  private static final int DEFAULT_MIN_TERM_FREQUENCY = 1;\n  private static final float DEFAULT_PERCENT_TERMS_TO_MATCH = 30; //default is a 3rd of selected terms must match\n\n  private final Analyzer analyzer;\n  private final String defaultFieldNames[];\n\n  public LikeThisQueryBuilder(Analyzer analyzer, String[] defaultFieldNames) {\n    this.analyzer = analyzer;\n    this.defaultFieldNames = defaultFieldNames;\n  }\n\n  /* (non-Javadoc)\n    * @see org.apache.lucene.xmlparser.QueryObjectBuilder#process(org.w3c.dom.Element)\n    */\n  @Override\n  public Query getQuery(Element e) throws ParserException {\n    String fieldsList = e.getAttribute(\"fieldNames\"); //a comma-delimited list of fields\n    String fields[] = defaultFieldNames;\n    if ((fieldsList != null) && (fieldsList.trim().length() > 0)) {\n      fields = fieldsList.trim().split(\",\");\n      //trim the fieldnames\n      for (int i = 0; i < fields.length; i++) {\n        fields[i] = fields[i].trim();\n      }\n    }\n\n    //Parse any \"stopWords\" attribute\n    //TODO MoreLikeThis needs to ideally have per-field stopWords lists - until then\n    //I use all analyzers/fields to generate multi-field compatible stop list\n    String stopWords = e.getAttribute(\"stopWords\");\n    Set<String> stopWordsSet = null;\n    if ((stopWords != null) && (fields != null)) {\n      stopWordsSet = new HashSet<String>();\n      for (String field : fields) {\n        try (TokenStream ts = analyzer.tokenStream(field, stopWords)) {\n          CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while (ts.incrementToken()) {\n            stopWordsSet.add(termAtt.toString());\n          }\n          ts.end();\n          ts.close();\n        } catch (IOException ioe) {\n          throw new ParserException(\"IoException parsing stop words list in \"\n              + getClass().getName() + \":\" + ioe.getLocalizedMessage());\n        }\n      }\n    }\n\n\n    MoreLikeThisQuery mlt = new MoreLikeThisQuery(DOMUtils.getText(e), fields, analyzer, fields[0]);\n    mlt.setMaxQueryTerms(DOMUtils.getAttribute(e, \"maxQueryTerms\", DEFAULT_MAX_QUERY_TERMS));\n    mlt.setMinTermFrequency(DOMUtils.getAttribute(e, \"minTermFrequency\", DEFAULT_MIN_TERM_FREQUENCY));\n    mlt.setPercentTermsToMatch(DOMUtils.getAttribute(e, \"percentTermsToMatch\", DEFAULT_PERCENT_TERMS_TO_MATCH) / 100);\n    mlt.setStopWords(stopWordsSet);\n    int minDocFreq = DOMUtils.getAttribute(e, \"minDocFreq\", -1);\n    if (minDocFreq >= 0) {\n      mlt.setMinDocFreq(minDocFreq);\n    }\n\n    mlt.setBoost(DOMUtils.getAttribute(e, \"boost\", 1.0f));\n\n    return mlt;\n  }\n\n}\n",
        "methodName": "getQuery",
        "exampleID": 16,
        "dataset": "spotbugs",
        "filepath": "/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java",
        "line": "73",
        "source": "fields",
        "sourceLine": "92",
        "qualifier": "Possible null pointer dereference of $$fields/$",
        "steps": [
            {
                "exampleID": 17
            }
        ],
        "line_number": "92"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/replicator/src/java/org/apache/lucene/replicator/PerSessionDirectoryFactory.java",
        "rawCode": "package org.apache.lucene.replicator;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.File;\nimport java.io.IOException;\n\nimport org.apache.lucene.replicator.ReplicationClient.SourceDirectoryFactory;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.FSDirectory;\n\n/**\n * A {@link SourceDirectoryFactory} which returns {@link FSDirectory} under a\n * dedicated session directory. When a session is over, the entire directory is\n * deleted.\n * \n * @lucene.experimental\n */\npublic class PerSessionDirectoryFactory implements SourceDirectoryFactory {\n  \n  private final File workDir;\n  \n  /** Constructor with the given sources mapping. */\n  public PerSessionDirectoryFactory(File workDir) {\n    this.workDir = workDir;\n  }\n  \n  private void rm(File file) throws IOException {\n    if (file.isDirectory()) {\n      for (File f : file.listFiles()) {\n        rm(f);\n      }\n    }\n    \n    // This should be either an empty directory, or a file\n    if (!file.delete() && file.exists()) {\n      throw new IOException(\"failed to delete \" + file);\n    }\n  }\n  \n  @Override\n  public Directory getDirectory(String sessionID, String source) throws IOException {\n    File sessionDir = new File(workDir, sessionID);\n    if (!sessionDir.exists() && !sessionDir.mkdirs()) {\n      throw new IOException(\"failed to create session directory \" + sessionDir);\n    }\n    File sourceDir = new File(sessionDir, source);\n    if (!sourceDir.mkdirs()) {\n      throw new IOException(\"failed to create source directory \" + sourceDir);\n    }\n    return FSDirectory.open(sourceDir);\n  }\n  \n  @Override\n  public void cleanupSession(String sessionID) throws IOException {\n    if (sessionID.isEmpty()) { // protect against deleting workDir entirely!\n      throw new IllegalArgumentException(\"sessionID cannot be empty\");\n    }\n    rm(new File(workDir, sessionID));\n  }\n  \n}\n",
        "methodName": "rm",
        "exampleID": 18,
        "dataset": "spotbugs",
        "filepath": "/lucene/replicator/src/java/org/apache/lucene/replicator/PerSessionDirectoryFactory.java",
        "line": "45",
        "source": "?",
        "sourceLine": "45",
        "qualifier": "Possible null pointer dereference of the $$value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 19
            }
        ],
        "line_number": "45"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/core/src/java/org/apache/lucene/search/BooleanScorer.java",
        "rawCode": "package org.apache.lucene.search;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.List;\n\nimport org.apache.lucene.index.AtomicReaderContext;\nimport org.apache.lucene.search.BooleanQuery.BooleanWeight;\n\n/* Description from Doug Cutting (excerpted from\n * LUCENE-1483):\n *\n * BooleanScorer uses an array to score windows of\n * 2K docs. So it scores docs 0-2K first, then docs 2K-4K,\n * etc. For each window it iterates through all query terms\n * and accumulates a score in table[doc%2K]. It also stores\n * in the table a bitmask representing which terms\n * contributed to the score. Non-zero scores are chained in\n * a linked list. At the end of scoring each window it then\n * iterates through the linked list and, if the bitmask\n * matches the boolean constraints, collects a hit. For\n * boolean queries with lots of frequent terms this can be\n * much faster, since it does not need to update a priority\n * queue for each posting, instead performing constant-time\n * operations per posting. The only downside is that it\n * results in hits being delivered out-of-order within the\n * window, which means it cannot be nested within other\n * scorers. But it works well as a top-level scorer.\n *\n * The new BooleanScorer2 implementation instead works by\n * merging priority queues of postings, albeit with some\n * clever tricks. For example, a pure conjunction (all terms\n * required) does not require a priority queue. Instead it\n * sorts the posting streams at the start, then repeatedly\n * skips the first to to the last. If the first ever equals\n * the last, then there's a hit. When some terms are\n * required and some terms are optional, the conjunction can\n * be evaluated first, then the optional terms can all skip\n * to the match and be added to the score. Thus the\n * conjunction can reduce the number of priority queue\n * updates for the optional terms. */\n\nfinal class BooleanScorer extends Scorer {\n  \n  private static final class BooleanScorerCollector extends Collector {\n    private BucketTable bucketTable;\n    private int mask;\n    private Scorer scorer;\n    \n    public BooleanScorerCollector(int mask, BucketTable bucketTable) {\n      this.mask = mask;\n      this.bucketTable = bucketTable;\n    }\n    \n    @Override\n    public void collect(final int doc) throws IOException {\n      final BucketTable table = bucketTable;\n      final int i = doc & BucketTable.MASK;\n      final Bucket bucket = table.buckets[i];\n      \n      if (bucket.doc != doc) {                    // invalid bucket\n        bucket.doc = doc;                         // set doc\n        bucket.score = scorer.score();            // initialize score\n        bucket.bits = mask;                       // initialize mask\n        bucket.coord = 1;                         // initialize coord\n\n        bucket.next = table.first;                // push onto valid list\n        table.first = bucket;\n      } else {                                    // valid bucket\n        bucket.score += scorer.score();           // increment score\n        bucket.bits |= mask;                      // add bits in mask\n        bucket.coord++;                           // increment coord\n      }\n    }\n    \n    @Override\n    public void setNextReader(AtomicReaderContext context) {\n      // not needed by this implementation\n    }\n    \n    @Override\n    public void setScorer(Scorer scorer) {\n      this.scorer = scorer;\n    }\n    \n    @Override\n    public boolean acceptsDocsOutOfOrder() {\n      return true;\n    }\n\n  }\n  \n  // An internal class which is used in score(Collector, int) for setting the\n  // current score. This is required since Collector exposes a setScorer method\n  // and implementations that need the score will call scorer.score().\n  // Therefore the only methods that are implemented are score() and doc().\n  private static final class BucketScorer extends Scorer {\n\n    double score;\n    int doc = NO_MORE_DOCS;\n    int freq;\n    \n    public BucketScorer(Weight weight) { super(weight); }\n    \n    @Override\n    public int advance(int target) { return NO_MORE_DOCS; }\n\n    @Override\n    public int docID() { return doc; }\n\n    @Override\n    public int freq() { return freq; }\n\n    @Override\n    public int nextDoc() { return NO_MORE_DOCS; }\n    \n    @Override\n    public float score() { return (float)score; }\n    \n    @Override\n    public long cost() { return 1; }\n\n  }\n\n  static final class Bucket {\n    int doc = -1;            // tells if bucket is valid\n    double score;             // incremental score\n    // TODO: break out bool anyProhibited, int\n    // numRequiredMatched; then we can remove 32 limit on\n    // required clauses\n    int bits;                // used for bool constraints\n    int coord;               // count of terms in score\n    Bucket next;             // next valid bucket\n  }\n  \n  /** A simple hash table of document scores within a range. */\n  static final class BucketTable {\n    public static final int SIZE = 1 << 11;\n    public static final int MASK = SIZE - 1;\n\n    final Bucket[] buckets = new Bucket[SIZE];\n    Bucket first = null;                          // head of valid list\n  \n    public BucketTable() {\n      // Pre-fill to save the lazy init when collecting\n      // each sub:\n      for(int idx=0;idx<SIZE;idx++) {\n        buckets[idx] = new Bucket();\n      }\n    }\n\n    public Collector newCollector(int mask) {\n      return new BooleanScorerCollector(mask, this);\n    }\n\n    public int size() { return SIZE; }\n  }\n\n  static final class SubScorer {\n    public Scorer scorer;\n    // TODO: re-enable this if BQ ever sends us required clauses\n    //public boolean required = false;\n    public boolean prohibited;\n    public Collector collector;\n    public SubScorer next;\n\n    public SubScorer(Scorer scorer, boolean required, boolean prohibited,\n        Collector collector, SubScorer next) {\n      if (required) {\n        throw new IllegalArgumentException(\"this scorer cannot handle required=true\");\n      }\n      this.scorer = scorer;\n      // TODO: re-enable this if BQ ever sends us required clauses\n      //this.required = required;\n      this.prohibited = prohibited;\n      this.collector = collector;\n      this.next = next;\n    }\n  }\n  \n  private SubScorer scorers = null;\n  private BucketTable bucketTable = new BucketTable();\n  private final float[] coordFactors;\n  // TODO: re-enable this if BQ ever sends us required clauses\n  //private int requiredMask = 0;\n  private final int minNrShouldMatch;\n  private int end;\n  private Bucket current;\n  // Any time a prohibited clause matches we set bit 0:\n  private static final int PROHIBITED_MASK = 1;\n  \n  BooleanScorer(BooleanWeight weight, boolean disableCoord, int minNrShouldMatch,\n      List<Scorer> optionalScorers, List<Scorer> prohibitedScorers, int maxCoord) throws IOException {\n    super(weight);\n    this.minNrShouldMatch = minNrShouldMatch;\n\n    if (optionalScorers != null && optionalScorers.size() > 0) {\n      for (Scorer scorer : optionalScorers) {\n        if (scorer.nextDoc() != NO_MORE_DOCS) {\n          scorers = new SubScorer(scorer, false, false, bucketTable.newCollector(0), scorers);\n        }\n      }\n    }\n    \n    if (prohibitedScorers != null && prohibitedScorers.size() > 0) {\n      for (Scorer scorer : prohibitedScorers) {\n        if (scorer.nextDoc() != NO_MORE_DOCS) {\n          scorers = new SubScorer(scorer, false, true, bucketTable.newCollector(PROHIBITED_MASK), scorers);\n        }\n      }\n    }\n\n    coordFactors = new float[optionalScorers.size() + 1];\n    for (int i = 0; i < coordFactors.length; i++) {\n      coordFactors[i] = disableCoord ? 1.0f : weight.coord(i, maxCoord); \n    }\n  }\n\n  // firstDocID is ignored since nextDoc() initializes 'current'\n  @Override\n  public boolean score(Collector collector, int max, int firstDocID) throws IOException {\n    // Make sure it's only BooleanScorer that calls us:\n    assert firstDocID == -1;\n    boolean more;\n    Bucket tmp;\n    BucketScorer bs = new BucketScorer(weight);\n\n    // The internal loop will set the score and doc before calling collect.\n    collector.setScorer(bs);\n    do {\n      bucketTable.first = null;\n      \n      while (current != null) {         // more queued \n\n        // check prohibited & required\n        if ((current.bits & PROHIBITED_MASK) == 0) {\n\n          // TODO: re-enable this if BQ ever sends us required\n          // clauses\n          //&& (current.bits & requiredMask) == requiredMask) {\n          \n          // NOTE: Lucene always passes max =\n          // Integer.MAX_VALUE today, because we never embed\n          // a BooleanScorer inside another (even though\n          // that should work)... but in theory an outside\n          // app could pass a different max so we must check\n          // it:\n          if (current.doc >= max){\n            tmp = current;\n            current = current.next;\n            tmp.next = bucketTable.first;\n            bucketTable.first = tmp;\n            continue;\n          }\n          \n          if (current.coord >= minNrShouldMatch) {\n            bs.score = current.score * coordFactors[current.coord];\n            bs.doc = current.doc;\n            bs.freq = current.coord;\n            collector.collect(current.doc);\n          }\n        }\n        \n        current = current.next;         // pop the queue\n      }\n      \n      if (bucketTable.first != null){\n        current = bucketTable.first;\n        bucketTable.first = current.next;\n        return true;\n      }\n\n      // refill the queue\n      more = false;\n      end += BucketTable.SIZE;\n      for (SubScorer sub = scorers; sub != null; sub = sub.next) {\n        int subScorerDocID = sub.scorer.docID();\n        if (subScorerDocID != NO_MORE_DOCS) {\n          more |= sub.scorer.score(sub.collector, end, subScorerDocID);\n        }\n      }\n      current = bucketTable.first;\n      \n    } while (current != null || more);\n\n    return false;\n  }\n  \n  @Override\n  public int advance(int target) {\n    throw new UnsupportedOperationException();\n  }\n\n  @Override\n  public int docID() {\n    throw new UnsupportedOperationException();\n  }\n\n  @Override\n  public int nextDoc() {\n    throw new UnsupportedOperationException();\n  }\n\n  @Override\n  public float score() {\n    throw new UnsupportedOperationException();\n  }\n\n  @Override\n  public int freq() throws IOException {\n    throw new UnsupportedOperationException();\n  }\n\n  @Override\n  public long cost() {\n    return Integer.MAX_VALUE;\n  }\n\n  @Override\n  public void score(Collector collector) throws IOException {\n    score(collector, Integer.MAX_VALUE, -1);\n  }\n  \n  @Override\n  public String toString() {\n    StringBuilder buffer = new StringBuilder();\n    buffer.append(\"boolean(\");\n    for (SubScorer sub = scorers; sub != null; sub = sub.next) {\n      buffer.append(sub.scorer.toString());\n      buffer.append(\" \");\n    }\n    buffer.append(\")\");\n    return buffer.toString();\n  }\n  \n  @Override\n  public Collection<ChildScorer> getChildren() {\n    throw new UnsupportedOperationException();\n  }\n}\n",
        "methodName": "<init>",
        "exampleID": 20,
        "dataset": "spotbugs",
        "filepath": "/lucene/core/src/java/org/apache/lucene/search/BooleanScorer.java",
        "line": "215",
        "source": "optionalScorers",
        "sourceLine": "231",
        "qualifier": "Possible null pointer dereference of $$optionalScorers/$",
        "steps": [
            {
                "exampleID": 21
            }
        ],
        "line_number": "231"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/core/src/java/org/apache/lucene/search/SloppyPhraseScorer.java",
        "rawCode": "package org.apache.lucene.search;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.LinkedHashMap;\n\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.similarities.Similarity;\nimport org.apache.lucene.util.OpenBitSet;\n\nfinal class SloppyPhraseScorer extends Scorer {\n  private PhrasePositions min, max;\n\n  private float sloppyFreq; //phrase frequency in current doc as computed by phraseFreq().\n\n  private final Similarity.SimScorer docScorer;\n  \n  private final int slop;\n  private final int numPostings;\n  private final PhraseQueue pq; // for advancing min position\n  \n  private int end; // current largest phrase position  \n\n  private boolean hasRpts; // flag indicating that there are repetitions (as checked in first candidate doc)\n  private boolean checkedRpts; // flag to only check for repetitions in first candidate doc\n  private boolean hasMultiTermRpts; //  \n  private PhrasePositions[][] rptGroups; // in each group are PPs that repeats each other (i.e. same term), sorted by (query) offset \n  private PhrasePositions[] rptStack; // temporary stack for switching colliding repeating pps \n  \n  private int numMatches;\n  private final long cost;\n  \n  SloppyPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n      int slop, Similarity.SimScorer docScorer) {\n    super(weight);\n    this.docScorer = docScorer;\n    this.slop = slop;\n    this.numPostings = postings==null ? 0 : postings.length;\n    pq = new PhraseQueue(postings.length);\n    // min(cost)\n    cost = postings[0].postings.cost();\n    // convert tps to a list of phrase positions.\n    // note: phrase-position differs from term-position in that its position\n    // reflects the phrase offset: pp.pos = tp.pos - offset.\n    // this allows to easily identify a matching (exact) phrase \n    // when all PhrasePositions have exactly the same position.\n    if (postings.length > 0) {\n      min = new PhrasePositions(postings[0].postings, postings[0].position, 0, postings[0].terms);\n      max = min;\n      max.doc = -1;\n      for (int i = 1; i < postings.length; i++) {\n        PhrasePositions pp = new PhrasePositions(postings[i].postings, postings[i].position, i, postings[i].terms);\n        max.next = pp;\n        max = pp;\n        max.doc = -1;\n      }\n      max.next = min; // make it cyclic for easier manipulation\n    }\n  }\n\n  /**\n   * Score a candidate doc for all slop-valid position-combinations (matches) \n   * encountered while traversing/hopping the PhrasePositions.\n   * <br> The score contribution of a match depends on the distance: \n   * <br> - highest score for distance=0 (exact match).\n   * <br> - score gets lower as distance gets higher.\n   * <br>Example: for query \"a b\"~2, a document \"x a b a y\" can be scored twice: \n   * once for \"a b\" (distance=0), and once for \"b a\" (distance=2).\n   * <br>Possibly not all valid combinations are encountered, because for efficiency  \n   * we always propagate the least PhrasePosition. This allows to base on \n   * PriorityQueue and move forward faster. \n   * As result, for example, document \"a b c b a\"\n   * would score differently for queries \"a b c\"~4 and \"c b a\"~4, although \n   * they really are equivalent. \n   * Similarly, for doc \"a b c b a f g\", query \"c b\"~2 \n   * would get same score as \"g f\"~2, although \"c b\"~2 could be matched twice.\n   * We may want to fix this in the future (currently not, for performance reasons).\n   */\n  private float phraseFreq() throws IOException {\n    if (!initPhrasePositions()) {\n      return 0.0f;\n    }\n    float freq = 0.0f;\n    numMatches = 0;\n    PhrasePositions pp = pq.pop();\n    int matchLength = end - pp.position;\n    int next = pq.top().position; \n    while (advancePP(pp)) {\n      if (hasRpts && !advanceRpts(pp)) {\n        break; // pps exhausted\n      }\n      if (pp.position > next) { // done minimizing current match-length \n        if (matchLength <= slop) {\n          freq += docScorer.computeSlopFactor(matchLength); // score match\n          numMatches++;\n        }      \n        pq.add(pp);\n        pp = pq.pop();\n        next = pq.top().position;\n        matchLength = end - pp.position;\n      } else {\n        int matchLength2 = end - pp.position;\n        if (matchLength2 < matchLength) {\n          matchLength = matchLength2;\n        }\n      }\n    }\n    if (matchLength <= slop) {\n      freq += docScorer.computeSlopFactor(matchLength); // score match\n      numMatches++;\n    }    \n    return freq;\n  }\n\n  /** advance a PhrasePosition and update 'end', return false if exhausted */\n  private boolean advancePP(PhrasePositions pp) throws IOException {\n    if (!pp.nextPosition()) {\n      return false;\n    }\n    if (pp.position > end) {\n      end = pp.position;\n    }\n    return true;\n  }\n  \n  /** pp was just advanced. If that caused a repeater collision, resolve by advancing the lesser\n   * of the two colliding pps. Note that there can only be one collision, as by the initialization\n   * there were no collisions before pp was advanced.  */\n  private boolean advanceRpts(PhrasePositions pp) throws IOException {\n    if (pp.rptGroup < 0) {\n      return true; // not a repeater\n    }\n    PhrasePositions[] rg = rptGroups[pp.rptGroup];\n    OpenBitSet bits = new OpenBitSet(rg.length); // for re-queuing after collisions are resolved\n    int k0 = pp.rptInd;\n    int k;\n    while((k=collide(pp)) >= 0) {\n      pp = lesser(pp, rg[k]); // always advance the lesser of the (only) two colliding pps\n      if (!advancePP(pp)) {\n        return false; // exhausted\n      }\n      if (k != k0) { // careful: mark only those currently in the queue\n        bits.set(k); // mark that pp2 need to be re-queued\n      }\n    }\n    // collisions resolved, now re-queue\n    // empty (partially) the queue until seeing all pps advanced for resolving collisions\n    int n = 0;\n    while (bits.cardinality() > 0) {\n      PhrasePositions pp2 = pq.pop();\n      rptStack[n++] = pp2;\n      if (pp2.rptGroup >= 0 && bits.get(pp2.rptInd)) {\n        bits.clear(pp2.rptInd);\n      }\n    }\n    // add back to queue\n    for (int i=n-1; i>=0; i--) {\n      pq.add(rptStack[i]);\n    }\n    return true;\n  }\n\n  /** compare two pps, but only by position and offset */\n  private PhrasePositions lesser(PhrasePositions pp, PhrasePositions pp2) {\n    if (pp.position < pp2.position ||\n        (pp.position == pp2.position && pp.offset < pp2.offset)) {\n      return pp;\n    }\n    return pp2;\n  }\n\n  /** index of a pp2 colliding with pp, or -1 if none */\n  private int collide(PhrasePositions pp) {\n    int tpPos = tpPos(pp);\n    PhrasePositions[] rg = rptGroups[pp.rptGroup];\n    for (int i=0; i<rg.length; i++) {\n      PhrasePositions pp2 = rg[i];\n      if (pp2 != pp && tpPos(pp2) == tpPos) {\n        return pp2.rptInd;\n      }\n    }\n    return -1;\n  }\n\n  /**\n   * Initialize PhrasePositions in place.\n   * A one time initialization for this scorer (on first doc matching all terms):\n   * <ul>\n   *  <li>Check if there are repetitions\n   *  <li>If there are, find groups of repetitions.\n   * </ul>\n   * Examples:\n   * <ol>\n   *  <li>no repetitions: <b>\"ho my\"~2</b>\n   *  <li>repetitions: <b>\"ho my my\"~2</b>\n   *  <li>repetitions: <b>\"my ho my\"~2</b>\n   * </ol>\n   * @return false if PPs are exhausted (and so current doc will not be a match) \n   */\n  private boolean initPhrasePositions() throws IOException {\n    end = Integer.MIN_VALUE;\n    if (!checkedRpts) {\n      return initFirstTime();\n    }\n    if (!hasRpts) {\n      initSimple();\n      return true; // PPs available\n    }\n    return initComplex();\n  }\n  \n  /** no repeats: simplest case, and most common. It is important to keep this piece of the code simple and efficient */\n  private void initSimple() throws IOException {\n    //System.err.println(\"initSimple: doc: \"+min.doc);\n    pq.clear();\n    // position pps and build queue from list\n    for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) {  // iterate cyclic list: done once handled max\n      pp.firstPosition();\n      if (pp.position > end) {\n        end = pp.position;\n      }\n      pq.add(pp);\n    }\n  }\n  \n  /** with repeats: not so simple. */\n  private boolean initComplex() throws IOException {\n    //System.err.println(\"initComplex: doc: \"+min.doc);\n    placeFirstPositions();\n    if (!advanceRepeatGroups()) {\n      return false; // PPs exhausted\n    }\n    fillQueue();\n    return true; // PPs available\n  }\n\n  /** move all PPs to their first position */\n  private void placeFirstPositions() throws IOException {\n    for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) { // iterate cyclic list: done once handled max\n      pp.firstPosition();\n    }\n  }\n\n  /** Fill the queue (all pps are already placed */\n  private void fillQueue() {\n    pq.clear();\n    for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) {  // iterate cyclic list: done once handled max\n      if (pp.position > end) {\n        end = pp.position;\n      }\n      pq.add(pp);\n    }\n  }\n\n  /** At initialization (each doc), each repetition group is sorted by (query) offset.\n   * This provides the start condition: no collisions.\n   * <p>Case 1: no multi-term repeats<br>\n   * It is sufficient to advance each pp in the group by one less than its group index.\n   * So lesser pp is not advanced, 2nd one advance once, 3rd one advanced twice, etc.\n   * <p>Case 2: multi-term repeats<br>\n   * \n   * @return false if PPs are exhausted. \n   */\n  private boolean advanceRepeatGroups() throws IOException {\n    for (PhrasePositions[] rg: rptGroups) { \n      if (hasMultiTermRpts) {\n        // more involved, some may not collide\n        int incr;\n        for (int i=0; i<rg.length; i+=incr) {\n          incr = 1;\n          PhrasePositions pp = rg[i];\n          int k;\n          while((k=collide(pp)) >= 0) {\n            PhrasePositions pp2 = lesser(pp, rg[k]);\n            if (!advancePP(pp2)) {  // at initialization always advance pp with higher offset\n              return false; // exhausted\n            }\n            if (pp2.rptInd < i) { // should not happen?\n              incr = 0;\n              break;\n            }\n          }\n        }\n      } else {\n        // simpler, we know exactly how much to advance\n        for (int j=1; j<rg.length; j++) {\n          for (int k=0; k<j; k++) {\n            if (!rg[j].nextPosition()) {\n              return false; // PPs exhausted\n            }\n          }\n        }\n      }\n    }\n    return true; // PPs available\n  }\n  \n  /** initialize with checking for repeats. Heavy work, but done only for the first candidate doc.<p>\n   * If there are repetitions, check if multi-term postings (MTP) are involved.<p>\n   * Without MTP, once PPs are placed in the first candidate doc, repeats (and groups) are visible.<br>\n   * With MTP, a more complex check is needed, up-front, as there may be \"hidden collisions\".<br>\n   * For example P1 has {A,B}, P1 has {B,C}, and the first doc is: \"A C B\". At start, P1 would point\n   * to \"A\", p2 to \"C\", and it will not be identified that P1 and P2 are repetitions of each other.<p>\n   * The more complex initialization has two parts:<br>\n   * (1) identification of repetition groups.<br>\n   * (2) advancing repeat groups at the start of the doc.<br>\n   * For (1), a possible solution is to just create a single repetition group, \n   * made of all repeating pps. But this would slow down the check for collisions, \n   * as all pps would need to be checked. Instead, we compute \"connected regions\" \n   * on the bipartite graph of postings and terms.  \n   */\n  private boolean initFirstTime() throws IOException {\n    //System.err.println(\"initFirstTime: doc: \"+min.doc);\n    checkedRpts = true;\n    placeFirstPositions();\n\n    LinkedHashMap<Term,Integer> rptTerms = repeatingTerms(); \n    hasRpts = !rptTerms.isEmpty();\n\n    if (hasRpts) {\n      rptStack = new PhrasePositions[numPostings]; // needed with repetitions\n      ArrayList<ArrayList<PhrasePositions>> rgs = gatherRptGroups(rptTerms);\n      sortRptGroups(rgs);\n      if (!advanceRepeatGroups()) {\n        return false; // PPs exhausted\n      }\n    }\n    \n    fillQueue();\n    return true; // PPs available\n  }\n\n  /** sort each repetition group by (query) offset. \n   * Done only once (at first doc) and allows to initialize faster for each doc. */\n  private void sortRptGroups(ArrayList<ArrayList<PhrasePositions>> rgs) {\n    rptGroups = new PhrasePositions[rgs.size()][];\n    Comparator<PhrasePositions> cmprtr = new Comparator<PhrasePositions>() {\n      @Override\n      public int compare(PhrasePositions pp1, PhrasePositions pp2) {\n        return pp1.offset - pp2.offset;\n      }\n    };\n    for (int i=0; i<rptGroups.length; i++) {\n      PhrasePositions[] rg = rgs.get(i).toArray(new PhrasePositions[0]);\n      Arrays.sort(rg, cmprtr);\n      rptGroups[i] = rg;\n      for (int j=0; j<rg.length; j++) {\n        rg[j].rptInd = j; // we use this index for efficient re-queuing\n      }\n    }\n  }\n\n  /** Detect repetition groups. Done once - for first doc */\n  private ArrayList<ArrayList<PhrasePositions>> gatherRptGroups(LinkedHashMap<Term,Integer> rptTerms) throws IOException {\n    PhrasePositions[] rpp = repeatingPPs(rptTerms); \n    ArrayList<ArrayList<PhrasePositions>> res = new ArrayList<ArrayList<PhrasePositions>>();\n    if (!hasMultiTermRpts) {\n      // simpler - no multi-terms - can base on positions in first doc\n      for (int i=0; i<rpp.length; i++) {\n        PhrasePositions pp = rpp[i];\n        if (pp.rptGroup >=0) continue; // already marked as a repetition\n        int tpPos = tpPos(pp);\n        for (int j=i+1; j<rpp.length; j++) {\n          PhrasePositions pp2 = rpp[j];\n          if (\n              pp2.rptGroup >=0        // already marked as a repetition\n              || pp2.offset == pp.offset // not a repetition: two PPs are originally in same offset in the query! \n              || tpPos(pp2) != tpPos) {  // not a repetition\n            continue; \n          }\n          // a repetition\n          int g = pp.rptGroup;\n          if (g < 0) {\n            g = res.size();\n            pp.rptGroup = g;  \n            ArrayList<PhrasePositions> rl = new ArrayList<PhrasePositions>(2);\n            rl.add(pp);\n            res.add(rl); \n          }\n          pp2.rptGroup = g;\n          res.get(g).add(pp2);\n        }\n      }\n    } else {\n      // more involved - has multi-terms\n      ArrayList<HashSet<PhrasePositions>> tmp = new ArrayList<HashSet<PhrasePositions>>();\n      ArrayList<OpenBitSet> bb = ppTermsBitSets(rpp, rptTerms);\n      unionTermGroups(bb);\n      HashMap<Term,Integer> tg = termGroups(rptTerms, bb);\n      HashSet<Integer> distinctGroupIDs = new HashSet<Integer>(tg.values());\n      for (int i=0; i<distinctGroupIDs.size(); i++) {\n        tmp.add(new HashSet<PhrasePositions>());\n      }\n      for (PhrasePositions pp : rpp) {\n        for (Term t: pp.terms) {\n          if (rptTerms.containsKey(t)) {\n            int g = tg.get(t);\n            tmp.get(g).add(pp);\n            assert pp.rptGroup==-1 || pp.rptGroup==g;  \n            pp.rptGroup = g;\n          }\n        }\n      }\n      for (HashSet<PhrasePositions> hs : tmp) {\n        res.add(new ArrayList<PhrasePositions>(hs));\n      }\n    }\n    return res;\n  }\n\n  /** Actual position in doc of a PhrasePosition, relies on that position = tpPos - offset) */\n  private final int tpPos(PhrasePositions pp) {\n    return pp.position + pp.offset;\n  }\n\n  /** find repeating terms and assign them ordinal values */\n  private LinkedHashMap<Term,Integer> repeatingTerms() {\n    LinkedHashMap<Term,Integer> tord = new LinkedHashMap<Term,Integer>();\n    HashMap<Term,Integer> tcnt = new HashMap<Term,Integer>();\n    for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) { // iterate cyclic list: done once handled max\n      for (Term t : pp.terms) {\n        Integer cnt0 = tcnt.get(t);\n        Integer cnt = cnt0==null ? new Integer(1) : new Integer(1+cnt0.intValue());\n        tcnt.put(t, cnt);\n        if (cnt==2) {\n          tord.put(t,tord.size());\n        }\n      }\n    }\n    return tord;\n  }\n\n  /** find repeating pps, and for each, if has multi-terms, update this.hasMultiTermRpts */\n  private PhrasePositions[] repeatingPPs(HashMap<Term,Integer> rptTerms) {\n    ArrayList<PhrasePositions> rp = new ArrayList<PhrasePositions>(); \n    for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) { // iterate cyclic list: done once handled max\n      for (Term t : pp.terms) {\n        if (rptTerms.containsKey(t)) {\n          rp.add(pp);\n          hasMultiTermRpts |= (pp.terms.length > 1);\n          break;\n        }\n      }\n    }\n    return rp.toArray(new PhrasePositions[0]);\n  }\n  \n  /** bit-sets - for each repeating pp, for each of its repeating terms, the term ordinal values is set */\n  private ArrayList<OpenBitSet> ppTermsBitSets(PhrasePositions[] rpp, HashMap<Term,Integer> tord) {\n    ArrayList<OpenBitSet> bb = new ArrayList<OpenBitSet>(rpp.length);\n    for (PhrasePositions pp : rpp) {\n      OpenBitSet b = new OpenBitSet(tord.size());\n      Integer ord;\n      for (Term t: pp.terms) {\n        if ((ord=tord.get(t))!=null) {\n          b.set(ord);\n        }\n      }\n      bb.add(b);\n    }\n    return bb;\n  }\n  \n  /** union (term group) bit-sets until they are disjoint (O(n^^2)), and each group have different terms */\n  private void unionTermGroups(ArrayList<OpenBitSet> bb) {\n    int incr;\n    for (int i=0; i<bb.size()-1; i+=incr) {\n      incr = 1;\n      int j = i+1;\n      while (j<bb.size()) {\n        if (bb.get(i).intersects(bb.get(j))) {\n          bb.get(i).union(bb.get(j));\n          bb.remove(j);\n          incr = 0;\n        } else {\n          ++j;\n        }\n      }\n    }\n  }\n  \n  /** map each term to the single group that contains it */ \n  private HashMap<Term,Integer> termGroups(LinkedHashMap<Term,Integer> tord, ArrayList<OpenBitSet> bb) throws IOException {\n    HashMap<Term,Integer> tg = new HashMap<Term,Integer>();\n    Term[] t = tord.keySet().toArray(new Term[0]);\n    for (int i=0; i<bb.size(); i++) { // i is the group no.\n      DocIdSetIterator bits = bb.get(i).iterator();\n      int ord;\n      while ((ord=bits.nextDoc())!=NO_MORE_DOCS) {\n        tg.put(t[ord],i);\n      }\n    }\n    return tg;\n  }\n\n  @Override\n  public int freq() {\n    return numMatches;\n  }\n  \n  float sloppyFreq() {\n    return sloppyFreq;\n  }\n  \n//  private void printQueue(PrintStream ps, PhrasePositions ext, String title) {\n//    //if (min.doc != ?) return;\n//    ps.println();\n//    ps.println(\"---- \"+title);\n//    ps.println(\"EXT: \"+ext);\n//    PhrasePositions[] t = new PhrasePositions[pq.size()];\n//    if (pq.size()>0) {\n//      t[0] = pq.pop();\n//      ps.println(\"  \" + 0 + \"  \" + t[0]);\n//      for (int i=1; i<t.length; i++) {\n//        t[i] = pq.pop();\n//        assert t[i-1].position <= t[i].position;\n//        ps.println(\"  \" + i + \"  \" + t[i]);\n//      }\n//      // add them back\n//      for (int i=t.length-1; i>=0; i--) {\n//        pq.add(t[i]);\n//      }\n//    }\n//  }\n  \n  private boolean advanceMin(int target) throws IOException {\n    if (!min.skipTo(target)) { \n      max.doc = NO_MORE_DOCS; // for further calls to docID() \n      return false;\n    }\n    min = min.next; // cyclic\n    max = max.next; // cyclic\n    return true;\n  }\n  \n  @Override\n  public int docID() {\n    return max.doc; \n  }\n\n  @Override\n  public int nextDoc() throws IOException {\n    return advance(max.doc + 1); // advance to the next doc after #docID()\n  }\n  \n  @Override\n  public float score() {\n    return docScorer.score(max.doc, sloppyFreq);\n  }\n\n  @Override\n  public int advance(int target) throws IOException {\n    assert target > docID();\n    do {\n      if (!advanceMin(target)) {\n        return NO_MORE_DOCS;\n      }\n      while (min.doc < max.doc) {\n        if (!advanceMin(max.doc)) {\n          return NO_MORE_DOCS;\n        }\n      }\n      // found a doc with all of the terms\n      sloppyFreq = phraseFreq(); // check for phrase\n      target = min.doc + 1; // next target in case sloppyFreq is still 0\n    } while (sloppyFreq == 0f);\n\n    // found a match\n    return max.doc;\n  }\n\n  @Override\n  public long cost() {\n    return cost;\n  }\n\n  @Override\n  public String toString() { return \"scorer(\" + weight + \")\"; }\n}\n",
        "methodName": "<init>",
        "exampleID": 22,
        "dataset": "spotbugs",
        "filepath": "/lucene/core/src/java/org/apache/lucene/search/SloppyPhraseScorer.java",
        "line": "59",
        "source": "postings",
        "sourceLine": "60",
        "qualifier": "Possible null pointer dereference of $$postings/$",
        "steps": [
            {
                "exampleID": 23
            }
        ],
        "line_number": "60"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/core/src/java/org/apache/lucene/util/RamUsageEstimator.java",
        "rawCode": "package org.apache.lucene.util;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.lang.management.ManagementFactory;\nimport java.lang.management.PlatformManagedObject;\nimport java.lang.reflect.*;\nimport java.text.DecimalFormat;\nimport java.text.DecimalFormatSymbols;\nimport java.util.*;\n\n/**\n * Estimates the size (memory representation) of Java objects.\n * \n * @see #sizeOf(Object)\n * @see #shallowSizeOf(Object)\n * @see #shallowSizeOfInstance(Class)\n * \n * @lucene.internal\n */\npublic final class RamUsageEstimator {\n  /**\n   * JVM diagnostic features.\n   */\n  public static enum JvmFeature {\n    OBJECT_REFERENCE_SIZE(\"Object reference size estimated using array index scale\"),\n    ARRAY_HEADER_SIZE(\"Array header size estimated using array based offset\"),\n    FIELD_OFFSETS(\"Shallow instance size based on field offsets\"),\n    OBJECT_ALIGNMENT(\"Object alignment retrieved from HotSpotDiagnostic MX bean\");\n\n    public final String description;\n\n    private JvmFeature(String description) {\n      this.description = description;\n    }\n    \n    @Override\n    public String toString() {\n      return super.name() + \" (\" + description + \")\";\n    }\n  }\n\n  /** JVM info string for debugging and reports. */\n  public final static String JVM_INFO_STRING;\n\n  /** One kilobyte bytes. */\n  public static final long ONE_KB = 1024;\n  \n  /** One megabyte bytes. */\n  public static final long ONE_MB = ONE_KB * ONE_KB;\n  \n  /** One gigabyte bytes.*/\n  public static final long ONE_GB = ONE_KB * ONE_MB;\n\n  /** No instantiation. */\n  private RamUsageEstimator() {}\n\n  public final static int NUM_BYTES_BOOLEAN = 1;\n  public final static int NUM_BYTES_BYTE = 1;\n  public final static int NUM_BYTES_CHAR = 2;\n  public final static int NUM_BYTES_SHORT = 2;\n  public final static int NUM_BYTES_INT = 4;\n  public final static int NUM_BYTES_FLOAT = 4;\n  public final static int NUM_BYTES_LONG = 8;\n  public final static int NUM_BYTES_DOUBLE = 8;\n\n  /** \n   * Number of bytes this jvm uses to represent an object reference. \n   */\n  public final static int NUM_BYTES_OBJECT_REF;\n\n  /**\n   * Number of bytes to represent an object header (no fields, no alignments).\n   */\n  public final static int NUM_BYTES_OBJECT_HEADER;\n\n  /**\n   * Number of bytes to represent an array header (no content, but with alignments).\n   */\n  public final static int NUM_BYTES_ARRAY_HEADER;\n  \n  /**\n   * A constant specifying the object alignment boundary inside the JVM. Objects will\n   * always take a full multiple of this constant, possibly wasting some space. \n   */\n  public final static int NUM_BYTES_OBJECT_ALIGNMENT;\n\n  /**\n   * Sizes of primitive classes.\n   */\n  private static final Map<Class<?>,Integer> primitiveSizes;\n  static {\n    primitiveSizes = new IdentityHashMap<Class<?>,Integer>();\n    primitiveSizes.put(boolean.class, Integer.valueOf(NUM_BYTES_BOOLEAN));\n    primitiveSizes.put(byte.class, Integer.valueOf(NUM_BYTES_BYTE));\n    primitiveSizes.put(char.class, Integer.valueOf(NUM_BYTES_CHAR));\n    primitiveSizes.put(short.class, Integer.valueOf(NUM_BYTES_SHORT));\n    primitiveSizes.put(int.class, Integer.valueOf(NUM_BYTES_INT));\n    primitiveSizes.put(float.class, Integer.valueOf(NUM_BYTES_FLOAT));\n    primitiveSizes.put(double.class, Integer.valueOf(NUM_BYTES_DOUBLE));\n    primitiveSizes.put(long.class, Integer.valueOf(NUM_BYTES_LONG));\n  }\n\n  /**\n   * A handle to <code>sun.misc.Unsafe</code>.\n   */\n  private final static Object theUnsafe;\n  \n  /**\n   * A handle to <code>sun.misc.Unsafe#fieldOffset(Field)</code>.\n   */\n  private final static Method objectFieldOffsetMethod;\n\n  /**\n   * All the supported \"internal\" JVM features detected at clinit. \n   */\n  private final static EnumSet<JvmFeature> supportedFeatures;\n\n  /**\n   * Initialize constants and try to collect information about the JVM internals. \n   */\n  static {\n    // Initialize empirically measured defaults. We'll modify them to the current\n    // JVM settings later on if possible.\n    int referenceSize = Constants.JRE_IS_64BIT ? 8 : 4;\n    int objectHeader = Constants.JRE_IS_64BIT ? 16 : 8;\n    // The following is objectHeader + NUM_BYTES_INT, but aligned (object alignment)\n    // so on 64 bit JVMs it'll be align(16 + 4, @8) = 24.\n    int arrayHeader = Constants.JRE_IS_64BIT ? 24 : 12;\n\n    supportedFeatures = EnumSet.noneOf(JvmFeature.class);\n\n    Class<?> unsafeClass = null;\n    Object tempTheUnsafe = null;\n    try {\n      unsafeClass = Class.forName(\"sun.misc.Unsafe\");\n      final Field unsafeField = unsafeClass.getDeclaredField(\"theUnsafe\");\n      unsafeField.setAccessible(true);\n      tempTheUnsafe = unsafeField.get(null);\n    } catch (Exception e) {\n      // Ignore.\n    }\n    theUnsafe = tempTheUnsafe;\n\n    // get object reference size by getting scale factor of Object[] arrays:\n    try {\n      final Method arrayIndexScaleM = unsafeClass.getMethod(\"arrayIndexScale\", Class.class);\n      referenceSize = ((Number) arrayIndexScaleM.invoke(theUnsafe, Object[].class)).intValue();\n      supportedFeatures.add(JvmFeature.OBJECT_REFERENCE_SIZE);\n    } catch (Exception e) {\n      // ignore.\n    }\n\n    // \"best guess\" based on reference size. We will attempt to modify\n    // these to exact values if there is supported infrastructure.\n    objectHeader = Constants.JRE_IS_64BIT ? (8 + referenceSize) : 8;\n    arrayHeader =  Constants.JRE_IS_64BIT ? (8 + 2 * referenceSize) : 12;\n\n    // get the object header size:\n    // - first try out if the field offsets are not scaled (see warning in Unsafe docs)\n    // - get the object header size by getting the field offset of the first field of a dummy object\n    // If the scaling is byte-wise and unsafe is available, enable dynamic size measurement for\n    // estimateRamUsage().\n    Method tempObjectFieldOffsetMethod = null;\n    try {\n      final Method objectFieldOffsetM = unsafeClass.getMethod(\"objectFieldOffset\", Field.class);\n      final Field dummy1Field = DummyTwoLongObject.class.getDeclaredField(\"dummy1\");\n      final int ofs1 = ((Number) objectFieldOffsetM.invoke(theUnsafe, dummy1Field)).intValue();\n      final Field dummy2Field = DummyTwoLongObject.class.getDeclaredField(\"dummy2\");\n      final int ofs2 = ((Number) objectFieldOffsetM.invoke(theUnsafe, dummy2Field)).intValue();\n      if (Math.abs(ofs2 - ofs1) == NUM_BYTES_LONG) {\n        final Field baseField = DummyOneFieldObject.class.getDeclaredField(\"base\");\n        objectHeader = ((Number) objectFieldOffsetM.invoke(theUnsafe, baseField)).intValue();\n        supportedFeatures.add(JvmFeature.FIELD_OFFSETS);\n        tempObjectFieldOffsetMethod = objectFieldOffsetM;\n      }\n    } catch (Exception e) {\n      // Ignore.\n    }\n    objectFieldOffsetMethod = tempObjectFieldOffsetMethod;\n\n    // Get the array header size by retrieving the array base offset\n    // (offset of the first element of an array).\n    try {\n      final Method arrayBaseOffsetM = unsafeClass.getMethod(\"arrayBaseOffset\", Class.class);\n      // we calculate that only for byte[] arrays, it's actually the same for all types:\n      arrayHeader = ((Number) arrayBaseOffsetM.invoke(theUnsafe, byte[].class)).intValue();\n      supportedFeatures.add(JvmFeature.ARRAY_HEADER_SIZE);\n    } catch (Exception e) {\n      // Ignore.\n    }\n\n    NUM_BYTES_OBJECT_REF = referenceSize;\n    NUM_BYTES_OBJECT_HEADER = objectHeader;\n    NUM_BYTES_ARRAY_HEADER = arrayHeader;\n    \n    // Try to get the object alignment (the default seems to be 8 on Hotspot, \n    // regardless of the architecture).\n    int objectAlignment = 8;\n    try {\n      final Class<? extends PlatformManagedObject> beanClazz =\n        Class.forName(\"com.sun.management.HotSpotDiagnosticMXBean\").asSubclass(PlatformManagedObject.class);\n      final Object hotSpotBean = ManagementFactory.getPlatformMXBean(beanClazz);\n      if (hotSpotBean != null) {\n        final Method getVMOptionMethod = beanClazz.getMethod(\"getVMOption\", String.class);\n        final Object vmOption = getVMOptionMethod.invoke(hotSpotBean, \"ObjectAlignmentInBytes\");\n        objectAlignment = Integer.parseInt(\n            vmOption.getClass().getMethod(\"getValue\").invoke(vmOption).toString()\n        );\n        supportedFeatures.add(JvmFeature.OBJECT_ALIGNMENT);\n      }\n    } catch (Exception e) {\n      // Ignore.\n    }\n\n    NUM_BYTES_OBJECT_ALIGNMENT = objectAlignment;\n\n    JVM_INFO_STRING = \"[JVM: \" +\n        Constants.JVM_NAME + \", \" + Constants.JVM_VERSION + \", \" + Constants.JVM_VENDOR + \", \" + \n        Constants.JAVA_VENDOR + \", \" + Constants.JAVA_VERSION + \"]\";\n  }\n\n  /**\n   * Cached information about a given class.   \n   */\n  private static final class ClassCache {\n    public final long alignedShallowInstanceSize;\n    public final Field[] referenceFields;\n\n    public ClassCache(long alignedShallowInstanceSize, Field[] referenceFields) {\n      this.alignedShallowInstanceSize = alignedShallowInstanceSize;\n      this.referenceFields = referenceFields;\n    }    \n  }\n\n  // Object with just one field to determine the object header size by getting the offset of the dummy field:\n  @SuppressWarnings(\"unused\")\n  private static final class DummyOneFieldObject {\n    public byte base;\n  }\n\n  // Another test object for checking, if the difference in offsets of dummy1 and dummy2 is 8 bytes.\n  // Only then we can be sure that those are real, unscaled offsets:\n  @SuppressWarnings(\"unused\")\n  private static final class DummyTwoLongObject {\n    public long dummy1, dummy2;\n  }\n  \n  /** \n   * Returns true, if the current JVM is fully supported by {@code RamUsageEstimator}.\n   * If this method returns {@code false} you are maybe using a 3rd party Java VM\n   * that is not supporting Oracle/Sun private APIs. The memory estimates can be \n   * imprecise then (no way of detecting compressed references, alignments, etc.). \n   * Lucene still tries to use sensible defaults.\n   */\n  public static boolean isSupportedJVM() {\n    return supportedFeatures.size() == JvmFeature.values().length;\n  }\n\n  /** \n   * Aligns an object size to be the next multiple of {@link #NUM_BYTES_OBJECT_ALIGNMENT}. \n   */\n  public static long alignObjectSize(long size) {\n    size += (long) NUM_BYTES_OBJECT_ALIGNMENT - 1L;\n    return size - (size % NUM_BYTES_OBJECT_ALIGNMENT);\n  }\n  \n  /** Returns the size in bytes of the byte[] object. */\n  public static long sizeOf(byte[] arr) {\n    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + arr.length);\n  }\n  \n  /** Returns the size in bytes of the boolean[] object. */\n  public static long sizeOf(boolean[] arr) {\n    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + arr.length);\n  }\n  \n  /** Returns the size in bytes of the char[] object. */\n  public static long sizeOf(char[] arr) {\n    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) NUM_BYTES_CHAR * arr.length);\n  }\n\n  /** Returns the size in bytes of the short[] object. */\n  public static long sizeOf(short[] arr) {\n    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) NUM_BYTES_SHORT * arr.length);\n  }\n  \n  /** Returns the size in bytes of the int[] object. */\n  public static long sizeOf(int[] arr) {\n    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) NUM_BYTES_INT * arr.length);\n  }\n  \n  /** Returns the size in bytes of the float[] object. */\n  public static long sizeOf(float[] arr) {\n    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) NUM_BYTES_FLOAT * arr.length);\n  }\n  \n  /** Returns the size in bytes of the long[] object. */\n  public static long sizeOf(long[] arr) {\n    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) NUM_BYTES_LONG * arr.length);\n  }\n  \n  /** Returns the size in bytes of the double[] object. */\n  public static long sizeOf(double[] arr) {\n    return alignObjectSize((long) NUM_BYTES_ARRAY_HEADER + (long) NUM_BYTES_DOUBLE * arr.length);\n  }\n\n  /** \n   * Estimates the RAM usage by the given object. It will\n   * walk the object tree and sum up all referenced objects.\n   * \n   * <p><b>Resource Usage:</b> This method internally uses a set of\n   * every object seen during traversals so it does allocate memory\n   * (it isn't side-effect free). After the method exits, this memory\n   * should be GCed.</p>\n   */\n  public static long sizeOf(Object obj) {\n    return measureObjectSize(obj);\n  }\n\n  /** \n   * Estimates a \"shallow\" memory usage of the given object. For arrays, this will be the\n   * memory taken by array storage (no subreferences will be followed). For objects, this\n   * will be the memory taken by the fields.\n   * \n   * JVM object alignments are also applied.\n   */\n  public static long shallowSizeOf(Object obj) {\n    if (obj == null) return 0;\n    final Class<?> clz = obj.getClass();\n    if (clz.isArray()) {\n      return shallowSizeOfArray(obj);\n    } else {\n      return shallowSizeOfInstance(clz);\n    }\n  }\n\n  /**\n   * Returns the shallow instance size in bytes an instance of the given class would occupy.\n   * This works with all conventional classes and primitive types, but not with arrays\n   * (the size then depends on the number of elements and varies from object to object).\n   * \n   * @see #shallowSizeOf(Object)\n   * @throws IllegalArgumentException if {@code clazz} is an array class. \n   */\n  public static long shallowSizeOfInstance(Class<?> clazz) {\n    if (clazz.isArray())\n      throw new IllegalArgumentException(\"This method does not work with array classes.\");\n    if (clazz.isPrimitive())\n      return primitiveSizes.get(clazz);\n    \n    long size = NUM_BYTES_OBJECT_HEADER;\n\n    // Walk type hierarchy\n    for (;clazz != null; clazz = clazz.getSuperclass()) {\n      final Field[] fields = clazz.getDeclaredFields();\n      for (Field f : fields) {\n        if (!Modifier.isStatic(f.getModifiers())) {\n          size = adjustForField(size, f);\n        }\n      }\n    }\n    return alignObjectSize(size);    \n  }\n\n  /**\n   * Return shallow size of any <code>array</code>.\n   */\n  private static long shallowSizeOfArray(Object array) {\n    long size = NUM_BYTES_ARRAY_HEADER;\n    final int len = Array.getLength(array);\n    if (len > 0) {\n      Class<?> arrayElementClazz = array.getClass().getComponentType();\n      if (arrayElementClazz.isPrimitive()) {\n        size += (long) len * primitiveSizes.get(arrayElementClazz);\n      } else {\n        size += (long) NUM_BYTES_OBJECT_REF * len;\n      }\n    }\n    return alignObjectSize(size);\n  }\n\n  /*\n   * Non-recursive version of object descend. This consumes more memory than recursive in-depth \n   * traversal but prevents stack overflows on long chains of objects\n   * or complex graphs (a max. recursion depth on my machine was ~5000 objects linked in a chain\n   * so not too much).  \n   */\n  private static long measureObjectSize(Object root) {\n    // Objects seen so far.\n    final IdentityHashSet<Object> seen = new IdentityHashSet<Object>();\n    // Class cache with reference Field and precalculated shallow size. \n    final IdentityHashMap<Class<?>, ClassCache> classCache = new IdentityHashMap<Class<?>, ClassCache>();\n    // Stack of objects pending traversal. Recursion caused stack overflows. \n    final ArrayList<Object> stack = new ArrayList<Object>();\n    stack.add(root);\n\n    long totalSize = 0;\n    while (!stack.isEmpty()) {\n      final Object ob = stack.remove(stack.size() - 1);\n\n      if (ob == null || seen.contains(ob)) {\n        continue;\n      }\n      seen.add(ob);\n\n      final Class<?> obClazz = ob.getClass();\n      if (obClazz.isArray()) {\n        /*\n         * Consider an array, possibly of primitive types. Push any of its references to\n         * the processing stack and accumulate this array's shallow size. \n         */\n        long size = NUM_BYTES_ARRAY_HEADER;\n        final int len = Array.getLength(ob);\n        if (len > 0) {\n          Class<?> componentClazz = obClazz.getComponentType();\n          if (componentClazz.isPrimitive()) {\n            size += (long) len * primitiveSizes.get(componentClazz);\n          } else {\n            size += (long) NUM_BYTES_OBJECT_REF * len;\n\n            // Push refs for traversal later.\n            for (int i = len; --i >= 0 ;) {\n              final Object o = Array.get(ob, i);\n              if (o != null && !seen.contains(o)) {\n                stack.add(o);\n              }\n            }            \n          }\n        }\n        totalSize += alignObjectSize(size);\n      } else {\n        /*\n         * Consider an object. Push any references it has to the processing stack\n         * and accumulate this object's shallow size. \n         */\n        try {\n          ClassCache cachedInfo = classCache.get(obClazz);\n          if (cachedInfo == null) {\n            classCache.put(obClazz, cachedInfo = createCacheEntry(obClazz));\n          }\n\n          for (Field f : cachedInfo.referenceFields) {\n            // Fast path to eliminate redundancies.\n            final Object o = f.get(ob);\n            if (o != null && !seen.contains(o)) {\n              stack.add(o);\n            }\n          }\n\n          totalSize += cachedInfo.alignedShallowInstanceSize;\n        } catch (IllegalAccessException e) {\n          // this should never happen as we enabled setAccessible().\n          throw new RuntimeException(\"Reflective field access failed?\", e);\n        }\n      }\n    }\n\n    // Help the GC (?).\n    seen.clear();\n    stack.clear();\n    classCache.clear();\n\n    return totalSize;\n  }\n\n  /**\n   * Create a cached information about shallow size and reference fields for \n   * a given class.\n   */\n  private static ClassCache createCacheEntry(final Class<?> clazz) {\n    ClassCache cachedInfo;\n    long shallowInstanceSize = NUM_BYTES_OBJECT_HEADER;\n    final ArrayList<Field> referenceFields = new ArrayList<Field>(32);\n    for (Class<?> c = clazz; c != null; c = c.getSuperclass()) {\n      final Field[] fields = c.getDeclaredFields();\n      for (final Field f : fields) {\n        if (!Modifier.isStatic(f.getModifiers())) {\n          shallowInstanceSize = adjustForField(shallowInstanceSize, f);\n\n          if (!f.getType().isPrimitive()) {\n            f.setAccessible(true);\n            referenceFields.add(f);\n          }\n        }\n      }\n    }\n\n    cachedInfo = new ClassCache(\n        alignObjectSize(shallowInstanceSize), \n        referenceFields.toArray(new Field[referenceFields.size()]));\n    return cachedInfo;\n  }\n\n  /**\n   * This method returns the maximum representation size of an object. <code>sizeSoFar</code>\n   * is the object's size measured so far. <code>f</code> is the field being probed.\n   * \n   * <p>The returned offset will be the maximum of whatever was measured so far and \n   * <code>f</code> field's offset and representation size (unaligned).\n   */\n  private static long adjustForField(long sizeSoFar, final Field f) {\n    final Class<?> type = f.getType();\n    final int fsize = type.isPrimitive() ? primitiveSizes.get(type) : NUM_BYTES_OBJECT_REF;\n    if (objectFieldOffsetMethod != null) {\n      try {\n        final long offsetPlusSize =\n          ((Number) objectFieldOffsetMethod.invoke(theUnsafe, f)).longValue() + fsize;\n        return Math.max(sizeSoFar, offsetPlusSize);\n      } catch (IllegalAccessException ex) {\n        throw new RuntimeException(\"Access problem with sun.misc.Unsafe\", ex);\n      } catch (InvocationTargetException ite) {\n        final Throwable cause = ite.getCause();\n        if (cause instanceof RuntimeException)\n          throw (RuntimeException) cause;\n        if (cause instanceof Error)\n          throw (Error) cause;\n        // this should never happen (Unsafe does not declare\n        // checked Exceptions for this method), but who knows?\n        throw new RuntimeException(\"Call to Unsafe's objectFieldOffset() throwed \"+\n          \"checked Exception when accessing field \" +\n          f.getDeclaringClass().getName() + \"#\" + f.getName(), cause);\n      }\n    } else {\n      // TODO: No alignments based on field type/ subclass fields alignments?\n      return sizeSoFar + fsize;\n    }\n  }\n\n  /** Return the set of unsupported JVM features that improve the estimation. */\n  public static EnumSet<JvmFeature> getUnsupportedFeatures() {\n    EnumSet<JvmFeature> unsupported = EnumSet.allOf(JvmFeature.class);\n    unsupported.removeAll(supportedFeatures);\n    return unsupported;\n  }\n\n  /** Return the set of supported JVM features that improve the estimation. */\n  public static EnumSet<JvmFeature> getSupportedFeatures() {\n    return EnumSet.copyOf(supportedFeatures);\n  }\n\n  /**\n   * Returns <code>size</code> in human-readable units (GB, MB, KB or bytes).\n   */\n  public static String humanReadableUnits(long bytes) {\n    return humanReadableUnits(bytes, \n        new DecimalFormat(\"0.#\", DecimalFormatSymbols.getInstance(Locale.ROOT)));\n  }\n\n  /**\n   * Returns <code>size</code> in human-readable units (GB, MB, KB or bytes). \n   */\n  public static String humanReadableUnits(long bytes, DecimalFormat df) {\n    if (bytes / ONE_GB > 0) {\n      return df.format((float) bytes / ONE_GB) + \" GB\";\n    } else if (bytes / ONE_MB > 0) {\n      return df.format((float) bytes / ONE_MB) + \" MB\";\n    } else if (bytes / ONE_KB > 0) {\n      return df.format((float) bytes / ONE_KB) + \" KB\";\n    } else {\n      return bytes + \" bytes\";\n    }\n  }\n\n  /**\n   * Return a human-readable size of a given object.\n   * @see #sizeOf(Object)\n   * @see #humanReadableUnits(long)\n   */\n  public static String humanSizeOf(Object object) {\n    return humanReadableUnits(sizeOf(object));\n  }\n\n  /**\n   * An identity hash set implemented using open addressing. No null keys are allowed.\n   * \n   * TODO: If this is useful outside this class, make it public - needs some work\n   */\n  static final class IdentityHashSet<KType> implements Iterable<KType> {\n    /**\n     * Default load factor.\n     */\n    public final static float DEFAULT_LOAD_FACTOR = 0.75f;\n\n    /**\n     * Minimum capacity for the set.\n     */\n    public final static int MIN_CAPACITY = 4;\n\n    /**\n     * All of set entries. Always of power of two length.\n     */\n    public Object[] keys;\n    \n    /**\n     * Cached number of assigned slots.\n     */\n    public int assigned;\n    \n    /**\n     * The load factor for this set (fraction of allocated or deleted slots before\n     * the buffers must be rehashed or reallocated).\n     */\n    public final float loadFactor;\n    \n    /**\n     * Cached capacity threshold at which we must resize the buffers.\n     */\n    private int resizeThreshold;\n    \n    /**\n     * Creates a hash set with the default capacity of 16.\n     * load factor of {@value #DEFAULT_LOAD_FACTOR}. `\n     */\n    public IdentityHashSet() {\n      this(16, DEFAULT_LOAD_FACTOR);\n    }\n    \n    /**\n     * Creates a hash set with the given capacity, load factor of\n     * {@value #DEFAULT_LOAD_FACTOR}.\n     */\n    public IdentityHashSet(int initialCapacity) {\n      this(initialCapacity, DEFAULT_LOAD_FACTOR);\n    }\n    \n    /**\n     * Creates a hash set with the given capacity and load factor.\n     */\n    public IdentityHashSet(int initialCapacity, float loadFactor) {\n      initialCapacity = Math.max(MIN_CAPACITY, initialCapacity);\n      \n      assert initialCapacity > 0 : \"Initial capacity must be between (0, \"\n          + Integer.MAX_VALUE + \"].\";\n      assert loadFactor > 0 && loadFactor < 1 : \"Load factor must be between (0, 1).\";\n      this.loadFactor = loadFactor;\n      allocateBuffers(roundCapacity(initialCapacity));\n    }\n    \n    /**\n     * Adds a reference to the set. Null keys are not allowed.\n     */\n    public boolean add(KType e) {\n      assert e != null : \"Null keys not allowed.\";\n      \n      if (assigned >= resizeThreshold) expandAndRehash();\n      \n      final int mask = keys.length - 1;\n      int slot = rehash(e) & mask;\n      Object existing;\n      while ((existing = keys[slot]) != null) {\n        if (e == existing) {\n          return false; // already found.\n        }\n        slot = (slot + 1) & mask;\n      }\n      assigned++;\n      keys[slot] = e;\n      return true;\n    }\n\n    /**\n     * Checks if the set contains a given ref.\n     */\n    public boolean contains(KType e) {\n      final int mask = keys.length - 1;\n      int slot = rehash(e) & mask;\n      Object existing;\n      while ((existing = keys[slot]) != null) {\n        if (e == existing) {\n          return true;\n        }\n        slot = (slot + 1) & mask;\n      }\n      return false;\n    }\n\n    /** Rehash via MurmurHash.\n     * \n     * <p>The implementation is based on the\n     * finalization step from Austin Appleby's\n     * <code>MurmurHash3</code>.\n     * \n     * @see \"http://sites.google.com/site/murmurhash/\"\n     */\n    private static int rehash(Object o) {\n      int k = System.identityHashCode(o);\n      k ^= k >>> 16;\n      k *= 0x85ebca6b;\n      k ^= k >>> 13;\n      k *= 0xc2b2ae35;\n      k ^= k >>> 16;\n      return k;\n    }\n    \n    /**\n     * Expand the internal storage buffers (capacity) or rehash current keys and\n     * values if there are a lot of deleted slots.\n     */\n    private void expandAndRehash() {\n      final Object[] oldKeys = this.keys;\n      \n      assert assigned >= resizeThreshold;\n      allocateBuffers(nextCapacity(keys.length));\n      \n      /*\n       * Rehash all assigned slots from the old hash table.\n       */\n      final int mask = keys.length - 1;\n      for (int i = 0; i < oldKeys.length; i++) {\n        final Object key = oldKeys[i];\n        if (key != null) {\n          int slot = rehash(key) & mask;\n          while (keys[slot] != null) {\n            slot = (slot + 1) & mask;\n          }\n          keys[slot] = key;\n        }\n      }\n      Arrays.fill(oldKeys, null);\n    }\n    \n    /**\n     * Allocate internal buffers for a given capacity.\n     * \n     * @param capacity\n     *          New capacity (must be a power of two).\n     */\n    private void allocateBuffers(int capacity) {\n      this.keys = new Object[capacity];\n      this.resizeThreshold = (int) (capacity * DEFAULT_LOAD_FACTOR);\n    }\n    \n    /**\n     * Return the next possible capacity, counting from the current buffers' size.\n     */\n    protected int nextCapacity(int current) {\n      assert current > 0 && Long.bitCount(current) == 1 : \"Capacity must be a power of two.\";\n      assert ((current << 1) > 0) : \"Maximum capacity exceeded (\"\n          + (0x80000000 >>> 1) + \").\";\n      \n      if (current < MIN_CAPACITY / 2) current = MIN_CAPACITY / 2;\n      return current << 1;\n    }\n    \n    /**\n     * Round the capacity to the next allowed value.\n     */\n    protected int roundCapacity(int requestedCapacity) {\n      // Maximum positive integer that is a power of two.\n      if (requestedCapacity > (0x80000000 >>> 1)) return (0x80000000 >>> 1);\n      \n      int capacity = MIN_CAPACITY;\n      while (capacity < requestedCapacity) {\n        capacity <<= 1;\n      }\n\n      return capacity;\n    }\n    \n    public void clear() {\n      assigned = 0;\n      Arrays.fill(keys, null);\n    }\n    \n    public int size() {\n      return assigned;\n    }\n    \n    public boolean isEmpty() {\n      return size() == 0;\n    }\n\n    @Override\n    public Iterator<KType> iterator() {\n      return new Iterator<KType>() {\n        int pos = -1;\n        Object nextElement = fetchNext();\n\n        @Override\n        public boolean hasNext() {\n          return nextElement != null;\n        }\n\n        @SuppressWarnings(\"unchecked\")\n        @Override\n        public KType next() {\n          Object r = this.nextElement;\n          if (r == null) {\n            throw new NoSuchElementException();\n          }\n          this.nextElement = fetchNext();\n          return (KType) r;\n        }\n\n        private Object fetchNext() {\n          pos++;\n          while (pos < keys.length && keys[pos] == null) {\n            pos++;\n          }\n\n          return (pos >= keys.length ? null : keys[pos]);\n        }\n\n        @Override\n        public void remove() {\n          throw new UnsupportedOperationException();\n        }\n      };\n    }\n  }\n}\n",
        "methodName": "<clinit>",
        "exampleID": 24,
        "dataset": "spotbugs",
        "filepath": "/lucene/core/src/java/org/apache/lucene/util/RamUsageEstimator.java",
        "line": "151",
        "source": "unsafeClass",
        "sourceLine": "200",
        "qualifier": "Possible null pointer dereference of $$unsafeClass/$",
        "steps": [
            {
                "exampleID": 25
            }
        ],
        "line_number": "200"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java",
        "rawCode": "package org.apache.lucene.util;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.BufferedOutputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.io.PrintStream;\nimport java.math.BigDecimal;\nimport java.math.BigInteger;\nimport java.nio.CharBuffer;\nimport java.util.Arrays;\nimport java.util.Enumeration;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.regex.Pattern;\nimport java.util.regex.PatternSyntaxException;\nimport java.util.zip.ZipEntry;\nimport java.util.zip.ZipFile;\n\nimport org.apache.lucene.codecs.Codec;\nimport org.apache.lucene.codecs.DocValuesFormat;\nimport org.apache.lucene.codecs.PostingsFormat;\nimport org.apache.lucene.codecs.lucene46.Lucene46Codec;\nimport org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;\nimport org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;\nimport org.apache.lucene.document.BinaryDocValuesField;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.DoubleField;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.document.FieldType.NumericType;\nimport org.apache.lucene.document.FloatField;\nimport org.apache.lucene.document.IntField;\nimport org.apache.lucene.document.LongField;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.SortedDocValuesField;\nimport org.apache.lucene.index.AtomicReader;\nimport org.apache.lucene.index.AtomicReaderContext;\nimport org.apache.lucene.index.CheckIndex;\nimport org.apache.lucene.index.CheckIndex.Status.DocValuesStatus;\nimport org.apache.lucene.index.CheckIndex.Status.FieldNormStatus;\nimport org.apache.lucene.index.CheckIndex.Status.StoredFieldStatus;\nimport org.apache.lucene.index.CheckIndex.Status.TermIndexStatus;\nimport org.apache.lucene.index.CheckIndex.Status.TermVectorStatus;\nimport org.apache.lucene.index.ConcurrentMergeScheduler;\nimport org.apache.lucene.index.DocsAndPositionsEnum;\nimport org.apache.lucene.index.DocsEnum;\nimport org.apache.lucene.index.FieldInfo.DocValuesType;\nimport org.apache.lucene.index.FieldInfos;\nimport org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.LogMergePolicy;\nimport org.apache.lucene.index.MergePolicy;\nimport org.apache.lucene.index.MergeScheduler;\nimport org.apache.lucene.index.MultiFields;\nimport org.apache.lucene.index.SegmentCommitInfo;\nimport org.apache.lucene.index.SegmentReader;\nimport org.apache.lucene.index.Terms;\nimport org.apache.lucene.index.TermsEnum;\nimport org.apache.lucene.index.TieredMergePolicy;\nimport org.apache.lucene.search.FieldDoc;\nimport org.apache.lucene.search.FilteredQuery;\nimport org.apache.lucene.search.FilteredQuery.FilterStrategy;\nimport org.apache.lucene.search.ScoreDoc;\nimport org.apache.lucene.search.TopDocs;\nimport org.apache.lucene.store.Directory;\nimport org.junit.Assert;\n\nimport com.carrotsearch.randomizedtesting.RandomizedContext;\nimport com.carrotsearch.randomizedtesting.generators.RandomInts;\nimport com.carrotsearch.randomizedtesting.generators.RandomPicks;\n\n/**\n * General utility methods for Lucene unit tests. \n */\npublic class _TestUtil {\n\n  // the max number of retries we're going to do in getTempDir\n  private static final int GET_TEMP_DIR_RETRY_THRESHOLD = 1000;\n  \n  /**\n   * Returns a temp directory, based on the given description. Creates the\n   * directory.\n   */\n  public static File getTempDir(String desc) {\n    if (desc.length() < 3) {\n      throw new IllegalArgumentException(\"description must be at least 3 characters\");\n    }\n    // always pull a long from master random. that way, the randomness of the test\n    // is not affected by whether it initialized the counter (in genTempFile) or not.\n    // note that the Random used by genTempFile is *not* the master Random, and therefore\n    // does not affect the randomness of the test.\n    final Random random = new Random(RandomizedContext.current().getRandom().nextLong());\n    int attempt = 0;\n    File f;\n    do {\n      f = genTempFile(random, desc, \"tmp\", LuceneTestCase.TEMP_DIR);\n    } while (!f.mkdir() && (attempt++) < GET_TEMP_DIR_RETRY_THRESHOLD);\n    \n    if (attempt > GET_TEMP_DIR_RETRY_THRESHOLD) {\n      throw new RuntimeException(\n          \"failed to get a temporary dir too many times. check your temp directory and consider manually cleaning it.\");\n    }\n    \n    LuceneTestCase.closeAfterSuite(new CloseableFile(f, LuceneTestCase.suiteFailureMarker));\n    return f;\n  }\n\n  /**\n   * Deletes a directory and everything underneath it.\n   */\n  public static void rmDir(File dir) throws IOException {\n    if (dir.exists()) {\n      if (dir.isFile() && !dir.delete()) {\n        throw new IOException(\"could not delete \" + dir);\n      }\n      for (File f : dir.listFiles()) {\n        if (f.isDirectory()) {\n          rmDir(f);\n        } else {\n          if (!f.delete()) {\n            throw new IOException(\"could not delete \" + f);\n          }\n        }\n      }\n      if (!dir.delete()) {\n        throw new IOException(\"could not delete \" + dir);\n      }\n    }\n  }\n\n  /** \n   * Convenience method: Unzip zipName + \".zip\" under destDir, removing destDir first \n   */\n  public static void unzip(File zipName, File destDir) throws IOException {\n    \n    ZipFile zipFile = new ZipFile(zipName);\n    \n    Enumeration<? extends ZipEntry> entries = zipFile.entries();\n    \n    rmDir(destDir);\n\n    destDir.mkdir();\n    LuceneTestCase.closeAfterSuite(new CloseableFile(destDir, LuceneTestCase.suiteFailureMarker));\n\n    while (entries.hasMoreElements()) {\n      ZipEntry entry = entries.nextElement();\n      \n      InputStream in = zipFile.getInputStream(entry);\n      File targetFile = new File(destDir, entry.getName());\n      if (entry.isDirectory()) {\n        // allow unzipping with directory structure\n        targetFile.mkdirs();\n      } else {\n        if (targetFile.getParentFile()!=null) {\n          // be on the safe side: do not rely on that directories are always extracted\n          // before their children (although this makes sense, but is it guaranteed?)\n          targetFile.getParentFile().mkdirs();   \n        }\n        OutputStream out = new BufferedOutputStream(new FileOutputStream(targetFile));\n        \n        byte[] buffer = new byte[8192];\n        int len;\n        while((len = in.read(buffer)) >= 0) {\n          out.write(buffer, 0, len);\n        }\n        \n        in.close();\n        out.close();\n      }\n    }\n    \n    zipFile.close();\n  }\n  \n  public static void syncConcurrentMerges(IndexWriter writer) {\n    syncConcurrentMerges(writer.getConfig().getMergeScheduler());\n  }\n\n  public static void syncConcurrentMerges(MergeScheduler ms) {\n    if (ms instanceof ConcurrentMergeScheduler)\n      ((ConcurrentMergeScheduler) ms).sync();\n  }\n\n  /** This runs the CheckIndex tool on the index in.  If any\n   *  issues are hit, a RuntimeException is thrown; else,\n   *  true is returned. */\n  public static CheckIndex.Status checkIndex(Directory dir) throws IOException {\n    return checkIndex(dir, true);\n  }\n\n  public static CheckIndex.Status checkIndex(Directory dir, boolean crossCheckTermVectors) throws IOException {\n    ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);\n    CheckIndex checker = new CheckIndex(dir);\n    checker.setCrossCheckTermVectors(crossCheckTermVectors);\n    checker.setInfoStream(new PrintStream(bos, false, \"UTF-8\"), false);\n    CheckIndex.Status indexStatus = checker.checkIndex(null);\n    if (indexStatus == null || indexStatus.clean == false) {\n      System.out.println(\"CheckIndex failed\");\n      System.out.println(bos.toString(\"UTF-8\"));\n      throw new RuntimeException(\"CheckIndex failed\");\n    } else {\n      if (LuceneTestCase.INFOSTREAM) {\n        System.out.println(bos.toString(\"UTF-8\"));\n      }\n      return indexStatus;\n    }\n  }\n  \n  /** This runs the CheckIndex tool on the Reader.  If any\n   *  issues are hit, a RuntimeException is thrown */\n  public static void checkReader(IndexReader reader) throws IOException {\n    for (AtomicReaderContext context : reader.leaves()) {\n      checkReader(context.reader(), true);\n    }\n  }\n  \n  public static void checkReader(AtomicReader reader, boolean crossCheckTermVectors) throws IOException {\n    ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);\n    PrintStream infoStream = new PrintStream(bos, false, \"UTF-8\");\n\n    FieldNormStatus fieldNormStatus = CheckIndex.testFieldNorms(reader, infoStream);\n    TermIndexStatus termIndexStatus = CheckIndex.testPostings(reader, infoStream);\n    StoredFieldStatus storedFieldStatus = CheckIndex.testStoredFields(reader, infoStream);\n    TermVectorStatus termVectorStatus = CheckIndex.testTermVectors(reader, infoStream, false, crossCheckTermVectors);\n    DocValuesStatus docValuesStatus = CheckIndex.testDocValues(reader, infoStream);\n    \n    if (fieldNormStatus.error != null || \n      termIndexStatus.error != null ||\n      storedFieldStatus.error != null ||\n      termVectorStatus.error != null ||\n      docValuesStatus.error != null) {\n      System.out.println(\"CheckReader failed\");\n      System.out.println(bos.toString(\"UTF-8\"));\n      throw new RuntimeException(\"CheckReader failed\");\n    } else {\n      if (LuceneTestCase.INFOSTREAM) {\n        System.out.println(bos.toString(\"UTF-8\"));\n      }\n    }\n  }\n\n  /** start and end are BOTH inclusive */\n  public static int nextInt(Random r, int start, int end) {\n    return RandomInts.randomIntBetween(r, start, end);\n  }\n\n  /** start and end are BOTH inclusive */\n  public static long nextLong(Random r, long start, long end) {\n    assert end >= start;\n    final BigInteger range = BigInteger.valueOf(end).add(BigInteger.valueOf(1)).subtract(BigInteger.valueOf(start));\n    if (range.compareTo(BigInteger.valueOf(Integer.MAX_VALUE)) <= 0) {\n      return start + r.nextInt(range.intValue());\n    } else {\n      // probably not evenly distributed when range is large, but OK for tests\n      final BigInteger augend = new BigDecimal(range).multiply(new BigDecimal(r.nextDouble())).toBigInteger();\n      final long result = BigInteger.valueOf(start).add(augend).longValue();\n      assert result >= start;\n      assert result <= end;\n      return result;\n    }\n  }\n\n  public static String randomSimpleString(Random r, int maxLength) {\n    return randomSimpleString(r, 0, maxLength);\n  }\n  \n  public static String randomSimpleString(Random r, int minLength, int maxLength) {\n    final int end = nextInt(r, minLength, maxLength);\n    if (end == 0) {\n      // allow 0 length\n      return \"\";\n    }\n    final char[] buffer = new char[end];\n    for (int i = 0; i < end; i++) {\n      buffer[i] = (char) _TestUtil.nextInt(r, 'a', 'z');\n    }\n    return new String(buffer, 0, end);\n  }\n\n  public static String randomSimpleStringRange(Random r, char minChar, char maxChar, int maxLength) {\n    final int end = nextInt(r, 0, maxLength);\n    if (end == 0) {\n      // allow 0 length\n      return \"\";\n    }\n    final char[] buffer = new char[end];\n    for (int i = 0; i < end; i++) {\n      buffer[i] = (char) _TestUtil.nextInt(r, minChar, maxChar);\n    }\n    return new String(buffer, 0, end);\n  }\n\n  public static String randomSimpleString(Random r) {\n    return randomSimpleString(r, 0, 10);\n  }\n\n  /** Returns random string, including full unicode range. */\n  public static String randomUnicodeString(Random r) {\n    return randomUnicodeString(r, 20);\n  }\n\n  /**\n   * Returns a random string up to a certain length.\n   */\n  public static String randomUnicodeString(Random r, int maxLength) {\n    final int end = nextInt(r, 0, maxLength);\n    if (end == 0) {\n      // allow 0 length\n      return \"\";\n    }\n    final char[] buffer = new char[end];\n    randomFixedLengthUnicodeString(r, buffer, 0, buffer.length);\n    return new String(buffer, 0, end);\n  }\n\n  /**\n   * Fills provided char[] with valid random unicode code\n   * unit sequence.\n   */\n  public static void randomFixedLengthUnicodeString(Random random, char[] chars, int offset, int length) {\n    int i = offset;\n    final int end = offset + length;\n    while(i < end) {\n      final int t = random.nextInt(5);\n      if (0 == t && i < length - 1) {\n        // Make a surrogate pair\n        // High surrogate\n        chars[i++] = (char) nextInt(random, 0xd800, 0xdbff);\n        // Low surrogate\n        chars[i++] = (char) nextInt(random, 0xdc00, 0xdfff);\n      } else if (t <= 1) {\n        chars[i++] = (char) random.nextInt(0x80);\n      } else if (2 == t) {\n        chars[i++] = (char) nextInt(random, 0x80, 0x7ff);\n      } else if (3 == t) {\n        chars[i++] = (char) nextInt(random, 0x800, 0xd7ff);\n      } else if (4 == t) {\n        chars[i++] = (char) nextInt(random, 0xe000, 0xffff);\n      }\n    }\n  }\n  \n  /**\n   * Returns a String thats \"regexpish\" (contains lots of operators typically found in regular expressions)\n   * If you call this enough times, you might get a valid regex!\n   */\n  public static String randomRegexpishString(Random r) {\n    return randomRegexpishString(r, 20);\n  }\n\n  /**\n   * Maximum recursion bound for '+' and '*' replacements in\n   * {@link #randomRegexpishString(Random, int)}.\n   */\n  private final static int maxRecursionBound = 5;\n\n  /**\n   * Operators for {@link #randomRegexpishString(Random, int)}.\n   */\n  private final static List<String> ops = Arrays.asList(\n      \".\", \"?\", \n      \"{0,\" + maxRecursionBound + \"}\",  // bounded replacement for '*'\n      \"{1,\" + maxRecursionBound + \"}\",  // bounded replacement for '+'\n      \"(\",\n      \")\",\n      \"-\",\n      \"[\",\n      \"]\",\n      \"|\"\n  );\n\n  /**\n   * Returns a String thats \"regexpish\" (contains lots of operators typically found in regular expressions)\n   * If you call this enough times, you might get a valid regex!\n   * \n   * <P>Note: to avoid practically endless backtracking patterns we replace asterisk and plus\n   * operators with bounded repetitions. See LUCENE-4111 for more info.\n   * \n   * @param maxLength A hint about maximum length of the regexpish string. It may be exceeded by a few characters.\n   */\n  public static String randomRegexpishString(Random r, int maxLength) {\n    final StringBuilder regexp = new StringBuilder(maxLength);\n    for (int i = nextInt(r, 0, maxLength); i > 0; i--) {\n      if (r.nextBoolean()) {\n        regexp.append((char) RandomInts.randomIntBetween(r, 'a', 'z'));\n      } else {\n        regexp.append(RandomPicks.randomFrom(r, ops));\n      }\n    }\n    return regexp.toString();\n  }\n\n  private static final String[] HTML_CHAR_ENTITIES = {\n      \"AElig\", \"Aacute\", \"Acirc\", \"Agrave\", \"Alpha\", \"AMP\", \"Aring\", \"Atilde\",\n      \"Auml\", \"Beta\", \"COPY\", \"Ccedil\", \"Chi\", \"Dagger\", \"Delta\", \"ETH\",\n      \"Eacute\", \"Ecirc\", \"Egrave\", \"Epsilon\", \"Eta\", \"Euml\", \"Gamma\", \"GT\",\n      \"Iacute\", \"Icirc\", \"Igrave\", \"Iota\", \"Iuml\", \"Kappa\", \"Lambda\", \"LT\",\n      \"Mu\", \"Ntilde\", \"Nu\", \"OElig\", \"Oacute\", \"Ocirc\", \"Ograve\", \"Omega\",\n      \"Omicron\", \"Oslash\", \"Otilde\", \"Ouml\", \"Phi\", \"Pi\", \"Prime\", \"Psi\",\n      \"QUOT\", \"REG\", \"Rho\", \"Scaron\", \"Sigma\", \"THORN\", \"Tau\", \"Theta\",\n      \"Uacute\", \"Ucirc\", \"Ugrave\", \"Upsilon\", \"Uuml\", \"Xi\", \"Yacute\", \"Yuml\",\n      \"Zeta\", \"aacute\", \"acirc\", \"acute\", \"aelig\", \"agrave\", \"alefsym\",\n      \"alpha\", \"amp\", \"and\", \"ang\", \"apos\", \"aring\", \"asymp\", \"atilde\",\n      \"auml\", \"bdquo\", \"beta\", \"brvbar\", \"bull\", \"cap\", \"ccedil\", \"cedil\",\n      \"cent\", \"chi\", \"circ\", \"clubs\", \"cong\", \"copy\", \"crarr\", \"cup\",\n      \"curren\", \"dArr\", \"dagger\", \"darr\", \"deg\", \"delta\", \"diams\", \"divide\",\n      \"eacute\", \"ecirc\", \"egrave\", \"empty\", \"emsp\", \"ensp\", \"epsilon\",\n      \"equiv\", \"eta\", \"eth\", \"euml\", \"euro\", \"exist\", \"fnof\", \"forall\",\n      \"frac12\", \"frac14\", \"frac34\", \"frasl\", \"gamma\", \"ge\", \"gt\", \"hArr\",\n      \"harr\", \"hearts\", \"hellip\", \"iacute\", \"icirc\", \"iexcl\", \"igrave\",\n      \"image\", \"infin\", \"int\", \"iota\", \"iquest\", \"isin\", \"iuml\", \"kappa\",\n      \"lArr\", \"lambda\", \"lang\", \"laquo\", \"larr\", \"lceil\", \"ldquo\", \"le\",\n      \"lfloor\", \"lowast\", \"loz\", \"lrm\", \"lsaquo\", \"lsquo\", \"lt\", \"macr\",\n      \"mdash\", \"micro\", \"middot\", \"minus\", \"mu\", \"nabla\", \"nbsp\", \"ndash\",\n      \"ne\", \"ni\", \"not\", \"notin\", \"nsub\", \"ntilde\", \"nu\", \"oacute\", \"ocirc\",\n      \"oelig\", \"ograve\", \"oline\", \"omega\", \"omicron\", \"oplus\", \"or\", \"ordf\",\n      \"ordm\", \"oslash\", \"otilde\", \"otimes\", \"ouml\", \"para\", \"part\", \"permil\",\n      \"perp\", \"phi\", \"pi\", \"piv\", \"plusmn\", \"pound\", \"prime\", \"prod\", \"prop\",\n      \"psi\", \"quot\", \"rArr\", \"radic\", \"rang\", \"raquo\", \"rarr\", \"rceil\",\n      \"rdquo\", \"real\", \"reg\", \"rfloor\", \"rho\", \"rlm\", \"rsaquo\", \"rsquo\",\n      \"sbquo\", \"scaron\", \"sdot\", \"sect\", \"shy\", \"sigma\", \"sigmaf\", \"sim\",\n      \"spades\", \"sub\", \"sube\", \"sum\", \"sup\", \"sup1\", \"sup2\", \"sup3\", \"supe\",\n      \"szlig\", \"tau\", \"there4\", \"theta\", \"thetasym\", \"thinsp\", \"thorn\",\n      \"tilde\", \"times\", \"trade\", \"uArr\", \"uacute\", \"uarr\", \"ucirc\", \"ugrave\",\n      \"uml\", \"upsih\", \"upsilon\", \"uuml\", \"weierp\", \"xi\", \"yacute\", \"yen\",\n      \"yuml\", \"zeta\", \"zwj\", \"zwnj\"\n  };\n  \n  public static String randomHtmlishString(Random random, int numElements) {\n    final int end = nextInt(random, 0, numElements);\n    if (end == 0) {\n      // allow 0 length\n      return \"\";\n    }\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < end; i++) {\n      int val = random.nextInt(25);\n      switch(val) {\n        case 0: sb.append(\"<p>\"); break;\n        case 1: {\n          sb.append(\"<\");\n          sb.append(\"    \".substring(nextInt(random, 0, 4)));\n          sb.append(randomSimpleString(random));\n          for (int j = 0 ; j < nextInt(random, 0, 10) ; ++j) {\n            sb.append(' ');\n            sb.append(randomSimpleString(random));\n            sb.append(\" \".substring(nextInt(random, 0, 1)));\n            sb.append('=');\n            sb.append(\" \".substring(nextInt(random, 0, 1)));\n            sb.append(\"\\\"\".substring(nextInt(random, 0, 1)));\n            sb.append(randomSimpleString(random));\n            sb.append(\"\\\"\".substring(nextInt(random, 0, 1)));\n          }\n          sb.append(\"    \".substring(nextInt(random, 0, 4)));\n          sb.append(\"/\".substring(nextInt(random, 0, 1)));\n          sb.append(\">\".substring(nextInt(random, 0, 1)));\n          break;\n        }\n        case 2: {\n          sb.append(\"</\");\n          sb.append(\"    \".substring(nextInt(random, 0, 4)));\n          sb.append(randomSimpleString(random));\n          sb.append(\"    \".substring(nextInt(random, 0, 4)));\n          sb.append(\">\".substring(nextInt(random, 0, 1)));\n          break;\n        }\n        case 3: sb.append(\">\"); break;\n        case 4: sb.append(\"</p>\"); break;\n        case 5: sb.append(\"<!--\"); break;\n        case 6: sb.append(\"<!--#\"); break;\n        case 7: sb.append(\"<script><!-- f('\"); break;\n        case 8: sb.append(\"</script>\"); break;\n        case 9: sb.append(\"<?\"); break;\n        case 10: sb.append(\"?>\"); break;\n        case 11: sb.append(\"\\\"\"); break;\n        case 12: sb.append(\"\\\\\\\"\"); break;\n        case 13: sb.append(\"'\"); break;\n        case 14: sb.append(\"\\\\'\"); break;\n        case 15: sb.append(\"-->\"); break;\n        case 16: {\n          sb.append(\"&\");\n          switch(nextInt(random, 0, 2)) {\n            case 0: sb.append(randomSimpleString(random)); break;\n            case 1: sb.append(HTML_CHAR_ENTITIES[random.nextInt(HTML_CHAR_ENTITIES.length)]); break;\n          }\n          sb.append(\";\".substring(nextInt(random, 0, 1)));\n          break;\n        }\n        case 17: {\n          sb.append(\"&#\");\n          if (0 == nextInt(random, 0, 1)) {\n            sb.append(nextInt(random, 0, Integer.MAX_VALUE - 1));\n            sb.append(\";\".substring(nextInt(random, 0, 1)));\n          }\n          break;\n        } \n        case 18: {\n          sb.append(\"&#x\");\n          if (0 == nextInt(random, 0, 1)) {\n            sb.append(Integer.toString(nextInt(random, 0, Integer.MAX_VALUE - 1), 16));\n            sb.append(\";\".substring(nextInt(random, 0, 1)));\n          }\n          break;\n        }\n          \n        case 19: sb.append(\";\"); break;\n        case 20: sb.append(nextInt(random, 0, Integer.MAX_VALUE - 1)); break;\n        case 21: sb.append(\"\\n\"); break;\n        case 22: sb.append(\"          \".substring(nextInt(random, 0, 10))); break;\n        case 23: {\n          sb.append(\"<\");\n          if (0 == nextInt(random, 0, 3)) {\n            sb.append(\"          \".substring(nextInt(random, 1, 10)));\n          }\n          if (0 == nextInt(random, 0, 1)) {\n            sb.append(\"/\");\n            if (0 == nextInt(random, 0, 3)) {\n              sb.append(\"          \".substring(nextInt(random, 1, 10)));\n            }\n          }\n          switch (nextInt(random, 0, 3)) {\n            case 0: sb.append(randomlyRecaseCodePoints(random, \"script\")); break;\n            case 1: sb.append(randomlyRecaseCodePoints(random, \"style\")); break;\n            case 2: sb.append(randomlyRecaseCodePoints(random, \"br\")); break;\n            // default: append nothing\n          }\n          sb.append(\">\".substring(nextInt(random, 0, 1)));\n          break;\n        }\n        default: sb.append(randomSimpleString(random));\n      }\n    }\n    return sb.toString();\n  }\n\n  /**\n   * Randomly upcases, downcases, or leaves intact each code point in the given string\n   */\n  public static String randomlyRecaseCodePoints(Random random, String str) {\n    StringBuilder builder = new StringBuilder();\n    int pos = 0;\n    while (pos < str.length()) {\n      int codePoint = str.codePointAt(pos);\n      pos += Character.charCount(codePoint);\n      switch (nextInt(random, 0, 2)) {\n        case 0: builder.appendCodePoint(Character.toUpperCase(codePoint)); break;\n        case 1: builder.appendCodePoint(Character.toLowerCase(codePoint)); break;\n        case 2: builder.appendCodePoint(codePoint); // leave intact\n      }\n    }\n    return builder.toString();\n  }\n\n  private static final int[] blockStarts = {\n    0x0000, 0x0080, 0x0100, 0x0180, 0x0250, 0x02B0, 0x0300, 0x0370, 0x0400, \n    0x0500, 0x0530, 0x0590, 0x0600, 0x0700, 0x0750, 0x0780, 0x07C0, 0x0800, \n    0x0900, 0x0980, 0x0A00, 0x0A80, 0x0B00, 0x0B80, 0x0C00, 0x0C80, 0x0D00, \n    0x0D80, 0x0E00, 0x0E80, 0x0F00, 0x1000, 0x10A0, 0x1100, 0x1200, 0x1380, \n    0x13A0, 0x1400, 0x1680, 0x16A0, 0x1700, 0x1720, 0x1740, 0x1760, 0x1780, \n    0x1800, 0x18B0, 0x1900, 0x1950, 0x1980, 0x19E0, 0x1A00, 0x1A20, 0x1B00, \n    0x1B80, 0x1C00, 0x1C50, 0x1CD0, 0x1D00, 0x1D80, 0x1DC0, 0x1E00, 0x1F00, \n    0x2000, 0x2070, 0x20A0, 0x20D0, 0x2100, 0x2150, 0x2190, 0x2200, 0x2300, \n    0x2400, 0x2440, 0x2460, 0x2500, 0x2580, 0x25A0, 0x2600, 0x2700, 0x27C0, \n    0x27F0, 0x2800, 0x2900, 0x2980, 0x2A00, 0x2B00, 0x2C00, 0x2C60, 0x2C80, \n    0x2D00, 0x2D30, 0x2D80, 0x2DE0, 0x2E00, 0x2E80, 0x2F00, 0x2FF0, 0x3000, \n    0x3040, 0x30A0, 0x3100, 0x3130, 0x3190, 0x31A0, 0x31C0, 0x31F0, 0x3200, \n    0x3300, 0x3400, 0x4DC0, 0x4E00, 0xA000, 0xA490, 0xA4D0, 0xA500, 0xA640, \n    0xA6A0, 0xA700, 0xA720, 0xA800, 0xA830, 0xA840, 0xA880, 0xA8E0, 0xA900, \n    0xA930, 0xA960, 0xA980, 0xAA00, 0xAA60, 0xAA80, 0xABC0, 0xAC00, 0xD7B0, \n    0xE000, 0xF900, 0xFB00, 0xFB50, 0xFE00, 0xFE10, \n    0xFE20, 0xFE30, 0xFE50, 0xFE70, 0xFF00, 0xFFF0, \n    0x10000, 0x10080, 0x10100, 0x10140, 0x10190, 0x101D0, 0x10280, 0x102A0, \n    0x10300, 0x10330, 0x10380, 0x103A0, 0x10400, 0x10450, 0x10480, 0x10800, \n    0x10840, 0x10900, 0x10920, 0x10A00, 0x10A60, 0x10B00, 0x10B40, 0x10B60, \n    0x10C00, 0x10E60, 0x11080, 0x12000, 0x12400, 0x13000, 0x1D000, 0x1D100, \n    0x1D200, 0x1D300, 0x1D360, 0x1D400, 0x1F000, 0x1F030, 0x1F100, 0x1F200, \n    0x20000, 0x2A700, 0x2F800, 0xE0000, 0xE0100, 0xF0000, 0x100000\n  };\n  \n  private static final int[] blockEnds = {\n    0x007F, 0x00FF, 0x017F, 0x024F, 0x02AF, 0x02FF, 0x036F, 0x03FF, 0x04FF, \n    0x052F, 0x058F, 0x05FF, 0x06FF, 0x074F, 0x077F, 0x07BF, 0x07FF, 0x083F, \n    0x097F, 0x09FF, 0x0A7F, 0x0AFF, 0x0B7F, 0x0BFF, 0x0C7F, 0x0CFF, 0x0D7F, \n    0x0DFF, 0x0E7F, 0x0EFF, 0x0FFF, 0x109F, 0x10FF, 0x11FF, 0x137F, 0x139F, \n    0x13FF, 0x167F, 0x169F, 0x16FF, 0x171F, 0x173F, 0x175F, 0x177F, 0x17FF, \n    0x18AF, 0x18FF, 0x194F, 0x197F, 0x19DF, 0x19FF, 0x1A1F, 0x1AAF, 0x1B7F, \n    0x1BBF, 0x1C4F, 0x1C7F, 0x1CFF, 0x1D7F, 0x1DBF, 0x1DFF, 0x1EFF, 0x1FFF, \n    0x206F, 0x209F, 0x20CF, 0x20FF, 0x214F, 0x218F, 0x21FF, 0x22FF, 0x23FF, \n    0x243F, 0x245F, 0x24FF, 0x257F, 0x259F, 0x25FF, 0x26FF, 0x27BF, 0x27EF, \n    0x27FF, 0x28FF, 0x297F, 0x29FF, 0x2AFF, 0x2BFF, 0x2C5F, 0x2C7F, 0x2CFF, \n    0x2D2F, 0x2D7F, 0x2DDF, 0x2DFF, 0x2E7F, 0x2EFF, 0x2FDF, 0x2FFF, 0x303F, \n    0x309F, 0x30FF, 0x312F, 0x318F, 0x319F, 0x31BF, 0x31EF, 0x31FF, 0x32FF, \n    0x33FF, 0x4DBF, 0x4DFF, 0x9FFF, 0xA48F, 0xA4CF, 0xA4FF, 0xA63F, 0xA69F, \n    0xA6FF, 0xA71F, 0xA7FF, 0xA82F, 0xA83F, 0xA87F, 0xA8DF, 0xA8FF, 0xA92F, \n    0xA95F, 0xA97F, 0xA9DF, 0xAA5F, 0xAA7F, 0xAADF, 0xABFF, 0xD7AF, 0xD7FF, \n    0xF8FF, 0xFAFF, 0xFB4F, 0xFDFF, 0xFE0F, 0xFE1F, \n    0xFE2F, 0xFE4F, 0xFE6F, 0xFEFF, 0xFFEF, 0xFFFF, \n    0x1007F, 0x100FF, 0x1013F, 0x1018F, 0x101CF, 0x101FF, 0x1029F, 0x102DF, \n    0x1032F, 0x1034F, 0x1039F, 0x103DF, 0x1044F, 0x1047F, 0x104AF, 0x1083F, \n    0x1085F, 0x1091F, 0x1093F, 0x10A5F, 0x10A7F, 0x10B3F, 0x10B5F, 0x10B7F, \n    0x10C4F, 0x10E7F, 0x110CF, 0x123FF, 0x1247F, 0x1342F, 0x1D0FF, 0x1D1FF, \n    0x1D24F, 0x1D35F, 0x1D37F, 0x1D7FF, 0x1F02F, 0x1F09F, 0x1F1FF, 0x1F2FF, \n    0x2A6DF, 0x2B73F, 0x2FA1F, 0xE007F, 0xE01EF, 0xFFFFF, 0x10FFFF\n  };\n  \n  /** Returns random string of length between 0-20 codepoints, all codepoints within the same unicode block. */\n  public static String randomRealisticUnicodeString(Random r) {\n    return randomRealisticUnicodeString(r, 20);\n  }\n  \n  /** Returns random string of length up to maxLength codepoints , all codepoints within the same unicode block. */\n  public static String randomRealisticUnicodeString(Random r, int maxLength) {\n    return randomRealisticUnicodeString(r, 0, maxLength);\n  }\n\n  /** Returns random string of length between min and max codepoints, all codepoints within the same unicode block. */\n  public static String randomRealisticUnicodeString(Random r, int minLength, int maxLength) {\n    final int end = nextInt(r, minLength, maxLength);\n    final int block = r.nextInt(blockStarts.length);\n    StringBuilder sb = new StringBuilder();\n    for (int i = 0; i < end; i++)\n      sb.appendCodePoint(nextInt(r, blockStarts[block], blockEnds[block]));\n    return sb.toString();\n  }\n  \n  /** Returns random string, with a given UTF-8 byte length*/\n  public static String randomFixedByteLengthUnicodeString(Random r, int length) {\n    \n    final char[] buffer = new char[length*3];\n    int bytes = length;\n    int i = 0;\n    for (; i < buffer.length && bytes != 0; i++) {\n      int t;\n      if (bytes >= 4) {\n        t = r.nextInt(5);\n      } else if (bytes >= 3) {\n        t = r.nextInt(4);\n      } else if (bytes >= 2) {\n        t = r.nextInt(2);\n      } else {\n        t = 0;\n      }\n      if (t == 0) {\n        buffer[i] = (char) r.nextInt(0x80);\n        bytes--;\n      } else if (1 == t) {\n        buffer[i] = (char) nextInt(r, 0x80, 0x7ff);\n        bytes -= 2;\n      } else if (2 == t) {\n        buffer[i] = (char) nextInt(r, 0x800, 0xd7ff);\n        bytes -= 3;\n      } else if (3 == t) {\n        buffer[i] = (char) nextInt(r, 0xe000, 0xffff);\n        bytes -= 3;\n      } else if (4 == t) {\n        // Make a surrogate pair\n        // High surrogate\n        buffer[i++] = (char) nextInt(r, 0xd800, 0xdbff);\n        // Low surrogate\n        buffer[i] = (char) nextInt(r, 0xdc00, 0xdfff);\n        bytes -= 4;\n      }\n\n    }\n    return new String(buffer, 0, i);\n  }\n\n  \n  /** Return a Codec that can read any of the\n   *  default codecs and formats, but always writes in the specified\n   *  format. */\n  public static Codec alwaysPostingsFormat(final PostingsFormat format) {\n    // TODO: we really need for postings impls etc to announce themselves\n    // (and maybe their params, too) to infostream on flush and merge.\n    // otherwise in a real debugging situation we won't know whats going on!\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"forcing postings format to:\" + format);\n    }\n    return new Lucene46Codec() {\n      @Override\n      public PostingsFormat getPostingsFormatForField(String field) {\n        return format;\n      }\n    };\n  }\n  \n  /** Return a Codec that can read any of the\n   *  default codecs and formats, but always writes in the specified\n   *  format. */\n  public static Codec alwaysDocValuesFormat(final DocValuesFormat format) {\n    // TODO: we really need for docvalues impls etc to announce themselves\n    // (and maybe their params, too) to infostream on flush and merge.\n    // otherwise in a real debugging situation we won't know whats going on!\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"forcing docvalues format to:\" + format);\n    }\n    return new Lucene46Codec() {\n      @Override\n      public DocValuesFormat getDocValuesFormatForField(String field) {\n        return format;\n      }\n    };\n  }\n\n  // TODO: generalize all 'test-checks-for-crazy-codecs' to\n  // annotations (LUCENE-3489)\n  public static String getPostingsFormat(String field) {\n    return getPostingsFormat(Codec.getDefault(), field);\n  }\n  \n  public static String getPostingsFormat(Codec codec, String field) {\n    PostingsFormat p = codec.postingsFormat();\n    if (p instanceof PerFieldPostingsFormat) {\n      return ((PerFieldPostingsFormat)p).getPostingsFormatForField(field).getName();\n    } else {\n      return p.getName();\n    }\n  }\n\n  public static String getDocValuesFormat(String field) {\n    return getDocValuesFormat(Codec.getDefault(), field);\n  }\n  \n  public static String getDocValuesFormat(Codec codec, String field) {\n    DocValuesFormat f = codec.docValuesFormat();\n    if (f instanceof PerFieldDocValuesFormat) {\n      return ((PerFieldDocValuesFormat) f).getDocValuesFormatForField(field).getName();\n    } else {\n      return f.getName();\n    }\n  }\n\n  // TODO: remove this, push this test to Lucene40/Lucene42 codec tests\n  public static boolean fieldSupportsHugeBinaryDocValues(String field) {\n    String dvFormat = getDocValuesFormat(field);\n    if (dvFormat.equals(\"Lucene40\") || dvFormat.equals(\"Lucene42\") || dvFormat.equals(\"Memory\")) {\n      return false;\n    }\n    return true;\n  }\n\n  public static boolean anyFilesExceptWriteLock(Directory dir) throws IOException {\n    String[] files = dir.listAll();\n    if (files.length > 1 || (files.length == 1 && !files[0].equals(\"write.lock\"))) {\n      return true;\n    } else {\n      return false;\n    }\n  }\n\n  /** just tries to configure things to keep the open file\n   * count lowish */\n  public static void reduceOpenFiles(IndexWriter w) {\n    // keep number of open files lowish\n    MergePolicy mp = w.getConfig().getMergePolicy();\n    if (mp instanceof LogMergePolicy) {\n      LogMergePolicy lmp = (LogMergePolicy) mp;\n      lmp.setMergeFactor(Math.min(5, lmp.getMergeFactor()));\n      lmp.setNoCFSRatio(1.0);\n    } else if (mp instanceof TieredMergePolicy) {\n      TieredMergePolicy tmp = (TieredMergePolicy) mp;\n      tmp.setMaxMergeAtOnce(Math.min(5, tmp.getMaxMergeAtOnce()));\n      tmp.setSegmentsPerTier(Math.min(5, tmp.getSegmentsPerTier()));\n      tmp.setNoCFSRatio(1.0);\n    }\n    MergeScheduler ms = w.getConfig().getMergeScheduler();\n    if (ms instanceof ConcurrentMergeScheduler) {\n      // wtf... shouldnt it be even lower since its 1 by default?!?!\n      ((ConcurrentMergeScheduler) ms).setMaxMergesAndThreads(3, 2);\n    }\n  }\n\n  /** Checks some basic behaviour of an AttributeImpl\n   * @param reflectedValues contains a map with \"AttributeClass#key\" as values\n   */\n  public static <T> void assertAttributeReflection(final AttributeImpl att, Map<String,T> reflectedValues) {\n    final Map<String,Object> map = new HashMap<String,Object>();\n    att.reflectWith(new AttributeReflector() {\n      @Override\n      public void reflect(Class<? extends Attribute> attClass, String key, Object value) {\n        map.put(attClass.getName() + '#' + key, value);\n      }\n    });\n    Assert.assertEquals(\"Reflection does not produce same map\", reflectedValues, map);\n  }\n  \n  /** \n   * insecure, fast version of File.createTempFile\n   * uses Random instead of SecureRandom.\n   */\n  public static File createTempFile(String prefix, String suffix, File directory)\n      throws IOException {\n    if (prefix.length() < 3) {\n      throw new IllegalArgumentException(\"prefix must be at least 3 characters\");\n    }\n    String newSuffix = suffix == null ? \".tmp\" : suffix;\n    // always pull a long from master random. that way, the randomness of the test\n    // is not affected by whether it initialized the counter (in genTempFile) or not.\n    // note that the Random used by genTempFile is *not* the master Random, and therefore\n    // does not affect the randomness of the test.\n    final Random random = new Random(RandomizedContext.current().getRandom().nextLong());\n    File result;\n    do {\n      result = genTempFile(random, prefix, newSuffix, directory);\n    } while (!result.createNewFile());\n    return result;\n  }\n\n  /* identify for differnt VM processes */\n  private static String counterBase;\n  \n  /* Temp file counter */\n  private static int counter;\n  private static final Object counterLock = new Object();\n\n  private static File genTempFile(Random random, String prefix, String suffix, File directory) {\n    final int identify;\n    synchronized (counterLock) {\n      if (counterBase == null) { // init once\n        counter = random.nextInt() & 0xFFFF; // up to five digits number\n        counterBase = Integer.toString(counter);\n      }\n      identify = counter++;\n    }\n    StringBuilder newName = new StringBuilder();\n    newName.append(prefix);\n    newName.append(counterBase);\n    newName.append(identify);\n    newName.append(suffix);\n    return new File(directory, newName.toString());\n  }\n\n  public static void assertEquals(TopDocs expected, TopDocs actual) {\n    Assert.assertEquals(\"wrong total hits\", expected.totalHits, actual.totalHits);\n    Assert.assertEquals(\"wrong maxScore\", expected.getMaxScore(), actual.getMaxScore(), 0.0);\n    Assert.assertEquals(\"wrong hit count\", expected.scoreDocs.length, actual.scoreDocs.length);\n    for(int hitIDX=0;hitIDX<expected.scoreDocs.length;hitIDX++) {\n      final ScoreDoc expectedSD = expected.scoreDocs[hitIDX];\n      final ScoreDoc actualSD = actual.scoreDocs[hitIDX];\n      Assert.assertEquals(\"wrong hit docID\", expectedSD.doc, actualSD.doc);\n      Assert.assertEquals(\"wrong hit score\", expectedSD.score, actualSD.score, 0.0);\n      if (expectedSD instanceof FieldDoc) {\n        Assert.assertTrue(actualSD instanceof FieldDoc);\n        Assert.assertArrayEquals(\"wrong sort field values\",\n                            ((FieldDoc) expectedSD).fields,\n                            ((FieldDoc) actualSD).fields);\n      } else {\n        Assert.assertFalse(actualSD instanceof FieldDoc);\n      }\n    }\n  }\n\n  // NOTE: this is likely buggy, and cannot clone fields\n  // with tokenStreamValues, etc.  Use at your own risk!!\n\n  // TODO: is there a pre-existing way to do this!!!\n  public static Document cloneDocument(Document doc1) {\n    final Document doc2 = new Document();\n    for(IndexableField f : doc1.getFields()) {\n      final Field field1 = (Field) f;\n      final Field field2;\n      final DocValuesType dvType = field1.fieldType().docValueType();\n      final NumericType numType = field1.fieldType().numericType();\n      if (dvType != null) {\n        switch(dvType) {\n          case NUMERIC:\n            field2 = new NumericDocValuesField(field1.name(), field1.numericValue().longValue());\n            break;\n          case BINARY:\n            field2 = new BinaryDocValuesField(field1.name(), field1.binaryValue());\n          break;\n          case SORTED:\n            field2 = new SortedDocValuesField(field1.name(), field1.binaryValue());\n            break;\n          default:\n            throw new IllegalStateException(\"unknown Type: \" + dvType);\n        }\n      } else if (numType != null) {\n        switch (numType) {\n          case INT:\n            field2 = new IntField(field1.name(), field1.numericValue().intValue(), field1.fieldType());\n            break;\n          case FLOAT:\n            field2 = new FloatField(field1.name(), field1.numericValue().intValue(), field1.fieldType());\n            break;\n          case LONG:\n            field2 = new LongField(field1.name(), field1.numericValue().intValue(), field1.fieldType());\n            break;\n          case DOUBLE:\n            field2 = new DoubleField(field1.name(), field1.numericValue().intValue(), field1.fieldType());\n            break;\n          default:\n            throw new IllegalStateException(\"unknown Type: \" + numType);\n        }\n      } else {\n        field2 = new Field(field1.name(), field1.stringValue(), field1.fieldType());\n      }\n      doc2.add(field2);\n    }\n\n    return doc2;\n  }\n\n  // Returns a DocsEnum, but randomly sometimes uses a\n  // DocsAndFreqsEnum, DocsAndPositionsEnum.  Returns null\n  // if field/term doesn't exist:\n  public static DocsEnum docs(Random random, IndexReader r, String field, BytesRef term, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {\n    final Terms terms = MultiFields.getTerms(r, field);\n    if (terms == null) {\n      return null;\n    }\n    final TermsEnum termsEnum = terms.iterator(null);\n    if (!termsEnum.seekExact(term)) {\n      return null;\n    }\n    return docs(random, termsEnum, liveDocs, reuse, flags);\n  }\n\n  // Returns a DocsEnum from a positioned TermsEnum, but\n  // randomly sometimes uses a DocsAndFreqsEnum, DocsAndPositionsEnum.\n  public static DocsEnum docs(Random random, TermsEnum termsEnum, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {\n    if (random.nextBoolean()) {\n      if (random.nextBoolean()) {\n        final int posFlags;\n        switch (random.nextInt(4)) {\n          case 0: posFlags = 0; break;\n          case 1: posFlags = DocsAndPositionsEnum.FLAG_OFFSETS; break;\n          case 2: posFlags = DocsAndPositionsEnum.FLAG_PAYLOADS; break;\n          default: posFlags = DocsAndPositionsEnum.FLAG_OFFSETS | DocsAndPositionsEnum.FLAG_PAYLOADS; break;\n        }\n        // TODO: cast to DocsAndPositionsEnum?\n        DocsAndPositionsEnum docsAndPositions = termsEnum.docsAndPositions(liveDocs, null, posFlags);\n        if (docsAndPositions != null) {\n          return docsAndPositions;\n        }\n      }\n      flags |= DocsEnum.FLAG_FREQS;\n    }\n    return termsEnum.docs(liveDocs, reuse, flags);\n  }\n  \n  public static CharSequence stringToCharSequence(String string, Random random) {\n    return bytesToCharSequence(new BytesRef(string), random);\n  }\n  \n  public static CharSequence bytesToCharSequence(BytesRef ref, Random random) {\n    switch(random.nextInt(5)) {\n    case 4:\n      CharsRef chars = new CharsRef(ref.length);\n      UnicodeUtil.UTF8toUTF16(ref.bytes, ref.offset, ref.length, chars);\n      return chars;\n    case 3:\n      return CharBuffer.wrap(ref.utf8ToString());\n    default:\n      return ref.utf8ToString();\n    }\n  }\n\n  /**\n   * Shutdown {@link ExecutorService} and wait for its.\n   */\n  public static void shutdownExecutorService(ExecutorService ex) {\n    if (ex != null) {\n      try {\n        ex.shutdown();\n        ex.awaitTermination(1, TimeUnit.SECONDS);\n      } catch (InterruptedException e) {\n        // Just report it on the syserr.\n        System.err.println(\"Could not properly shutdown executor service.\");\n        e.printStackTrace(System.err);\n      }\n    }\n  }\n\n  /**\n   * Returns a valid (compiling) Pattern instance with random stuff inside. Be careful\n   * when applying random patterns to longer strings as certain types of patterns\n   * may explode into exponential times in backtracking implementations (such as Java's).\n   */\n  public static Pattern randomPattern(Random random) {\n    final String nonBmpString = \"AB\\uD840\\uDC00C\";\n    while (true) {\n      try {\n        Pattern p = Pattern.compile(_TestUtil.randomRegexpishString(random));\n        String replacement = null;\n        // ignore bugs in Sun's regex impl\n        try {\n          replacement = p.matcher(nonBmpString).replaceAll(\"_\");\n        } catch (StringIndexOutOfBoundsException jdkBug) {\n          System.out.println(\"WARNING: your jdk is buggy!\");\n          System.out.println(\"Pattern.compile(\\\"\" + p.pattern() + \n              \"\\\").matcher(\\\"AB\\\\uD840\\\\uDC00C\\\").replaceAll(\\\"_\\\"); should not throw IndexOutOfBounds!\");\n        }\n        // Make sure the result of applying the pattern to a string with extended\n        // unicode characters is a valid utf16 string. See LUCENE-4078 for discussion.\n        if (replacement != null && UnicodeUtil.validUTF16String(replacement)) {\n          return p;\n        }\n      } catch (PatternSyntaxException ignored) {\n        // Loop trying until we hit something that compiles.\n      }\n    }\n  }\n    \n  \n  public static final FilterStrategy randomFilterStrategy(final Random random) {\n    switch(random.nextInt(6)) {\n      case 5:\n      case 4:\n        return new FilteredQuery.RandomAccessFilterStrategy() {\n          @Override\n          protected boolean useRandomAccess(Bits bits, int firstFilterDoc) {\n            return LuceneTestCase.random().nextBoolean();\n          }\n        };\n      case 3:\n        return FilteredQuery.RANDOM_ACCESS_FILTER_STRATEGY;\n      case 2:\n        return FilteredQuery.LEAP_FROG_FILTER_FIRST_STRATEGY;\n      case 1:\n        return FilteredQuery.LEAP_FROG_QUERY_FIRST_STRATEGY;\n      case 0: \n        return FilteredQuery.QUERY_FIRST_FILTER_STRATEGY;\n      default:\n        return FilteredQuery.RANDOM_ACCESS_FILTER_STRATEGY;\n    }\n  }\n\n  /**\n   * Returns a random string in the specified length range consisting \n   * entirely of whitespace characters \n   * @see #WHITESPACE_CHARACTERS\n   */\n  public static String randomWhitespace(Random r, int minLength, int maxLength) {\n    final int end = nextInt(r, minLength, maxLength);\n    StringBuilder out = new StringBuilder();\n    for (int i = 0; i < end; i++) {\n      int offset = nextInt(r, 0, WHITESPACE_CHARACTERS.length-1);\n      char c = WHITESPACE_CHARACTERS[offset];\n      // sanity check\n      Assert.assertTrue(\"Not really whitespace? (@\"+offset+\"): \" + c, Character.isWhitespace(c));\n      out.append(c);\n    }\n    return out.toString();\n  }\n  \n  /** List of characters that match {@link Character#isWhitespace} */\n  public static final char[] WHITESPACE_CHARACTERS = new char[] {\n    // :TODO: is this list exhaustive?\n    '\\u0009',\n    '\\n',    \n    '\\u000B',\n    '\\u000C',\n    '\\r',    \n    '\\u001C',\n    '\\u001D',\n    '\\u001E',\n    '\\u001F',\n    '\\u0020',\n    // '\\u0085', faild sanity check?\n    '\\u1680',\n    '\\u180E',\n    '\\u2000',\n    '\\u2001',\n    '\\u2002',\n    '\\u2003',\n    '\\u2004',\n    '\\u2005',\n    '\\u2006',\n    '\\u2008',\n    '\\u2009',\n    '\\u200A',\n    '\\u2028',\n    '\\u2029',\n    '\\u205F',\n    '\\u3000',\n  };\n}\n",
        "methodName": "rmDir",
        "exampleID": 26,
        "dataset": "spotbugs",
        "filepath": "/lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java",
        "line": "141",
        "source": "?",
        "sourceLine": "141",
        "qualifier": "Possible null pointer dereference of the $$value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 27
            }
        ],
        "line_number": "141"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.solr;\n\nimport static com.google.common.base.Preconditions.checkNotNull;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.StringWriter;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Comparator;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Properties;\nimport java.util.logging.ConsoleHandler;\nimport java.util.logging.Handler;\nimport java.util.logging.Level;\nimport java.util.regex.Pattern;\n\nimport javax.xml.xpath.XPathExpressionException;\n\nimport org.apache.commons.codec.Charsets;\nimport org.apache.commons.io.FileUtils;\nimport org.apache.lucene.analysis.MockAnalyzer;\nimport org.apache.lucene.index.IndexWriterConfig;\nimport org.apache.lucene.util.IOUtils;\nimport org.apache.lucene.util.LuceneTestCase;\nimport org.apache.lucene.util.QuickPatchThreadsFilter;\nimport org.apache.lucene.util._TestUtil;\nimport org.apache.solr.client.solrj.util.ClientUtils;\nimport org.apache.solr.cloud.IpTables;\nimport org.apache.solr.common.SolrDocument;\nimport org.apache.solr.common.SolrDocumentList;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.SolrInputDocument;\nimport org.apache.solr.common.SolrInputField;\nimport org.apache.solr.common.cloud.SolrZkClient;\nimport org.apache.solr.common.params.CommonParams;\nimport org.apache.solr.common.params.ModifiableSolrParams;\nimport org.apache.solr.common.params.SolrParams;\nimport org.apache.solr.common.util.XML;\nimport org.apache.solr.core.ConfigSolr;\nimport org.apache.solr.core.ConfigSolrXmlOld;\nimport org.apache.solr.core.CoreContainer;\nimport org.apache.solr.core.CoreDescriptor;\nimport org.apache.solr.core.SolrConfig;\nimport org.apache.solr.core.SolrCore;\nimport org.apache.solr.core.SolrResourceLoader;\nimport org.apache.solr.handler.JsonUpdateRequestHandler;\nimport org.apache.solr.request.LocalSolrQueryRequest;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.request.SolrRequestHandler;\nimport org.apache.solr.schema.IndexSchema;\nimport org.apache.solr.schema.SchemaField;\nimport org.apache.solr.search.SolrIndexSearcher;\nimport org.apache.solr.servlet.DirectSolrConnection;\nimport org.apache.solr.util.AbstractSolrTestCase;\nimport org.apache.solr.util.RevertDefaultThreadHandlerRule;\nimport org.apache.solr.util.TestHarness;\nimport org.junit.AfterClass;\nimport org.junit.BeforeClass;\nimport org.junit.ClassRule;\nimport org.junit.Rule;\nimport org.junit.rules.RuleChain;\nimport org.junit.rules.TestRule;\nimport org.noggit.CharArr;\nimport org.noggit.JSONUtil;\nimport org.noggit.ObjectBuilder;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.xml.sax.SAXException;\n\nimport com.carrotsearch.randomizedtesting.RandomizedContext;\nimport com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;\nimport com.carrotsearch.randomizedtesting.rules.SystemPropertiesRestoreRule;\n\n/**\n * A junit4 Solr test harness that extends LuceneTestCaseJ4. To change which core is used when loading the schema and solrconfig.xml, simply\n * invoke the {@link #initCore(String, String, String, String)} method.\n * \n * Unlike {@link AbstractSolrTestCase}, a new core is not created for each test method.\n */\n@ThreadLeakFilters(defaultFilters = true, filters = {\n    SolrIgnoredThreadsFilter.class,\n    QuickPatchThreadsFilter.class\n})\npublic abstract class SolrTestCaseJ4 extends LuceneTestCase {\n  private static String coreName = ConfigSolrXmlOld.DEFAULT_DEFAULT_CORE_NAME;\n  public static int DEFAULT_CONNECTION_TIMEOUT = 30000;  // default socket connection timeout in ms\n\n\n  @ClassRule\n  public static TestRule solrClassRules = \n    RuleChain.outerRule(new SystemPropertiesRestoreRule())\n             .around(new RevertDefaultThreadHandlerRule());\n\n  @Rule\n  public TestRule solrTestRules = \n    RuleChain.outerRule(new SystemPropertiesRestoreRule());\n\n  @BeforeClass \n  @SuppressWarnings(\"unused\")\n  private static void beforeClass() {\n    System.setProperty(\"jetty.testMode\", \"true\");\n    System.setProperty(\"enable.update.log\", usually() ? \"true\" : \"false\");\n    System.setProperty(\"tests.shardhandler.randomSeed\", Long.toString(random().nextLong()));\n    System.setProperty(\"solr.clustering.enabled\", \"false\");\n    setupLogging();\n    startTrackingSearchers();\n    startTrackingZkClients();\n    ignoreException(\"ignore_exception\");\n    newRandomConfig();\n  }\n\n  @AfterClass\n  @SuppressWarnings(\"unused\")\n  private static void afterClass() throws Exception {\n    deleteCore();\n    resetExceptionIgnores();\n    endTrackingSearchers();\n    endTrackingZkClients();\n    resetFactory();\n    coreName = ConfigSolrXmlOld.DEFAULT_DEFAULT_CORE_NAME;\n    System.clearProperty(\"jetty.testMode\");\n    System.clearProperty(\"tests.shardhandler.randomSeed\");\n    System.clearProperty(\"enable.update.log\");\n    System.clearProperty(\"useCompoundFile\");\n    \n    IpTables.unblockAllPorts();\n  }\n\n  private static boolean changedFactory = false;\n  private static String savedFactory;\n  /** Use a different directory factory.  Passing \"null\" sets to an FS-based factory */\n  public static void useFactory(String factory) throws Exception {\n    // allow calling more than once so a subclass can override a base class\n    if (!changedFactory) {\n      savedFactory = System.getProperty(\"solr.DirectoryFactory\");\n    }\n\n    if (factory == null) {\n      factory = random().nextInt(100) < 75 ? \"solr.NRTCachingDirectoryFactory\" : \"solr.StandardDirectoryFactory\"; // test the default most of the time\n    }\n    System.setProperty(\"solr.directoryFactory\", factory);\n    changedFactory = true;\n  }\n\n  public static void resetFactory() throws Exception {\n    if (!changedFactory) return;\n    changedFactory = false;\n    if (savedFactory != null) {\n      System.setProperty(\"solr.directoryFactory\", savedFactory);\n    } else {\n      System.clearProperty(\"solr.directoryFactory\");\n    }\n  }\n\n  /**\n   * Call this from @BeforeClass to set up the test harness and update handler with no cores.\n   *\n   * @param solrHome The solr home directory.\n   * @param xmlStr - the text of an XML file to use. If null, use the what's the absolute minimal file.\n   * @throws Exception Lost of file-type things can go wrong.\n   */\n  public static void setupNoCoreTest(File solrHome, String xmlStr) throws Exception {\n\n    File tmpFile = new File(solrHome, ConfigSolr.SOLR_XML_FILE);\n    if (xmlStr == null) {\n      xmlStr = \"<solr></solr>\";\n    }\n    FileUtils.write(tmpFile, xmlStr, IOUtils.CHARSET_UTF_8.toString());\n\n    SolrResourceLoader loader = new SolrResourceLoader(solrHome.getAbsolutePath());\n    h = new TestHarness(loader, ConfigSolr.fromFile(loader, new File(solrHome, \"solr.xml\")));\n    lrf = h.getRequestFactory(\"standard\", 0, 20, CommonParams.VERSION, \"2.2\");\n  }\n  \n  /** sets system properties based on \n   * {@link #newIndexWriterConfig(org.apache.lucene.util.Version, org.apache.lucene.analysis.Analyzer)}\n   * \n   * configs can use these system properties to vary the indexwriter settings\n   */\n  public static void newRandomConfig() {\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    System.setProperty(\"useCompoundFile\", String.valueOf(iwc.getUseCompoundFile()));\n\n    System.setProperty(\"solr.tests.maxBufferedDocs\", String.valueOf(iwc.getMaxBufferedDocs()));\n    System.setProperty(\"solr.tests.ramBufferSizeMB\", String.valueOf(iwc.getRAMBufferSizeMB()));\n    System.setProperty(\"solr.tests.mergeScheduler\", iwc.getMergeScheduler().getClass().getName());\n\n    // don't ask iwc.getMaxThreadStates(), sometimes newIWC uses \n    // RandomDocumentsWriterPerThreadPool and all hell breaks loose\n    int maxIndexingThreads = rarely(random())\n      ? _TestUtil.nextInt(random(), 5, 20) // crazy value\n      : _TestUtil.nextInt(random(), 1, 4); // reasonable value\n    System.setProperty(\"solr.tests.maxIndexingThreads\", String.valueOf(maxIndexingThreads));\n  }\n\n  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    log.info(\"###Starting \" + getTestName());  // returns <unknown>???\n  }\n\n  @Override\n  public void tearDown() throws Exception {\n    log.info(\"###Ending \" + getTestName());    \n    super.tearDown();\n  }\n\n  public static SolrLogFormatter formatter;\n\n  public static void setupLogging() {\n    boolean register = false;\n    Handler[] handlers = java.util.logging.Logger.getLogger(\"\").getHandlers();\n    ConsoleHandler consoleHandler = null;\n    for (Handler handler : handlers) {\n      if (handler instanceof ConsoleHandler) {\n        consoleHandler = (ConsoleHandler)handler;\n        break;\n      }\n    }\n\n    if (consoleHandler == null) {\n      consoleHandler = new ConsoleHandler();\n      register = true;\n    }\n\n    consoleHandler.setLevel(Level.ALL);\n    formatter = new SolrLogFormatter();\n    consoleHandler.setFormatter(formatter);\n\n    if (register) {\n      java.util.logging.Logger.getLogger(\"\").addHandler(consoleHandler);\n    }\n  }\n\n  public static void setLoggingLevel(Level level) {\n    java.util.logging.Logger logger = java.util.logging.Logger.getLogger(\"\");\n    logger.setLevel(level);\n  }\n\n\n  /** Call initCore in @BeforeClass to instantiate a solr core in your test class.\n   * deleteCore will be called for you via SolrTestCaseJ4 @AfterClass */\n  public static void initCore(String config, String schema) throws Exception {\n    initCore(config, schema, TEST_HOME());\n  }\n\n  /** Call initCore in @BeforeClass to instantiate a solr core in your test class.\n   * deleteCore will be called for you via SolrTestCaseJ4 @AfterClass */\n  public static void initCore(String config, String schema, String solrHome) throws Exception {\n    assertNotNull(solrHome);\n    configString = config;\n    schemaString = schema;\n    testSolrHome = solrHome;\n    if (solrHome != null) {\n      System.setProperty(\"solr.solr.home\", solrHome);\n    }\n    initCore();\n  }\n\n  /** Call initCore in @BeforeClass to instantiate a solr core in your test class.\n   * deleteCore will be called for you via SolrTestCaseJ4 @AfterClass */\n  public static void initCore(String config, String schema, String solrHome, String pCoreName) throws Exception {\n    coreName=pCoreName;\n    initCore(config,schema,solrHome);\n  }\n  \n  static long numOpens;\n  static long numCloses;\n  public static void startTrackingSearchers() {\n    numOpens = SolrIndexSearcher.numOpens.getAndSet(0);\n    numCloses = SolrIndexSearcher.numCloses.getAndSet(0);\n    if (numOpens != 0 || numCloses != 0) {\n      // NOTE: some other tests don't use this base class and hence won't reset the counts.\n      log.warn(\"startTrackingSearchers: numOpens=\"+numOpens+\" numCloses=\"+numCloses);\n      numOpens = numCloses = 0;\n    }\n  }\n  static long zkClientNumOpens;\n  static long zkClientNumCloses;\n  public static void startTrackingZkClients() {\n    zkClientNumOpens = SolrZkClient.numOpens.get();\n    zkClientNumCloses = SolrZkClient.numCloses.get();\n  }\n\n  public static void endTrackingSearchers() {\n     long endNumOpens = SolrIndexSearcher.numOpens.get();\n     long endNumCloses = SolrIndexSearcher.numCloses.get();\n\n     // wait a bit in case any ending threads have anything to release\n     int retries = 0;\n     while (endNumOpens - numOpens != endNumCloses - numCloses) {\n       if (retries++ > 120) {\n         break;\n       }\n       try {\n         Thread.sleep(1000);\n       } catch (InterruptedException e) {}\n       endNumOpens = SolrIndexSearcher.numOpens.get();\n       endNumCloses = SolrIndexSearcher.numCloses.get();\n     }\n\n     SolrIndexSearcher.numOpens.getAndSet(0);\n     SolrIndexSearcher.numCloses.getAndSet(0);\n\n     if (endNumOpens-numOpens != endNumCloses-numCloses) {\n       String msg = \"ERROR: SolrIndexSearcher opens=\" + (endNumOpens-numOpens) + \" closes=\" + (endNumCloses-numCloses);\n       log.error(msg);\n       // if its TestReplicationHandler, ignore it. the test is broken and gets no love\n       if (\"TestReplicationHandler\".equals(RandomizedContext.current().getTargetClass().getSimpleName())) {\n         log.warn(\"TestReplicationHandler wants to fail!: \" + msg);\n       } else {\n         fail(msg);\n       }\n     }\n  }\n  \n  public static void endTrackingZkClients() {\n    long endNumOpens = SolrZkClient.numOpens.get();\n    long endNumCloses = SolrZkClient.numCloses.get();\n\n    SolrZkClient.numOpens.getAndSet(0);\n    SolrZkClient.numCloses.getAndSet(0);\n\n    if (endNumOpens-zkClientNumOpens != endNumCloses-zkClientNumCloses) {\n      String msg = \"ERROR: SolrZkClient opens=\" + (endNumOpens-zkClientNumOpens) + \" closes=\" + (endNumCloses-zkClientNumCloses);\n      log.error(msg);\n      fail(msg);\n    }\n }\n  \n  /** Causes an exception matching the regex pattern to not be logged. */\n  public static void ignoreException(String pattern) {\n    if (SolrException.ignorePatterns == null)\n      SolrException.ignorePatterns = new HashSet<String>();\n    SolrException.ignorePatterns.add(pattern);\n  }\n\n  public static void unIgnoreException(String pattern) {\n    if (SolrException.ignorePatterns != null)\n      SolrException.ignorePatterns.remove(pattern);\n  }\n\n\n  public static void resetExceptionIgnores() {\n    SolrException.ignorePatterns = null;\n    ignoreException(\"ignore_exception\");  // always ignore \"ignore_exception\"    \n  }\n\n  protected static String getClassName() {\n    return getTestClass().getName();\n  }\n\n  protected static String getSimpleClassName() {\n    return getTestClass().getSimpleName();\n  }\n\n  protected static String configString;\n  protected static String schemaString;\n  protected static String testSolrHome;\n\n  protected static SolrConfig solrConfig;\n\n  /**\n   * Harness initialized by initTestHarness.\n   *\n   * <p>\n   * For use in test methods as needed.\n   * </p>\n   */\n  protected static TestHarness h;\n\n  /**\n   * LocalRequestFactory initialized by initTestHarness using sensible\n   * defaults.\n   *\n   * <p>\n   * For use in test methods as needed.\n   * </p>\n   */\n  protected static TestHarness.LocalRequestFactory lrf;\n\n\n  /**\n   * Subclasses must define this method to return the name of the\n   * schema.xml they wish to use.\n   */\n  public static String getSchemaFile() {\n    return schemaString;\n  }\n\n  /**\n   * Subclasses must define this method to return the name of the\n   * solrconfig.xml they wish to use.\n   */\n  public static String getSolrConfigFile() {\n    return configString;\n  }\n\n  /**\n   * The directory used to story the index managed by the TestHarness h\n   */\n  protected static File dataDir;\n  \n  // hack due to File dataDir\n  protected static String hdfsDataDir;\n\n  /**\n   * Initializes things your test might need\n   *\n   * <ul>\n   * <li>Creates a dataDir in the \"java.io.tmpdir\"</li>\n   * <li>initializes the TestHarness h using this data directory, and getSchemaPath()</li>\n   * <li>initializes the LocalRequestFactory lrf using sensible defaults.</li>\n   * </ul>\n   *\n   */\n\n  public static Logger log = LoggerFactory.getLogger(SolrTestCaseJ4.class);\n\n  private static String factoryProp;\n\n  public static void createTempDir() {\n    String cname = getSimpleClassName();\n    dataDir = new File(TEMP_DIR,\n            \"solrtest-\" + cname + \"-\" + System.currentTimeMillis());\n    dataDir.mkdirs();\n    System.err.println(\"Creating dataDir: \" + dataDir.getAbsolutePath());\n  }\n\n  public static void initCore() throws Exception {\n    log.info(\"####initCore\");\n\n    ignoreException(\"ignore_exception\");\n    factoryProp = System.getProperty(\"solr.directoryFactory\");\n    if (factoryProp == null) {\n      System.setProperty(\"solr.directoryFactory\",\"solr.RAMDirectoryFactory\");\n    }\n    if (dataDir == null) {\n      createTempDir();\n    }\n\n    // other  methods like starting a jetty instance need these too\n    System.setProperty(\"solr.test.sys.prop1\", \"propone\");\n    System.setProperty(\"solr.test.sys.prop2\", \"proptwo\");\n\n    String configFile = getSolrConfigFile();\n    if (configFile != null) {\n      createCore();\n    }\n    log.info(\"####initCore end\");\n  }\n\n  public static void createCore() {\n    assertNotNull(testSolrHome);\n    solrConfig = TestHarness.createConfig(testSolrHome, coreName, getSolrConfigFile());\n    h = new TestHarness( coreName, hdfsDataDir == null ? dataDir.getAbsolutePath() : hdfsDataDir,\n            solrConfig,\n            getSchemaFile());\n    lrf = h.getRequestFactory\n            (\"standard\",0,20,CommonParams.VERSION,\"2.2\");\n  }\n\n  public static CoreContainer createCoreContainer(String solrHome, String solrXML) {\n    testSolrHome = checkNotNull(solrHome);\n    if (dataDir == null)\n      createTempDir();\n    h = new TestHarness(solrHome, solrXML);\n    lrf = h.getRequestFactory(\"standard\", 0, 20, CommonParams.VERSION, \"2.2\");\n    return h.getCoreContainer();\n  }\n\n  public static CoreContainer createDefaultCoreContainer(String solrHome) {\n    testSolrHome = checkNotNull(solrHome);\n    if (dataDir == null)\n      createTempDir();\n    h = new TestHarness(\"collection1\", dataDir.getAbsolutePath(), \"solrconfig.xml\", \"schema.xml\");\n    lrf = h.getRequestFactory(\"standard\", 0, 20, CommonParams.VERSION, \"2.2\");\n    return h.getCoreContainer();\n  }\n\n  public static boolean hasInitException(String message) {\n    for (Map.Entry<String, Exception> entry : h.getCoreContainer().getCoreInitFailures().entrySet()) {\n      if (entry.getValue().getMessage().indexOf(message) != -1)\n        return true;\n    }\n    return false;\n  }\n\n  public static boolean hasInitException(Class<? extends Exception> exceptionType) {\n    for (Map.Entry<String, Exception> entry : h.getCoreContainer().getCoreInitFailures().entrySet()) {\n      if (exceptionType.isAssignableFrom(entry.getValue().getClass()))\n        return true;\n    }\n    return false;\n  }\n\n  /** Subclasses that override setUp can optionally call this method\n   * to log the fact that their setUp process has ended.\n   */\n  public void postSetUp() {\n    log.info(\"####POSTSETUP \" + getTestName());\n  }\n\n\n  /** Subclasses that override tearDown can optionally call this method\n   * to log the fact that the tearDown process has started.  This is necessary\n   * since subclasses will want to call super.tearDown() at the *end* of their\n   * tearDown method.\n   */\n  public void preTearDown() {\n    log.info(\"####PRETEARDOWN \" + getTestName());\n  }\n\n  /**\n   * Shuts down the test harness, and makes the best attempt possible\n   * to delete dataDir, unless the system property \"solr.test.leavedatadir\"\n   * is set.\n   */\n  public static void deleteCore() {\n    log.info(\"###deleteCore\" );\n    if (h != null) { h.close(); }\n    if (dataDir != null) {\n      String skip = System.getProperty(\"solr.test.leavedatadir\");\n      if (null != skip && 0 != skip.trim().length()) {\n        System.err.println(\"NOTE: per solr.test.leavedatadir, dataDir will not be removed: \" + dataDir.getAbsolutePath());\n      } else {\n        if (!recurseDelete(dataDir)) {\n          System.err.println(\"!!!! WARNING: best effort to remove \" + dataDir.getAbsolutePath() + \" FAILED !!!!!\");\n        }\n      }\n    }\n\n    if (factoryProp == null) {\n      System.clearProperty(\"solr.directoryFactory\");\n    }\n    \n    dataDir = null;\n    solrConfig = null;\n    h = null;\n    lrf = null;\n    configString = schemaString = null;\n  }\n\n\n  /** Validates an update XML String is successful\n   */\n  public static void assertU(String update) {\n    assertU(null, update);\n  }\n\n  /** Validates an update XML String is successful\n   */\n  public static void assertU(String message, String update) {\n    checkUpdateU(message, update, true);\n  }\n\n  /** Validates an update XML String failed\n   */\n  public static void assertFailedU(String update) {\n    assertFailedU(null, update);\n  }\n\n  /** Validates an update XML String failed\n   */\n  public static void assertFailedU(String message, String update) {\n    checkUpdateU(message, update, false);\n  }\n\n  /** Checks the success or failure of an update message\n   */\n  private static void checkUpdateU(String message, String update, boolean shouldSucceed) {\n    try {\n      String m = (null == message) ? \"\" : message + \" \";\n      if (shouldSucceed) {\n           String res = h.validateUpdate(update);\n         if (res != null) fail(m + \"update was not successful: \" + res);\n      } else {\n           String res = h.validateErrorUpdate(update);\n         if (res != null) fail(m + \"update succeeded, but should have failed: \" + res);\n      }\n    } catch (SAXException e) {\n      throw new RuntimeException(\"Invalid XML\", e);\n    }\n  }\n\n  /** Validates a query matches some XPath test expressions and closes the query */\n  public static void assertQ(SolrQueryRequest req, String... tests) {\n    assertQ(null, req, tests);\n  }\n\n  /** Validates a query matches some XPath test expressions and closes the query */\n  public static void assertQ(String message, SolrQueryRequest req, String... tests) {\n    try {\n      String m = (null == message) ? \"\" : message + \" \";\n      String response = h.query(req);\n\n      if (req.getParams().getBool(\"facet\", false)) {\n        // add a test to ensure that faceting did not throw an exception\n        // internally, where it would be added to facet_counts/exception\n        String[] allTests = new String[tests.length+1];\n        System.arraycopy(tests,0,allTests,1,tests.length);\n        allTests[0] = \"*[count(//lst[@name='facet_counts']/*[@name='exception'])=0]\";\n        tests = allTests;\n      }\n\n      String results = h.validateXPath(response, tests);\n\n      if (null != results) {\n        String msg = \"REQUEST FAILED: xpath=\" + results\n            + \"\\n\\txml response was: \" + response\n            + \"\\n\\trequest was:\" + req.getParamString();\n\n        log.error(msg);\n        throw new RuntimeException(msg);\n      }\n\n    } catch (XPathExpressionException e1) {\n      throw new RuntimeException(\"XPath is invalid\", e1);\n    } catch (Exception e2) {\n      SolrException.log(log,\"REQUEST FAILED: \" + req.getParamString(), e2);\n      throw new RuntimeException(\"Exception during query\", e2);\n    }\n  }\n\n  /** Makes a query request and returns the JSON string response */\n  public static String JQ(SolrQueryRequest req) throws Exception {\n    SolrParams params = req.getParams();\n    if (!\"json\".equals(params.get(\"wt\",\"xml\")) || params.get(\"indent\")==null) {\n      ModifiableSolrParams newParams = new ModifiableSolrParams(params);\n      newParams.set(\"wt\",\"json\");\n      if (params.get(\"indent\")==null) newParams.set(\"indent\",\"true\");\n      req.setParams(newParams);\n    }\n\n    String response;\n    boolean failed=true;\n    try {\n      response = h.query(req);\n      failed = false;\n    } finally {\n      if (failed) {\n        log.error(\"REQUEST FAILED: \" + req.getParamString());\n      }\n    }\n\n    return response;\n  }\n\n  /**\n   * Validates a query matches some JSON test expressions using the default double delta tolerance.\n   * @see JSONTestUtil#DEFAULT_DELTA\n   * @see #assertJQ(SolrQueryRequest,double,String...)\n   */\n  public static void assertJQ(SolrQueryRequest req, String... tests) throws Exception {\n    assertJQ(req, JSONTestUtil.DEFAULT_DELTA, tests);\n  }\n  /**\n   * Validates a query matches some JSON test expressions and closes the\n   * query. The text expression is of the form path:JSON.  To facilitate\n   * easy embedding in Java strings, the JSON tests can have double quotes\n   * replaced with single quotes.\n   * <p>\n   * Please use this with care: this makes it easy to match complete\n   * structures, but doing so can result in fragile tests if you are\n   * matching more than what you want to test.\n   * </p>\n   * @param req Solr request to execute\n   * @param delta tolerance allowed in comparing float/double values\n   * @param tests JSON path expression + '==' + expected value\n   */\n  public static void assertJQ(SolrQueryRequest req, double delta, String... tests) throws Exception {\n    SolrParams params =  null;\n    try {\n      params = req.getParams();\n      if (!\"json\".equals(params.get(\"wt\",\"xml\")) || params.get(\"indent\")==null) {\n        ModifiableSolrParams newParams = new ModifiableSolrParams(params);\n        newParams.set(\"wt\",\"json\");\n        if (params.get(\"indent\")==null) newParams.set(\"indent\",\"true\");\n        req.setParams(newParams);\n      }\n\n      String response;\n      boolean failed=true;\n      try {\n        response = h.query(req);\n        failed = false;\n      } finally {\n        if (failed) {\n          log.error(\"REQUEST FAILED: \" + req.getParamString());\n        }\n      }\n\n      for (String test : tests) {\n        if (test == null || test.length()==0) continue;\n        String testJSON = json(test);\n\n        try {\n          failed = true;\n          String err = JSONTestUtil.match(response, testJSON, delta);\n          failed = false;\n          if (err != null) {\n            log.error(\"query failed JSON validation. error=\" + err +\n                \"\\n expected =\" + testJSON +\n                \"\\n response = \" + response +\n                \"\\n request = \" + req.getParamString()\n            );\n            throw new RuntimeException(err);\n          }\n        } finally {\n          if (failed) {\n            log.error(\"JSON query validation threw an exception.\" + \n                \"\\n expected =\" + testJSON +\n                \"\\n response = \" + response +\n                \"\\n request = \" + req.getParamString()\n            );\n          }\n        }\n      }\n    } finally {\n      // restore the params\n      if (params != null && params != req.getParams()) req.setParams(params);\n    }\n  }  \n\n\n  /** Makes sure a query throws a SolrException with the listed response code */\n  public static void assertQEx(String message, SolrQueryRequest req, int code ) {\n    try {\n      ignoreException(\".\");\n      h.query(req);\n      fail( message );\n    } catch (SolrException sex) {\n      assertEquals( code, sex.code() );\n    } catch (Exception e2) {\n      throw new RuntimeException(\"Exception during query\", e2);\n    } finally {\n      unIgnoreException(\".\");\n    }\n  }\n\n  public static void assertQEx(String message, SolrQueryRequest req, SolrException.ErrorCode code ) {\n    try {\n      ignoreException(\".\");\n      h.query(req);\n      fail( message );\n    } catch (SolrException e) {\n      assertEquals( code.code, e.code() );\n    } catch (Exception e2) {\n      throw new RuntimeException(\"Exception during query\", e2);\n    } finally {\n      unIgnoreException(\".\");\n    }\n  }\n\n\n  /**\n   * @see TestHarness#optimize\n   */\n  public static String optimize(String... args) {\n    return TestHarness.optimize(args);\n  }\n  /**\n   * @see TestHarness#commit\n   */\n  public static String commit(String... args) {\n    return TestHarness.commit(args);\n  }\n\n  /**\n   * Generates a simple &lt;add&gt;&lt;doc&gt;... XML String with no options\n   *\n   * @param fieldsAndValues 0th and Even numbered args are fields names odds are field values.\n   * @see #add\n   * @see #doc\n   */\n  public static String adoc(String... fieldsAndValues) {\n    XmlDoc d = doc(fieldsAndValues);\n    return add(d);\n  }\n\n  /**\n   * Generates a simple &lt;add&gt;&lt;doc&gt;... XML String with no options\n   */\n  public static String adoc(SolrInputDocument sdoc) {\n    StringWriter out = new StringWriter(512);\n    try {\n      out.append(\"<add>\");\n      ClientUtils.writeXML(sdoc, out);\n      out.append(\"</add>\");\n    } catch (IOException e) {\n      throw new RuntimeException(\"Inexplicable IO error from StringWriter\", e);\n    }\n    return out.toString();\n  }\n\n\n  /**\n   * Generates an &lt;add&gt;&lt;doc&gt;... XML String with options\n   * on the add.\n   *\n   * @param doc the Document to add\n   * @param args 0th and Even numbered args are param names, Odds are param values.\n   * @see #add\n   * @see #doc\n   */\n  public static String add(XmlDoc doc, String... args) {\n    try {\n      StringWriter r = new StringWriter();\n\n      // this is annoying\n      if (null == args || 0 == args.length) {\n        r.write(\"<add>\");\n        r.write(doc.xml);\n        r.write(\"</add>\");\n      } else {\n        XML.writeUnescapedXML(r, \"add\", doc.xml, (Object[])args);\n      }\n\n      return r.getBuffer().toString();\n    } catch (IOException e) {\n      throw new RuntimeException\n        (\"this should never happen with a StringWriter\", e);\n    }\n  }\n\n  /**\n   * Generates a &lt;delete&gt;... XML string for an ID\n   *\n   * @see TestHarness#deleteById\n   */\n  public static String delI(String id) {\n    return TestHarness.deleteById(id);\n  }\n  /**\n   * Generates a &lt;delete&gt;... XML string for an query\n   *\n   * @see TestHarness#deleteByQuery\n   */\n  public static String delQ(String q) {\n    return TestHarness.deleteByQuery(q);\n  }\n\n  /**\n   * Generates a simple &lt;doc&gt;... XML String with no options\n   *\n   * @param fieldsAndValues 0th and Even numbered args are fields names, Odds are field values.\n   * @see TestHarness#makeSimpleDoc\n   */\n  public static XmlDoc doc(String... fieldsAndValues) {\n    XmlDoc d = new XmlDoc();\n    d.xml = TestHarness.makeSimpleDoc(fieldsAndValues).toString();\n    return d;\n  }\n\n  public static ModifiableSolrParams params(String... params) {\n    ModifiableSolrParams msp = new ModifiableSolrParams();\n    for (int i=0; i<params.length; i+=2) {\n      msp.add(params[i], params[i+1]);\n    }\n    return msp;\n  }\n\n  public static Map map(Object... params) {\n    LinkedHashMap ret = new LinkedHashMap();\n    for (int i=0; i<params.length; i+=2) {\n      Object o = ret.put(params[i], params[i+1]);\n      // TODO: handle multi-valued map?\n    }\n    return ret;\n  }\n\n  /**\n   * Generates a SolrQueryRequest using the LocalRequestFactory\n   * @see #lrf\n   */\n  public static SolrQueryRequest req(String... q) {\n    return lrf.makeRequest(q);\n  }\n\n  /**\n   * Generates a SolrQueryRequest using the LocalRequestFactory\n   * @see #lrf\n   */\n  public static SolrQueryRequest req(String[] params, String... moreParams) {\n    String[] allParams = moreParams;\n    if (params.length!=0) {\n      int len = params.length + moreParams.length;\n      allParams = new String[len];\n      System.arraycopy(params,0,allParams,0,params.length);\n      System.arraycopy(moreParams,0,allParams,params.length,moreParams.length);\n    }\n\n    return lrf.makeRequest(allParams);\n  }\n\n  /**\n   * Generates a SolrQueryRequest\n   */\n  public static SolrQueryRequest req(SolrParams params, String... moreParams) {\n    ModifiableSolrParams mp = new ModifiableSolrParams(params);\n    for (int i=0; i<moreParams.length; i+=2) {\n      mp.add(moreParams[i], moreParams[i+1]);\n    }\n    return new LocalSolrQueryRequest(h.getCore(), mp);\n  }\n\n  /** Necessary to make method signatures un-ambiguous */\n  public static class XmlDoc {\n    public String xml;\n    @Override\n    public String toString() { return xml; }\n  }\n\n  public static boolean recurseDelete(File f) {\n    if (f.isDirectory()) {\n      for (File sub : f.listFiles()) {\n        if (!recurseDelete(sub)) {\n          System.err.println(\"!!!! WARNING: best effort to remove \" + sub.getAbsolutePath() + \" FAILED !!!!!\");\n          return false;\n        }\n      }\n    }\n    return f.delete();\n  }\n  \n  public void clearIndex() {\n    assertU(delQ(\"*:*\"));\n  }\n\n  /** Send JSON update commands */\n  public static String updateJ(String json, SolrParams args) throws Exception {\n    SolrCore core = h.getCore();\n    if (args == null) {\n      args = params(\"wt\",\"json\",\"indent\",\"true\");\n    } else {\n      ModifiableSolrParams newArgs = new ModifiableSolrParams(args);\n      if (newArgs.get(\"wt\") == null) newArgs.set(\"wt\",\"json\");\n      if (newArgs.get(\"indent\") == null) newArgs.set(\"indent\",\"true\");\n      args = newArgs;\n    }\n    DirectSolrConnection connection = new DirectSolrConnection(core);\n    SolrRequestHandler handler = core.getRequestHandler(\"/update/json\");\n    if (handler == null) {\n      handler = new JsonUpdateRequestHandler();\n      handler.init(null);\n    }\n    return connection.request(handler, args, json);\n  }\n\n  public static SolrInputDocument sdoc(Object... fieldsAndValues) {\n    SolrInputDocument sd = new SolrInputDocument();\n    for (int i=0; i<fieldsAndValues.length; i+=2) {\n      sd.addField((String)fieldsAndValues[i], fieldsAndValues[i+1]);\n    }\n    return sd;\n  }\n\n  public static List<SolrInputDocument> sdocs(SolrInputDocument... docs) {\n    return Arrays.asList(docs);\n  }\n\n  /** Converts \"test JSON\" and returns standard JSON.\n   *  Currently this only consists of changing unescaped single quotes to double quotes,\n   *  and escaped single quotes to single quotes.\n   *\n   * The primary purpose is to be able to easily embed JSON strings in a JAVA string\n   * with the best readability.\n   *\n   * This transformation is automatically applied to JSON test srings (like assertJQ).\n   */\n  public static String json(String testJSON) {\n    testJSON = nonEscapedSingleQuotePattern.matcher(testJSON).replaceAll(\"\\\"\");\n    testJSON = escapedSingleQuotePattern.matcher(testJSON).replaceAll(\"'\");\n    return testJSON;\n  }\n  private static Pattern nonEscapedSingleQuotePattern = Pattern.compile(\"(?<!\\\\\\\\)\\'\");\n  private static Pattern escapedSingleQuotePattern = Pattern.compile(\"\\\\\\\\\\'\");\n\n\n  /** Creates JSON from a SolrInputDocument.  Doesn't currently handle boosts. */\n  public static String json(SolrInputDocument doc) {\n     CharArr out = new CharArr();\n    try {\n      out.append('{');\n      boolean firstField = true;\n      for (SolrInputField sfield : doc) {\n        if (firstField) firstField=false;\n        else out.append(',');\n        JSONUtil.writeString(sfield.getName(), 0, sfield.getName().length(), out);\n        out.append(':');\n\n        if (sfield.getValueCount() > 1) {\n          out.append('[');\n          boolean firstVal = true;\n          for (Object val : sfield) {\n            if (firstVal) firstVal=false;\n            else out.append(',');\n            out.append(JSONUtil.toJSON(val));\n          }\n          out.append(']');\n        } else {\n          out.append(JSONUtil.toJSON(sfield.getValue()));\n        }\n      }\n      out.append('}');\n    } catch (IOException e) {\n      // should never happen\n    }\n    return out.toString();\n  }\n\n  /** Creates a JSON add command from a SolrInputDocument list.  Doesn't currently handle boosts. */\n  public static String jsonAdd(SolrInputDocument... docs) {\n    CharArr out = new CharArr();\n    try {\n      out.append('[');\n      boolean firstField = true;\n      for (SolrInputDocument doc : docs) {\n        if (firstField) firstField=false;\n        else out.append(',');\n        out.append(json(doc));\n      }\n      out.append(']');\n    } catch (IOException e) {\n      // should never happen\n    }\n    return out.toString();\n  }\n\n    /** Creates a JSON delete command from an id list */\n  public static String jsonDelId(Object... ids) {\n    CharArr out = new CharArr();\n    try {\n      out.append('{');\n      boolean first = true;\n      for (Object id : ids) {\n        if (first) first=false;\n        else out.append(',');\n        out.append(\"\\\"delete\\\":{\\\"id\\\":\");\n        out.append(JSONUtil.toJSON(id));\n        out.append('}');\n      }\n      out.append('}');\n    } catch (IOException e) {\n      // should never happen\n    }\n    return out.toString();\n  }\n\n\n  /** Creates a JSON deleteByQuery command */\n  public static String jsonDelQ(String... queries) {\n    CharArr out = new CharArr();\n    try {\n      out.append('{');\n      boolean first = true;\n      for (Object q : queries) {\n        if (first) first=false;\n        else out.append(',');\n        out.append(\"\\\"delete\\\":{\\\"query\\\":\");\n        out.append(JSONUtil.toJSON(q));\n        out.append('}');\n      }\n      out.append('}');\n    } catch (IOException e) {\n      // should never happen\n    }\n    return out.toString();\n  }\n\n\n  public static Long addAndGetVersion(SolrInputDocument sdoc, SolrParams params) throws Exception {\n    if (params==null || params.get(\"versions\") == null) {\n      ModifiableSolrParams mparams = new ModifiableSolrParams(params);\n      mparams.set(\"versions\",\"true\");\n      params = mparams;\n    }\n    String response = updateJ(jsonAdd(sdoc), params);\n    Map rsp = (Map)ObjectBuilder.fromJSON(response);\n    List lst = (List)rsp.get(\"adds\");\n    if (lst == null || lst.size() == 0) return null;\n    return (Long) lst.get(1);\n  }\n\n  public static Long deleteAndGetVersion(String id, SolrParams params) throws Exception {\n    if (params==null || params.get(\"versions\") == null) {\n      ModifiableSolrParams mparams = new ModifiableSolrParams(params);\n      mparams.set(\"versions\",\"true\");\n      params = mparams;\n    }\n    String response = updateJ(jsonDelId(id), params);\n    Map rsp = (Map)ObjectBuilder.fromJSON(response);\n    List lst = (List)rsp.get(\"deletes\");\n    if (lst == null || lst.size() == 0) return null;\n    return (Long) lst.get(1);\n  }\n\n  public static Long deleteByQueryAndGetVersion(String q, SolrParams params) throws Exception {\n    if (params==null || params.get(\"versions\") == null) {\n      ModifiableSolrParams mparams = new ModifiableSolrParams(params);\n      mparams.set(\"versions\",\"true\");\n      params = mparams;\n    }\n    String response = updateJ(jsonDelQ(q), params);\n    Map rsp = (Map)ObjectBuilder.fromJSON(response);\n    List lst = (List)rsp.get(\"deleteByQuery\");\n    if (lst == null || lst.size() == 0) return null;\n    return (Long) lst.get(1);\n  }\n\n  /////////////////////////////////////////////////////////////////////////////////////\n  //////////////////////////// random document / index creation ///////////////////////\n  /////////////////////////////////////////////////////////////////////////////////////\n  \n  public abstract static class Vals {\n    public abstract Comparable get();\n    public String toJSON(Comparable val) {\n      return JSONUtil.toJSON(val);\n    }\n\n    protected int between(int min, int max) {\n      return min != max ? random().nextInt(max-min+1) + min : min;\n    }\n  }\n\n  public abstract static class IVals extends Vals {\n    public abstract int getInt();\n  }\n\n  public static class IRange extends IVals {\n    final int min;\n    final int max;\n    public IRange(int min, int max) {\n      this.min = min;\n      this.max = max;\n    }\n\n    @Override\n    public int getInt() {\n      return between(min,max);\n    }\n\n    @Override\n    public Comparable get() {\n      return getInt();\n    }\n  }\n\n  public static class IValsPercent extends IVals {\n    final int[] percentAndValue;\n    public IValsPercent(int... percentAndValue) {\n      this.percentAndValue = percentAndValue;\n    }\n\n    @Override\n    public int getInt() {\n      int r = between(0,99);\n      int cumulative = 0;\n      for (int i=0; i<percentAndValue.length; i+=2) {\n        cumulative += percentAndValue[i];\n        if (r < cumulative) {\n          return percentAndValue[i+1];\n        }\n      }\n\n      return percentAndValue[percentAndValue.length-1];\n    }\n\n    @Override\n    public Comparable get() {\n      return getInt();\n    }\n  }\n\n  public static class FVal extends Vals {\n    final float min;\n    final float max;\n    public FVal(float min, float max) {\n      this.min = min;\n      this.max = max;\n    }\n\n    public float getFloat() {\n      if (min >= max) return min;\n      return min + random().nextFloat() *  (max - min);\n    }\n\n    @Override\n    public Comparable get() {\n      return getFloat();\n    }\n  }  \n\n  public static class SVal extends Vals {\n    char start;\n    char end;\n    int minLength;\n    int maxLength;\n\n    public SVal() {\n      this('a','z',1,10);\n    }\n\n    public SVal(char start, char end, int minLength, int maxLength) {\n      this.start = start;\n      this.end = end;\n      this.minLength = minLength;\n      this.maxLength = maxLength;\n    }\n\n    @Override\n    public Comparable get() {\n      char[] arr = new char[between(minLength,maxLength)];\n      for (int i=0; i<arr.length; i++) {\n        arr[i] = (char)between(start, end);\n      }\n      return new String(arr);\n    }\n  }\n\n  public static final IRange ZERO_ONE = new IRange(0,1);\n  public static final IRange ZERO_TWO = new IRange(0,2);\n  public static final IRange ONE_ONE = new IRange(1,1);\n\n  public static class Doc implements Comparable {\n    public Comparable id;\n    public List<Fld> fields;\n    public int order; // the order this document was added to the index\n\n\n    @Override\n    public String toString() {\n      return \"Doc(\"+order+\"):\"+fields.toString();\n    }\n\n    @Override\n    public int hashCode() {\n      return id.hashCode();\n    }\n\n    @Override\n    public boolean equals(Object o) {\n      if (!(o instanceof Doc)) return false;\n      Doc other = (Doc)o;\n      return this==other || id != null && id.equals(other.id);\n    }\n\n    @Override\n    public int compareTo(Object o) {\n      if (!(o instanceof Doc)) return this.getClass().hashCode() - o.getClass().hashCode();\n      Doc other = (Doc)o;\n      return this.id.compareTo(other.id);\n    }\n\n    public List<Comparable> getValues(String field) {\n      for (Fld fld : fields) {\n        if (fld.ftype.fname.equals(field)) return fld.vals;\n      }\n      return null;\n    }\n\n    public Comparable getFirstValue(String field) {\n      List<Comparable> vals = getValues(field);\n      return vals==null || vals.size()==0 ? null : vals.get(0);\n    }\n\n    public Map<String,Object> toObject(IndexSchema schema) {\n      Map<String,Object> result = new HashMap<String,Object>();\n      for (Fld fld : fields) {\n        SchemaField sf = schema.getField(fld.ftype.fname);\n        if (!sf.multiValued()) {\n          result.put(fld.ftype.fname, fld.vals.get(0));\n        } else {\n          result.put(fld.ftype.fname, fld.vals);\n        }\n      }\n      return result;\n    }\n\n  }\n\n  public static class Fld {\n    public FldType ftype;\n    public List<Comparable> vals;\n    @Override\n    public String toString() {\n      return ftype.fname + \"=\" + (vals.size()==1 ? vals.get(0).toString() : vals.toString());\n    }\n  }\n\n  protected class FldType {\n    public String fname;\n    public IVals numValues;\n    public Vals vals;\n\n    public FldType(String fname, Vals vals) {\n      this(fname, ZERO_ONE, vals);\n    }\n\n    public FldType(String fname, IVals numValues, Vals vals) {\n      this.fname = fname;\n      this.numValues = numValues;\n      this.vals = vals;      \n    }\n\n    public Comparable createValue() {\n      return vals.get();\n    }\n\n    public List<Comparable> createValues() {\n      int nVals = numValues.getInt();\n      if (nVals <= 0) return null;\n      List<Comparable> vals = new ArrayList<Comparable>(nVals);\n      for (int i=0; i<nVals; i++)\n        vals.add(createValue());\n      return vals;\n    }\n\n    public Fld createField() {\n      List<Comparable> vals = createValues();\n      if (vals == null) return null;\n\n      Fld fld = new Fld();\n      fld.ftype = this;\n      fld.vals = vals;\n      return fld;          \n    }\n\n  }\n\n\n  public Map<Comparable,Doc> indexDocs(List<FldType> descriptor, Map<Comparable,Doc> model, int nDocs) throws Exception {\n    if (model == null) {\n      model = new LinkedHashMap<Comparable,Doc>();\n    }\n\n    // commit an average of 10 times for large sets, or 10% of the time for small sets\n    int commitOneOutOf = Math.max(nDocs/10, 10);\n\n    for (int i=0; i<nDocs; i++) {\n      Doc doc = createDoc(descriptor);\n      // doc.order = order++;\n      updateJ(toJSON(doc), null);\n      model.put(doc.id, doc);\n\n      // commit 10% of the time\n      if (random().nextInt(commitOneOutOf)==0) {\n        assertU(commit());\n      }\n\n      // duplicate 10% of the docs\n      if (random().nextInt(10)==0) {\n        updateJ(toJSON(doc), null);\n        model.put(doc.id, doc);        \n      }\n    }\n\n    // optimize 10% of the time\n    if (random().nextInt(10)==0) {\n      assertU(optimize());\n    } else {\n      if (random().nextInt(10) == 0) {\n        assertU(commit());\n      } else {\n        assertU(commit(\"softCommit\",\"true\"));\n      }\n    }\n\n    // merging segments no longer selects just adjacent segments hence ids (doc.order) can be shuffled.\n    // we need to look at the index to determine the order.\n    String responseStr = h.query(req(\"q\",\"*:*\", \"fl\",\"id\", \"sort\",\"_docid_ asc\", \"rows\",Integer.toString(model.size()*2), \"wt\",\"json\", \"indent\",\"true\"));\n    Object response = ObjectBuilder.fromJSON(responseStr);\n\n    response = ((Map)response).get(\"response\");\n    response = ((Map)response).get(\"docs\");\n    List<Map> docList = (List<Map>)response;\n    int order = 0;\n    for (Map doc : docList) {\n      Object id = doc.get(\"id\");\n      Doc modelDoc = model.get(id);\n      if (modelDoc == null) continue;  // may be some docs in the index that aren't modeled\n      modelDoc.order = order++;\n    }\n\n    // make sure we updated the order of all docs in the model\n    assertEquals(order, model.size());\n\n    return model;\n  }\n\n  public static Doc createDoc(List<FldType> descriptor) {\n    Doc doc = new Doc();\n    doc.fields = new ArrayList<Fld>();\n    for (FldType ftype : descriptor) {\n      Fld fld = ftype.createField();\n      if (fld != null) {\n        doc.fields.add(fld);\n        if (\"id\".equals(ftype.fname))\n          doc.id = fld.vals.get(0);\n      }\n    }\n    return doc;\n  }\n\n  public static Comparator<Doc> createSort(IndexSchema schema, List<FldType> fieldTypes, String[] out) {\n    StringBuilder sortSpec = new StringBuilder();\n    int nSorts = random().nextInt(4);\n    List<Comparator<Doc>> comparators = new ArrayList<Comparator<Doc>>();\n    for (int i=0; i<nSorts; i++) {\n      if (i>0) sortSpec.append(',');\n\n      int which = random().nextInt(fieldTypes.size()+2);\n      boolean asc = random().nextBoolean();\n      if (which == fieldTypes.size()) {\n        // sort by score\n        sortSpec.append(\"score\").append(asc ? \" asc\" : \" desc\");\n        comparators.add(createComparator(\"score\", asc, false, false, false));\n      } else if (which == fieldTypes.size() + 1) {\n        // sort by docid\n        sortSpec.append(\"_docid_\").append(asc ? \" asc\" : \" desc\");\n        comparators.add(createComparator(\"_docid_\", asc, false, false, false));\n      } else {\n        String field = fieldTypes.get(which).fname;\n        sortSpec.append(field).append(asc ? \" asc\" : \" desc\");\n        SchemaField sf = schema.getField(field);\n        comparators.add(createComparator(field, asc, sf.sortMissingLast(), sf.sortMissingFirst(), !(sf.sortMissingLast()||sf.sortMissingFirst()) ));\n      }\n    }\n\n    out[0] = sortSpec.length() > 0 ? sortSpec.toString() : null;\n\n    if (comparators.size() == 0) {\n      // default sort is by score desc\n      comparators.add(createComparator(\"score\", false, false, false, false));      \n    }\n\n    return createComparator(comparators);\n  }\n\n  public static Comparator<Doc> createComparator(final String field, final boolean asc, final boolean sortMissingLast, final boolean sortMissingFirst, final boolean sortMissingAsZero) {\n    final int mul = asc ? 1 : -1;\n\n    if (field.equals(\"_docid_\")) {\n     return new Comparator<Doc>() {\n      @Override\n      public int compare(Doc o1, Doc o2) {\n        return (o1.order - o2.order) * mul;\n      }\n     };\n    }\n\n    if (field.equals(\"score\")) {\n      return createComparator(\"score_f\", asc, sortMissingLast, sortMissingFirst, sortMissingAsZero);\n    }\n\n    return new Comparator<Doc>() {\n      private Comparable zeroVal(Comparable template) {\n        if (template == null) return null;\n        if (template instanceof String) return null;  // fast-path for string\n        if (template instanceof Integer) return 0;\n        if (template instanceof Long) return (long)0;\n        if (template instanceof Float) return (float)0;\n        if (template instanceof Double) return (double)0;\n        if (template instanceof Short) return (short)0;\n        if (template instanceof Byte) return (byte)0;\n        if (template instanceof Character) return (char)0;\n        return null;\n      }\n\n      @Override\n      public int compare(Doc o1, Doc o2) {\n        Comparable v1 = o1.getFirstValue(field);\n        Comparable v2 = o2.getFirstValue(field);\n\n        v1 = v1 == null ? zeroVal(v2) : v1;\n        v2 = v2 == null ? zeroVal(v1) : v2;\n\n        int c = 0;\n        if (v1 == v2) {\n          c = 0;\n        } else if (v1 == null) {\n          if (sortMissingLast) c = mul;\n          else if (sortMissingFirst) c = -mul;\n          else c = -1;\n        } else if (v2 == null) {\n          if (sortMissingLast) c = -mul;\n          else if (sortMissingFirst) c = mul;\n          else c = 1;\n        } else {\n          c = v1.compareTo(v2);\n        }\n\n        c = c * mul;\n\n        return c;\n      }\n    };\n  }\n\n  public static Comparator<Doc> createComparator(final List<Comparator<Doc>> comparators) {\n    return new Comparator<Doc>() {\n      @Override\n      public int compare(Doc o1, Doc o2) {\n        int c = 0;\n        for (Comparator<Doc> comparator : comparators) {\n          c = comparator.compare(o1, o2);\n          if (c!=0) return c;\n        }\n        return o1.order - o2.order;\n      }\n    };\n  }\n\n\n  public static String toJSON(Doc doc) {\n    CharArr out = new CharArr();\n    try {\n      out.append(\"{\\\"add\\\":{\\\"doc\\\":{\");\n      boolean firstField = true;\n      for (Fld fld : doc.fields) {\n        if (firstField) firstField=false;\n        else out.append(',');\n        JSONUtil.writeString(fld.ftype.fname, 0, fld.ftype.fname.length(), out);\n        out.append(':');\n        if (fld.vals.size() > 1) {\n          out.append('[');\n        }\n        boolean firstVal = true;\n        for (Comparable val : fld.vals) {\n          if (firstVal) firstVal=false;\n          else out.append(',');\n          out.append(JSONUtil.toJSON(val));\n        }\n        if (fld.vals.size() > 1) {\n          out.append(']');\n        }\n      }\n      out.append(\"}}}\");\n    } catch (IOException e) {\n      // should never happen\n    }\n    return out.toString();\n  }\n\n  /** Return a Map from field value to a list of document ids */\n  public Map<Comparable, List<Comparable>> invertField(Map<Comparable, Doc> model, String field) {\n    Map<Comparable, List<Comparable>> value_to_id = new HashMap<Comparable, List<Comparable>>();\n\n    // invert field\n    for (Comparable key : model.keySet()) {\n      Doc doc = model.get(key);\n      List<Comparable> vals = doc.getValues(field);\n      if (vals == null) continue;\n      for (Comparable val : vals) {\n        List<Comparable> ids = value_to_id.get(val);\n        if (ids == null) {\n          ids = new ArrayList<Comparable>(2);\n          value_to_id.put(val, ids);\n        }\n        ids.add(key);\n      }\n    }\n\n    return value_to_id;\n  }\n\n\n  /** Gets a resource from the context classloader as {@link File}. This method should only be used,\n   * if a real file is needed. To get a stream, code should prefer\n   * {@link Class#getResourceAsStream} using {@code this.getClass()}.\n   */\n  public static File getFile(String name) {\n    try {\n      File file = new File(name);\n      if (!file.exists()) {\n        file = new File(Thread.currentThread().getContextClassLoader().getResource(name).toURI());\n      }\n      return file;\n    } catch (Exception e) {\n      /* more friendly than NPE */\n      throw new RuntimeException(\"Cannot find resource: \" + new File(name).getAbsolutePath());\n    }\n  }\n  \n  public static String TEST_HOME() {\n    return getFile(\"solr/collection1\").getParent();\n  }\n\n  public static Throwable getRootCause(Throwable t) {\n    Throwable result = t;\n    for (Throwable cause = t; null != cause; cause = cause.getCause()) {\n      result = cause;\n    }\n    return result;\n  }\n\n  public static void assertXmlFile(final File file, String... xpath)\n      throws IOException, SAXException {\n\n    try {\n      String xml = FileUtils.readFileToString(file, \"UTF-8\");\n      String results = TestHarness.validateXPath(xml, xpath);\n      if (null != results) {\n        String msg = \"File XPath failure: file=\" + file.getPath() + \" xpath=\"\n            + results + \"\\n\\nxml was: \" + xml;\n        fail(msg);\n      }\n    } catch (XPathExpressionException e2) {\n      throw new RuntimeException(\"XPath is invalid\", e2);\n    }\n  }\n                                                         \n  /**\n   * Fails if the number of documents in the given SolrDocumentList differs\n   * from the given number of expected values, or if any of the values in the\n   * given field don't match the expected values in the same order.\n   */\n  public static void assertFieldValues(SolrDocumentList documents, String fieldName, Object... expectedValues) {\n    if (documents.size() != expectedValues.length) {\n      fail(\"Number of documents (\" + documents.size()\n          + \") is different from number of expected values (\" + expectedValues.length);\n    }\n    for (int docNum = 1 ; docNum <= documents.size() ; ++docNum) {\n      SolrDocument doc = documents.get(docNum - 1);\n      Object expected = expectedValues[docNum - 1];\n      Object actual = doc.get(fieldName);\n      if (null != expected && null != actual) {\n        if ( ! expected.equals(actual)) {\n          fail( \"Unexpected \" + fieldName + \" field value in document #\" + docNum\n              + \": expected=[\" + expected + \"], actual=[\" + actual + \"]\");\n        }\n      }\n    }\n  }\n\n  public static void copyMinConf(File dstRoot) throws IOException {\n    copyMinConf(dstRoot, null);\n  }\n\n  // Creates a minimal conf dir. Optionally adding in a core.properties file from the string passed in\n  // the string to write to the core.properties file may be null in which case nothing is done with it.\n  // propertiesContent may be an empty string, which will actually work.\n  public static void copyMinConf(File dstRoot, String propertiesContent) throws IOException {\n\n    File subHome = new File(dstRoot, \"conf\");\n    if (! dstRoot.exists()) {\n      assertTrue(\"Failed to make subdirectory \", dstRoot.mkdirs());\n    }\n    if (propertiesContent != null) {\n      FileUtils.writeStringToFile(new File(dstRoot, \"core.properties\"), propertiesContent, Charsets.UTF_8.toString());\n    }\n    String top = SolrTestCaseJ4.TEST_HOME() + \"/collection1/conf\";\n    FileUtils.copyFile(new File(top, \"schema-tiny.xml\"), new File(subHome, \"schema.xml\"));\n    FileUtils.copyFile(new File(top, \"solrconfig-minimal.xml\"), new File(subHome, \"solrconfig.xml\"));\n    FileUtils.copyFile(new File(top, \"solrconfig.snippet.randomindexconfig.xml\"), new File(subHome, \"solrconfig.snippet.randomindexconfig.xml\"));\n  }\n\n  // Creates minimal full setup, including the old solr.xml file that used to be hard coded in ConfigSolrXmlOld\n  // TODO: remove for 5.0\n  public static void copyMinFullSetup(File dstRoot) throws IOException {\n    if (! dstRoot.exists()) {\n      assertTrue(\"Failed to make subdirectory \", dstRoot.mkdirs());\n    }\n    File xmlF = new File(SolrTestCaseJ4.TEST_HOME(), \"solr.xml\");\n    FileUtils.copyFile(xmlF, new File(dstRoot, \"solr.xml\"));\n    copyMinConf(dstRoot);\n  }\n\n  // Creates a consistent configuration, _including_ solr.xml at dstRoot. Creates collection1/conf and copies\n  // the stock files in there. Seems to be indicated for some tests when we remove the default, hard-coded\n  // solr.xml from being automatically synthesized from SolrConfigXmlOld.DEFAULT_SOLR_XML.\n  public static void copySolrHomeToTemp(File dstRoot, String collection) throws IOException {\n    copySolrHomeToTemp(dstRoot, collection, false);\n  }\n  public static void copySolrHomeToTemp(File dstRoot, String collection, boolean newStyle) throws IOException {\n    if (!dstRoot.exists()) {\n      assertTrue(\"Failed to make subdirectory \", dstRoot.mkdirs());\n    }\n\n    if (newStyle) {\n      FileUtils.copyFile(new File(SolrTestCaseJ4.TEST_HOME(), \"solr-no-core.xml\"), new File(dstRoot, \"solr.xml\"));\n    } else {\n      FileUtils.copyFile(new File(SolrTestCaseJ4.TEST_HOME(), \"solr.xml\"), new File(dstRoot, \"solr.xml\"));\n    }\n\n    File subHome = new File(dstRoot, collection + File.separator + \"conf\");\n    String top = SolrTestCaseJ4.TEST_HOME() + \"/collection1/conf\";\n    FileUtils.copyFile(new File(top, \"currency.xml\"), new File(subHome, \"currency.xml\"));\n    FileUtils.copyFile(new File(top, \"mapping-ISOLatin1Accent.txt\"), new File(subHome, \"mapping-ISOLatin1Accent.txt\"));\n    FileUtils.copyFile(new File(top, \"old_synonyms.txt\"), new File(subHome, \"old_synonyms.txt\"));\n    FileUtils.copyFile(new File(top, \"open-exchange-rates.json\"), new File(subHome, \"open-exchange-rates.json\"));\n    FileUtils.copyFile(new File(top, \"protwords.txt\"), new File(subHome, \"protwords.txt\"));\n    FileUtils.copyFile(new File(top, \"schema.xml\"), new File(subHome, \"schema.xml\"));\n    FileUtils.copyFile(new File(top, \"solrconfig.snippet.randomindexconfig.xml\"), new File(subHome, \"solrconfig.snippet.randomindexconfig.xml\"));\n    FileUtils.copyFile(new File(top, \"solrconfig.xml\"), new File(subHome, \"solrconfig.xml\"));\n    FileUtils.copyFile(new File(top, \"stopwords.txt\"), new File(subHome, \"stopwords.txt\"));\n    FileUtils.copyFile(new File(top, \"synonyms.txt\"), new File(subHome, \"synonyms.txt\"));\n  }\n\n  public static CoreDescriptorBuilder buildCoreDescriptor(CoreContainer container, String name, String instancedir) {\n    return new CoreDescriptorBuilder(container, name, instancedir);\n  }\n\n  public static class CoreDescriptorBuilder {\n\n    final String name;\n    final String instanceDir;\n    final CoreContainer container;\n    final Properties properties = new Properties();\n\n    public CoreDescriptorBuilder(CoreContainer container, String name, String instancedir) {\n      this.name = name;\n      this.instanceDir = instancedir;\n      this.container = container;\n    }\n\n    public CoreDescriptorBuilder withSchema(String schema) {\n      properties.setProperty(CoreDescriptor.CORE_SCHEMA, schema);\n      return this;\n    }\n\n    public CoreDescriptorBuilder withConfig(String config) {\n      properties.setProperty(CoreDescriptor.CORE_CONFIG, config);\n      return this;\n    }\n\n    public CoreDescriptorBuilder withDataDir(String datadir) {\n      properties.setProperty(CoreDescriptor.CORE_DATADIR, datadir);\n      return this;\n    }\n\n    public CoreDescriptor build() {\n      return new CoreDescriptor(container, name, instanceDir, properties);\n    }\n\n    public CoreDescriptorBuilder isTransient(boolean isTransient) {\n      properties.setProperty(CoreDescriptor.CORE_TRANSIENT, Boolean.toString(isTransient));\n      return this;\n    }\n\n    public CoreDescriptorBuilder loadOnStartup(boolean loadOnStartup) {\n      properties.setProperty(CoreDescriptor.CORE_LOADONSTARTUP, Boolean.toString(loadOnStartup));\n      return this;\n    }\n  }\n\n}\n",
        "methodName": "recurseDelete",
        "exampleID": 28,
        "dataset": "spotbugs",
        "filepath": "/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java",
        "line": "939",
        "source": "?",
        "sourceLine": "939",
        "qualifier": "Possible null pointer dereference of the $$value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 29
            }
        ],
        "line_number": "939"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java",
        "rawCode": "package org.apache.solr.cloud;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport org.apache.solr.client.solrj.SolrResponse;\nimport org.apache.solr.client.solrj.SolrServerException;\nimport org.apache.solr.client.solrj.impl.HttpSolrServer;\nimport org.apache.solr.client.solrj.request.AbstractUpdateRequest;\nimport org.apache.solr.client.solrj.request.CoreAdminRequest;\nimport org.apache.solr.client.solrj.request.UpdateRequest;\nimport org.apache.solr.client.solrj.response.UpdateResponse;\nimport org.apache.solr.cloud.DistributedQueue.QueueEvent;\nimport org.apache.solr.cloud.Overseer.LeaderStatus;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.SolrException.ErrorCode;\nimport org.apache.solr.common.cloud.Aliases;\nimport org.apache.solr.common.cloud.ClosableThread;\nimport org.apache.solr.common.cloud.ClusterState;\nimport org.apache.solr.common.cloud.CompositeIdRouter;\nimport org.apache.solr.common.cloud.DocCollection;\nimport org.apache.solr.common.cloud.DocRouter;\nimport org.apache.solr.common.cloud.ImplicitDocRouter;\nimport org.apache.solr.common.cloud.PlainIdRouter;\nimport org.apache.solr.common.cloud.Replica;\nimport org.apache.solr.common.cloud.RoutingRule;\nimport org.apache.solr.common.cloud.Slice;\nimport org.apache.solr.common.cloud.ZkCoreNodeProps;\nimport org.apache.solr.common.cloud.ZkNodeProps;\nimport org.apache.solr.common.cloud.ZkStateReader;\nimport org.apache.solr.common.cloud.ZooKeeperException;\nimport org.apache.solr.common.params.CoreAdminParams;\nimport org.apache.solr.common.params.CoreAdminParams.CoreAdminAction;\nimport org.apache.solr.common.params.MapSolrParams;\nimport org.apache.solr.common.params.ModifiableSolrParams;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.common.util.SimpleOrderedMap;\nimport org.apache.solr.common.util.StrUtils;\nimport org.apache.solr.handler.component.ShardHandler;\nimport org.apache.solr.handler.component.ShardRequest;\nimport org.apache.solr.handler.component.ShardResponse;\nimport org.apache.solr.update.SolrIndexSplitter;\nimport org.apache.zookeeper.KeeperException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\n\nimport static org.apache.solr.cloud.Assign.Node;\nimport static org.apache.solr.cloud.Assign.getNodesForNewShard;\nimport static org.apache.solr.common.cloud.ZkStateReader.COLLECTION_PROP;\nimport static org.apache.solr.common.cloud.ZkStateReader.REPLICA_PROP;\nimport static org.apache.solr.common.cloud.ZkStateReader.SHARD_ID_PROP;\n\n\npublic class OverseerCollectionProcessor implements Runnable, ClosableThread {\n  \n  public static final String NUM_SLICES = \"numShards\";\n  \n  public static final String REPLICATION_FACTOR = \"replicationFactor\";\n  \n  public static final String MAX_SHARDS_PER_NODE = \"maxShardsPerNode\";\n  \n  public static final String CREATE_NODE_SET = \"createNodeSet\";\n  \n  public static final String DELETECOLLECTION = \"deletecollection\";\n\n  public static final String CREATECOLLECTION = \"createcollection\";\n\n  public static final String RELOADCOLLECTION = \"reloadcollection\";\n  \n  public static final String CREATEALIAS = \"createalias\";\n  \n  public static final String DELETEALIAS = \"deletealias\";\n  \n  public static final String SPLITSHARD = \"splitshard\";\n\n  public static final String DELETESHARD = \"deleteshard\";\n\n  public static final String ROUTER = \"router\";\n\n  public static final String SHARDS_PROP = \"shards\";\n\n  public static final String CREATESHARD = \"createshard\";\n\n  public static final String DELETEREPLICA = \"deletereplica\";\n\n  public static final String MIGRATE = \"migrate\";\n\n  public static final String COLL_CONF = \"collection.configName\";\n\n  public static final String COLL_PROP_PREFIX = \"property.\";\n\n  public static final Map<String,Object> COLL_PROPS = ZkNodeProps.makeMap(\n      ROUTER, DocRouter.DEFAULT_NAME,\n      REPLICATION_FACTOR, \"1\",\n      MAX_SHARDS_PER_NODE, \"1\",\n      \"external\",null );\n\n\n  // TODO: use from Overseer?\n  private static final String QUEUE_OPERATION = \"operation\";\n  \n  private static Logger log = LoggerFactory\n      .getLogger(OverseerCollectionProcessor.class);\n  \n  private DistributedQueue workQueue;\n  \n  private String myId;\n\n  private ShardHandler shardHandler;\n\n  private String adminPath;\n\n  private ZkStateReader zkStateReader;\n\n  private boolean isClosed;\n  \n  public OverseerCollectionProcessor(ZkStateReader zkStateReader, String myId, ShardHandler shardHandler, String adminPath) {\n    this(zkStateReader, myId, shardHandler, adminPath, Overseer.getCollectionQueue(zkStateReader.getZkClient()));\n  }\n\n  protected OverseerCollectionProcessor(ZkStateReader zkStateReader, String myId, ShardHandler shardHandler, String adminPath, DistributedQueue workQueue) {\n    this.zkStateReader = zkStateReader;\n    this.myId = myId;\n    this.shardHandler = shardHandler;\n    this.adminPath = adminPath;\n    this.workQueue = workQueue;\n  }\n  \n  @Override\n  public void run() {\n       log.info(\"Process current queue of collection creations\");\n       LeaderStatus isLeader = amILeader();\n       while (isLeader == LeaderStatus.DONT_KNOW) {\n         log.debug(\"am_i_leader unclear {}\", isLeader);\n         isLeader = amILeader();  // not a no, not a yes, try ask again\n       }\n       while (!this.isClosed) {\n         try {\n           isLeader = amILeader();\n           if (LeaderStatus.NO == isLeader) {\n             break;\n           }\n           else if (LeaderStatus.YES != isLeader) {\n             log.debug(\"am_i_leader unclear {}\", isLeader);                  \n             continue; // not a no, not a yes, try asking again\n           }\n           \n           QueueEvent head = workQueue.peek(true);\n           final ZkNodeProps message = ZkNodeProps.load(head.getBytes());\n           log.info(\"Overseer Collection Processor: Get the message id:\" + head.getId() + \" message:\" + message.toString());\n           final String operation = message.getStr(QUEUE_OPERATION);\n           SolrResponse response = processMessage(message, operation);\n           head.setBytes(SolrResponse.serializable(response));\n           workQueue.remove(head);\n          log.info(\"Overseer Collection Processor: Message id:\" + head.getId() + \" complete, response:\"+ response.getResponse().toString());\n        } catch (KeeperException e) {\n          if (e.code() == KeeperException.Code.SESSIONEXPIRED\n              || e.code() == KeeperException.Code.CONNECTIONLOSS) {\n             log.warn(\"Overseer cannot talk to ZK\");\n             return;\n           }\n           SolrException.log(log, \"\", e);\n           throw new ZooKeeperException(\n               SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n         } catch (InterruptedException e) {\n           Thread.currentThread().interrupt();\n           return;\n         } catch (Throwable e) {\n           SolrException.log(log, \"\", e);\n         }\n       }\n  }\n  \n  public void close() {\n    isClosed = true;\n  }\n  \n  protected LeaderStatus amILeader() {\n    try {\n      ZkNodeProps props = ZkNodeProps.load(zkStateReader.getZkClient().getData(\n          \"/overseer_elect/leader\", null, null, true));\n      if (myId.equals(props.getStr(\"id\"))) {\n        return LeaderStatus.YES;\n      }\n    } catch (KeeperException e) {\n      if (e.code() == KeeperException.Code.CONNECTIONLOSS) {\n        log.error(\"\", e);\n        return LeaderStatus.DONT_KNOW;\n      } else if (e.code() == KeeperException.Code.SESSIONEXPIRED) {\n        log.info(\"\", e);\n      } else {\n        log.warn(\"\", e);\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    log.info(\"According to ZK I (id=\" + myId + \") am no longer a leader.\");\n    return LeaderStatus.NO;\n  }\n  \n  \n  protected SolrResponse processMessage(ZkNodeProps message, String operation) {\n    log.warn(\"OverseerCollectionProcessor.processMessage : \"+ operation + \" , \"+ message.toString());\n\n    NamedList results = new NamedList();\n    try {\n      if (CREATECOLLECTION.equals(operation)) {\n        createCollection(zkStateReader.getClusterState(), message, results);\n      } else if (DELETECOLLECTION.equals(operation)) {\n        deleteCollection(message, results);\n      } else if (RELOADCOLLECTION.equals(operation)) {\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.RELOAD.toString());\n        collectionCmd(zkStateReader.getClusterState(), message, params, results, ZkStateReader.ACTIVE);\n      } else if (CREATEALIAS.equals(operation)) {\n        createAlias(zkStateReader.getAliases(), message);\n      } else if (DELETEALIAS.equals(operation)) {\n        deleteAlias(zkStateReader.getAliases(), message);\n      } else if (SPLITSHARD.equals(operation))  {\n        splitShard(zkStateReader.getClusterState(), message, results);\n      } else if (CREATESHARD.equals(operation))  {\n        createShard(zkStateReader.getClusterState(), message, results);\n      } else if (DELETESHARD.equals(operation)) {\n        deleteShard(zkStateReader.getClusterState(), message, results);\n      } else if (DELETEREPLICA.equals(operation)) {\n        deleteReplica(zkStateReader.getClusterState(), message, results);\n      } else if (MIGRATE.equals(operation)) {\n        migrate(zkStateReader.getClusterState(), message, results);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"Unknown operation:\"\n            + operation);\n      }\n\n    } catch (Throwable t) {\n      SolrException.log(log, \"Collection \" + operation + \" of \" + operation\n          + \" failed\", t);\n      results.add(\"Operation \" + operation + \" caused exception:\", t);\n      SimpleOrderedMap nl = new SimpleOrderedMap();\n      nl.add(\"msg\", t.getMessage());\n      nl.add(\"rspCode\", t instanceof SolrException ? ((SolrException)t).code() : -1);\n      results.add(\"exception\", nl);\n    } \n    \n    return new OverseerSolrResponse(results);\n  }\n\n  private void deleteReplica(ClusterState clusterState, ZkNodeProps message, NamedList results) throws KeeperException, InterruptedException {\n    checkRequired(message, COLLECTION_PROP, SHARD_ID_PROP,REPLICA_PROP);\n    String collectionName = message.getStr(COLLECTION_PROP);\n    String shard = message.getStr(SHARD_ID_PROP);\n    String replicaName = message.getStr(REPLICA_PROP);\n    DocCollection coll = clusterState.getCollection(collectionName);\n    Slice slice = coll.getSlice(shard);\n    if(slice==null){\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"Invalid shard name : \"+shard+\" in collection : \"+ collectionName);\n    }\n    Replica replica = slice.getReplica(replicaName);\n    if(replica == null){\n      ArrayList<String> l = new ArrayList<String>();\n      for (Replica r : slice.getReplicas()) l.add(r.getName());\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"Invalid replica : \" + replicaName + \" in shard/collection : \"\n          + shard + \"/\"+ collectionName + \" available replicas are \"+ StrUtils.join(l,','));\n    }\n\n    String baseUrl = replica.getStr(ZkStateReader.BASE_URL_PROP);\n    String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n    \n    // assume the core exists and try to unload it\n    Map m = ZkNodeProps.makeMap(\"qt\", adminPath, CoreAdminParams.ACTION,\n        CoreAdminAction.UNLOAD.toString(), CoreAdminParams.CORE, core);\n    \n    ShardRequest sreq = new ShardRequest();\n    sreq.purpose = 1;\n    if (baseUrl.startsWith(\"http://\")) baseUrl = baseUrl.substring(7);\n    sreq.shards = new String[] {baseUrl};\n    sreq.actualShards = sreq.shards;\n    sreq.params = new ModifiableSolrParams(new MapSolrParams(m));\n    try {\n      shardHandler.submit(sreq, baseUrl, sreq.params);\n    } catch (Exception e) {\n      log.warn(\"Exception trying to unload core \" + sreq, e);\n    }\n    \n    collectShardResponses(!Slice.ACTIVE.equals(replica.getStr(Slice.STATE)) ? new NamedList() : results, false, null);\n    \n    if (waitForCoreNodeGone(collectionName, shard, replicaName, 5000)) return;//check if the core unload removed the corenode zk enry\n    deleteCoreNode(collectionName, replicaName, replica, core); // try and ensure core info is removed from clusterstate\n    if(waitForCoreNodeGone(collectionName, shard, replicaName, 30000)) return;\n\n    throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not  remove replica : \" + collectionName + \"/\" + shard+\"/\" + replicaName);\n  }\n\n  private boolean waitForCoreNodeGone(String collectionName, String shard, String replicaName, int timeoutms) throws InterruptedException {\n    long waitUntil = System.currentTimeMillis() + timeoutms;\n    boolean deleted = false;\n    while (System.currentTimeMillis() < waitUntil) {\n      Thread.sleep(100);\n      deleted = zkStateReader.getClusterState().getCollection(collectionName).getSlice(shard).getReplica(replicaName) == null;\n      if (deleted) break;\n    }\n    return deleted;\n  }\n\n  private void deleteCoreNode(String collectionName, String replicaName, Replica replica, String core) throws KeeperException, InterruptedException {\n    ZkNodeProps m = new ZkNodeProps(\n        Overseer.QUEUE_OPERATION, Overseer.DELETECORE,\n        ZkStateReader.CORE_NAME_PROP, core,\n        ZkStateReader.NODE_NAME_PROP, replica.getStr(ZkStateReader.NODE_NAME_PROP),\n        ZkStateReader.COLLECTION_PROP, collectionName,\n        ZkStateReader.CORE_NODE_NAME_PROP, replicaName);\n    Overseer.getInQueue(zkStateReader.getZkClient()).offer(ZkStateReader.toJSON(m));\n  }\n\n  private void checkRequired(ZkNodeProps message, String... props) {\n    for (String prop : props) {\n      if(message.get(prop) == null){\n        throw new SolrException(ErrorCode.BAD_REQUEST, StrUtils.join(Arrays.asList(props),',') +\" are required params\" );\n      }\n    }\n\n  }\n\n  private void deleteCollection(ZkNodeProps message, NamedList results)\n      throws KeeperException, InterruptedException {\n    String collection = message.getStr(\"name\");\n    try {\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n      params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);\n      params.set(CoreAdminParams.DELETE_DATA_DIR, true);\n      collectionCmd(zkStateReader.getClusterState(), message, params, results,\n          null);\n      \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION,\n          Overseer.REMOVECOLLECTION, \"name\", collection);\n      Overseer.getInQueue(zkStateReader.getZkClient()).offer(\n          ZkStateReader.toJSON(m));\n      \n      // wait for a while until we don't see the collection\n      long now = System.currentTimeMillis();\n      long timeout = now + 30000;\n      boolean removed = false;\n      while (System.currentTimeMillis() < timeout) {\n        Thread.sleep(100);\n        removed = !zkStateReader.getClusterState().hasCollection(message.getStr(collection));\n        if (removed) {\n          Thread.sleep(100); // just a bit of time so it's more likely other\n                             // readers see on return\n          break;\n        }\n      }\n      if (!removed) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Could not fully remove collection: \" + message.getStr(\"name\"));\n      }\n      \n    } finally {\n      \n      try {\n        if (zkStateReader.getZkClient().exists(\n            ZkStateReader.COLLECTIONS_ZKNODE + \"/\" + collection, true)) {\n          zkStateReader.getZkClient().clean(\n              ZkStateReader.COLLECTIONS_ZKNODE + \"/\" + collection);\n        }\n      } catch (InterruptedException e) {\n        SolrException.log(log, \"Cleaning up collection in zk was interrupted:\"\n            + collection, e);\n        Thread.currentThread().interrupt();\n      } catch (KeeperException e) {\n        SolrException.log(log, \"Problem cleaning up collection in zk:\"\n            + collection, e);\n      }\n    }\n  }\n\n  private void createAlias(Aliases aliases, ZkNodeProps message) {\n    String aliasName = message.getStr(\"name\");\n    String collections = message.getStr(\"collections\");\n    \n    Map<String,Map<String,String>> newAliasesMap = new HashMap<String,Map<String,String>>();\n    Map<String,String> newCollectionAliasesMap = new HashMap<String,String>();\n    Map<String,String> prevColAliases = aliases.getCollectionAliasMap();\n    if (prevColAliases != null) {\n      newCollectionAliasesMap.putAll(prevColAliases);\n    }\n    newCollectionAliasesMap.put(aliasName, collections);\n    newAliasesMap.put(\"collection\", newCollectionAliasesMap);\n    Aliases newAliases = new Aliases(newAliasesMap);\n    byte[] jsonBytes = null;\n    if (newAliases.collectionAliasSize() > 0) { // only sub map right now\n      jsonBytes  = ZkStateReader.toJSON(newAliases.getAliasMap());\n    }\n    try {\n      zkStateReader.getZkClient().setData(ZkStateReader.ALIASES,\n          jsonBytes, true);\n      \n      checkForAlias(aliasName, collections);\n      // some fudge for other nodes\n      Thread.sleep(100);\n    } catch (KeeperException e) {\n      log.error(\"\", e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    } catch (InterruptedException e) {\n      log.warn(\"\", e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n\n  }\n  \n  private void checkForAlias(String name, String value) {\n\n    long now = System.currentTimeMillis();\n    long timeout = now + 30000;\n    boolean success = false;\n    Aliases aliases = null;\n    while (System.currentTimeMillis() < timeout) {\n      aliases = zkStateReader.getAliases();\n      String collections = aliases.getCollectionAlias(name);\n      if (collections != null && collections.equals(value)) {\n        success = true;\n        break;\n      }\n    }\n    if (!success) {\n      log.warn(\"Timeout waiting to be notified of Alias change...\");\n    }\n  }\n  \n  private void checkForAliasAbsence(String name) {\n\n    long now = System.currentTimeMillis();\n    long timeout = now + 30000;\n    boolean success = false;\n    Aliases aliases = null;\n    while (System.currentTimeMillis() < timeout) {\n      aliases = zkStateReader.getAliases();\n      String collections = aliases.getCollectionAlias(name);\n      if (collections == null) {\n        success = true;\n        break;\n      }\n    }\n    if (!success) {\n      log.warn(\"Timeout waiting to be notified of Alias change...\");\n    }\n  }\n\n  private void deleteAlias(Aliases aliases, ZkNodeProps message) {\n    String aliasName = message.getStr(\"name\");\n\n    Map<String,Map<String,String>> newAliasesMap = new HashMap<String,Map<String,String>>();\n    Map<String,String> newCollectionAliasesMap = new HashMap<String,String>();\n    newCollectionAliasesMap.putAll(aliases.getCollectionAliasMap());\n    newCollectionAliasesMap.remove(aliasName);\n    newAliasesMap.put(\"collection\", newCollectionAliasesMap);\n    Aliases newAliases = new Aliases(newAliasesMap);\n    byte[] jsonBytes = null;\n    if (newAliases.collectionAliasSize() > 0) { // only sub map right now\n      jsonBytes  = ZkStateReader.toJSON(newAliases.getAliasMap());\n    }\n    try {\n      zkStateReader.getZkClient().setData(ZkStateReader.ALIASES,\n          jsonBytes, true);\n      checkForAliasAbsence(aliasName);\n      // some fudge for other nodes\n      Thread.sleep(100);\n    } catch (KeeperException e) {\n      log.error(\"\", e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    } catch (InterruptedException e) {\n      log.warn(\"\", e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n  }\n\n  private boolean createShard(ClusterState clusterState, ZkNodeProps message, NamedList results) throws KeeperException, InterruptedException {\n    log.info(\"Create shard invoked: {}\", message);\n    String collectionName = message.getStr(COLLECTION_PROP);\n    String shard = message.getStr(SHARD_ID_PROP);\n    if(collectionName == null || shard ==null)\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"'collection' and 'shard' are required parameters\" );\n    int numSlices = 1;\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    int maxShardsPerNode = collection.getInt(MAX_SHARDS_PER_NODE, 1);\n    int repFactor = message.getInt(REPLICATION_FACTOR, collection.getInt(REPLICATION_FACTOR, 1));\n    String createNodeSetStr = message.getStr(CREATE_NODE_SET);\n\n    ArrayList<Node> sortedNodeList = getNodesForNewShard(clusterState, collectionName, numSlices, maxShardsPerNode, repFactor, createNodeSetStr);\n\n    Overseer.getInQueue(zkStateReader.getZkClient()).offer(ZkStateReader.toJSON(message));\n    // wait for a while until we see the shard\n    long waitUntil = System.currentTimeMillis() + 30000;\n    boolean created = false;\n    while (System.currentTimeMillis() < waitUntil) {\n      Thread.sleep(100);\n      created = zkStateReader.getClusterState().getCollection(collectionName).getSlice(shard) != null;\n      if (created) break;\n    }\n    if (!created)\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not fully create shard: \" + message.getStr(\"name\"));\n\n\n    String configName = message.getStr(COLL_CONF);\n    String sliceName = shard;\n    for (int j = 1; j <= repFactor; j++) {\n      String nodeName = sortedNodeList.get(((j - 1)) % sortedNodeList.size()).nodeName;\n      String shardName = collectionName + \"_\" + sliceName + \"_replica\" + j;\n      log.info(\"Creating shard \" + shardName + \" as part of slice \"\n          + sliceName + \" of collection \" + collectionName + \" on \"\n          + nodeName);\n\n      // Need to create new params for each request\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n      params.set(CoreAdminParams.NAME, shardName);\n      params.set(COLL_CONF, configName);\n      params.set(CoreAdminParams.COLLECTION, collectionName);\n      params.set(CoreAdminParams.SHARD, sliceName);\n      params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n      addPropertyParams(message, params);\n\n      ShardRequest sreq = new ShardRequest();\n      params.set(\"qt\", adminPath);\n      sreq.purpose = 1;\n      String replica = zkStateReader.getZkClient()\n          .getBaseUrlForNodeName(nodeName);\n      if (replica.startsWith(\"http://\")) replica = replica.substring(7);\n      sreq.shards = new String[]{replica};\n      sreq.actualShards = sreq.shards;\n      sreq.params = params;\n\n      shardHandler.submit(sreq, replica, sreq.params);\n\n    }\n\n    ShardResponse srsp;\n    do {\n      srsp = shardHandler.takeCompletedOrError();\n      if (srsp != null) {\n        processResponse(results, srsp);\n      }\n    } while (srsp != null);\n\n    log.info(\"Finished create command on all shards for collection: \"\n        + collectionName);\n\n    return true;\n  }\n\n\n  private boolean splitShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Split shard invoked\");\n    String collectionName = message.getStr(\"collection\");\n    String slice = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    String splitKey = message.getStr(\"split.key\");\n\n    DocCollection collection = clusterState.getCollection(collectionName);\n    DocRouter router = collection.getRouter() != null ? collection.getRouter() : DocRouter.DEFAULT;\n\n    Slice parentSlice = null;\n\n    if (slice == null)  {\n      if (router instanceof CompositeIdRouter) {\n        Collection<Slice> searchSlices = router.getSearchSlicesSingle(splitKey, new ModifiableSolrParams(), collection);\n        if (searchSlices.isEmpty()) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \"Unable to find an active shard for split.key: \" + splitKey);\n        }\n        if (searchSlices.size() > 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Splitting a split.key: \" + splitKey + \" which spans multiple shards is not supported\");\n        }\n        parentSlice = searchSlices.iterator().next();\n        slice = parentSlice.getName();\n        log.info(\"Split by route.key: {}, parent shard is: {} \", splitKey, slice);\n      } else  {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"Split by route key can only be used with CompositeIdRouter or subclass. Found router: \" + router.getClass().getName());\n      }\n    } else  {\n      parentSlice = clusterState.getSlice(collectionName, slice);\n    }\n\n    if (parentSlice == null) {\n      if(clusterState.hasCollection(collectionName)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"No collection with the specified name exists: \" + collectionName);\n      }      \n    }\n    \n    // find the leader for the shard\n    Replica parentShardLeader = clusterState.getLeader(collectionName, slice);\n    DocRouter.Range range = parentSlice.getRange();\n    if (range == null) {\n      range = new PlainIdRouter().fullRange();\n    }\n\n    List<DocRouter.Range> subRanges = null;\n    String rangesStr = message.getStr(CoreAdminParams.RANGES);\n    if (rangesStr != null)  {\n      String[] ranges = rangesStr.split(\",\");\n      if (ranges.length == 0 || ranges.length == 1) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"There must be at least two ranges specified to split a shard\");\n      } else  {\n        subRanges = new ArrayList<DocRouter.Range>(ranges.length);\n        for (int i = 0; i < ranges.length; i++) {\n          String r = ranges[i];\n          try {\n            subRanges.add(DocRouter.DEFAULT.fromString(r));\n          } catch (Exception e) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Exception in parsing hexadecimal hash range: \" + r, e);\n          }\n          if (!subRanges.get(i).isSubsetOf(range)) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash range: \" + r + \" is not a subset of parent shard's range: \" + range.toString());\n          }\n        }\n        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order\n        Collections.sort(temp);\n        if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"Specified hash ranges: \" + rangesStr + \" do not cover the entire range of parent shard: \" + range);\n        }\n        for (int i = 1; i < temp.size(); i++) {\n          if (temp.get(i - 1).max + 1 != temp.get(i).min) {\n            throw new SolrException(ErrorCode.BAD_REQUEST,\n                \"Specified hash ranges: \" + rangesStr + \" either overlap with each other or \" +\n                    \"do not cover the entire range of parent shard: \" + range);\n          }\n        }\n      }\n    } else if (splitKey != null)  {\n      if (router instanceof CompositeIdRouter) {\n        CompositeIdRouter compositeIdRouter = (CompositeIdRouter) router;\n        subRanges = compositeIdRouter.partitionRangeByKey(splitKey, range);\n        if (subRanges.size() == 1)  {\n          throw new SolrException(ErrorCode.BAD_REQUEST,\n              \"The split.key: \" + splitKey + \" has a hash range that is exactly equal to hash range of shard: \" + slice);\n        }\n        for (DocRouter.Range subRange : subRanges) {\n          if (subRange.min == subRange.max) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"The split.key: \" + splitKey + \" must be a compositeId\");\n          }\n        }\n        log.info(\"Partitioning parent shard \" + slice + \" range: \" + parentSlice.getRange() + \" yields: \" + subRanges);\n        rangesStr = \"\";\n        for (int i = 0; i < subRanges.size(); i++) {\n          DocRouter.Range subRange = subRanges.get(i);\n          rangesStr += subRange.toString();\n          if (i < subRanges.size() - 1)\n            rangesStr += ',';\n        }\n      }\n    } else  {\n      // todo: fixed to two partitions?\n      subRanges = router.partitionRange(2, range);\n    }\n\n    try {\n      List<String> subSlices = new ArrayList<String>(subRanges.size());\n      List<String> subShardNames = new ArrayList<String>(subRanges.size());\n      String nodeName = parentShardLeader.getNodeName();\n      for (int i = 0; i < subRanges.size(); i++) {\n        String subSlice = slice + \"_\" + i;\n        subSlices.add(subSlice);\n        String subShardName = collectionName + \"_\" + subSlice + \"_replica1\";\n        subShardNames.add(subShardName);\n\n        Slice oSlice = clusterState.getSlice(collectionName, subSlice);\n        if (oSlice != null) {\n          if (Slice.ACTIVE.equals(oSlice.getState())) {\n            throw new SolrException(ErrorCode.BAD_REQUEST, \"Sub-shard: \" + subSlice + \" exists in active state. Aborting split shard.\");\n          } else if (Slice.CONSTRUCTION.equals(oSlice.getState()) || Slice.RECOVERY.equals(oSlice.getState()))  {\n            // delete the shards\n            for (String sub : subSlices) {\n              log.info(\"Sub-shard: {} already exists therefore requesting its deletion\", sub);\n              Map<String, Object> propMap = new HashMap<String, Object>();\n              propMap.put(Overseer.QUEUE_OPERATION, \"deleteshard\");\n              propMap.put(COLLECTION_PROP, collectionName);\n              propMap.put(SHARD_ID_PROP, sub);\n              ZkNodeProps m = new ZkNodeProps(propMap);\n              try {\n                deleteShard(clusterState, m, new NamedList());\n              } catch (Exception e) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to delete already existing sub shard: \" + sub, e);\n              }\n            }\n          }\n        }\n      }\n\n      // do not abort splitshard if the unloading fails\n      // this can happen because the replicas created previously may be down\n      // the only side effect of this is that the sub shard may end up having more replicas than we want\n      collectShardResponses(results, false, null);\n\n      for (int i=0; i<subRanges.size(); i++)  {\n        String subSlice = subSlices.get(i);\n        String subShardName = subShardNames.get(i);\n        DocRouter.Range subRange = subRanges.get(i);\n\n        log.info(\"Creating shard \" + subShardName + \" as part of slice \"\n            + subSlice + \" of collection \" + collectionName + \" on \"\n            + nodeName);\n\n        ModifiableSolrParams params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n        params.set(CoreAdminParams.NAME, subShardName);\n        params.set(CoreAdminParams.COLLECTION, collectionName);\n        params.set(CoreAdminParams.SHARD, subSlice);\n        params.set(CoreAdminParams.SHARD_RANGE, subRange.toString());\n        params.set(CoreAdminParams.SHARD_STATE, Slice.CONSTRUCTION);\n        params.set(CoreAdminParams.SHARD_PARENT, parentSlice.getName());\n        addPropertyParams(message, params);\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard leaders\");\n\n      for (String subShardName : subShardNames) {\n        // wait for parent leader to acknowledge the sub-shard core\n        log.info(\"Asking parent leader to wait for: \" + subShardName + \" to be alive on: \" + nodeName);\n        String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName), subShardName);\n        CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n        cmd.setCoreName(subShardName);\n        cmd.setNodeName(nodeName);\n        cmd.setCoreNodeName(coreNodeName);\n        cmd.setState(ZkStateReader.ACTIVE);\n        cmd.setCheckLive(true);\n        cmd.setOnlyIfLeader(true);\n        sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD timed out waiting for subshard leaders to come up\");\n      \n      log.info(\"Successfully created all sub-shards for collection \"\n          + collectionName + \" parent shard: \" + slice + \" on: \" + parentShardLeader);\n\n      log.info(\"Splitting shard \" + parentShardLeader.getName() + \" as part of slice \"\n          + slice + \" of collection \" + collectionName + \" on \"\n          + parentShardLeader);\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n      params.set(CoreAdminParams.CORE, parentShardLeader.getStr(\"core\"));\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n        params.add(CoreAdminParams.TARGET_CORE, subShardName);\n      }\n      params.set(CoreAdminParams.RANGES, rangesStr);\n\n      sendShardRequest(parentShardLeader.getNodeName(), params);\n      collectShardResponses(results, true, \"SPLITSHARD failed to invoke SPLIT core admin command\");\n\n      log.info(\"Index on shard: \" + nodeName + \" split into two successfully\");\n\n      // apply buffered updates on sub-shards\n      for (int i = 0; i < subShardNames.size(); i++) {\n        String subShardName = subShardNames.get(i);\n\n        log.info(\"Applying buffered updates on : \" + subShardName);\n\n        params = new ModifiableSolrParams();\n        params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n        params.set(CoreAdminParams.NAME, subShardName);\n\n        sendShardRequest(nodeName, params);\n      }\n\n      collectShardResponses(results, true,\n          \"SPLITSHARD failed while asking sub shard leaders to apply buffered updates\");\n\n      log.info(\"Successfully applied buffered updates on : \" + subShardNames);\n\n      // Replica creation for the new Slices\n\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n\n      // TODO: Have replication factor decided in some other way instead of numShards for the parent\n\n      int repFactor = clusterState.getSlice(collectionName, slice).getReplicas().size();\n\n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n\n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      \n      Collections.shuffle(nodeList);\n\n      // TODO: Have maxShardsPerNode param for this operation?\n\n      // Remove the node that hosts the parent shard for replica creation.\n      nodeList.remove(nodeName);\n      \n      // TODO: change this to handle sharding a slice into > 2 sub-shards.\n\n      for (int i = 1; i <= subSlices.size(); i++) {\n        Collections.shuffle(nodeList);\n        String sliceName = subSlices.get(i - 1);\n        for (int j = 2; j <= repFactor; j++) {\n          String subShardNodeName = nodeList.get((repFactor * (i - 1) + (j - 2)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + (j);\n\n          log.info(\"Creating replica shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + subShardNodeName);\n\n          // Need to create new params for each request\n          params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          addPropertyParams(message, params);\n          // TODO:  Figure the config used by the parent shard and use it.\n          //params.set(\"collection.configName\", configName);\n          \n          //Not using this property. Do we really need to use it?\n          //params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n\n          sendShardRequest(subShardNodeName, params);\n\n          String coreNodeName = waitForCoreNodeName(collection, zkStateReader.getZkClient().getBaseUrlForNodeName(subShardNodeName), shardName);\n          // wait for the replicas to be seen as active on sub shard leader\n          log.info(\"Asking sub shard leader to wait for: \" + shardName + \" to be alive on: \" + subShardNodeName);\n          CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n          cmd.setCoreName(subShardNames.get(i-1));\n          cmd.setNodeName(subShardNodeName);\n          cmd.setCoreNodeName(coreNodeName);\n          cmd.setState(ZkStateReader.RECOVERING);\n          cmd.setCheckLive(true);\n          cmd.setOnlyIfLeader(true);\n          sendShardRequest(nodeName, new ModifiableSolrParams(cmd.getParams()));\n        }\n      }\n\n      collectShardResponses(results, true,\n          \"SPLTSHARD failed to create subshard replicas or timed out waiting for them to come up\");\n\n      log.info(\"Successfully created all replica shards for all sub-slices \" + subSlices);\n\n      commit(results, slice, parentShardLeader);\n\n      if (repFactor == 1) {\n        // switch sub shard states to 'active'\n        log.info(\"Replication factor is 1 so switching shard states\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        propMap.put(slice, Slice.INACTIVE);\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.ACTIVE);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      } else  {\n        log.info(\"Requesting shard state be set to 'recovery'\");\n        DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());\n        Map<String, Object> propMap = new HashMap<String, Object>();\n        propMap.put(Overseer.QUEUE_OPERATION, \"updateshardstate\");\n        for (String subSlice : subSlices) {\n          propMap.put(subSlice, Slice.RECOVERY);\n        }\n        propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);\n        ZkNodeProps m = new ZkNodeProps(propMap);\n        inQueue.offer(ZkStateReader.toJSON(m));\n      }\n\n      return true;\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error executing split operation for collection: \" + collectionName + \" parent shard: \" + slice, e);\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, e);\n    }\n  }\n\n  private void commit(NamedList results, String slice, Replica parentShardLeader) {\n    log.info(\"Calling soft commit to make sub shard updates visible\");\n    String coreUrl = new ZkCoreNodeProps(parentShardLeader).getCoreUrl();\n    // HttpShardHandler is hard coded to send a QueryRequest hence we go direct\n    // and we force open a searcher so that we have documents to show upon switching states\n    UpdateResponse updateResponse = null;\n    try {\n      updateResponse = softCommit(coreUrl);\n      processResponse(results, null, coreUrl, updateResponse, slice);\n    } catch (Exception e) {\n      processResponse(results, e, coreUrl, updateResponse, slice);\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to call distrib softCommit on: \" + coreUrl, e);\n    }\n  }\n\n  static UpdateResponse softCommit(String url) throws SolrServerException, IOException {\n    HttpSolrServer server = null;\n    try {\n      server = new HttpSolrServer(url);\n      server.setConnectionTimeout(30000);\n      server.setSoTimeout(120000);\n      UpdateRequest ureq = new UpdateRequest();\n      ureq.setParams(new ModifiableSolrParams());\n      ureq.setAction(AbstractUpdateRequest.ACTION.COMMIT, false, true, true);\n      return ureq.process(server);\n    } finally {\n      if (server != null) {\n        server.shutdown();\n      }\n    }\n  }\n  \n  private String waitForCoreNodeName(DocCollection collection, String msgBaseUrl, String msgCore) {\n    int retryCount = 320;\n    while (retryCount-- > 0) {\n      Map<String,Slice> slicesMap = zkStateReader.getClusterState()\n          .getSlicesMap(collection.getName());\n      if (slicesMap != null) {\n        \n        for (Slice slice : slicesMap.values()) {\n          for (Replica replica : slice.getReplicas()) {\n            // TODO: for really large clusters, we could 'index' on this\n            \n            String baseUrl = replica.getStr(ZkStateReader.BASE_URL_PROP);\n            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n            \n            if (baseUrl.equals(msgBaseUrl) && core.equals(msgCore)) {\n              return replica.getName();\n            }\n          }\n        }\n      }\n      try {\n        Thread.sleep(1000);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n    }\n    throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not find coreNodeName\");\n  }\n\n  private void collectShardResponses(NamedList results, boolean abortOnError, String msgOnError) {\n    ShardResponse srsp;\n    do {\n      srsp = shardHandler.takeCompletedOrError();\n      if (srsp != null) {\n        processResponse(results, srsp);\n        Throwable exception = srsp.getException();\n        if (abortOnError && exception != null)  {\n          // drain pending requests\n          while (srsp != null)  {\n            srsp = shardHandler.takeCompletedOrError();\n          }\n          throw new SolrException(ErrorCode.SERVER_ERROR, msgOnError, exception);\n        }\n      }\n    } while (srsp != null);\n  }\n\n  \n  private void deleteShard(ClusterState clusterState, ZkNodeProps message, NamedList results) {\n    log.info(\"Delete shard invoked\");\n    String collection = message.getStr(ZkStateReader.COLLECTION_PROP);\n\n    String sliceId = message.getStr(ZkStateReader.SHARD_ID_PROP);\n    Slice slice = clusterState.getSlice(collection, sliceId);\n    \n    if (slice == null) {\n      if(clusterState.hasCollection(collection)) {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No shard with the specified name exists: \" + slice);\n      } else {\n        throw new SolrException(ErrorCode.BAD_REQUEST,\n            \"No collection with the specified name exists: \" + collection);\n      }      \n    }\n    // For now, only allow for deletions of Inactive slices or custom hashes (range==null).\n    // TODO: Add check for range gaps on Slice deletion\n    if (!(slice.getRange() == null || slice.getState().equals(Slice.INACTIVE) || slice.getState().equals(Slice.RECOVERY))) {\n      throw new SolrException(ErrorCode.BAD_REQUEST,\n          \"The slice: \" + slice.getName() + \" is currently \"\n          + slice.getState() + \". Only INACTIVE (or custom-hashed) slices can be deleted.\");\n    }\n\n    try {\n      ModifiableSolrParams params = new ModifiableSolrParams();\n      params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());\n      params.set(CoreAdminParams.DELETE_INDEX, \"true\");\n      sliceCmd(clusterState, params, null, slice);\n\n      ShardResponse srsp;\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION,\n          Overseer.REMOVESHARD, ZkStateReader.COLLECTION_PROP, collection,\n          ZkStateReader.SHARD_ID_PROP, sliceId);\n      Overseer.getInQueue(zkStateReader.getZkClient()).offer(ZkStateReader.toJSON(m));\n\n      // wait for a while until we don't see the shard\n      long now = System.currentTimeMillis();\n      long timeout = now + 30000;\n      boolean removed = false;\n      while (System.currentTimeMillis() < timeout) {\n        Thread.sleep(100);\n        removed = zkStateReader.getClusterState().getSlice(collection, sliceId) == null;\n        if (removed) {\n          Thread.sleep(100); // just a bit of time so it's more likely other readers see on return\n          break;\n        }\n      }\n      if (!removed) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Could not fully remove collection: \" + collection + \" shard: \" + sliceId);\n      }\n\n      log.info(\"Successfully deleted collection: \" + collection + \", shard: \" + sliceId);\n\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Error executing delete operation for collection: \" + collection + \" shard: \" + sliceId, e);\n    }\n  }\n\n  private void migrate(ClusterState clusterState, ZkNodeProps message, NamedList results) throws KeeperException, InterruptedException {\n    String sourceCollectionName = message.getStr(\"collection\");\n    String splitKey = message.getStr(\"split.key\");\n    String targetCollectionName = message.getStr(\"target.collection\");\n    int timeout = message.getInt(\"forward.timeout\", 10 * 60) * 1000;\n\n    DocCollection sourceCollection = clusterState.getCollection(sourceCollectionName);\n    if (sourceCollection == null) {\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"Unknown source collection: \" + sourceCollectionName);\n    }\n    DocCollection targetCollection = clusterState.getCollection(targetCollectionName);\n    if (targetCollection == null) {\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"Unknown target collection: \" + sourceCollectionName);\n    }\n    if (!(sourceCollection.getRouter() instanceof CompositeIdRouter))  {\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"Source collection must use a compositeId router\");\n    }\n    if (!(targetCollection.getRouter() instanceof CompositeIdRouter))  {\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"Target collection must use a compositeId router\");\n    }\n    CompositeIdRouter sourceRouter = (CompositeIdRouter) sourceCollection.getRouter();\n    CompositeIdRouter targetRouter = (CompositeIdRouter) targetCollection.getRouter();\n    Collection<Slice> sourceSlices = sourceRouter.getSearchSlicesSingle(splitKey, null, sourceCollection);\n    if (sourceSlices.isEmpty()) {\n      throw new SolrException(ErrorCode.BAD_REQUEST,\n          \"No active slices available in source collection: \" + sourceCollection + \"for given split.key: \" + splitKey);\n    }\n    Collection<Slice> targetSlices = targetRouter.getSearchSlicesSingle(splitKey, null, targetCollection);\n    if (targetSlices.isEmpty()) {\n      throw new SolrException(ErrorCode.BAD_REQUEST,\n          \"No active slices available in target collection: \" + targetCollection + \"for given split.key: \" + splitKey);\n    }\n\n    for (Slice sourceSlice : sourceSlices) {\n      for (Slice targetSlice : targetSlices) {\n        log.info(\"Migrating source shard: {} to target shard: {} for split.key = \" + splitKey, sourceSlice, targetSlice);\n        migrateKey(clusterState, sourceCollection, sourceSlice, targetCollection, targetSlice, splitKey, timeout, results);\n      }\n    }\n  }\n\n  private void migrateKey(ClusterState clusterState, DocCollection sourceCollection, Slice sourceSlice, DocCollection targetCollection, Slice targetSlice, String splitKey, int timeout, NamedList results) throws KeeperException, InterruptedException {\n    String tempSourceCollectionName = \"split_\" + sourceSlice.getName() + \"_temp_\" + targetSlice.getName();\n    if (clusterState.hasCollection(tempSourceCollectionName)) {\n      log.info(\"Deleting temporary collection: \" + tempSourceCollectionName);\n      Map<String, Object> props = ZkNodeProps.makeMap(\n          QUEUE_OPERATION, DELETECOLLECTION,\n          \"name\", tempSourceCollectionName);\n      try {\n        deleteCollection(new ZkNodeProps(props), results);\n      } catch (Exception e) {\n        log.warn(\"Unable to clean up existing temporary collection: \" + tempSourceCollectionName, e);\n      }\n    }\n\n    CompositeIdRouter sourceRouter = (CompositeIdRouter) sourceCollection.getRouter();\n    DocRouter.Range keyHashRange = sourceRouter.keyHashRange(splitKey);\n\n    log.info(\"Hash range for split.key: {} is: {}\", splitKey, keyHashRange);\n    // intersect source range, keyHashRange and target range\n    // this is the range that has to be split from source and transferred to target\n    DocRouter.Range splitRange = intersect(targetSlice.getRange(), intersect(sourceSlice.getRange(), keyHashRange));\n    if (splitRange == null) {\n      log.info(\"No common hashes between source shard: {} and target shard: {}\", sourceSlice.getName(), targetSlice.getName());\n      return;\n    }\n    log.info(\"Common hash range between source shard: {} and target shard: {} = \" + splitRange, sourceSlice.getName(), targetSlice.getName());\n\n    Replica targetLeader = targetSlice.getLeader();\n\n    log.info(\"Asking target leader node: \" + targetLeader.getNodeName() + \" core: \"\n        + targetLeader.getStr(\"core\") + \" to buffer updates\");\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTBUFFERUPDATES.toString());\n    params.set(CoreAdminParams.NAME, targetLeader.getStr(\"core\"));\n    sendShardRequest(targetLeader.getNodeName(), params);\n    collectShardResponses(results, true, \"MIGRATE failed to request node to buffer updates\");\n\n    ZkNodeProps m = new ZkNodeProps(\n        Overseer.QUEUE_OPERATION, Overseer.ADD_ROUTING_RULE,\n        COLLECTION_PROP, sourceCollection.getName(),\n        SHARD_ID_PROP, sourceSlice.getName(),\n        \"routeKey\", SolrIndexSplitter.getRouteKey(splitKey) + \"!\",\n        \"range\", splitRange.toString(),\n        \"targetCollection\", targetCollection.getName(),\n        \"expireAt\", String.valueOf(System.currentTimeMillis() + timeout));\n    log.info(\"Adding routing rule: \" + m);\n    Overseer.getInQueue(zkStateReader.getZkClient()).offer(\n        ZkStateReader.toJSON(m));\n\n    // wait for a while until we see the new rule\n    log.info(\"Waiting to see routing rule updated in clusterstate\");\n    long waitUntil = System.currentTimeMillis() + 60000;\n    boolean added = false;\n    while (System.currentTimeMillis() < waitUntil) {\n      Thread.sleep(100);\n      Map<String, RoutingRule> rules = zkStateReader.getClusterState().getSlice(sourceCollection.getName(), sourceSlice.getName()).getRoutingRules();\n      if (rules != null) {\n        RoutingRule rule = rules.get(SolrIndexSplitter.getRouteKey(splitKey) + \"!\");\n        if (rule != null && rule.getRouteRanges().contains(splitRange)) {\n          added = true;\n          break;\n        }\n      }\n    }\n    if (!added) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not add routing rule: \" + m);\n    }\n\n    log.info(\"Routing rule added successfully\");\n\n    // Create temp core on source shard\n    Replica sourceLeader = sourceSlice.getLeader();\n\n    // create a temporary collection with just one node on the shard leader\n    String configName = zkStateReader.readConfigName(sourceCollection.getName());\n    Map<String, Object> props = ZkNodeProps.makeMap(\n        QUEUE_OPERATION, CREATECOLLECTION,\n        \"name\", tempSourceCollectionName,\n        REPLICATION_FACTOR, 1,\n        NUM_SLICES, 1,\n        COLL_CONF, configName,\n        CREATE_NODE_SET, sourceLeader.getNodeName());\n    log.info(\"Creating temporary collection: \" + props);\n    createCollection(clusterState, new ZkNodeProps(props), results);\n    // refresh cluster state\n    clusterState = zkStateReader.getClusterState();\n    Slice tempSourceSlice = clusterState.getCollection(tempSourceCollectionName).getSlices().iterator().next();\n    Replica tempSourceLeader = clusterState.getLeader(tempSourceCollectionName, tempSourceSlice.getName());\n\n    String tempCollectionReplica1 = tempSourceCollectionName + \"_\" + tempSourceSlice.getName() + \"_replica1\";\n    String coreNodeName = waitForCoreNodeName(clusterState.getCollection(tempSourceCollectionName),\n        zkStateReader.getZkClient().getBaseUrlForNodeName(sourceLeader.getNodeName()), tempCollectionReplica1);\n    // wait for the replicas to be seen as active on temp source leader\n    log.info(\"Asking source leader to wait for: \" + tempCollectionReplica1 + \" to be alive on: \" + sourceLeader.getNodeName());\n    CoreAdminRequest.WaitForState cmd = new CoreAdminRequest.WaitForState();\n    cmd.setCoreName(tempCollectionReplica1);\n    cmd.setNodeName(sourceLeader.getNodeName());\n    cmd.setCoreNodeName(coreNodeName);\n    cmd.setState(ZkStateReader.ACTIVE);\n    cmd.setCheckLive(true);\n    cmd.setOnlyIfLeader(true);\n    sendShardRequest(tempSourceLeader.getNodeName(), new ModifiableSolrParams(cmd.getParams()));\n\n    collectShardResponses(results, true,\n        \"MIGRATE failed to create temp collection leader or timed out waiting for it to come up\");\n\n    log.info(\"Asking source leader to split index\");\n    params = new ModifiableSolrParams();\n    params.set(CoreAdminParams.ACTION, CoreAdminAction.SPLIT.toString());\n    params.set(CoreAdminParams.CORE, sourceLeader.getStr(\"core\"));\n    params.add(CoreAdminParams.TARGET_CORE, tempSourceLeader.getStr(\"core\"));\n    params.set(CoreAdminParams.RANGES, splitRange.toString());\n    params.set(\"split.key\", splitKey);\n\n    sendShardRequest(sourceLeader.getNodeName(), params);\n    collectShardResponses(results, true, \"MIGRATE failed to invoke SPLIT core admin command\");\n\n    log.info(\"Creating a replica of temporary collection: {} on the target leader node: {}\",\n        tempSourceCollectionName, targetLeader.getNodeName());\n    params = new ModifiableSolrParams();\n    params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n    String tempCollectionReplica2 = tempSourceCollectionName + \"_\" + tempSourceSlice.getName() + \"_replica2\";\n    params.set(CoreAdminParams.NAME, tempCollectionReplica2);\n    params.set(CoreAdminParams.COLLECTION, tempSourceCollectionName);\n    params.set(CoreAdminParams.SHARD, tempSourceSlice.getName());\n    sendShardRequest(targetLeader.getNodeName(), params);\n\n    coreNodeName = waitForCoreNodeName(clusterState.getCollection(tempSourceCollectionName),\n        zkStateReader.getZkClient().getBaseUrlForNodeName(targetLeader.getNodeName()), tempCollectionReplica2);\n    // wait for the replicas to be seen as active on temp source leader\n    log.info(\"Asking temp source leader to wait for: \" + tempCollectionReplica2 + \" to be alive on: \" + targetLeader.getNodeName());\n    cmd = new CoreAdminRequest.WaitForState();\n    cmd.setCoreName(tempSourceLeader.getStr(\"core\"));\n    cmd.setNodeName(targetLeader.getNodeName());\n    cmd.setCoreNodeName(coreNodeName);\n    cmd.setState(ZkStateReader.ACTIVE); // todo introduce asynchronous actions\n    cmd.setCheckLive(true);\n    cmd.setOnlyIfLeader(true);\n    sendShardRequest(tempSourceLeader.getNodeName(), new ModifiableSolrParams(cmd.getParams()));\n\n    collectShardResponses(results, true,\n        \"MIGRATE failed to create temp collection replica or timed out waiting for them to come up\");\n\n    log.info(\"Successfully created replica of temp source collection on target leader node\");\n\n    log.info(\"Requesting merge of temp source collection replica to target leader\");\n    params = new ModifiableSolrParams();\n    params.set(CoreAdminParams.ACTION, CoreAdminAction.MERGEINDEXES.toString());\n    params.set(CoreAdminParams.CORE, targetLeader.getStr(\"core\"));\n    params.set(CoreAdminParams.SRC_CORE, tempCollectionReplica2);\n    sendShardRequest(targetLeader.getNodeName(), params);\n    collectShardResponses(results, true,\n        \"MIGRATE failed to merge \" + tempCollectionReplica2 + \" to \" + targetLeader.getStr(\"core\") + \" on node: \" + targetLeader.getNodeName());\n\n    log.info(\"Asking target leader to apply buffered updates\");\n    params = new ModifiableSolrParams();\n    params.set(CoreAdminParams.ACTION, CoreAdminAction.REQUESTAPPLYUPDATES.toString());\n    params.set(CoreAdminParams.NAME, targetLeader.getStr(\"core\"));\n    sendShardRequest(targetLeader.getNodeName(), params);\n    collectShardResponses(results, true,\n        \"MIGRATE failed to request node to apply buffered updates\");\n\n    try {\n      log.info(\"Deleting temporary collection: \" + tempSourceCollectionName);\n      props = ZkNodeProps.makeMap(\n          QUEUE_OPERATION, DELETECOLLECTION,\n          \"name\", tempSourceCollectionName);\n      deleteCollection(new ZkNodeProps(props), results);\n    } catch (Exception e) {\n      log.error(\"Unable to delete temporary collection: \" + tempSourceCollectionName\n          + \". Please remove it manually\", e);\n    }\n  }\n\n  private DocRouter.Range intersect(DocRouter.Range a, DocRouter.Range b) {\n    if (a == null || b == null || !a.overlaps(b)) {\n      return null;\n    } else if (a.isSubsetOf(b))\n      return a;\n    else if (b.isSubsetOf(a))\n      return b;\n    else if (b.includes(a.max)) {\n      return new DocRouter.Range(b.min, a.max);\n    } else  {\n      return new DocRouter.Range(a.min, b.max);\n    }\n  }\n\n  private void sendShardRequest(String nodeName, ModifiableSolrParams params) {\n    ShardRequest sreq = new ShardRequest();\n    params.set(\"qt\", adminPath);\n    sreq.purpose = 1;\n    String replica = zkStateReader.getZkClient().getBaseUrlForNodeName(nodeName);\n    if (replica.startsWith(\"http://\")) replica = replica.substring(7);\n    sreq.shards = new String[]{replica};\n    sreq.actualShards = sreq.shards;\n    sreq.params = params;\n\n    shardHandler.submit(sreq, replica, sreq.params);\n  }\n\n  private void addPropertyParams(ZkNodeProps message, ModifiableSolrParams params) {\n    // Now add the property.key=value pairs\n    for (String key : message.keySet()) {\n      if (key.startsWith(COLL_PROP_PREFIX)) {\n        params.set(key, message.getStr(key));\n      }\n    }\n  }\n  private void createCollection(ClusterState clusterState, ZkNodeProps message, NamedList results) throws KeeperException, InterruptedException {\n    String collectionName = message.getStr(\"name\");\n    if (clusterState.hasCollection(collectionName)) {\n      throw new SolrException(ErrorCode.BAD_REQUEST, \"collection already exists: \" + collectionName);\n    }\n    \n    try {\n      // look at the replication factor and see if it matches reality\n      // if it does not, find best nodes to create more cores\n      \n      int repFactor = message.getInt( REPLICATION_FACTOR, 1);\n      Integer numSlices = message.getInt(NUM_SLICES, null);\n      String router = message.getStr(\"router.name\", DocRouter.DEFAULT_NAME);\n      List<String> shardNames = new ArrayList<>();\n      if(ImplicitDocRouter.NAME.equals(router)){\n        Overseer.getShardNames(shardNames, message.getStr(\"shards\",null));\n        numSlices = shardNames.size();\n      } else {\n        Overseer.getShardNames(numSlices,shardNames);\n      }\n\n      if (numSlices == null ) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, NUM_SLICES + \" is a required param\");\n      }\n\n      int maxShardsPerNode = message.getInt(MAX_SHARDS_PER_NODE, 1);\n      String createNodeSetStr; \n      List<String> createNodeList = ((createNodeSetStr = message.getStr(CREATE_NODE_SET)) == null)?null:StrUtils.splitSmart(createNodeSetStr, \",\", true);\n      \n      if (repFactor <= 0) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, REPLICATION_FACTOR + \" must be greater than 0\");\n      }\n      \n      if (numSlices <= 0) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, NUM_SLICES + \" must be > 0\");\n      }\n      \n      // we need to look at every node and see how many cores it serves\n      // add our new cores to existing nodes serving the least number of cores\n      // but (for now) require that each core goes on a distinct node.\n      \n      // TODO: add smarter options that look at the current number of cores per\n      // node?\n      // for now we just go random\n      Set<String> nodes = clusterState.getLiveNodes();\n      List<String> nodeList = new ArrayList<String>(nodes.size());\n      nodeList.addAll(nodes);\n      if (createNodeList != null) nodeList.retainAll(createNodeList);\n      Collections.shuffle(nodeList);\n      \n      if (nodeList.size() <= 0) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"Cannot create collection \" + collectionName\n            + \". No live Solr-instances\" + ((createNodeList != null)?\" among Solr-instances specified in \" + CREATE_NODE_SET + \":\" + createNodeSetStr:\"\"));\n      }\n      \n      if (repFactor > nodeList.size()) {\n        log.warn(\"Specified \"\n            + REPLICATION_FACTOR\n            + \" of \"\n            + repFactor\n            + \" on collection \"\n            + collectionName\n            + \" is higher than or equal to the number of Solr instances currently live or part of your \" + CREATE_NODE_SET + \"(\"\n            + nodeList.size()\n            + \"). Its unusual to run two replica of the same slice on the same Solr-instance.\");\n      }\n      \n      int maxShardsAllowedToCreate = maxShardsPerNode * nodeList.size();\n      int requestedShardsToCreate = numSlices * repFactor;\n      if (maxShardsAllowedToCreate < requestedShardsToCreate) {\n        throw new SolrException(ErrorCode.BAD_REQUEST, \"Cannot create collection \" + collectionName + \". Value of \"\n            + MAX_SHARDS_PER_NODE + \" is \" + maxShardsPerNode\n            + \", and the number of live nodes is \" + nodeList.size()\n            + \". This allows a maximum of \" + maxShardsAllowedToCreate\n            + \" to be created. Value of \" + NUM_SLICES + \" is \" + numSlices\n            + \" and value of \" + REPLICATION_FACTOR + \" is \" + repFactor\n            + \". This requires \" + requestedShardsToCreate\n            + \" shards to be created (higher than the allowed number)\");\n      }\n      String configName = createConfNode(collectionName, message);\n\n      Overseer.getInQueue(zkStateReader.getZkClient()).offer(ZkStateReader.toJSON(message));\n\n      // wait for a while until we don't see the collection\n      long waitUntil = System.currentTimeMillis() + 30000;\n      boolean created = false;\n      while (System.currentTimeMillis() < waitUntil) {\n        Thread.sleep(100);\n        created = zkStateReader.getClusterState().getCollections().contains(message.getStr(\"name\"));\n        if(created) break;\n      }\n      if (!created)\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not fully createcollection: \" + message.getStr(\"name\"));\n\n      log.info(\"going to create cores replicas shardNames {} , repFactor : {}\", shardNames, repFactor);\n      for (int i = 1; i <= shardNames.size(); i++) {\n        String sliceName = shardNames.get(i-1);\n        for (int j = 1; j <= repFactor; j++) {\n          String nodeName = nodeList.get((repFactor * (i - 1) + (j - 1)) % nodeList.size());\n          String shardName = collectionName + \"_\" + sliceName + \"_replica\" + j;\n          log.info(\"Creating shard \" + shardName + \" as part of slice \"\n              + sliceName + \" of collection \" + collectionName + \" on \"\n              + nodeName);\n\n          // Need to create new params for each request\n          ModifiableSolrParams params = new ModifiableSolrParams();\n          params.set(CoreAdminParams.ACTION, CoreAdminAction.CREATE.toString());\n\n          params.set(CoreAdminParams.NAME, shardName);\n          params.set(COLL_CONF, configName);\n          params.set(CoreAdminParams.COLLECTION, collectionName);\n          params.set(CoreAdminParams.SHARD, sliceName);\n          params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n          addPropertyParams(message, params);\n\n          ShardRequest sreq = new ShardRequest();\n          params.set(\"qt\", adminPath);\n          sreq.purpose = 1;\n          String replica = zkStateReader.getZkClient()\n            .getBaseUrlForNodeName(nodeName);\n          if (replica.startsWith(\"http://\")) replica = replica.substring(7);\n          sreq.shards = new String[] {replica};\n          sreq.actualShards = sreq.shards;\n          sreq.params = params;\n\n          shardHandler.submit(sreq, replica, sreq.params);\n\n        }\n      }\n\n      ShardResponse srsp;\n      do {\n        srsp = shardHandler.takeCompletedOrError();\n        if (srsp != null) {\n          processResponse(results, srsp);\n        }\n      } while (srsp != null);\n\n      log.info(\"Finished create command on all shards for collection: \"\n          + collectionName);\n\n    } catch (SolrException ex) {\n      throw ex;\n    } catch (Exception ex) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, null, ex);\n    }\n  }\n\n  private String createConfNode(String coll, ZkNodeProps message) throws KeeperException, InterruptedException {\n    String configName = message.getStr(OverseerCollectionProcessor.COLL_CONF);\n    if(configName == null){\n      // if there is only one conf, use that\n      List<String> configNames=null;\n      try {\n        configNames = zkStateReader.getZkClient().getChildren(ZkController.CONFIGS_ZKNODE, null, true);\n        if (configNames != null && configNames.size() == 1) {\n          configName = configNames.get(0);\n          // no config set named, but there is only 1 - use it\n          log.info(\"Only one config set found in zk - using it:\" + configName);\n        }\n      } catch (KeeperException.NoNodeException e) {\n\n      }\n\n    }\n\n    if(configName!= null){\n      log.info(\"creating collections conf node {} \",ZkStateReader.COLLECTIONS_ZKNODE + \"/\" + coll);\n      zkStateReader.getZkClient().makePath(ZkStateReader.COLLECTIONS_ZKNODE + \"/\" + coll,\n          ZkStateReader.toJSON(ZkNodeProps.makeMap(ZkController.CONFIGNAME_PROP,configName)),true );\n\n    } else {\n      String msg = \"Could not obtain config name\";\n      log.warn(msg);\n    }\n    return configName;\n\n  }\n\n  private void collectionCmd(ClusterState clusterState, ZkNodeProps message, ModifiableSolrParams params, NamedList results, String stateMatcher) {\n    log.info(\"Executing Collection Cmd : \" + params);\n    String collectionName = message.getStr(\"name\");\n    \n    DocCollection coll = clusterState.getCollection(collectionName);\n    \n    for (Map.Entry<String,Slice> entry : coll.getSlicesMap().entrySet()) {\n      Slice slice = entry.getValue();\n      sliceCmd(clusterState, params, stateMatcher, slice);\n    }\n    \n    ShardResponse srsp;\n    do {\n      srsp = shardHandler.takeCompletedOrError();\n      if (srsp != null) {\n        processResponse(results, srsp);\n      }\n    } while (srsp != null);\n\n  }\n\n  private void sliceCmd(ClusterState clusterState, ModifiableSolrParams params, String stateMatcher, Slice slice) {\n    Map<String,Replica> shards = slice.getReplicasMap();\n    Set<Map.Entry<String,Replica>> shardEntries = shards.entrySet();\n    for (Map.Entry<String,Replica> shardEntry : shardEntries) {\n      final ZkNodeProps node = shardEntry.getValue();\n      if (clusterState.liveNodesContain(node.getStr(ZkStateReader.NODE_NAME_PROP)) && (stateMatcher != null ? node.getStr(ZkStateReader.STATE_PROP).equals(stateMatcher) : true)) {\n        // For thread safety, only simple clone the ModifiableSolrParams\n        ModifiableSolrParams cloneParams = new ModifiableSolrParams();\n        cloneParams.add(params);\n        cloneParams.set(CoreAdminParams.CORE,\n            node.getStr(ZkStateReader.CORE_NAME_PROP));\n\n        String replica = node.getStr(ZkStateReader.BASE_URL_PROP);\n        ShardRequest sreq = new ShardRequest();\n        sreq.nodeName = node.getStr(ZkStateReader.NODE_NAME_PROP);\n        // yes, they must use same admin handler path everywhere...\n        cloneParams.set(\"qt\", adminPath);\n        sreq.purpose = 1;\n        // TODO: this sucks\n        if (replica.startsWith(\"http://\")) replica = replica.substring(7);\n        sreq.shards = new String[] {replica};\n        sreq.actualShards = sreq.shards;\n        sreq.params = cloneParams;\n        log.info(\"Collection Admin sending CoreAdmin cmd to \" + replica\n            + \" params:\" + sreq.params);\n        shardHandler.submit(sreq, replica, sreq.params);\n      }\n    }\n  }\n\n  private void processResponse(NamedList results, ShardResponse srsp) {\n    Throwable e = srsp.getException();\n    String nodeName = srsp.getNodeName();\n    SolrResponse solrResponse = srsp.getSolrResponse();\n    String shard = srsp.getShard();\n\n    processResponse(results, e, nodeName, solrResponse, shard);\n  }\n\n  private void processResponse(NamedList results, Throwable e, String nodeName, SolrResponse solrResponse, String shard) {\n    if (e != null) {\n      log.error(\"Error from shard: \" + shard, e);\n\n      SimpleOrderedMap failure = (SimpleOrderedMap) results.get(\"failure\");\n      if (failure == null) {\n        failure = new SimpleOrderedMap();\n        results.add(\"failure\", failure);\n      }\n\n      failure.add(nodeName, e.getClass().getName() + \":\" + e.getMessage());\n\n    } else {\n\n      SimpleOrderedMap success = (SimpleOrderedMap) results.get(\"success\");\n      if (success == null) {\n        success = new SimpleOrderedMap();\n        results.add(\"success\", success);\n      }\n\n      success.add(nodeName, solrResponse.getResponse());\n    }\n  }\n\n  @Override\n  public boolean isClosed() {\n    return isClosed;\n  }\n\n}\n",
        "methodName": "splitShard",
        "exampleID": 30,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java",
        "line": "660",
        "source": "subRanges",
        "sourceLine": "687",
        "qualifier": "Possible null pointer dereference of $$subRanges/$",
        "steps": [
            {
                "exampleID": 31
            }
        ],
        "line_number": "687"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java",
        "rawCode": "package org.apache.solr.core;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.Set;\n\nimport org.apache.lucene.store.AlreadyClosedException;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.IOContext.Context;\nimport org.apache.lucene.store.NRTCachingDirectory;\nimport org.apache.lucene.store.NativeFSLockFactory;\nimport org.apache.lucene.store.NoLockFactory;\nimport org.apache.lucene.store.RateLimitedDirectoryWrapper;\nimport org.apache.lucene.store.SimpleFSLockFactory;\nimport org.apache.lucene.store.SingleInstanceLockFactory;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.SolrException.ErrorCode;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.store.blockcache.BlockDirectory;\nimport org.apache.solr.store.hdfs.HdfsDirectory;\nimport org.apache.solr.store.hdfs.HdfsLockFactory;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * A {@link DirectoryFactory} impl base class for caching Directory instances\n * per path. Most DirectoryFactory implementations will want to extend this\n * class and simply implement {@link DirectoryFactory#create(String, DirContext)}.\n * \n * This is an expert class and these API's are subject to change.\n * \n */\npublic abstract class CachingDirectoryFactory extends DirectoryFactory {\n  protected class CacheValue {\n    final public String path;\n    final public Directory directory;\n    // for debug\n    //final Exception originTrace;\n    // use the setter!\n    private boolean deleteOnClose = false;\n    \n    public CacheValue(String path, Directory directory) {\n      this.path = path;\n      this.directory = directory;\n      this.closeEntries.add(this);\n      // for debug\n      // this.originTrace = new RuntimeException(\"Originated from:\");\n    }\n    public int refCnt = 1;\n    // has doneWithDirectory(Directory) been called on this?\n    public boolean closeCacheValueCalled = false;\n    public boolean doneWithDir = false;\n    private boolean deleteAfterCoreClose = false;\n    public Set<CacheValue> removeEntries = new HashSet<CacheValue>();\n    public Set<CacheValue> closeEntries = new HashSet<CacheValue>();\n\n    public void setDeleteOnClose(boolean deleteOnClose, boolean deleteAfterCoreClose) {\n      if (deleteOnClose) {\n        removeEntries.add(this);\n      }\n      this.deleteOnClose = deleteOnClose;\n      this.deleteAfterCoreClose = deleteAfterCoreClose;\n    }\n    \n    @Override\n    public String toString() {\n      return \"CachedDir<<\" + \"refCount=\" + refCnt + \";path=\" + path + \";done=\" + doneWithDir + \">>\";\n    }\n  }\n  \n  private static Logger log = LoggerFactory\n      .getLogger(CachingDirectoryFactory.class);\n  \n  protected Map<String,CacheValue> byPathCache = new HashMap<String,CacheValue>();\n  \n  protected Map<Directory,CacheValue> byDirectoryCache = new IdentityHashMap<Directory,CacheValue>();\n  \n  protected Map<Directory,List<CloseListener>> closeListeners = new HashMap<Directory,List<CloseListener>>();\n  \n  protected Set<CacheValue> removeEntries = new HashSet<CacheValue>();\n\n  private Double maxWriteMBPerSecFlush;\n\n  private Double maxWriteMBPerSecMerge;\n\n  private Double maxWriteMBPerSecRead;\n\n  private Double maxWriteMBPerSecDefault;\n\n  private boolean closed;\n  \n  public interface CloseListener {\n    public void postClose();\n\n    public void preClose();\n  }\n  \n  @Override\n  public void addCloseListener(Directory dir, CloseListener closeListener) {\n    synchronized (this) {\n      if (!byDirectoryCache.containsKey(dir)) {\n        throw new IllegalArgumentException(\"Unknown directory: \" + dir\n            + \" \" + byDirectoryCache);\n      }\n      List<CloseListener> listeners = closeListeners.get(dir);\n      if (listeners == null) {\n        listeners = new ArrayList<CloseListener>();\n        closeListeners.put(dir, listeners);\n      }\n      listeners.add(closeListener);\n      \n      closeListeners.put(dir, listeners);\n    }\n  }\n  \n  @Override\n  public void doneWithDirectory(Directory directory) throws IOException {\n    synchronized (this) {\n      CacheValue cacheValue = byDirectoryCache.get(directory);\n      if (cacheValue == null) {\n        throw new IllegalArgumentException(\"Unknown directory: \" + directory\n            + \" \" + byDirectoryCache);\n      }\n      cacheValue.doneWithDir = true;\n      log.debug(\"Done with dir: {}\", cacheValue);\n      if (cacheValue.refCnt == 0 && !closed) {\n        boolean cl = closeCacheValue(cacheValue);\n        if (cl) {\n          removeFromCache(cacheValue);\n        }\n      }\n    }\n  }\n  \n  /*\n   * (non-Javadoc)\n   * \n   * @see org.apache.solr.core.DirectoryFactory#close()\n   */\n  @Override\n  public void close() throws IOException {\n    synchronized (this) {\n      log.info(\"Closing \" + this.getClass().getSimpleName() + \" - \" + byDirectoryCache.size() + \" directories currently being tracked\");\n      this.closed = true;\n      Collection<CacheValue> values = byDirectoryCache.values();\n      for (CacheValue val : values) {\n        log.debug(\"Closing {} - currently tracking: {}\", \n                  this.getClass().getSimpleName(), val);\n        try {\n          // if there are still refs out, we have to wait for them\n          int cnt = 0;\n          while(val.refCnt != 0) {\n            wait(100);\n            \n            if (cnt++ >= 120) {\n              String msg = \"Timeout waiting for all directory ref counts to be released - gave up waiting on \" + val;\n              log.error(msg);\n              // debug\n              // val.originTrace.printStackTrace();\n              throw new SolrException(ErrorCode.SERVER_ERROR, msg);\n            }\n          }\n          assert val.refCnt == 0 : val.refCnt;\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error closing directory\", t);\n        }\n      }\n      \n      values = byDirectoryCache.values();\n      Set<CacheValue> closedDirs = new HashSet<CacheValue>();\n      for (CacheValue val : values) {\n        try {\n          for (CacheValue v : val.closeEntries) {\n            assert v.refCnt == 0 : val.refCnt;\n            log.debug(\"Closing directory when closing factory: \" + v.path);\n            boolean cl = closeCacheValue(v);\n            if (cl) {\n              closedDirs.add(v);\n            }\n          }\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error closing directory\", t);\n        }\n      }\n\n      for (CacheValue val : removeEntries) {\n        log.info(\"Removing directory after core close: \" + val.path);\n        try {\n          removeDirectory(val);\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error removing directory\", t);\n        }\n      }\n      \n      for (CacheValue v : closedDirs) {\n        removeFromCache(v);\n      }\n    }\n  }\n\n  private void removeFromCache(CacheValue v) {\n    log.debug(\"Removing from cache: {}\", v);\n    byDirectoryCache.remove(v.directory);\n    byPathCache.remove(v.path);\n  }\n\n  // be sure this is called with the this sync lock\n  // returns true if we closed the cacheValue, false if it will be closed later\n  private boolean closeCacheValue(CacheValue cacheValue) {\n    log.info(\"looking to close \" + cacheValue.path + \" \" + cacheValue.closeEntries.toString());\n    List<CloseListener> listeners = closeListeners.remove(cacheValue.directory);\n    if (listeners != null) {\n      for (CloseListener listener : listeners) {\n        try {\n          listener.preClose();\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error executing preClose for directory\", t);\n        }\n      }\n    }\n    cacheValue.closeCacheValueCalled = true;\n    if (cacheValue.deleteOnClose) {\n      // see if we are a subpath\n      Collection<CacheValue> values = byPathCache.values();\n      \n      Collection<CacheValue> cacheValues = new ArrayList<CacheValue>(values);\n      cacheValues.remove(cacheValue);\n      for (CacheValue otherCacheValue : cacheValues) {\n        // if we are a parent path and a sub path is not already closed, get a sub path to close us later\n        if (isSubPath(cacheValue, otherCacheValue) && !otherCacheValue.closeCacheValueCalled) {\n          // we let the sub dir remove and close us\n          if (!otherCacheValue.deleteAfterCoreClose && cacheValue.deleteAfterCoreClose) {\n            otherCacheValue.deleteAfterCoreClose = true;\n          }\n          otherCacheValue.removeEntries.addAll(cacheValue.removeEntries);\n          otherCacheValue.closeEntries.addAll(cacheValue.closeEntries);\n          cacheValue.closeEntries.clear();\n          cacheValue.removeEntries.clear();\n          return false;\n        }\n      }\n    }\n\n    boolean cl = false;\n    for (CacheValue val : cacheValue.closeEntries) {\n      close(val);\n      if (val == cacheValue) {\n        cl = true;\n      }\n    }\n\n    for (CacheValue val : cacheValue.removeEntries) {\n      if (!val.deleteAfterCoreClose) {\n        log.info(\"Removing directory before core close: \" + val.path);\n        try {\n          removeDirectory(val);\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error removing directory\", t);\n        }\n      } else {\n        removeEntries.add(val);\n      }\n    }\n    \n    if (listeners != null) {\n      for (CloseListener listener : listeners) {\n        try {\n          listener.postClose();\n        } catch (Throwable t) {\n          SolrException.log(log, \"Error executing postClose for directory\", t);\n        }\n      }\n    }\n    return cl;\n  }\n\n  private void close(CacheValue val) {\n    try {\n      log.info(\"Closing directory: \" + val.path);\n      val.directory.close();\n    } catch (Throwable t) {\n      SolrException.log(log, \"Error closing directory\", t);\n    }\n  }\n\n  private boolean isSubPath(CacheValue cacheValue, CacheValue otherCacheValue) {\n    int one = cacheValue.path.lastIndexOf('/');\n    int two = otherCacheValue.path.lastIndexOf('/');\n    \n    return otherCacheValue.path.startsWith(cacheValue.path + \"/\") && two > one;\n  }\n\n  @Override\n  protected abstract Directory create(String path, DirContext dirContext) throws IOException;\n  \n  @Override\n  public boolean exists(String path) throws IOException {\n    // back compat behavior\n    File dirFile = new File(path);\n    return dirFile.canRead() && dirFile.list().length > 0;\n  }\n  \n  /*\n   * (non-Javadoc)\n   * \n   * @see org.apache.solr.core.DirectoryFactory#get(java.lang.String,\n   * java.lang.String, boolean)\n   */\n  @Override\n  public final Directory get(String path,  DirContext dirContext, String rawLockType)\n      throws IOException {\n    String fullPath = normalize(path);\n    synchronized (this) {\n      if (closed) {\n        throw new AlreadyClosedException(\"Already closed\");\n      }\n      \n      final CacheValue cacheValue = byPathCache.get(fullPath);\n      Directory directory = null;\n      if (cacheValue != null) {\n        directory = cacheValue.directory;\n      }\n      \n      if (directory == null) { \n        directory = create(fullPath, dirContext);\n        \n        directory = rateLimit(directory);\n        \n        CacheValue newCacheValue = new CacheValue(fullPath, directory);\n        \n        injectLockFactory(directory, fullPath, rawLockType);\n        \n        byDirectoryCache.put(directory, newCacheValue);\n        byPathCache.put(fullPath, newCacheValue);\n        log.info(\"return new directory for \" + fullPath);\n      } else {\n        cacheValue.refCnt++;\n        log.debug(\"Reusing cached directory: {}\", cacheValue);\n      }\n      \n      return directory;\n    }\n  }\n\n  private Directory rateLimit(Directory directory) {\n    if (maxWriteMBPerSecDefault != null || maxWriteMBPerSecFlush != null || maxWriteMBPerSecMerge != null || maxWriteMBPerSecRead != null) {\n      directory = new RateLimitedDirectoryWrapper(directory);\n      if (maxWriteMBPerSecDefault != null) {\n        ((RateLimitedDirectoryWrapper)directory).setMaxWriteMBPerSec(maxWriteMBPerSecDefault, Context.DEFAULT);\n      }\n      if (maxWriteMBPerSecFlush != null) {\n        ((RateLimitedDirectoryWrapper)directory).setMaxWriteMBPerSec(maxWriteMBPerSecFlush, Context.FLUSH);\n      }\n      if (maxWriteMBPerSecMerge != null) {\n        ((RateLimitedDirectoryWrapper)directory).setMaxWriteMBPerSec(maxWriteMBPerSecMerge, Context.MERGE);\n      }\n      if (maxWriteMBPerSecRead != null) {\n        ((RateLimitedDirectoryWrapper)directory).setMaxWriteMBPerSec(maxWriteMBPerSecRead, Context.READ);\n      }\n    }\n    return directory;\n  }\n  \n  /*\n   * (non-Javadoc)\n   * \n   * @see\n   * org.apache.solr.core.DirectoryFactory#incRef(org.apache.lucene.store.Directory\n   * )\n   */\n  @Override\n  public void incRef(Directory directory) {\n    synchronized (this) {\n      if (closed) {\n        throw new SolrException(ErrorCode.SERVICE_UNAVAILABLE, \"Already closed\");\n      }\n      CacheValue cacheValue = byDirectoryCache.get(directory);\n      if (cacheValue == null) {\n        throw new IllegalArgumentException(\"Unknown directory: \" + directory);\n      }\n      \n      cacheValue.refCnt++;\n      log.debug(\"incRef'ed: {}\", cacheValue);\n    }\n  }\n  \n  @Override\n  public void init(NamedList args) {\n    maxWriteMBPerSecFlush = (Double) args.get(\"maxWriteMBPerSecFlush\");\n    maxWriteMBPerSecMerge = (Double) args.get(\"maxWriteMBPerSecMerge\");\n    maxWriteMBPerSecRead = (Double) args.get(\"maxWriteMBPerSecRead\");\n    maxWriteMBPerSecDefault = (Double) args.get(\"maxWriteMBPerSecDefault\");\n  }\n  \n  /*\n   * (non-Javadoc)\n   * \n   * @see\n   * org.apache.solr.core.DirectoryFactory#release(org.apache.lucene.store.Directory\n   * )\n   */\n  @Override\n  public void release(Directory directory) throws IOException {\n    if (directory == null) {\n      throw new NullPointerException();\n    }\n    synchronized (this) {\n      // don't check if already closed here - we need to able to release\n      // while #close() waits.\n      \n      CacheValue cacheValue = byDirectoryCache.get(directory);\n      if (cacheValue == null) {\n        throw new IllegalArgumentException(\"Unknown directory: \" + directory\n            + \" \" + byDirectoryCache);\n      }\n      log.debug(\"Releasing directory: \" + cacheValue.path + \" \" + (cacheValue.refCnt - 1) + \" \" + cacheValue.doneWithDir);\n\n      cacheValue.refCnt--;\n      \n      assert cacheValue.refCnt >= 0 : cacheValue.refCnt;\n\n      if (cacheValue.refCnt == 0 && cacheValue.doneWithDir && !closed) {\n        boolean cl = closeCacheValue(cacheValue);\n        if (cl) {\n          removeFromCache(cacheValue);\n        }\n      }\n    }\n  }\n  \n  @Override\n  public void remove(String path) throws IOException {\n    remove(path, false);\n  }\n  \n  @Override\n  public void remove(Directory dir) throws IOException {\n    remove(dir, false);\n  }\n  \n  @Override\n  public void remove(String path, boolean deleteAfterCoreClose) throws IOException {\n    synchronized (this) {\n      CacheValue val = byPathCache.get(normalize(path));\n      if (val == null) {\n        throw new IllegalArgumentException(\"Unknown directory \" + path);\n      }\n      val.setDeleteOnClose(true, deleteAfterCoreClose);\n    }\n  }\n  \n  @Override\n  public void remove(Directory dir, boolean deleteAfterCoreClose) throws IOException {\n    synchronized (this) {\n      CacheValue val = byDirectoryCache.get(dir);\n      if (val == null) {\n        throw new IllegalArgumentException(\"Unknown directory \" + dir);\n      }\n      val.setDeleteOnClose(true, deleteAfterCoreClose);\n    }\n  }\n  \n  private static Directory injectLockFactory(Directory dir, String lockPath,\n      String rawLockType) throws IOException {\n    if (null == rawLockType) {\n      // we default to \"simple\" for backwards compatibility\n      log.warn(\"No lockType configured for \" + dir + \" assuming 'simple'\");\n      rawLockType = \"simple\";\n    }\n    final String lockType = rawLockType.toLowerCase(Locale.ROOT).trim();\n    \n    if (\"simple\".equals(lockType)) {\n      // multiple SimpleFSLockFactory instances should be OK\n      dir.setLockFactory(new SimpleFSLockFactory(lockPath));\n    } else if (\"native\".equals(lockType)) {\n      dir.setLockFactory(new NativeFSLockFactory(lockPath));\n    } else if (\"single\".equals(lockType)) {\n      if (!(dir.getLockFactory() instanceof SingleInstanceLockFactory)) dir\n          .setLockFactory(new SingleInstanceLockFactory());\n    } else if (\"hdfs\".equals(lockType)) {\n      Directory del = dir;\n      \n      if (dir instanceof NRTCachingDirectory) {\n        del = ((NRTCachingDirectory) del).getDelegate();\n      }\n      \n      if (del instanceof BlockDirectory) {\n        del = ((BlockDirectory) del).getDirectory();\n      }\n      \n      if (!(del instanceof HdfsDirectory)) {\n        throw new SolrException(ErrorCode.FORBIDDEN, \"Directory: \"\n            + del.getClass().getName()\n            + \", but hdfs lock factory can only be used with HdfsDirectory\");\n      }\n\n      dir.setLockFactory(new HdfsLockFactory(((HdfsDirectory)del).getHdfsDirPath(), ((HdfsDirectory)del).getConfiguration()));\n    } else if (\"none\".equals(lockType)) {\n      // Recipe for disaster\n      log.error(\"CONFIGURATION WARNING: locks are disabled on \" + dir);\n      dir.setLockFactory(NoLockFactory.getNoLockFactory());\n    } else {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unrecognized lockType: \" + rawLockType);\n    }\n    return dir;\n  }\n  \n  protected synchronized void removeDirectory(CacheValue cacheValue) throws IOException {\n     // this page intentionally left blank\n  }\n  \n  @Override\n  public String normalize(String path) throws IOException {\n    path = stripTrailingSlash(path);\n    return path;\n  }\n  \n  protected String stripTrailingSlash(String path) {\n    if (path.endsWith(\"/\")) {\n      path = path.substring(0, path.length() - 1);\n    }\n    return path;\n  }\n  \n  /**\n   * Test only method for inspecting the cache\n   * @return paths in the cache which have not been marked \"done\"\n   *\n   * @see #doneWithDirectory\n   * @lucene.internal\n   */\n  public synchronized Set<String> getLivePaths() {\n    HashSet<String> livePaths = new HashSet<String>();\n    for (CacheValue val : byPathCache.values()) {\n      if (!val.doneWithDir) {\n        livePaths.add(val.path);\n      }\n    }\n    return livePaths;\n  }\n}\n",
        "methodName": "exists",
        "exampleID": 32,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java",
        "line": "325",
        "source": "?",
        "sourceLine": "325",
        "qualifier": "Possible null pointer dereference of the $$value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 33
            }
        ],
        "line_number": "325"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/core/CorePropertiesLocator.java",
        "rawCode": "package org.apache.solr.core;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport com.google.common.base.Charsets;\nimport com.google.common.collect.Lists;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.util.IOUtils;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.io.OutputStreamWriter;\nimport java.io.Writer;\nimport java.util.List;\nimport java.util.Properties;\n\n/**\n * Persists CoreDescriptors as properties files\n */\npublic class CorePropertiesLocator implements CoresLocator {\n\n  public static final String PROPERTIES_FILENAME = \"core.properties\";\n\n  private static final Logger logger = LoggerFactory.getLogger(CoresLocator.class);\n\n  private final File rootDirectory;\n\n  public CorePropertiesLocator(String coreDiscoveryRoot) {\n    this.rootDirectory = new File(coreDiscoveryRoot);\n  }\n\n  @Override\n  public void create(CoreContainer cc, CoreDescriptor... coreDescriptors) {\n    for (CoreDescriptor cd : coreDescriptors) {\n      File propFile = new File(new File(cd.getInstanceDir()), PROPERTIES_FILENAME);\n      if (propFile.exists())\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                                \"Could not create a new core in \" + cd.getInstanceDir()\n                              + \"as another core is already defined there\");\n      writePropertiesFile(cd, propFile);\n    }\n  }\n\n  // TODO, this isn't atomic!  If we crash in the middle of a rename, we\n  // could end up with two cores with identical names, in which case one of\n  // them won't start up.  Are we happy with this?\n\n  @Override\n  public void persist(CoreContainer cc, CoreDescriptor... coreDescriptors) {\n    for (CoreDescriptor cd : coreDescriptors) {\n      File propFile = new File(new File(cd.getInstanceDir()), PROPERTIES_FILENAME);\n      writePropertiesFile(cd, propFile);\n    }\n  }\n\n  private void writePropertiesFile(CoreDescriptor cd, File propfile)  {\n    Properties p = buildCoreProperties(cd);\n    Writer os = null;\n    try {\n      propfile.getParentFile().mkdirs();\n      os = new OutputStreamWriter(new FileOutputStream(propfile), Charsets.UTF_8);\n      p.store(os, \"Written by CorePropertiesLocator\");\n    }\n    catch (IOException e) {\n      logger.error(\"Couldn't persist core properties to {}: {}\", propfile.getAbsolutePath(), e);\n    }\n    finally {\n      IOUtils.closeQuietly(os);\n    }\n  }\n\n  @Override\n  public void delete(CoreContainer cc, CoreDescriptor... coreDescriptors) {\n    if (coreDescriptors == null) {\n      return;\n    }\n    for (CoreDescriptor cd : coreDescriptors) {\n      if (cd == null) continue;\n      File instanceDir = new File(cd.getInstanceDir());\n      File propertiesFile = new File(instanceDir, PROPERTIES_FILENAME);\n      propertiesFile.renameTo(new File(instanceDir, PROPERTIES_FILENAME + \".unloaded\"));\n      // This is a best-effort: the core.properties file may already have been\n      // deleted by the core unload, so we don't worry about checking if the\n      // rename has succeeded.\n    }\n  }\n\n  @Override\n  public void rename(CoreContainer cc, CoreDescriptor oldCD, CoreDescriptor newCD) {\n    persist(cc, newCD);\n  }\n\n  @Override\n  public void swap(CoreContainer cc, CoreDescriptor cd1, CoreDescriptor cd2) {\n    persist(cc, cd1, cd2);\n  }\n\n  @Override\n  public List<CoreDescriptor> discover(CoreContainer cc) {\n    logger.info(\"Looking for core definitions underneath {}\", rootDirectory.getAbsolutePath());\n    List<CoreDescriptor> cds = Lists.newArrayList();\n    discoverUnder(rootDirectory, cds, cc);\n    logger.info(\"Found {} core definitions\", cds.size());\n    return cds;\n  }\n\n  private void discoverUnder(File root, List<CoreDescriptor> cds, CoreContainer cc) {\n    if (!root.exists())\n      return;\n    for (File child : root.listFiles()) {\n      File propertiesFile = new File(child, PROPERTIES_FILENAME);\n      if (propertiesFile.exists()) {\n        CoreDescriptor cd = buildCoreDescriptor(propertiesFile, cc);\n        logger.info(\"Found core {} in {}\", cd.getName(), cd.getInstanceDir());\n        cds.add(cd);\n        continue;\n      }\n      if (child.isDirectory())\n        discoverUnder(child, cds, cc);\n    }\n  }\n\n  protected CoreDescriptor buildCoreDescriptor(File propertiesFile, CoreContainer cc) {\n    FileInputStream fis = null;\n    try {\n      File instanceDir = propertiesFile.getParentFile();\n      Properties coreProperties = new Properties();\n      fis = new FileInputStream(propertiesFile);\n      coreProperties.load(new InputStreamReader(fis, Charsets.UTF_8));\n      String name = createName(coreProperties, instanceDir);\n      return new CoreDescriptor(cc, name, instanceDir.getAbsolutePath(), coreProperties);\n    }\n    catch (IOException e) {\n      logger.error(\"Couldn't load core descriptor from {}:{}\", propertiesFile.getAbsolutePath(), e.toString());\n      return null;\n    }\n    finally {\n      IOUtils.closeQuietly(fis);\n    }\n  }\n\n  protected static String createName(Properties p, File instanceDir) {\n    return p.getProperty(CoreDescriptor.CORE_NAME, instanceDir.getName());\n  }\n\n  protected Properties buildCoreProperties(CoreDescriptor cd) {\n    Properties p = new Properties();\n    p.putAll(cd.getPersistableStandardProperties());\n    p.putAll(cd.getPersistableUserProperties());\n    // We don't persist the instance directory, as that's defined by the location\n    // of the properties file.\n    p.remove(CoreDescriptor.CORE_INSTDIR);\n    return p;\n  }\n\n}\n",
        "methodName": "discoverUnder",
        "exampleID": 34,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/core/CorePropertiesLocator.java",
        "line": "130",
        "source": "?",
        "sourceLine": "130",
        "qualifier": "Possible null pointer dereference of the $$the value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 35
            }
        ],
        "line_number": "130"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/core/SolrCore.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.solr.core;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.io.Writer;\nimport java.lang.reflect.Constructor;\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.IdentityHashMap;\nimport java.util.LinkedHashMap;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.Properties;\nimport java.util.Set;\nimport java.util.StringTokenizer;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.locks.ReentrantLock;\n\nimport javax.xml.parsers.ParserConfigurationException;\n\nimport org.apache.commons.io.IOUtils;\nimport org.apache.lucene.codecs.Codec;\nimport org.apache.lucene.index.DirectoryReader;\nimport org.apache.lucene.index.IndexDeletionPolicy;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.search.BooleanQuery;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.IOContext;\nimport org.apache.lucene.store.IndexInput;\nimport org.apache.lucene.store.LockObtainFailedException;\nimport org.apache.solr.cloud.CloudDescriptor;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.cloud.Slice;\nimport org.apache.solr.common.params.CommonParams;\nimport org.apache.solr.common.params.CommonParams.EchoParamStyle;\nimport org.apache.solr.common.params.SolrParams;\nimport org.apache.solr.common.util.ExecutorUtil;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.common.util.SimpleOrderedMap;\nimport org.apache.solr.core.DirectoryFactory.DirContext;\nimport org.apache.solr.handler.SnapPuller;\nimport org.apache.solr.handler.admin.ShowFileRequestHandler;\nimport org.apache.solr.handler.component.AnalyticsComponent;\nimport org.apache.solr.handler.component.DebugComponent;\nimport org.apache.solr.handler.component.FacetComponent;\nimport org.apache.solr.handler.component.HighlightComponent;\nimport org.apache.solr.handler.component.MoreLikeThisComponent;\nimport org.apache.solr.handler.component.QueryComponent;\nimport org.apache.solr.handler.component.RealTimeGetComponent;\nimport org.apache.solr.handler.component.SearchComponent;\nimport org.apache.solr.handler.component.StatsComponent;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.request.SolrRequestHandler;\nimport org.apache.solr.response.BinaryResponseWriter;\nimport org.apache.solr.response.CSVResponseWriter;\nimport org.apache.solr.response.JSONResponseWriter;\nimport org.apache.solr.response.PHPResponseWriter;\nimport org.apache.solr.response.PHPSerializedResponseWriter;\nimport org.apache.solr.response.PythonResponseWriter;\nimport org.apache.solr.response.QueryResponseWriter;\nimport org.apache.solr.response.RawResponseWriter;\nimport org.apache.solr.response.RubyResponseWriter;\nimport org.apache.solr.response.SchemaXmlResponseWriter;\nimport org.apache.solr.response.SolrQueryResponse;\nimport org.apache.solr.response.XMLResponseWriter;\nimport org.apache.solr.response.transform.TransformerFactory;\nimport org.apache.solr.schema.FieldType;\nimport org.apache.solr.schema.IndexSchema;\nimport org.apache.solr.schema.IndexSchemaFactory;\nimport org.apache.solr.schema.SimilarityFactory;\nimport org.apache.solr.search.QParserPlugin;\nimport org.apache.solr.search.SolrFieldCacheMBean;\nimport org.apache.solr.search.SolrIndexSearcher;\nimport org.apache.solr.search.ValueSourceParser;\nimport org.apache.solr.update.DefaultSolrCoreState;\nimport org.apache.solr.update.DirectUpdateHandler2;\nimport org.apache.solr.update.SolrCoreState;\nimport org.apache.solr.update.SolrCoreState.IndexWriterCloser;\nimport org.apache.solr.update.SolrIndexWriter;\nimport org.apache.solr.update.UpdateHandler;\nimport org.apache.solr.update.VersionInfo;\nimport org.apache.solr.update.processor.DistributedUpdateProcessorFactory;\nimport org.apache.solr.update.processor.LogUpdateProcessorFactory;\nimport org.apache.solr.update.processor.RunUpdateProcessorFactory;\nimport org.apache.solr.update.processor.UpdateRequestProcessorChain;\nimport org.apache.solr.update.processor.UpdateRequestProcessorFactory;\nimport org.apache.solr.util.DefaultSolrThreadFactory;\nimport org.apache.solr.util.PropertiesInputStream;\nimport org.apache.solr.util.RefCounted;\nimport org.apache.solr.util.plugin.NamedListInitializedPlugin;\nimport org.apache.solr.util.plugin.PluginInfoInitialized;\nimport org.apache.solr.util.plugin.SolrCoreAware;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.xml.sax.SAXException;\n\n\n/**\n *\n */\npublic final class SolrCore implements SolrInfoMBean {\n  public static final String version=\"1.0\";  \n\n  // These should *only* be used for debugging or monitoring purposes\n  public static final AtomicLong numOpens = new AtomicLong();\n  public static final AtomicLong numCloses = new AtomicLong();\n  public static Map<SolrCore,Exception> openHandles = Collections.synchronizedMap(new IdentityHashMap<SolrCore, Exception>());\n\n  \n  public static Logger log = LoggerFactory.getLogger(SolrCore.class);\n\n  private String name;\n  private String logid; // used to show what name is set\n  private CoreDescriptor coreDescriptor;\n\n  private boolean isReloaded = false;\n\n  private final SolrConfig solrConfig;\n  private final SolrResourceLoader resourceLoader;\n  private volatile IndexSchema schema;\n  private final String dataDir;\n  private final UpdateHandler updateHandler;\n  private final SolrCoreState solrCoreState;\n  \n  private final long startTime;\n  private final RequestHandlers reqHandlers;\n  private final Map<String,SearchComponent> searchComponents;\n  private final Map<String,UpdateRequestProcessorChain> updateProcessorChains;\n  private final Map<String, SolrInfoMBean> infoRegistry;\n  private IndexDeletionPolicyWrapper solrDelPolicy;\n  private DirectoryFactory directoryFactory;\n  private IndexReaderFactory indexReaderFactory;\n  private final Codec codec;\n\n  private final ReentrantLock ruleExpiryLock;\n\n  public long getStartTime() { return startTime; }\n\n  static int boolean_query_max_clause_count = Integer.MIN_VALUE;\n  // only change the BooleanQuery maxClauseCount once for ALL cores...\n  void booleanQueryMaxClauseCount()  {\n    synchronized(SolrCore.class) {\n      if (boolean_query_max_clause_count == Integer.MIN_VALUE) {\n        boolean_query_max_clause_count = solrConfig.booleanQueryMaxClauseCount;\n        BooleanQuery.setMaxClauseCount(boolean_query_max_clause_count);\n      } else if (boolean_query_max_clause_count != solrConfig.booleanQueryMaxClauseCount ) {\n        log.debug(\"BooleanQuery.maxClauseCount= \" +boolean_query_max_clause_count+ \", ignoring \" +solrConfig.booleanQueryMaxClauseCount);\n      }\n    }\n  }\n\n  \n  /**\n   * The SolrResourceLoader used to load all resources for this core.\n   * @since solr 1.3\n   */\n  public SolrResourceLoader getResourceLoader() {\n    return resourceLoader;\n  }\n\n  /**\n   * Gets the configuration resource name used by this core instance.\n   * @since solr 1.3\n   */\n  public String getConfigResource() {\n    return solrConfig.getResourceName();\n  }\n\n  /**\n   * Gets the configuration object used by this core instance.\n   */\n  public SolrConfig getSolrConfig() {\n    return solrConfig;\n  }\n  \n  /**\n   * Gets the schema resource name used by this core instance.\n   * @since solr 1.3\n   */\n  public String getSchemaResource() {\n    return getLatestSchema().getResourceName();\n  }\n\n  /** @return the latest snapshot of the schema used by this core instance. */\n  public IndexSchema getLatestSchema() { \n    return schema;\n  }\n  \n  /** Sets the latest schema snapshot to be used by this core instance. */\n  public void setLatestSchema(IndexSchema replacementSchema) {\n    schema = replacementSchema;\n  }\n  \n  public String getDataDir() {\n    return dataDir;\n  }\n\n  public String getIndexDir() {\n    synchronized (searcherLock) {\n      if (_searcher == null) return getNewIndexDir();\n      SolrIndexSearcher searcher = _searcher.get();\n      return searcher.getPath() == null ? dataDir + \"index/\" : searcher\n          .getPath();\n    }\n  }\n\n\n  /**\n   * Returns the indexdir as given in index.properties. If index.properties exists in dataDir and\n   * there is a property <i>index</i> available and it points to a valid directory\n   * in dataDir that is returned Else dataDir/index is returned. Only called for creating new indexSearchers\n   * and indexwriters. Use the getIndexDir() method to know the active index directory\n   *\n   * @return the indexdir as given in index.properties\n   */\n  public String getNewIndexDir() {\n    String result = dataDir + \"index/\";\n    Properties p = new Properties();\n    Directory dir = null;\n    try {\n      dir = getDirectoryFactory().get(getDataDir(), DirContext.META_DATA, getSolrConfig().indexConfig.lockType);\n      if (dir.fileExists(SnapPuller.INDEX_PROPERTIES)){\n        final IndexInput input = dir.openInput(SnapPuller.INDEX_PROPERTIES, IOContext.DEFAULT);\n  \n        final InputStream is = new PropertiesInputStream(input);\n        try {\n          p.load(new InputStreamReader(is, \"UTF-8\"));\n          \n          String s = p.getProperty(\"index\");\n          if (s != null && s.trim().length() > 0) {\n              result = dataDir + s;\n          }\n          \n        } catch (Exception e) {\n          log.error(\"Unable to load \" + SnapPuller.INDEX_PROPERTIES, e);\n        } finally {\n          IOUtils.closeQuietly(is);\n        }\n      }\n    } catch (IOException e) {\n      SolrException.log(log, \"\", e);\n    } finally {\n      if (dir != null) {\n        try {\n          getDirectoryFactory().release(dir);\n        } catch (IOException e) {\n          SolrException.log(log, \"\", e);\n        }\n      }\n    }\n    if (!result.equals(lastNewIndexDir)) {\n      log.info(\"New index directory detected: old=\"+lastNewIndexDir + \" new=\" + result);\n    }\n    lastNewIndexDir = result;\n    return result;\n  }\n  private String lastNewIndexDir; // for debugging purposes only... access not synchronized, but that's ok\n\n  \n  public DirectoryFactory getDirectoryFactory() {\n    return directoryFactory;\n  }\n  \n  public IndexReaderFactory getIndexReaderFactory() {\n    return indexReaderFactory;\n  }\n  \n  @Override\n  public String getName() {\n    return name;\n  }\n\n  public void setName(String v) {\n    this.name = v;\n    this.logid = (v==null)?\"\":(\"[\"+v+\"] \");\n    this.coreDescriptor = new CoreDescriptor(v, this.coreDescriptor);\n  }\n\n  public String getLogId()\n  {\n    return this.logid;\n  }\n\n  /**\n   * Returns a Map of name vs SolrInfoMBean objects. The returned map is an instance of\n   * a ConcurrentHashMap and therefore no synchronization is needed for putting, removing\n   * or iterating over it.\n   *\n   * @return the Info Registry map which contains SolrInfoMBean objects keyed by name\n   * @since solr 1.3\n   */\n  public Map<String, SolrInfoMBean> getInfoRegistry() {\n    return infoRegistry;\n  }\n\n   private void initDeletionPolicy() {\n     PluginInfo info = solrConfig.getPluginInfo(IndexDeletionPolicy.class.getName());\n     IndexDeletionPolicy delPolicy = null;\n     if(info != null){\n       delPolicy = createInstance(info.className,IndexDeletionPolicy.class,\"Deletion Policy for SOLR\");\n       if (delPolicy instanceof NamedListInitializedPlugin) {\n         ((NamedListInitializedPlugin) delPolicy).init(info.initArgs);\n       }\n     } else {\n       delPolicy = new SolrDeletionPolicy();\n     }     \n     solrDelPolicy = new IndexDeletionPolicyWrapper(delPolicy);\n   }\n\n  private void initListeners() {\n    final Class<SolrEventListener> clazz = SolrEventListener.class;\n    final String label = \"Event Listener\";\n    for (PluginInfo info : solrConfig.getPluginInfos(SolrEventListener.class.getName())) {\n      String event = info.attributes.get(\"event\");\n      if(\"firstSearcher\".equals(event) ){\n        SolrEventListener obj = createInitInstance(info,clazz,label,null);\n        firstSearcherListeners.add(obj);\n        log.info(logid + \"Added SolrEventListener for firstSearcher: \" + obj);\n      } else if(\"newSearcher\".equals(event) ){\n        SolrEventListener obj = createInitInstance(info,clazz,label,null);\n        newSearcherListeners.add(obj);\n        log.info(logid + \"Added SolrEventListener for newSearcher: \" + obj);\n      }\n    }\n  }\n\n  final List<SolrEventListener> firstSearcherListeners = new ArrayList<SolrEventListener>();\n  final List<SolrEventListener> newSearcherListeners = new ArrayList<SolrEventListener>();\n\n  /**\n   * NOTE: this function is not thread safe.  However, it is safe to call within the\n   * <code>inform( SolrCore core )</code> function for <code>SolrCoreAware</code> classes.\n   * Outside <code>inform</code>, this could potentially throw a ConcurrentModificationException\n   * \n   * @see SolrCoreAware\n   */\n  public void registerFirstSearcherListener( SolrEventListener listener )\n  {\n    firstSearcherListeners.add( listener );\n  }\n\n  /**\n   * NOTE: this function is not thread safe.  However, it is safe to call within the\n   * <code>inform( SolrCore core )</code> function for <code>SolrCoreAware</code> classes.\n   * Outside <code>inform</code>, this could potentially throw a ConcurrentModificationException\n   * \n   * @see SolrCoreAware\n   */\n  public void registerNewSearcherListener( SolrEventListener listener )\n  {\n    newSearcherListeners.add( listener );\n  }\n\n  /**\n   * NOTE: this function is not thread safe.  However, it is safe to call within the\n   * <code>inform( SolrCore core )</code> function for <code>SolrCoreAware</code> classes.\n   * Outside <code>inform</code>, this could potentially throw a ConcurrentModificationException\n   * \n   * @see SolrCoreAware\n   */\n  public QueryResponseWriter registerResponseWriter( String name, QueryResponseWriter responseWriter ){\n    return responseWriters.put(name, responseWriter);\n  }\n  \n  public SolrCore reload(SolrCore prev) throws IOException,\n      ParserConfigurationException, SAXException {\n    return reload(prev.getResourceLoader(), prev);\n  }\n  \n  public SolrCore reload(SolrResourceLoader resourceLoader, SolrCore prev) throws IOException,\n      ParserConfigurationException, SAXException {\n    \n    SolrConfig config = new SolrConfig(resourceLoader, getSolrConfig().getName(), null);\n    \n    IndexSchema schema = IndexSchemaFactory.buildIndexSchema(getLatestSchema().getResourceName(), config);\n    \n    solrCoreState.increfSolrCoreState();\n    \n    if (!getNewIndexDir().equals(getIndexDir())) {\n      // the directory is changing, don't pass on state\n      prev = null;\n    }\n    \n    SolrCore core = new SolrCore(getName(), getDataDir(), config,\n        schema, coreDescriptor, updateHandler, this.solrDelPolicy, prev);\n    core.solrDelPolicy = this.solrDelPolicy;\n    \n    core.getUpdateHandler().getSolrCoreState().newIndexWriter(core, false);\n    \n    core.getSearcher(true, false, null, true);\n    \n    return core;\n  }\n\n\n  // gets a non-caching searcher\n  public SolrIndexSearcher newSearcher(String name) throws IOException {\n    return new SolrIndexSearcher(this, getNewIndexDir(), getLatestSchema(), getSolrConfig().indexConfig, \n                                 name, false, directoryFactory);\n  }\n\n\n   private void initDirectoryFactory() {\n    DirectoryFactory dirFactory;\n    PluginInfo info = solrConfig.getPluginInfo(DirectoryFactory.class.getName());\n    if (info != null) {\n      log.info(info.className);\n      dirFactory = getResourceLoader().newInstance(info.className, DirectoryFactory.class);\n      dirFactory.init(info.initArgs);\n    } else {\n      log.info(\"solr.NRTCachingDirectoryFactory\");\n      dirFactory = new NRTCachingDirectoryFactory();\n    }\n    // And set it\n    directoryFactory = dirFactory;\n  }\n\n  private void initIndexReaderFactory() {\n    IndexReaderFactory indexReaderFactory;\n    PluginInfo info = solrConfig.getPluginInfo(IndexReaderFactory.class.getName());\n    if (info != null) {\n      indexReaderFactory = resourceLoader.newInstance(info.className, IndexReaderFactory.class);\n      indexReaderFactory.init(info.initArgs);\n    } else {\n      indexReaderFactory = new StandardIndexReaderFactory();\n    } \n    this.indexReaderFactory = indexReaderFactory;\n  }\n  \n  // protect via synchronized(SolrCore.class)\n  private static Set<String> dirs = new HashSet<String>();\n\n  void initIndex(boolean reload) throws IOException {\n \n      String indexDir = getNewIndexDir();\n      boolean indexExists = getDirectoryFactory().exists(indexDir);\n      boolean firstTime;\n      synchronized (SolrCore.class) {\n        firstTime = dirs.add(getDirectoryFactory().normalize(indexDir));\n      }\n      boolean removeLocks = solrConfig.unlockOnStartup;\n\n      initIndexReaderFactory();\n\n      if (indexExists && firstTime && !reload) {\n        \n        Directory dir = directoryFactory.get(indexDir, DirContext.DEFAULT,\n            getSolrConfig().indexConfig.lockType);\n        try {\n          if (IndexWriter.isLocked(dir)) {\n            if (removeLocks) {\n              log.warn(\n                  logid\n                      + \"WARNING: Solr index directory '{}' is locked.  Unlocking...\",\n                  indexDir);\n              IndexWriter.unlock(dir);\n            } else {\n              log.error(logid\n                  + \"Solr index directory '{}' is locked.  Throwing exception\",\n                  indexDir);\n              throw new LockObtainFailedException(\n                  \"Index locked for write for core \" + name);\n            }\n            \n          }\n        } finally {\n          directoryFactory.release(dir);\n        }\n      }\n\n      // Create the index if it doesn't exist.\n      if(!indexExists) {\n        log.warn(logid+\"Solr index directory '\" + new File(indexDir) + \"' doesn't exist.\"\n                + \" Creating new index...\");\n\n        SolrIndexWriter writer = SolrIndexWriter.create(\"SolrCore.initIndex\", indexDir, getDirectoryFactory(), true, \n                                                        getLatestSchema(), solrConfig.indexConfig, solrDelPolicy, codec);\n        writer.close();\n      }\n\n \n  }\n\n  /** Creates an instance by trying a constructor that accepts a SolrCore before\n   *  trying the default (no arg) constructor.\n   *@param className the instance class to create\n   *@param cast the class or interface that the instance should extend or implement\n   *@param msg a message helping compose the exception error if any occurs.\n   *@return the desired instance\n   *@throws SolrException if the object could not be instantiated\n   */\n  private <T> T createInstance(String className, Class<T> cast, String msg) {\n    Class<? extends T> clazz = null;\n    if (msg == null) msg = \"SolrCore Object\";\n    try {\n        clazz = getResourceLoader().findClass(className, cast);\n        //most of the classes do not have constructors which takes SolrCore argument. It is recommended to obtain SolrCore by implementing SolrCoreAware.\n        // So invariably always it will cause a  NoSuchMethodException. So iterate though the list of available constructors\n        Constructor<?>[] cons =  clazz.getConstructors();\n        for (Constructor<?> con : cons) {\n          Class<?>[] types = con.getParameterTypes();\n          if(types.length == 1 && types[0] == SolrCore.class){\n            return cast.cast(con.newInstance(this));\n          }\n        }\n        return getResourceLoader().newInstance(className, cast);//use the empty constructor\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      // The JVM likes to wrap our helpful SolrExceptions in things like\n      // \"InvocationTargetException\" that have no useful getMessage\n      if (null != e.getCause() && e.getCause() instanceof SolrException) {\n        SolrException inner = (SolrException) e.getCause();\n        throw inner;\n      }\n\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\"Error Instantiating \"+msg+\", \"+className+ \" failed to instantiate \" +cast.getName(), e);\n    }\n  }\n  \n  private UpdateHandler createReloadedUpdateHandler(String className, String msg, UpdateHandler updateHandler) {\n    Class<? extends UpdateHandler> clazz = null;\n    if (msg == null) msg = \"SolrCore Object\";\n    try {\n        clazz = getResourceLoader().findClass(className, UpdateHandler.class);\n        //most of the classes do not have constructors which takes SolrCore argument. It is recommended to obtain SolrCore by implementing SolrCoreAware.\n        // So invariably always it will cause a  NoSuchMethodException. So iterate though the list of available constructors\n        Constructor<?>[] cons =  clazz.getConstructors();\n        for (Constructor<?> con : cons) {\n          Class<?>[] types = con.getParameterTypes();\n          if(types.length == 2 && types[0] == SolrCore.class && types[1] == UpdateHandler.class){\n            return UpdateHandler.class.cast(con.newInstance(this, updateHandler));\n          } \n        }\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\"Error Instantiating \"+msg+\", \"+className+ \" could not find proper constructor for \" + UpdateHandler.class.getName());\n    } catch (SolrException e) {\n      throw e;\n    } catch (Exception e) {\n      // The JVM likes to wrap our helpful SolrExceptions in things like\n      // \"InvocationTargetException\" that have no useful getMessage\n      if (null != e.getCause() && e.getCause() instanceof SolrException) {\n        SolrException inner = (SolrException) e.getCause();\n        throw inner;\n      }\n\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\"Error Instantiating \"+msg+\", \"+className+ \" failed to instantiate \" + UpdateHandler.class.getName(), e);\n    }\n  }\n\n  public <T extends Object> T createInitInstance(PluginInfo info,Class<T> cast, String msg, String defClassName){\n    if(info == null) return null;\n    T o = createInstance(info.className == null ? defClassName : info.className,cast, msg);\n    if (o instanceof PluginInfoInitialized) {\n      ((PluginInfoInitialized) o).init(info);\n    } else if (o instanceof NamedListInitializedPlugin) {\n      ((NamedListInitializedPlugin) o).init(info.initArgs);\n    }\n    if(o instanceof SearchComponent) {\n      ((SearchComponent) o).setName(info.name);\n    }\n    return o;\n  }\n\n  public SolrEventListener createEventListener(String className) {\n    return createInstance(className, SolrEventListener.class, \"Event Listener\");\n  }\n\n  public SolrRequestHandler createRequestHandler(String className) {\n    return createInstance(className, SolrRequestHandler.class, \"Request Handler\");\n  }\n\n  private UpdateHandler createUpdateHandler(String className) {\n    return createInstance(className, UpdateHandler.class, \"Update Handler\");\n  }\n  \n  private UpdateHandler createUpdateHandler(String className, UpdateHandler updateHandler) {\n    return createReloadedUpdateHandler(className, \"Update Handler\", updateHandler);\n  }\n\n  private QueryResponseWriter createQueryResponseWriter(String className) {\n    return createInstance(className, QueryResponseWriter.class, \"Query Response Writer\");\n  }\n  \n  /**\n   * Creates a new core and register it in the list of cores.\n   * If a core with the same name already exists, it will be stopped and replaced by this one.\n   *\n   * @param dataDir the index directory\n   * @param config a solr config instance\n   * @param schema a solr schema instance\n   *\n   * @since solr 1.3\n   */\n  public SolrCore(String name, String dataDir, SolrConfig config, IndexSchema schema, CoreDescriptor cd) {\n    this(name, dataDir, config, schema, cd, null, null, null);\n  }\n\n\n  /**\n   * Creates a new core that is to be loaded lazily. i.e. lazyLoad=\"true\" in solr.xml\n   * @since solr 4.1\n   */\n  public SolrCore(String name, CoreDescriptor cd) {\n    coreDescriptor = cd;\n    this.setName(name);\n    this.schema = null;\n    this.dataDir = null;\n    this.solrConfig = null;\n    this.startTime = System.currentTimeMillis();\n    this.maxWarmingSearchers = 2;  // we don't have a config yet, just pick a number.\n    this.resourceLoader = null;\n    this.updateHandler = null;\n    this.isReloaded = true;\n    this.reqHandlers = null;\n    this.searchComponents = null;\n    this.updateProcessorChains = null;\n    this.infoRegistry = null;\n    this.codec = null;\n    this.ruleExpiryLock = null;\n\n    solrCoreState = null;\n  }\n  /**\n   * Creates a new core and register it in the list of cores.\n   * If a core with the same name already exists, it will be stopped and replaced by this one.\n   *@param dataDir the index directory\n   *@param config a solr config instance\n   *@param schema a solr schema instance\n   *\n   *@since solr 1.3\n   */\n  public SolrCore(String name, String dataDir, SolrConfig config, IndexSchema schema, CoreDescriptor cd, UpdateHandler updateHandler, IndexDeletionPolicyWrapper delPolicy, SolrCore prev) {\n    coreDescriptor = cd;\n    this.setName( name );\n    resourceLoader = config.getResourceLoader();\n    this.solrConfig = config;\n    \n    if (updateHandler == null) {\n      initDirectoryFactory();\n    }\n    \n    if (dataDir == null) {\n      if (cd.usingDefaultDataDir()) dataDir = config.getDataDir();\n      if (dataDir == null) {\n        try {\n          dataDir = cd.getDataDir();\n          if (!directoryFactory.isAbsolute(dataDir)) {\n            dataDir = directoryFactory.getDataHome(cd);\n          }\n        } catch (IOException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, null, e);\n        }\n      }\n    }\n\n    dataDir = SolrResourceLoader.normalizeDir(dataDir);\n    log.info(logid+\"Opening new SolrCore at \" + resourceLoader.getInstanceDir() + \", dataDir=\"+dataDir);\n\n    if (null != cd && null != cd.getCloudDescriptor()) {\n      // we are evidently running in cloud mode.  \n      //\n      // In cloud mode, version field is required for correct consistency\n      // ideally this check would be more fine grained, and individual features\n      // would assert it when they initialize, but DistributedUpdateProcessor\n      // is currently a big ball of wax that does more then just distributing\n      // updates (ie: partial document updates), so it needs to work in no cloud\n      // mode as well, and can't assert version field support on init.\n\n      try {\n        VersionInfo.getAndCheckVersionField(schema);\n      } catch (SolrException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                                \"Schema will not work with SolrCloud mode: \" +\n                                e.getMessage(), e);\n      }\n    }\n\n    //Initialize JMX\n    if (config.jmxConfig.enabled) {\n      infoRegistry = new JmxMonitoredMap<String, SolrInfoMBean>(name, String.valueOf(this.hashCode()), config.jmxConfig);\n    } else  {\n      log.info(\"JMX monitoring not detected for core: \" + name);\n      infoRegistry = new ConcurrentHashMap<String, SolrInfoMBean>();\n    }\n\n    infoRegistry.put(\"fieldCache\", new SolrFieldCacheMBean());\n\n    if (schema==null) {\n      schema = IndexSchemaFactory.buildIndexSchema(IndexSchema.DEFAULT_SCHEMA_FILE, config);\n    }\n    this.schema = schema;\n    final SimilarityFactory similarityFactory = schema.getSimilarityFactory(); \n    if (similarityFactory instanceof SolrCoreAware) {\n      // Similarity needs SolrCore before inform() is called on all registered SolrCoreAware listeners below\n      ((SolrCoreAware)similarityFactory).inform(this);\n    }\n\n    this.dataDir = dataDir;\n    this.startTime = System.currentTimeMillis();\n    this.maxWarmingSearchers = config.maxWarmingSearchers;\n\n    booleanQueryMaxClauseCount();\n  \n    final CountDownLatch latch = new CountDownLatch(1);\n\n    try {\n      \n      initListeners();\n      \n      if (delPolicy == null) {\n        initDeletionPolicy();\n      } else {\n        this.solrDelPolicy = delPolicy;\n      }\n      \n      this.codec = initCodec(solrConfig, schema);\n      \n      if (updateHandler == null) {\n        solrCoreState = new DefaultSolrCoreState(getDirectoryFactory());\n      } else {\n        solrCoreState = updateHandler.getSolrCoreState();\n        directoryFactory = solrCoreState.getDirectoryFactory();\n        this.isReloaded = true;\n      }\n      \n      initIndex(prev != null);\n      \n      initWriters();\n      initQParsers();\n      initValueSourceParsers();\n      initTransformerFactories();\n      \n      this.searchComponents = Collections\n          .unmodifiableMap(loadSearchComponents());\n      \n      // Processors initialized before the handlers\n      updateProcessorChains = loadUpdateProcessorChains();\n      reqHandlers = new RequestHandlers(this);\n      reqHandlers.initHandlersFromConfig(solrConfig);\n\n      // Handle things that should eventually go away\n      initDeprecatedSupport();\n      \n      // cause the executor to stall so firstSearcher events won't fire\n      // until after inform() has been called for all components.\n      // searchExecutor must be single-threaded for this to work\n      searcherExecutor.submit(new Callable<Void>() {\n        @Override\n        public Void call() throws Exception {\n          latch.await();\n          return null;\n        }\n      });\n      \n      // use the (old) writer to open the first searcher\n      RefCounted<IndexWriter> iwRef = null;\n      if (prev != null) {\n        iwRef = prev.getUpdateHandler().getSolrCoreState().getIndexWriter(null);\n        if (iwRef != null) {\n          final IndexWriter iw = iwRef.get();\n          final SolrCore core = this;\n          newReaderCreator = new Callable<DirectoryReader>() {\n            // this is used during a core reload\n\n            @Override\n            public DirectoryReader call() throws Exception {\n              if(getSolrConfig().nrtMode) {\n                // if in NRT mode, need to open from the previous writer\n                return indexReaderFactory.newReader(iw, core);\n              } else {\n                // if not NRT, need to create a new reader from the directory\n                return indexReaderFactory.newReader(iw.getDirectory(), core);\n              }\n            }\n          };\n        }\n      }\n      \n      String updateHandlerClass = solrConfig.getUpdateHandlerInfo().className;\n      \n      if (updateHandler == null) {\n        this.updateHandler = createUpdateHandler(updateHandlerClass == null ? DirectUpdateHandler2.class\n            .getName() : updateHandlerClass);\n      } else {\n        this.updateHandler = createUpdateHandler(\n            updateHandlerClass == null ? DirectUpdateHandler2.class.getName()\n                : updateHandlerClass, updateHandler);\n      }\n      infoRegistry.put(\"updateHandler\", this.updateHandler);\n\n      try {\n        getSearcher(false, false, null, true);\n      } finally {\n        newReaderCreator = null;\n        if (iwRef != null) iwRef.decref();\n      }\n      \n      // Finally tell anyone who wants to know\n      resourceLoader.inform(resourceLoader);\n      resourceLoader.inform(this); // last call before the latch is released.\n    } catch (Throwable e) {\n      latch.countDown();//release the latch, otherwise we block trying to do the close.  This should be fine, since counting down on a latch of 0 is still fine\n      //close down the searcher and any other resources, if it exists, as this is not recoverable\n      close();\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \n                              e.getMessage(), e);\n    } finally {\n      // allow firstSearcher events to fire and make sure it is released\n      latch.countDown();\n    }\n\n    infoRegistry.put(\"core\", this);\n    \n    // register any SolrInfoMBeans SolrResourceLoader initialized\n    //\n    // this must happen after the latch is released, because a JMX server impl may\n    // choose to block on registering until properties can be fetched from an MBean,\n    // and a SolrCoreAware MBean may have properties that depend on getting a Searcher\n    // from the core.\n    resourceLoader.inform(infoRegistry);\n    \n    CoreContainer cc = cd.getCoreContainer();\n\n    if (cc != null && cc.isZooKeeperAware()) {\n      SolrRequestHandler realtimeGetHandler = reqHandlers.get(\"/get\");\n      if (realtimeGetHandler == null) {\n        log.warn(\"WARNING: RealTimeGetHandler is not registered at /get. \" +\n            \"SolrCloud will always use full index replication instead of the more efficient PeerSync method.\");\n      }\n\n      if (Slice.CONSTRUCTION.equals(cd.getCloudDescriptor().getShardState())) {\n        // set update log to buffer before publishing the core\n        getUpdateHandler().getUpdateLog().bufferUpdates();\n\n        cd.getCloudDescriptor().setShardState(null);\n        cd.getCloudDescriptor().setShardRange(null);\n        cd.getCloudDescriptor().setShardParent(null);\n      }\n    }\n    // For debugging   \n//    numOpens.incrementAndGet();\n//    openHandles.put(this, new RuntimeException(\"unclosed core - name:\" + getName() + \" refs: \" + refCount.get()));\n\n    ruleExpiryLock = new ReentrantLock();\n  }\n    \n  private Codec initCodec(SolrConfig solrConfig, final IndexSchema schema) {\n    final PluginInfo info = solrConfig.getPluginInfo(CodecFactory.class.getName());\n    final CodecFactory factory;\n    if (info != null) {\n      factory = schema.getResourceLoader().newInstance(info.className, CodecFactory.class);\n      factory.init(info.initArgs);\n    } else {\n      factory = new CodecFactory() {\n        @Override\n        public Codec getCodec() {\n          return Codec.getDefault();\n        }\n      };\n    }\n    if (factory instanceof SolrCoreAware) {\n      // CodecFactory needs SolrCore before inform() is called on all registered\n      // SolrCoreAware listeners, at the end of the SolrCore constructor\n      ((SolrCoreAware)factory).inform(this);\n    } else {\n      for (FieldType ft : schema.getFieldTypes().values()) {\n        if (null != ft.getPostingsFormat()) {\n          String msg = \"FieldType '\" + ft.getTypeName() + \"' is configured with a postings format, but the codec does not support it: \" + factory.getClass();\n          log.error(msg);\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, msg);\n        }\n        if (null != ft.getDocValuesFormat()) {\n          String msg = \"FieldType '\" + ft.getTypeName() + \"' is configured with a docValues format, but the codec does not support it: \" + factory.getClass();\n          log.error(msg);\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, msg);\n        }\n      }\n    }\n    return factory.getCodec();\n  }\n\n  /**\n   * Load the request processors\n   */\n   private Map<String,UpdateRequestProcessorChain> loadUpdateProcessorChains() {\n    Map<String, UpdateRequestProcessorChain> map = new HashMap<String, UpdateRequestProcessorChain>();\n    UpdateRequestProcessorChain def = initPlugins(map,UpdateRequestProcessorChain.class, UpdateRequestProcessorChain.class.getName());\n    if(def == null){\n      def = map.get(null);\n    } \n    if (def == null) {\n      log.info(\"no updateRequestProcessorChain defined as default, creating implicit default\");\n      // construct the default chain\n      UpdateRequestProcessorFactory[] factories = new UpdateRequestProcessorFactory[]{\n              new LogUpdateProcessorFactory(),\n              new DistributedUpdateProcessorFactory(),\n              new RunUpdateProcessorFactory()\n      };\n      def = new UpdateRequestProcessorChain(factories, this);\n    }\n    map.put(null, def);\n    map.put(\"\", def);\n    return map;\n  }\n   \n  public SolrCoreState getSolrCoreState() {\n    return solrCoreState;\n  }  \n\n  /**\n   * @return an update processor registered to the given name.  Throw an exception if this chain is undefined\n   */    \n  public UpdateRequestProcessorChain getUpdateProcessingChain( final String name )\n  {\n    UpdateRequestProcessorChain chain = updateProcessorChains.get( name );\n    if( chain == null ) {\n      throw new SolrException( SolrException.ErrorCode.BAD_REQUEST,\n          \"unknown UpdateRequestProcessorChain: \"+name );\n    }\n    return chain;\n  }\n  \n  // this core current usage count\n  private final AtomicInteger refCount = new AtomicInteger(1);\n\n  /** expert: increments the core reference count */\n  public void open() {\n    refCount.incrementAndGet();\n  }\n  \n  /**\n   * Close all resources allocated by the core if it is no longer in use...\n   * <ul>\n   *   <li>searcher</li>\n   *   <li>updateHandler</li>\n   *   <li>all CloseHooks will be notified</li>\n   *   <li>All MBeans will be unregistered from MBeanServer if JMX was enabled\n   *       </li>\n   * </ul>\n   * <p>   \n   * <p>\n   * The behavior of this method is determined by the result of decrementing\n   * the core's reference count (A core is created with a reference count of 1)...\n   * </p>\n   * <ul>\n   *   <li>If reference count is > 0, the usage count is decreased by 1 and no\n   *       resources are released.\n   *   </li>\n   *   <li>If reference count is == 0, the resources are released.\n   *   <li>If reference count is &lt; 0, and error is logged and no further action\n   *       is taken.\n   *   </li>\n   * </ul>\n   * @see #isClosed() \n   */\n  public void close() {\n    int count = refCount.decrementAndGet();\n    if (count > 0) return; // close is called often, and only actually closes if nothing is using it.\n    if (count < 0) {\n      log.error(\"Too many close [count:{}] on {}. Please report this exception to solr-user@lucene.apache.org\", count, this );\n      assert false : \"Too many closes on SolrCore\";\n      return;\n    }\n    log.info(logid+\" CLOSING SolrCore \" + this);\n\n\n    if( closeHooks != null ) {\n       for( CloseHook hook : closeHooks ) {\n         try {\n           hook.preClose( this );\n         } catch (Throwable e) {\n           SolrException.log(log, e);           \n         }\n      }\n    }\n\n\n    try {\n      infoRegistry.clear();\n    } catch (Throwable e) {\n      SolrException.log(log, e);\n    }\n\n    try {\n      if (null != updateHandler) {\n        updateHandler.close();\n      }\n    } catch (Throwable e) {\n      SolrException.log(log,e);\n    }\n    \n    boolean coreStateClosed = false;\n    try {\n      if (solrCoreState != null) {\n        if (updateHandler instanceof IndexWriterCloser) {\n          coreStateClosed = solrCoreState.decrefSolrCoreState((IndexWriterCloser) updateHandler);\n        } else {\n          coreStateClosed = solrCoreState.decrefSolrCoreState(null);\n        }\n      }\n    } catch (Throwable e) {\n      SolrException.log(log, e);\n    }\n    \n    try {\n      ExecutorUtil.shutdownAndAwaitTermination(searcherExecutor);\n    } catch (Throwable e) {\n      SolrException.log(log, e);\n    }\n\n    try {\n      // Since we waited for the searcherExecutor to shut down,\n      // there should be no more searchers warming in the background\n      // that we need to take care of.\n      //\n      // For the case that a searcher was registered *before* warming\n      // then the searchExecutor will throw an exception when getSearcher()\n      // tries to use it, and the exception handling code should close it.\n      closeSearcher();\n    } catch (Throwable e) {\n      SolrException.log(log,e);\n    }\n    \n    if (coreStateClosed) {\n      \n      try {\n        directoryFactory.close();\n      } catch (Throwable t) {\n        SolrException.log(log, t);\n      }\n      \n    }\n\n    \n    if( closeHooks != null ) {\n       for( CloseHook hook : closeHooks ) {\n         try {\n           hook.postClose( this );\n         } catch (Throwable e) {\n           SolrException.log(log, e);\n         }\n      }\n    }\n    \n    // For debugging \n//    numCloses.incrementAndGet();\n//    openHandles.remove(this);\n  }\n\n  /** Current core usage count. */\n  public int getOpenCount() {\n    return refCount.get();\n  }\n  \n  /** Whether this core is closed. */\n  public boolean isClosed() {\n      return refCount.get() <= 0;\n  }\n  \n  @Override\n  protected void finalize() throws Throwable {\n    try {\n      if (getOpenCount() != 0) {\n        log.error(\"REFCOUNT ERROR: unreferenced \" + this + \" (\" + getName()\n            + \") has a reference count of \" + getOpenCount());\n      }\n    } finally {\n      super.finalize();\n    }\n  }\n\n  private Collection<CloseHook> closeHooks = null;\n\n   /**\n    * Add a close callback hook\n    */\n   public void addCloseHook( CloseHook hook )\n   {\n     if( closeHooks == null ) {\n       closeHooks = new ArrayList<CloseHook>();\n     }\n     closeHooks.add( hook );\n   }\n\n  /** @lucene.internal\n   *  Debugging aid only.  No non-test code should be released with uncommented verbose() calls.  */\n  public static boolean VERBOSE = Boolean.parseBoolean(System.getProperty(\"tests.verbose\",\"false\"));\n  public static void verbose(Object... args) {\n    if (!VERBOSE) return;\n    StringBuilder sb = new StringBuilder(\"VERBOSE:\");\n//    sb.append(Thread.currentThread().getName());\n//    sb.append(':');\n    for (Object o : args) {\n      sb.append(' ');\n      sb.append(o==null ? \"(null)\" : o.toString());\n    }\n    // System.out.println(sb.toString());\n    log.info(sb.toString());\n  }\n\n\n  ////////////////////////////////////////////////////////////////////////////////\n  // Request Handler\n  ////////////////////////////////////////////////////////////////////////////////\n\n  /**\n   * Get the request handler registered to a given name.  \n   * \n   * This function is thread safe.\n   */\n  public SolrRequestHandler getRequestHandler(String handlerName) {\n    return reqHandlers.get(handlerName);\n  }\n\n  /**\n   * Returns an unmodifiable Map containing the registered handlers of the specified type.\n   */\n  public <T extends SolrRequestHandler> Map<String,T> getRequestHandlers(Class<T> clazz) {\n    return reqHandlers.getAll(clazz);\n  }\n  \n  /**\n   * Returns an unmodifiable Map containing the registered handlers\n   */\n  public Map<String,SolrRequestHandler> getRequestHandlers() {\n    return reqHandlers.getRequestHandlers();\n  }\n\n\n  /**\n   * Registers a handler at the specified location.  If one exists there, it will be replaced.\n   * To remove a handler, register <code>null</code> at its path\n   * \n   * Once registered the handler can be accessed through:\n   * <pre>\n   *   http://${host}:${port}/${context}/${handlerName}\n   * or:  \n   *   http://${host}:${port}/${context}/select?qt=${handlerName}\n   * </pre>  \n   * \n   * Handlers <em>must</em> be initialized before getting registered.  Registered\n   * handlers can immediately accept requests.\n   * \n   * This call is thread safe.\n   *  \n   * @return the previous <code>SolrRequestHandler</code> registered to this name <code>null</code> if none.\n   */\n  public SolrRequestHandler registerRequestHandler(String handlerName, SolrRequestHandler handler) {\n    return reqHandlers.register(handlerName,handler);\n  }\n  \n  /**\n   * Register the default search components\n   */\n  private Map<String, SearchComponent> loadSearchComponents()\n  {\n    Map<String, SearchComponent> components = new HashMap<String, SearchComponent>();\n    initPlugins(components,SearchComponent.class);\n    for (Map.Entry<String, SearchComponent> e : components.entrySet()) {\n      SearchComponent c = e.getValue();\n      if (c instanceof HighlightComponent) {\n        HighlightComponent hl = (HighlightComponent) c;\n        if(!HighlightComponent.COMPONENT_NAME.equals(e.getKey())){\n          components.put(HighlightComponent.COMPONENT_NAME,hl);\n        }\n        break;\n      }\n    }\n    addIfNotPresent(components,HighlightComponent.COMPONENT_NAME,HighlightComponent.class);\n    addIfNotPresent(components,QueryComponent.COMPONENT_NAME,QueryComponent.class);\n    addIfNotPresent(components,FacetComponent.COMPONENT_NAME,FacetComponent.class);\n    addIfNotPresent(components,MoreLikeThisComponent.COMPONENT_NAME,MoreLikeThisComponent.class);\n    addIfNotPresent(components,StatsComponent.COMPONENT_NAME,StatsComponent.class);\n    addIfNotPresent(components,DebugComponent.COMPONENT_NAME,DebugComponent.class);\n    addIfNotPresent(components,RealTimeGetComponent.COMPONENT_NAME,RealTimeGetComponent.class);\n    addIfNotPresent(components,AnalyticsComponent.COMPONENT_NAME,AnalyticsComponent.class);\n    return components;\n  }\n  private <T> void addIfNotPresent(Map<String ,T> registry, String name, Class<? extends  T> c){\n    if(!registry.containsKey(name)){\n      T searchComp = resourceLoader.newInstance(c.getName(), c);\n      if (searchComp instanceof NamedListInitializedPlugin){\n        ((NamedListInitializedPlugin)searchComp).init( new NamedList<String>() );\n      }\n      if(searchComp instanceof SearchComponent) {\n        ((SearchComponent)searchComp).setName(name);\n      }\n      registry.put(name, searchComp);\n      if (searchComp instanceof SolrInfoMBean){\n        infoRegistry.put(((SolrInfoMBean)searchComp).getName(), (SolrInfoMBean)searchComp);\n      }\n    }\n  }\n  \n  /**\n   * @return a Search Component registered to a given name.  Throw an exception if the component is undefined\n   */\n  public SearchComponent getSearchComponent( String name )\n  {\n    SearchComponent component = searchComponents.get( name );\n    if( component == null ) {\n      throw new SolrException( SolrException.ErrorCode.BAD_REQUEST,\n          \"Unknown Search Component: \"+name );\n    }\n    return component;\n  }\n\n  /**\n   * Accessor for all the Search Components\n   * @return An unmodifiable Map of Search Components\n   */\n  public Map<String, SearchComponent> getSearchComponents() {\n    return searchComponents;\n  }\n\n  ////////////////////////////////////////////////////////////////////////////////\n  // Update Handler\n  ////////////////////////////////////////////////////////////////////////////////\n\n  /**\n   * RequestHandlers need access to the updateHandler so they can all talk to the\n   * same RAM indexer.  \n   */\n  public UpdateHandler getUpdateHandler() {\n    return updateHandler;\n  }\n\n  ////////////////////////////////////////////////////////////////////////////////\n  // Searcher Control\n  ////////////////////////////////////////////////////////////////////////////////\n\n  // The current searcher used to service queries.\n  // Don't access this directly!!!! use getSearcher() to\n  // get it (and it will increment the ref count at the same time).\n  // This reference is protected by searcherLock.\n  private RefCounted<SolrIndexSearcher> _searcher;\n\n  // All of the normal open searchers.  Don't access this directly.\n  // protected by synchronizing on searcherLock.\n  private final LinkedList<RefCounted<SolrIndexSearcher>> _searchers = new LinkedList<RefCounted<SolrIndexSearcher>>();\n  private final LinkedList<RefCounted<SolrIndexSearcher>> _realtimeSearchers = new LinkedList<RefCounted<SolrIndexSearcher>>();\n\n  final ExecutorService searcherExecutor = Executors.newSingleThreadExecutor(\n      new DefaultSolrThreadFactory(\"searcherExecutor\"));\n  private int onDeckSearchers;  // number of searchers preparing\n  // Lock ordering: one can acquire the openSearcherLock and then the searcherLock, but not vice-versa.\n  private Object searcherLock = new Object();  // the sync object for the searcher\n  private ReentrantLock openSearcherLock = new ReentrantLock(true);     // used to serialize opens/reopens for absolute ordering\n  private final int maxWarmingSearchers;  // max number of on-deck searchers allowed\n\n  private RefCounted<SolrIndexSearcher> realtimeSearcher;\n  private Callable<DirectoryReader> newReaderCreator;\n\n  /**\n  * Return a registered {@link RefCounted}&lt;{@link SolrIndexSearcher}&gt; with\n  * the reference count incremented.  It <b>must</b> be decremented when no longer needed.\n  * This method should not be called from SolrCoreAware.inform() since it can result\n  * in a deadlock if useColdSearcher==false.\n  * If handling a normal request, the searcher should be obtained from\n   * {@link org.apache.solr.request.SolrQueryRequest#getSearcher()} instead.\n  */\n  public RefCounted<SolrIndexSearcher> getSearcher() {\n    return getSearcher(false,true,null);\n  }\n\n  /**\n  * Returns the current registered searcher with its reference count incremented, or null if none are registered.\n  */\n  public RefCounted<SolrIndexSearcher> getRegisteredSearcher() {\n    synchronized (searcherLock) {\n      if (_searcher != null) {\n        _searcher.incref();\n      }\n      return _searcher;\n    }\n  }\n\n  /**\n   * Return the newest normal {@link RefCounted}&lt;{@link SolrIndexSearcher}&gt; with\n   * the reference count incremented.  It <b>must</b> be decremented when no longer needed.\n   * If no searcher is currently open, then if openNew==true a new searcher will be opened,\n   * or null is returned if openNew==false.\n   */\n  public RefCounted<SolrIndexSearcher> getNewestSearcher(boolean openNew) {\n    synchronized (searcherLock) {\n      if (!_searchers.isEmpty()) {\n        RefCounted<SolrIndexSearcher> newest = _searchers.getLast();\n        newest.incref();\n        return newest;\n      }\n    }\n\n    return openNew ? getRealtimeSearcher() : null;\n  }\n\n  /** Gets the latest real-time searcher w/o forcing open a new searcher if one already exists.\n   * The reference count will be incremented.\n   */\n  public RefCounted<SolrIndexSearcher> getRealtimeSearcher() {\n    synchronized (searcherLock) {\n      if (realtimeSearcher != null) {\n        realtimeSearcher.incref();\n        return realtimeSearcher;\n      }\n    }\n\n    // use the searcher lock to prevent multiple people from trying to open at once\n    openSearcherLock.lock();\n    try {\n\n      // try again\n      synchronized (searcherLock) {\n        if (realtimeSearcher != null) {\n          realtimeSearcher.incref();\n          return realtimeSearcher;\n        }\n      }\n\n      // force a new searcher open\n      return openNewSearcher(true, true);\n    } finally {\n      openSearcherLock.unlock();\n    }\n  }\n\n\n  public RefCounted<SolrIndexSearcher> getSearcher(boolean forceNew, boolean returnSearcher, final Future[] waitSearcher) {\n    return getSearcher(forceNew, returnSearcher, waitSearcher, false);\n  }\n\n\n  /** Opens a new searcher and returns a RefCounted&lt;SolrIndexSearcher&gt; with it's reference incremented.\n   *\n   * \"realtime\" means that we need to open quickly for a realtime view of the index, hence don't do any\n   * autowarming and add to the _realtimeSearchers queue rather than the _searchers queue (so it won't\n   * be used for autowarming by a future normal searcher).  A \"realtime\" searcher will currently never\n   * become \"registered\" (since it currently lacks caching).\n   *\n   * realtimeSearcher is updated to the latest opened searcher, regardless of the value of \"realtime\".\n   *\n   * This method acquires openSearcherLock - do not call with searckLock held!\n   */\n  public RefCounted<SolrIndexSearcher>  openNewSearcher(boolean updateHandlerReopens, boolean realtime) {\n    if (isClosed()) { // catch some errors quicker\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"openNewSearcher called on closed core\");\n    }\n\n    SolrIndexSearcher tmp;\n    RefCounted<SolrIndexSearcher> newestSearcher = null;\n    boolean nrt = solrConfig.nrtMode && updateHandlerReopens;\n\n    openSearcherLock.lock();\n    try {\n      String newIndexDir = getNewIndexDir();\n      String indexDirFile = null;\n      String newIndexDirFile = null;\n\n      // if it's not a normal near-realtime update, check that paths haven't changed.\n      if (!nrt) {\n        indexDirFile = getDirectoryFactory().normalize(getIndexDir());\n        newIndexDirFile = getDirectoryFactory().normalize(newIndexDir);\n      }\n\n      synchronized (searcherLock) {\n        newestSearcher = realtimeSearcher;\n        if (newestSearcher != null) {\n          newestSearcher.incref();      // the matching decref is in the finally block\n        }\n      }\n\n      if (newestSearcher != null && (nrt || indexDirFile.equals(newIndexDirFile))) {\n\n        DirectoryReader newReader;\n        DirectoryReader currentReader = newestSearcher.get().getIndexReader();\n\n        // SolrCore.verbose(\"start reopen from\",previousSearcher,\"writer=\",writer);\n        \n        RefCounted<IndexWriter> writer = getUpdateHandler().getSolrCoreState()\n            .getIndexWriter(null);\n        try {\n          if (writer != null && solrConfig.nrtMode) {\n            // if in NRT mode, open from the writer\n            newReader = DirectoryReader.openIfChanged(currentReader, writer.get(), true);\n          } else {\n            // verbose(\"start reopen without writer, reader=\", currentReader);\n            // if not in NRT mode, just re-open the reader\n            newReader = DirectoryReader.openIfChanged(currentReader);\n            // verbose(\"reopen result\", newReader);\n          }\n        } finally {\n          if (writer != null) {\n            writer.decref();\n          }\n        }\n\n        if (newReader == null) {\n          // if this is a request for a realtime searcher, just return the same searcher if there haven't been any changes.\n          if (realtime) {\n            newestSearcher.incref();\n            return newestSearcher;\n          }\n\n          currentReader.incRef();\n          newReader = currentReader;\n        }\n\n       // for now, turn off caches if this is for a realtime reader (caches take a little while to instantiate)\n        tmp = new SolrIndexSearcher(this, newIndexDir, getLatestSchema(), getSolrConfig().indexConfig, \n            (realtime ? \"realtime\":\"main\"), newReader, true, !realtime, true, directoryFactory);\n\n      } else {\n        // newestSearcher == null at this point\n\n        if (newReaderCreator != null) {\n          // this is set in the constructor if there is a currently open index writer\n          // so that we pick up any uncommitted changes and so we don't go backwards\n          // in time on a core reload\n          DirectoryReader newReader = newReaderCreator.call();\n          tmp = new SolrIndexSearcher(this, newIndexDir, getLatestSchema(), getSolrConfig().indexConfig, \n              (realtime ? \"realtime\":\"main\"), newReader, true, !realtime, true, directoryFactory);\n        } else if (solrConfig.nrtMode) {\n          RefCounted<IndexWriter> writer = getUpdateHandler().getSolrCoreState().getIndexWriter(this);\n          DirectoryReader newReader = null;\n          try {\n            newReader = indexReaderFactory.newReader(writer.get(), this);\n          } finally {\n            writer.decref();\n          }\n          tmp = new SolrIndexSearcher(this, newIndexDir, getLatestSchema(), getSolrConfig().indexConfig, \n              (realtime ? \"realtime\":\"main\"), newReader, true, !realtime, true, directoryFactory);\n        } else {\n         // normal open that happens at startup\n        // verbose(\"non-reopen START:\");\n        tmp = new SolrIndexSearcher(this, newIndexDir, getLatestSchema(), getSolrConfig().indexConfig,\n                                    \"main\", true, directoryFactory);\n        // verbose(\"non-reopen DONE: searcher=\",tmp);\n        }\n      }\n\n      List<RefCounted<SolrIndexSearcher>> searcherList = realtime ? _realtimeSearchers : _searchers;\n      RefCounted<SolrIndexSearcher> newSearcher = newHolder(tmp, searcherList);    // refcount now at 1\n\n      // Increment reference again for \"realtimeSearcher\" variable.  It should be at 2 after.\n      // When it's decremented by both the caller of this method, and by realtimeSearcher being replaced,\n      // it will be closed.\n      newSearcher.incref();\n\n      synchronized (searcherLock) {\n        if (realtimeSearcher != null) {\n          realtimeSearcher.decref();\n        }\n        realtimeSearcher = newSearcher;\n        searcherList.add(realtimeSearcher);\n      }\n\n      return newSearcher;\n\n    } catch (Exception e) {\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Error opening new searcher\", e);\n    }\n    finally {\n      openSearcherLock.unlock();\n      if (newestSearcher != null) {\n        newestSearcher.decref();\n      }\n    }\n\n  }\n  \n  /**\n   * Get a {@link SolrIndexSearcher} or start the process of creating a new one.\n   * <p>\n   * The registered searcher is the default searcher used to service queries.\n   * A searcher will normally be registered after all of the warming\n   * and event handlers (newSearcher or firstSearcher events) have run.\n   * In the case where there is no registered searcher, the newly created searcher will\n   * be registered before running the event handlers (a slow searcher is better than no searcher).\n   *\n   * <p>\n   * These searchers contain read-only IndexReaders. To access a non read-only IndexReader,\n   * see newSearcher(String name, boolean readOnly).\n   *\n   * <p>\n   * If <tt>forceNew==true</tt> then\n   *  A new searcher will be opened and registered regardless of whether there is already\n   *    a registered searcher or other searchers in the process of being created.\n   * <p>\n   * If <tt>forceNew==false</tt> then:<ul>\n   *   <li>If a searcher is already registered, that searcher will be returned</li>\n   *   <li>If no searcher is currently registered, but at least one is in the process of being created, then\n   * this call will block until the first searcher is registered</li>\n   *   <li>If no searcher is currently registered, and no searchers in the process of being registered, a new\n   * searcher will be created.</li>\n   * </ul>\n   * <p>\n   * If <tt>returnSearcher==true</tt> then a {@link RefCounted}&lt;{@link SolrIndexSearcher}&gt; will be returned with\n   * the reference count incremented.  It <b>must</b> be decremented when no longer needed.\n   * <p>\n   * If <tt>waitSearcher!=null</tt> and a new {@link SolrIndexSearcher} was created,\n   * then it is filled in with a Future that will return after the searcher is registered.  The Future may be set to\n   * <tt>null</tt> in which case the SolrIndexSearcher created has already been registered at the time\n   * this method returned.\n   * <p>\n   * @param forceNew             if true, force the open of a new index searcher regardless if there is already one open.\n   * @param returnSearcher       if true, returns a {@link SolrIndexSearcher} holder with the refcount already incremented.\n   * @param waitSearcher         if non-null, will be filled in with a {@link Future} that will return after the new searcher is registered.\n   * @param updateHandlerReopens if true, the UpdateHandler will be used when reopening a {@link SolrIndexSearcher}.\n   */\n  public RefCounted<SolrIndexSearcher> getSearcher(boolean forceNew, boolean returnSearcher, final Future[] waitSearcher, boolean updateHandlerReopens) {\n    // it may take some time to open an index.... we may need to make\n    // sure that two threads aren't trying to open one at the same time\n    // if it isn't necessary.\n\n    synchronized (searcherLock) {\n      // see if we can return the current searcher\n      if (_searcher!=null && !forceNew) {\n        if (returnSearcher) {\n          _searcher.incref();\n          return _searcher;\n        } else {\n          return null;\n        }\n      }\n\n      // check to see if we can wait for someone else's searcher to be set\n      if (onDeckSearchers>0 && !forceNew && _searcher==null) {\n        try {\n          searcherLock.wait();\n        } catch (InterruptedException e) {\n          log.info(SolrException.toStr(e));\n        }\n      }\n\n      // check again: see if we can return right now\n      if (_searcher!=null && !forceNew) {\n        if (returnSearcher) {\n          _searcher.incref();\n          return _searcher;\n        } else {\n          return null;\n        }\n      }\n\n      // At this point, we know we need to open a new searcher...\n      // first: increment count to signal other threads that we are\n      //        opening a new searcher.\n      onDeckSearchers++;\n      if (onDeckSearchers < 1) {\n        // should never happen... just a sanity check\n        log.error(logid+\"ERROR!!! onDeckSearchers is \" + onDeckSearchers);\n        onDeckSearchers=1;  // reset\n      } else if (onDeckSearchers > maxWarmingSearchers) {\n        onDeckSearchers--;\n        String msg=\"Error opening new searcher. exceeded limit of maxWarmingSearchers=\"+maxWarmingSearchers + \", try again later.\";\n        log.warn(logid+\"\"+ msg);\n        // HTTP 503==service unavailable, or 409==Conflict\n        throw new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE,msg);\n      } else if (onDeckSearchers > 1) {\n        log.warn(logid+\"PERFORMANCE WARNING: Overlapping onDeckSearchers=\" + onDeckSearchers);\n      }\n    }\n\n    // a signal to decrement onDeckSearchers if something goes wrong.\n    final boolean[] decrementOnDeckCount=new boolean[]{true};\n    RefCounted<SolrIndexSearcher> currSearcherHolder = null;     // searcher we are autowarming from\n    RefCounted<SolrIndexSearcher> searchHolder = null;\n    boolean success = false;\n\n    openSearcherLock.lock();\n    try {\n      searchHolder = openNewSearcher(updateHandlerReopens, false);\n       // the searchHolder will be incremented once already (and it will eventually be assigned to _searcher when registered)\n       // increment it again if we are going to return it to the caller.\n      if (returnSearcher) {\n        searchHolder.incref();\n      }\n\n\n      final RefCounted<SolrIndexSearcher> newSearchHolder = searchHolder;\n      final SolrIndexSearcher newSearcher = newSearchHolder.get();\n\n\n      boolean alreadyRegistered = false;\n      synchronized (searcherLock) {\n        if (_searcher == null) {\n          // if there isn't a current searcher then we may\n          // want to register this one before warming is complete instead of waiting.\n          if (solrConfig.useColdSearcher) {\n            registerSearcher(newSearchHolder);\n            decrementOnDeckCount[0]=false;\n            alreadyRegistered=true;\n          }\n        } else {\n          // get a reference to the current searcher for purposes of autowarming.\n          currSearcherHolder=_searcher;\n          currSearcherHolder.incref();\n        }\n      }\n\n\n      final SolrIndexSearcher currSearcher = currSearcherHolder==null ? null : currSearcherHolder.get();\n\n      Future future=null;\n\n      // warm the new searcher based on the current searcher.\n      // should this go before the other event handlers or after?\n      if (currSearcher != null) {\n        future = searcherExecutor.submit(\n            new Callable() {\n              @Override\n              public Object call() throws Exception {\n                try {\n                  newSearcher.warm(currSearcher);\n                } catch (Throwable e) {\n                  SolrException.log(log,e);\n                }\n                return null;\n              }\n            }\n        );\n      }\n\n      if (currSearcher==null && firstSearcherListeners.size() > 0) {\n        future = searcherExecutor.submit(\n            new Callable() {\n              @Override\n              public Object call() throws Exception {\n                try {\n                  for (SolrEventListener listener : firstSearcherListeners) {\n                    listener.newSearcher(newSearcher,null);\n                  }\n                } catch (Throwable e) {\n                  SolrException.log(log,null,e);\n                }\n                return null;\n              }\n            }\n        );\n      }\n\n      if (currSearcher!=null && newSearcherListeners.size() > 0) {\n        future = searcherExecutor.submit(\n            new Callable() {\n              @Override\n              public Object call() throws Exception {\n                try {\n                  for (SolrEventListener listener : newSearcherListeners) {\n                    listener.newSearcher(newSearcher, currSearcher);\n                  }\n                } catch (Throwable e) {\n                  SolrException.log(log,null,e);\n                }\n                return null;\n              }\n            }\n        );\n      }\n\n      // WARNING: this code assumes a single threaded executor (that all tasks\n      // queued will finish first).\n      final RefCounted<SolrIndexSearcher> currSearcherHolderF = currSearcherHolder;\n      if (!alreadyRegistered) {\n        future = searcherExecutor.submit(\n            new Callable() {\n              @Override\n              public Object call() throws Exception {\n                try {\n                  // registerSearcher will decrement onDeckSearchers and\n                  // do a notify, even if it fails.\n                  registerSearcher(newSearchHolder);\n                } catch (Throwable e) {\n                  SolrException.log(log, e);\n                } finally {\n                  // we are all done with the old searcher we used\n                  // for warming...\n                  if (currSearcherHolderF!=null) currSearcherHolderF.decref();\n                }\n                return null;\n              }\n            }\n        );\n      }\n\n      if (waitSearcher != null) {\n        waitSearcher[0] = future;\n      }\n\n      success = true;\n\n      // Return the searcher as the warming tasks run in parallel\n      // callers may wait on the waitSearcher future returned.\n      return returnSearcher ? newSearchHolder : null;\n\n    } catch (Exception e) {\n      if (e instanceof SolrException) throw (SolrException)e;\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n    } finally {\n\n      if (!success) {\n        synchronized (searcherLock) {\n          onDeckSearchers--;\n\n          if (onDeckSearchers < 0) {\n            // sanity check... should never happen\n            log.error(logid+\"ERROR!!! onDeckSearchers after decrement=\" + onDeckSearchers);\n            onDeckSearchers=0; // try and recover\n          }\n          // if we failed, we need to wake up at least one waiter to continue the process\n          searcherLock.notify();\n        }\n\n        if (currSearcherHolder != null) {\n          currSearcherHolder.decref();\n        }\n\n        if (searchHolder != null) {\n          searchHolder.decref();      // decrement 1 for _searcher (searchHolder will never become _searcher now)\n          if (returnSearcher) {\n            searchHolder.decref();    // decrement 1 because we won't be returning the searcher to the user\n          }\n        }\n      }\n\n      // we want to do this after we decrement onDeckSearchers so another thread\n      // doesn't increment first and throw a false warning.\n      openSearcherLock.unlock();\n\n    }\n\n  }\n\n\n  private RefCounted<SolrIndexSearcher> newHolder(SolrIndexSearcher newSearcher, final List<RefCounted<SolrIndexSearcher>> searcherList) {\n    RefCounted<SolrIndexSearcher> holder = new RefCounted<SolrIndexSearcher>(newSearcher) {\n      @Override\n      public void close() {\n        try {\n          synchronized(searcherLock) {\n            // it's possible for someone to get a reference via the _searchers queue\n            // and increment the refcount while RefCounted.close() is being called.\n            // we check the refcount again to see if this has happened and abort the close.\n            // This relies on the RefCounted class allowing close() to be called every\n            // time the counter hits zero.\n            if (refcount.get() > 0) return;\n            searcherList.remove(this);\n          }\n          resource.close();\n        } catch (Throwable e) {\n          // do not allow decref() operations to fail since they are typically called in finally blocks\n          // and throwing another exception would be very unexpected.\n          SolrException.log(log, \"Error closing searcher:\" + this, e);\n        }\n      }\n    };\n    holder.incref();  // set ref count to 1 to account for this._searcher\n    return holder;\n  }\n\n  public boolean isReloaded() {\n    return isReloaded;\n  }\n\n  // Take control of newSearcherHolder (which should have a reference count of at\n  // least 1 already.  If the caller wishes to use the newSearcherHolder directly\n  // after registering it, then they should increment the reference count *before*\n  // calling this method.\n  //\n  // onDeckSearchers will also be decremented (it should have been incremented\n  // as a result of opening a new searcher).\n  private void registerSearcher(RefCounted<SolrIndexSearcher> newSearcherHolder) {\n    synchronized (searcherLock) {\n      try {\n        if (_searcher != null) {\n          _searcher.decref();   // dec refcount for this._searcher\n          _searcher=null;\n        }\n\n        _searcher = newSearcherHolder;\n        SolrIndexSearcher newSearcher = newSearcherHolder.get();\n\n        /***\n        // a searcher may have been warming asynchronously while the core was being closed.\n        // if this happens, just close the searcher.\n        if (isClosed()) {\n          // NOTE: this should not happen now - see close() for details.\n          // *BUT* if we left it enabled, this could still happen before\n          // close() stopped the executor - so disable this test for now.\n          log.error(\"Ignoring searcher register on closed core:\" + newSearcher);\n          _searcher.decref();\n        }\n        ***/\n\n        newSearcher.register(); // register subitems (caches)\n        log.info(logid+\"Registered new searcher \" + newSearcher);\n\n      } catch (Throwable e) {\n        // an exception in register() shouldn't be fatal.\n        log(e);\n      } finally {\n        // wake up anyone waiting for a searcher\n        // even in the face of errors.\n        onDeckSearchers--;\n        searcherLock.notifyAll();\n      }\n    }\n  }\n\n\n\n  public void closeSearcher() {\n    log.info(logid+\"Closing main searcher on request.\");\n    synchronized (searcherLock) {\n      if (realtimeSearcher != null) {\n        realtimeSearcher.decref();\n        realtimeSearcher = null;\n      }\n      if (_searcher != null) {\n        _searcher.decref();   // dec refcount for this._searcher\n        _searcher = null; // isClosed() does check this\n        infoRegistry.remove(\"currentSearcher\");\n      }\n    }\n  }\n\n  public void execute(SolrRequestHandler handler, SolrQueryRequest req, SolrQueryResponse rsp) {\n    if (handler==null) {\n      String msg = \"Null Request Handler '\" +\n        req.getParams().get(CommonParams.QT) + \"'\";\n      \n      if (log.isWarnEnabled()) log.warn(logid + msg + \":\" + req);\n      \n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, msg);\n    }\n\n    preDecorateResponse(req, rsp);\n\n    // TODO: this doesn't seem to be working correctly and causes problems with the example server and distrib (for example /spell)\n    // if (req.getParams().getBool(ShardParams.IS_SHARD,false) && !(handler instanceof SearchHandler))\n    //   throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\"isShard is only acceptable with search handlers\");\n\n    handler.handleRequest(req,rsp);\n\n    postDecorateResponse(handler, req, rsp);\n\n    if (log.isInfoEnabled() && rsp.getToLog().size() > 0) {\n      log.info(rsp.getToLogAsString(logid));\n    }\n  }\n\n  public static void preDecorateResponse(SolrQueryRequest req, SolrQueryResponse rsp) {\n    // setup response header\n    final NamedList<Object> responseHeader = new SimpleOrderedMap<Object>();\n    rsp.add(\"responseHeader\", responseHeader);\n\n    // toLog is a local ref to the same NamedList used by the response\n    NamedList<Object> toLog = rsp.getToLog();\n\n    // for back compat, we set these now just in case other code\n    // are expecting them during handleRequest\n    toLog.add(\"webapp\", req.getContext().get(\"webapp\"));\n    toLog.add(\"path\", req.getContext().get(\"path\"));\n    toLog.add(\"params\", \"{\" + req.getParamString() + \"}\");\n  }\n\n  /** Put status, QTime, and possibly request handler and params, in the response header */\n  public static void postDecorateResponse\n      (SolrRequestHandler handler, SolrQueryRequest req, SolrQueryResponse rsp) {\n    // TODO should check that responseHeader has not been replaced by handler\n    NamedList<Object> responseHeader = rsp.getResponseHeader();\n    final int qtime=(int)(rsp.getEndTime() - req.getStartTime());\n    int status = 0;\n    Exception exception = rsp.getException();\n    if( exception != null ){\n      if( exception instanceof SolrException )\n        status = ((SolrException)exception).code();\n      else\n        status = 500;\n    }\n    responseHeader.add(\"status\",status);\n    responseHeader.add(\"QTime\",qtime);\n\n    if (rsp.getToLog().size() > 0) {\n      rsp.getToLog().add(\"status\",status);\n      rsp.getToLog().add(\"QTime\",qtime);\n    }\n\n    SolrParams params = req.getParams();\n    if( null != handler && params.getBool(CommonParams.HEADER_ECHO_HANDLER, false) ) {\n      responseHeader.add(\"handler\", handler.getName() );\n    }\n\n    // Values for echoParams... false/true/all or false/explicit/all ???\n    String ep = params.get( CommonParams.HEADER_ECHO_PARAMS, null );\n    if( ep != null ) {\n      EchoParamStyle echoParams = EchoParamStyle.get( ep );\n      if( echoParams == null ) {\n        throw new SolrException( SolrException.ErrorCode.BAD_REQUEST,\"Invalid value '\" + ep + \"' for \" + CommonParams.HEADER_ECHO_PARAMS \n            + \" parameter, use '\" + EchoParamStyle.EXPLICIT + \"' or '\" + EchoParamStyle.ALL + \"'\" );\n      }\n      if( echoParams == EchoParamStyle.EXPLICIT ) {\n        responseHeader.add(\"params\", req.getOriginalParams().toNamedList());\n      } else if( echoParams == EchoParamStyle.ALL ) {\n        responseHeader.add(\"params\", req.getParams().toNamedList());\n      }\n    }\n  }\n\n  final public static void log(Throwable e) {\n    SolrException.log(log,null,e);\n  }\n\n  \n  \n  private QueryResponseWriter defaultResponseWriter;\n  private final Map<String, QueryResponseWriter> responseWriters = new HashMap<String, QueryResponseWriter>();\n  public static final Map<String ,QueryResponseWriter> DEFAULT_RESPONSE_WRITERS ;\n  static{\n    HashMap<String, QueryResponseWriter> m= new HashMap<String, QueryResponseWriter>();\n    m.put(\"xml\", new XMLResponseWriter());\n    m.put(\"standard\", m.get(\"xml\"));\n    m.put(\"json\", new JSONResponseWriter());\n    m.put(\"python\", new PythonResponseWriter());\n    m.put(\"php\", new PHPResponseWriter());\n    m.put(\"phps\", new PHPSerializedResponseWriter());\n    m.put(\"ruby\", new RubyResponseWriter());\n    m.put(\"raw\", new RawResponseWriter());\n    m.put(\"javabin\", new BinaryResponseWriter());\n    m.put(\"csv\", new CSVResponseWriter());\n    m.put(\"schema.xml\", new SchemaXmlResponseWriter());\n    DEFAULT_RESPONSE_WRITERS = Collections.unmodifiableMap(m);\n  }\n  \n  /** Configure the query response writers. There will always be a default writer; additional\n   * writers may also be configured. */\n  private void initWriters() {\n    // use link map so we iterate in the same order\n    Map<PluginInfo,QueryResponseWriter> writers = new LinkedHashMap<PluginInfo,QueryResponseWriter>();\n    for (PluginInfo info : solrConfig.getPluginInfos(QueryResponseWriter.class.getName())) {\n      try {\n        QueryResponseWriter writer;\n        String startup = info.attributes.get(\"startup\") ;\n        if( startup != null ) {\n          if( \"lazy\".equals(startup) ) {\n            log.info(\"adding lazy queryResponseWriter: \" + info.className);\n            writer = new LazyQueryResponseWriterWrapper(this, info.className, info.initArgs );\n          } else {\n            throw new Exception( \"Unknown startup value: '\"+startup+\"' for: \"+info.className );\n          }\n        } else {\n          writer = createQueryResponseWriter(info.className);\n        }\n        writers.put(info,writer);\n        QueryResponseWriter old = registerResponseWriter(info.name, writer);\n        if(old != null) {\n          log.warn(\"Multiple queryResponseWriter registered to the same name: \" + info.name + \" ignoring: \" + old.getClass().getName());\n        }\n        if(info.isDefault()){\n          if(defaultResponseWriter != null)\n            log.warn(\"Multiple default queryResponseWriter registered, using: \" + info.name);\n          defaultResponseWriter = writer;\n        }\n        log.info(\"created \"+info.name+\": \" + info.className);\n      } catch (Exception ex) {\n          SolrException e = new SolrException\n            (SolrException.ErrorCode.SERVER_ERROR, \"QueryResponseWriter init failure\", ex);\n          SolrException.log(log,null,e);\n          throw e;\n      }\n    }\n\n    // we've now registered all handlers, time to init them in the same order\n    for (Map.Entry<PluginInfo,QueryResponseWriter> entry : writers.entrySet()) {\n      PluginInfo info = entry.getKey();\n      QueryResponseWriter writer = entry.getValue();\n      responseWriters.put(info.name, writer);\n      if (writer instanceof PluginInfoInitialized) {\n        ((PluginInfoInitialized) writer).init(info);\n      } else{\n        writer.init(info.initArgs);\n      }\n    }\n\n    NamedList emptyList = new NamedList();\n    for (Map.Entry<String, QueryResponseWriter> entry : DEFAULT_RESPONSE_WRITERS.entrySet()) {\n      if(responseWriters.get(entry.getKey()) == null) {\n        responseWriters.put(entry.getKey(), entry.getValue());\n        // call init so any logic in the default writers gets invoked\n        entry.getValue().init(emptyList);\n      }\n    }\n    \n    // configure the default response writer; this one should never be null\n    if (defaultResponseWriter == null) {\n      defaultResponseWriter = responseWriters.get(\"standard\");\n    }\n\n  }\n  \n  /** Finds a writer by name, or returns the default writer if not found. */\n  public final QueryResponseWriter getQueryResponseWriter(String writerName) {\n    if (writerName != null) {\n        QueryResponseWriter writer = responseWriters.get(writerName);\n        if (writer != null) {\n            return writer;\n        }\n    }\n    return defaultResponseWriter;\n  }\n\n  /** Returns the appropriate writer for a request. If the request specifies a writer via the\n   * 'wt' parameter, attempts to find that one; otherwise return the default writer.\n   */\n  public final QueryResponseWriter getQueryResponseWriter(SolrQueryRequest request) {\n    return getQueryResponseWriter(request.getParams().get(CommonParams.WT)); \n  }\n\n  private final Map<String, QParserPlugin> qParserPlugins = new HashMap<String, QParserPlugin>();\n\n  /** Configure the query parsers. */\n  private void initQParsers() {\n    initPlugins(qParserPlugins,QParserPlugin.class);\n    // default parsers\n    for (int i=0; i<QParserPlugin.standardPlugins.length; i+=2) {\n     try {\n       String name = (String)QParserPlugin.standardPlugins[i];\n       if (null == qParserPlugins.get(name)) {\n         Class<QParserPlugin> clazz = (Class<QParserPlugin>)QParserPlugin.standardPlugins[i+1];\n         QParserPlugin plugin = clazz.newInstance();\n         qParserPlugins.put(name, plugin);\n         plugin.init(null);\n         infoRegistry.put(name, plugin);\n       }\n     } catch (Exception e) {\n       throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n     }\n    }\n  }\n\n  public QParserPlugin getQueryPlugin(String parserName) {\n    QParserPlugin plugin = qParserPlugins.get(parserName);\n    if (plugin != null) return plugin;\n    throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown query parser '\"+parserName+\"'\");\n  }\n  \n  private final HashMap<String, ValueSourceParser> valueSourceParsers = new HashMap<String, ValueSourceParser>();\n  \n  /** Configure the ValueSource (function) plugins */\n  private void initValueSourceParsers() {\n    initPlugins(valueSourceParsers,ValueSourceParser.class);\n    // default value source parsers\n    for (Map.Entry<String, ValueSourceParser> entry : ValueSourceParser.standardValueSourceParsers.entrySet()) {\n      try {\n        String name = entry.getKey();\n        if (null == valueSourceParsers.get(name)) {\n          ValueSourceParser valueSourceParser = entry.getValue();\n          valueSourceParsers.put(name, valueSourceParser);\n          valueSourceParser.init(null);\n        }\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n  }\n  \n\n  private final HashMap<String, TransformerFactory> transformerFactories = new HashMap<String, TransformerFactory>();\n  \n  /** Configure the TransformerFactory plugins */\n  private void initTransformerFactories() {\n    // Load any transformer factories\n    initPlugins(transformerFactories,TransformerFactory.class);\n    \n    // Tell each transformer what its name is\n    for( Map.Entry<String, TransformerFactory> entry : TransformerFactory.defaultFactories.entrySet() ) {\n      try {\n        String name = entry.getKey();\n        if (null == valueSourceParsers.get(name)) {\n          TransformerFactory f = entry.getValue();\n          transformerFactories.put(name, f);\n          // f.init(null); default ones don't need init\n        }\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n  }\n  \n  public TransformerFactory getTransformerFactory(String name) {\n    return transformerFactories.get(name);\n  }\n\n  public void addTransformerFactory(String name, TransformerFactory factory){\n    transformerFactories.put(name, factory);\n  }\n  \n\n  /**\n   * @param registry The map to which the instance should be added to. The key is the name attribute\n   * @param type the class or interface that the instance should extend or implement.\n   * @param defClassName If PluginInfo does not have a classname, use this as the classname\n   * @return The default instance . The one with (default=true)\n   */\n  public <T> T initPlugins(Map<String ,T> registry, Class<T> type, String defClassName){\n    return initPlugins(solrConfig.getPluginInfos(type.getName()), registry, type, defClassName);\n  }\n\n  public <T> T initPlugins(List<PluginInfo> pluginInfos, Map<String, T> registry, Class<T> type, String defClassName) {\n    T def = null;\n    for (PluginInfo info : pluginInfos) {\n      T o = createInitInstance(info,type, type.getSimpleName(), defClassName);\n      registry.put(info.name, o);\n      if(info.isDefault()){\n        def = o;\n      }\n    }\n    return def;\n  }\n\n  /**For a given List of PluginInfo return the instances as a List\n   * @param defClassName The default classname if PluginInfo#className == null\n   * @return The instances initialized\n   */\n  public <T> List<T> initPlugins(List<PluginInfo> pluginInfos, Class<T> type, String defClassName) {\n    if(pluginInfos.isEmpty()) return Collections.emptyList();\n    List<T> result = new ArrayList<T>();\n    for (PluginInfo info : pluginInfos) result.add(createInitInstance(info,type, type.getSimpleName(), defClassName));\n    return result;\n  }\n\n  /**\n   *\n   * @param registry The map to which the instance should be added to. The key is the name attribute\n   * @param type The type of the Plugin. These should be standard ones registered by type.getName() in SolrConfig\n   * @return     The default if any\n   */\n  public <T> T initPlugins(Map<String, T> registry, Class<T> type) {\n    return initPlugins(registry, type, null);\n  }\n\n  public ValueSourceParser getValueSourceParser(String parserName) {\n    return valueSourceParsers.get(parserName);\n  }\n  \n  /**\n   * Manage anything that should be taken care of in case configs change\n   */\n  private void initDeprecatedSupport()\n  {\n    // TODO -- this should be removed in deprecation release...\n    String gettable = solrConfig.get(\"admin/gettableFiles\", null );\n    if( gettable != null ) {\n      log.warn( \n          \"solrconfig.xml uses deprecated <admin/gettableFiles>, Please \"+\n          \"update your config to use the ShowFileRequestHandler.\" );\n      if( getRequestHandler( \"/admin/file\" ) == null ) {\n        NamedList<String> invariants = new NamedList<String>();\n        \n        // Hide everything...\n        Set<String> hide = new HashSet<String>();\n\n        for (String file : solrConfig.getResourceLoader().listConfigDir()) {\n          hide.add(file.toUpperCase(Locale.ROOT));\n        }    \n        \n        // except the \"gettable\" list\n        StringTokenizer st = new StringTokenizer( gettable );\n        while( st.hasMoreTokens() ) {\n          hide.remove( st.nextToken().toUpperCase(Locale.ROOT) );\n        }\n        for( String s : hide ) {\n          invariants.add( ShowFileRequestHandler.HIDDEN, s );\n        }\n        \n        NamedList<Object> args = new NamedList<Object>();\n        args.add( \"invariants\", invariants );\n        ShowFileRequestHandler handler = new ShowFileRequestHandler();\n        handler.init( args );\n        reqHandlers.register(\"/admin/file\", handler);\n\n        log.warn( \"adding ShowFileRequestHandler with hidden files: \"+hide );\n      }\n    }\n\n    String facetSort = solrConfig.get(\"//bool[@name='facet.sort']\", null);\n    if (facetSort != null) {\n      log.warn( \n          \"solrconfig.xml uses deprecated <bool name='facet.sort'>. Please \"+\n          \"update your config to use <string name='facet.sort'>.\");\n    }\n  } \n\n  public CoreDescriptor getCoreDescriptor() {\n    return coreDescriptor;\n  }\n\n  public IndexDeletionPolicyWrapper getDeletionPolicy(){\n    return solrDelPolicy;\n  }\n\n  public ReentrantLock getRuleExpiryLock() {\n    return ruleExpiryLock;\n  }\n\n  /////////////////////////////////////////////////////////////////////\n  // SolrInfoMBean stuff: Statistics and Module Info\n  /////////////////////////////////////////////////////////////////////\n\n  @Override\n  public String getVersion() {\n    return SolrCore.version;\n  }\n\n  @Override\n  public String getDescription() {\n    return \"SolrCore\";\n  }\n\n  @Override\n  public Category getCategory() {\n    return Category.CORE;\n  }\n\n  @Override\n  public String getSource() {\n    return \"$URL$\";\n  }\n\n  @Override\n  public URL[] getDocs() {\n    return null;\n  }\n\n  @Override\n  public NamedList getStatistics() {\n    NamedList<Object> lst = new SimpleOrderedMap<Object>();\n    lst.add(\"coreName\", name==null ? \"(null)\" : name);\n    lst.add(\"startTime\", new Date(startTime));\n    lst.add(\"refCount\", getOpenCount());\n    lst.add(\"instanceDir\", resourceLoader.getInstanceDir());\n    lst.add(\"indexDir\", getIndexDir());\n\n    CoreDescriptor cd = getCoreDescriptor();\n    if (cd != null) {\n      if (null != cd && cd.getCoreContainer() != null) {\n        lst.add(\"aliases\", getCoreDescriptor().getCoreContainer().getCoreNames(this));\n      }\n      CloudDescriptor cloudDesc = cd.getCloudDescriptor();\n      if (cloudDesc != null) {\n        String collection = cloudDesc.getCollectionName();\n        if (collection == null) {\n          collection = \"_notset_\";\n        }\n        lst.add(\"collection\", collection);\n        String shard = cloudDesc.getShardId();\n        if (shard == null) {\n          shard = \"_auto_\";\n        }\n        lst.add(\"shard\", shard);\n      }\n    }\n    \n    return lst;\n  }\n  \n  public Codec getCodec() {\n    return codec;\n  }\n\n  public final class LazyQueryResponseWriterWrapper implements QueryResponseWriter {\n    private SolrCore _core;\n    private String _className;\n    private NamedList _args;\n    private QueryResponseWriter _writer;\n\n    public LazyQueryResponseWriterWrapper(SolrCore core, String className, NamedList args) {\n      _core = core;\n      _className = className;\n      _args = args;\n      _writer = null;\n    }\n\n    public synchronized QueryResponseWriter getWrappedWriter()\n    {\n      if( _writer == null ) {\n        try {\n          QueryResponseWriter writer = createQueryResponseWriter(_className);\n          writer.init( _args );\n          _writer = writer;\n        }\n        catch( Exception ex ) {\n          throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, \"lazy loading error\", ex );\n        }\n      }\n      return _writer;\n    }\n\n\n    @Override\n    public void init(NamedList args) {\n      // do nothing\n    }\n\n    @Override\n    public void write(Writer writer, SolrQueryRequest request, SolrQueryResponse response) throws IOException {\n      getWrappedWriter().write(writer, request, response);\n    }\n\n    @Override\n    public String getContentType(SolrQueryRequest request, SolrQueryResponse response) {\n      return getWrappedWriter().getContentType(request, response);\n    }\n  }\n}\n\n\n\n",
        "methodName": "<init>",
        "exampleID": 36,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/core/SolrCore.java",
        "line": "692",
        "source": "cd",
        "sourceLine": "855",
        "qualifier": "Possible null pointer dereference of $$cd/$",
        "steps": [
            {
                "exampleID": 37
            }
        ],
        "line_number": "855"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/core/StandardDirectoryFactory.java",
        "rawCode": "package org.apache.solr.core;\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.File;\nimport java.io.IOException;\n\nimport org.apache.commons.io.FileUtils;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.FSDirectory;\nimport org.apache.lucene.store.IOContext;\nimport org.apache.lucene.store.NRTCachingDirectory;\nimport org.apache.lucene.store.RateLimitedDirectoryWrapper;\nimport org.apache.solr.core.CachingDirectoryFactory.CacheValue;\n\n/**\n * Directory provider which mimics original Solr \n * {@link org.apache.lucene.store.FSDirectory} based behavior.\n * \n * File based DirectoryFactory implementations generally extend\n * this class.\n * \n */\npublic class StandardDirectoryFactory extends CachingDirectoryFactory {\n\n  @Override\n  protected Directory create(String path, DirContext dirContext) throws IOException {\n    return FSDirectory.open(new File(path));\n  }\n  \n  @Override\n  public String normalize(String path) throws IOException {\n    String cpath = new File(path).getCanonicalPath();\n    \n    return super.normalize(cpath);\n  }\n  \n  @Override\n  public boolean exists(String path) throws IOException {\n    // we go by the persistent storage ... \n    File dirFile = new File(path);\n    return dirFile.canRead() && dirFile.list().length > 0;\n  }\n  \n  public boolean isPersistent() {\n    return true;\n  }\n  \n  @Override\n  public boolean isAbsolute(String path) {\n    // back compat\n    return new File(path).isAbsolute();\n  }\n  \n  @Override\n  protected void removeDirectory(CacheValue cacheValue) throws IOException {\n    File dirFile = new File(cacheValue.path);\n    FileUtils.deleteDirectory(dirFile);\n  }\n  \n  /**\n   * Override for more efficient moves.\n   * \n   * Intended for use with replication - use\n   * carefully - some Directory wrappers will\n   * cache files for example.\n   * \n   * This implementation works with two wrappers:\n   * NRTCachingDirectory and RateLimitedDirectoryWrapper.\n   * \n   * You should first {@link Directory#sync(java.util.Collection)} any file that will be \n   * moved or avoid cached files through settings.\n   * \n   * @throws IOException\n   *           If there is a low-level I/O error.\n   */\n  @Override\n  public void move(Directory fromDir, Directory toDir, String fileName, IOContext ioContext)\n      throws IOException {\n    \n    Directory baseFromDir = getBaseDir(fromDir);\n    Directory baseToDir = getBaseDir(toDir);\n    \n    if (baseFromDir instanceof FSDirectory && baseToDir instanceof FSDirectory) {\n      File dir1 = ((FSDirectory) baseFromDir).getDirectory();\n      File dir2 = ((FSDirectory) baseToDir).getDirectory();\n      File indexFileInTmpDir = new File(dir1, fileName);\n      File indexFileInIndex = new File(dir2, fileName);\n      boolean success = indexFileInTmpDir.renameTo(indexFileInIndex);\n      if (success) {\n        return;\n      }\n    }\n\n    super.move(fromDir, toDir, fileName, ioContext);\n  }\n\n  // special hack to work with NRTCachingDirectory and RateLimitedDirectoryWrapper\n  private Directory getBaseDir(Directory dir) {\n    Directory baseDir;\n    if (dir instanceof NRTCachingDirectory) {\n      baseDir = ((NRTCachingDirectory)dir).getDelegate();\n    } else if (dir instanceof RateLimitedDirectoryWrapper) {\n      baseDir = ((RateLimitedDirectoryWrapper)dir).getDelegate();\n    } else {\n      baseDir = dir;\n    }\n    \n    return baseDir;\n  }\n\n}\n",
        "methodName": "exists",
        "exampleID": 38,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/core/StandardDirectoryFactory.java",
        "line": "56",
        "source": "?",
        "sourceLine": "56",
        "qualifier": "Possible null pointer dereference of the $$value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 39
            }
        ],
        "line_number": "56"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrOutputFormat.java",
        "rawCode": "/**\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.solr.hadoop;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.net.URI;\nimport java.util.Arrays;\nimport java.util.HashSet;\nimport java.util.Locale;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.zip.ZipEntry;\nimport java.util.zip.ZipOutputStream;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.filecache.DistributedCache;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.JobContext;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class SolrOutputFormat<K, V> extends FileOutputFormat<K, V> {\n\n  private static final Logger LOG = LoggerFactory.getLogger(SolrOutputFormat.class);\n\n  /**\n   * The parameter used to pass the solr config zip file information. This will\n   * be the hdfs path to the configuration zip file\n   */\n  public static final String SETUP_OK = \"solr.output.format.setup\";\n\n  /** The key used to pass the zip file name through the configuration. */\n  public static final String ZIP_NAME = \"solr.zip.name\";\n\n  /**\n   * The base name of the zip file containing the configuration information.\n   * This file is passed via the distributed cache using a unique name, obtained\n   * via {@link #getZipName(Configuration jobConf)}.\n   */\n  public static final String ZIP_FILE_BASE_NAME = \"solr.zip\";\n\n  /**\n   * The key used to pass the boolean configuration parameter that instructs for\n   * regular or zip file output\n   */\n  public static final String OUTPUT_ZIP_FILE = \"solr.output.zip.format\";\n\n  static int defaultSolrWriterThreadCount = 0;\n\n  public static final String SOLR_WRITER_THREAD_COUNT = \"solr.record.writer.num.threads\";\n\n  static int defaultSolrWriterQueueSize = 1;\n\n  public static final String SOLR_WRITER_QUEUE_SIZE = \"solr.record.writer.max.queues.size\";\n\n  static int defaultSolrBatchSize = 20;\n\n  public static final String SOLR_RECORD_WRITER_BATCH_SIZE = \"solr.record.writer.batch.size\";\n\n  public static final String SOLR_RECORD_WRITER_MAX_SEGMENTS = \"solr.record.writer.maxSegments\";\n\n  public static String getSetupOk() {\n    return SETUP_OK;\n  }\n\n  /** Get the number of threads used for index writing */\n  public static void setSolrWriterThreadCount(int count, Configuration conf) {\n    conf.setInt(SOLR_WRITER_THREAD_COUNT, count);\n  }\n\n  /** Set the number of threads used for index writing */\n  public static int getSolrWriterThreadCount(Configuration conf) {\n    return conf.getInt(SOLR_WRITER_THREAD_COUNT, defaultSolrWriterThreadCount);\n  }\n\n  /**\n   * Set the maximum size of the the queue for documents to be written to the\n   * index.\n   */\n  public static void setSolrWriterQueueSize(int count, Configuration conf) {\n    conf.setInt(SOLR_WRITER_QUEUE_SIZE, count);\n  }\n\n  /** Return the maximum size for the number of documents pending index writing. */\n  public static int getSolrWriterQueueSize(Configuration conf) {\n    return conf.getInt(SOLR_WRITER_QUEUE_SIZE, defaultSolrWriterQueueSize);\n  }\n\n  /**\n   * Return the file name portion of the configuration zip file, from the\n   * configuration.\n   */\n  public static String getZipName(Configuration conf) {\n    return conf.get(ZIP_NAME, ZIP_FILE_BASE_NAME);\n  }\n\n  /**\n   * configure the job to output zip files of the output index, or full\n   * directory trees. Zip files are about 1/5th the size of the raw index, and\n   * much faster to write, but take more cpu to create.\n   * \n   * @param output true if should output zip files\n   * @param conf to use\n   */\n  public static void setOutputZipFormat(boolean output, Configuration conf) {\n    conf.setBoolean(OUTPUT_ZIP_FILE, output);\n  }\n\n  /**\n   * return true if the output should be a zip file of the index, rather than\n   * the raw index\n   * \n   * @param conf to use\n   * @return true if output zip files is on\n   */\n  public static boolean isOutputZipFormat(Configuration conf) {\n    return conf.getBoolean(OUTPUT_ZIP_FILE, false);\n  }\n  \n  public static String getOutputName(JobContext job) {\n    return FileOutputFormat.getOutputName(job);\n  }\n\n  @Override\n  public void checkOutputSpecs(JobContext job) throws IOException {\n    super.checkOutputSpecs(job);\n    if (job.getConfiguration().get(SETUP_OK) == null) {\n      throw new IOException(\"Solr home cache not set up!\");\n    }\n  }\n\n\n  @Override\n  public RecordWriter<K, V> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {\n    Utils.getLogConfigFile(context.getConfiguration());\n    Path workDir = getDefaultWorkFile(context, \"\");\n    int batchSize = getBatchSize(context.getConfiguration());\n    return new SolrRecordWriter<K, V>(context, workDir, batchSize);\n  }\n\n  public static void setupSolrHomeCache(File solrHomeDir, Job job) throws IOException{\n    File solrHomeZip = createSolrHomeZip(solrHomeDir);\n    addSolrConfToDistributedCache(job, solrHomeZip);\n  }\n\n  public static File createSolrHomeZip(File solrHomeDir) throws IOException {\n    return createSolrHomeZip(solrHomeDir, false);\n  }\n\n  private static File createSolrHomeZip(File solrHomeDir, boolean safeToModify) throws IOException {\n    if (solrHomeDir == null || !(solrHomeDir.exists() && solrHomeDir.isDirectory())) {\n      throw new IOException(\"Invalid solr home: \" + solrHomeDir);\n    }\n    File solrHomeZip = File.createTempFile(\"solr\", \".zip\");\n    createZip(solrHomeDir, solrHomeZip);\n    return solrHomeZip;\n  }\n\n  public static void addSolrConfToDistributedCache(Job job, File solrHomeZip)\n      throws IOException {\n    // Make a reasonably unique name for the zip file in the distributed cache\n    // to avoid collisions if multiple jobs are running.\n    String hdfsZipName = UUID.randomUUID().toString() + '.'\n        + ZIP_FILE_BASE_NAME;\n    Configuration jobConf = job.getConfiguration();\n    jobConf.set(ZIP_NAME, hdfsZipName);\n\n    Path zipPath = new Path(\"/tmp\", getZipName(jobConf));\n    FileSystem fs = FileSystem.get(jobConf);\n    fs.copyFromLocalFile(new Path(solrHomeZip.toString()), zipPath);\n    final URI baseZipUrl = fs.getUri().resolve(\n        zipPath.toString() + '#' + getZipName(jobConf));\n\n    DistributedCache.addCacheArchive(baseZipUrl, jobConf);\n    LOG.debug(\"Set Solr distributed cache: {}\", Arrays.asList(job.getCacheArchives()));\n    LOG.debug(\"Set zipPath: {}\", zipPath);\n    // Actually send the path for the configuration zip file\n    jobConf.set(SETUP_OK, zipPath.toString());\n  }\n\n  private static void createZip(File dir, File out) throws IOException {\n    HashSet<File> files = new HashSet<File>();\n    // take only conf/ and lib/\n    for (String allowedDirectory : SolrRecordWriter\n        .getAllowedConfigDirectories()) {\n      File configDir = new File(dir, allowedDirectory);\n      boolean configDirExists;\n      /** If the directory does not exist, and is required, bail out */\n      if (!(configDirExists = configDir.exists())\n          && SolrRecordWriter.isRequiredConfigDirectory(allowedDirectory)) {\n        throw new IOException(String.format(Locale.ENGLISH,\n            \"required configuration directory %s is not present in %s\",\n            allowedDirectory, dir));\n      }\n      if (!configDirExists) {\n        continue;\n      }\n      listFiles(configDir, files); // Store the files in the existing, allowed\n                                   // directory configDir, in the list of files\n                                   // to store in the zip file\n    }\n\n    out.delete();\n    int subst = dir.toString().length();\n    ZipOutputStream zos = new ZipOutputStream(new FileOutputStream(out));\n    byte[] buf = new byte[1024];\n    for (File f : files) {\n      ZipEntry ze = new ZipEntry(f.toString().substring(subst));\n      zos.putNextEntry(ze);\n      InputStream is = new FileInputStream(f);\n      int cnt;\n      while ((cnt = is.read(buf)) >= 0) {\n        zos.write(buf, 0, cnt);\n      }\n      is.close();\n      zos.flush();\n      zos.closeEntry();\n    }\n    \n    ZipEntry ze = new ZipEntry(\"solr.xml\");\n    zos.putNextEntry(ze);\n    zos.write(\"<cores><core name=\\\"collection1\\\" instanceDir=\\\".\\\"/></cores>\".getBytes(\"UTF-8\"));\n    zos.flush();\n    zos.closeEntry();\n    zos.close();\n  }\n\n  private static void listFiles(File dir, Set<File> files) throws IOException {\n    File[] list = dir.listFiles();\n    \n    if (list == null && dir.isFile())  {\n      files.add(dir);\n      return;\n    }\n    \n    for (File f : list) {\n      if (f.isFile()) {\n        files.add(f);\n      } else {\n        listFiles(f, files);\n      }\n    }\n  }\n\n  public static int getBatchSize(Configuration jobConf) {\n    // TODO Auto-generated method stub\n    return jobConf.getInt(SolrOutputFormat.SOLR_RECORD_WRITER_BATCH_SIZE,\n        defaultSolrBatchSize);\n  }\n\n  public static void setBatchSize(int count, Configuration jobConf) {\n    jobConf.setInt(SOLR_RECORD_WRITER_BATCH_SIZE, count);\n  }\n\n}\n",
        "methodName": "listFiles",
        "exampleID": 40,
        "dataset": "spotbugs",
        "filepath": "/solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrOutputFormat.java",
        "line": "254",
        "source": "list",
        "sourceLine": "259",
        "qualifier": "Possible null pointer dereference of $$list/$",
        "steps": [
            {
                "exampleID": 41
            }
        ],
        "line_number": "259"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter.java",
        "rawCode": "/**\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.solr.hadoop;\n\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Properties;\nimport java.util.Set;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.filecache.DistributedCache;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.mapreduce.RecordWriter;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;\nimport org.apache.hadoop.mapreduce.TaskID;\nimport org.apache.solr.hadoop.SolrOutputFormat;\nimport org.apache.solr.client.solrj.SolrServerException;\nimport org.apache.solr.client.solrj.embedded.EmbeddedSolrServer;\nimport org.apache.solr.common.SolrInputDocument;\nimport org.apache.solr.core.CoreContainer;\nimport org.apache.solr.core.CoreDescriptor;\nimport org.apache.solr.core.HdfsDirectoryFactory;\nimport org.apache.solr.core.SolrCore;\nimport org.apache.solr.core.SolrResourceLoader;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nclass SolrRecordWriter<K, V> extends RecordWriter<K, V> {\n  \n  private static final Logger LOG = LoggerFactory.getLogger(SolrRecordWriter.class);\n\n  public final static List<String> allowedConfigDirectories = new ArrayList<String>(\n      Arrays.asList(new String[] { \"conf\", \"lib\", \"solr.xml\" }));\n\n  public final static Set<String> requiredConfigDirectories = new HashSet<String>();\n  \n  static {\n    requiredConfigDirectories.add(\"conf\");\n  }\n\n  /**\n   * Return the list of directories names that may be included in the\n   * configuration data passed to the tasks.\n   * \n   * @return an UnmodifiableList of directory names\n   */\n  public static List<String> getAllowedConfigDirectories() {\n    return Collections.unmodifiableList(allowedConfigDirectories);\n  }\n\n  /**\n   * check if the passed in directory is required to be present in the\n   * configuration data set.\n   * \n   * @param directory The directory to check\n   * @return true if the directory is required.\n   */\n  public static boolean isRequiredConfigDirectory(final String directory) {\n    return requiredConfigDirectories.contains(directory);\n  }\n\n  /** The path that the final index will be written to */\n\n  /** The location in a local temporary directory that the index is built in. */\n\n//  /**\n//   * If true, create a zip file of the completed index in the final storage\n//   * location A .zip will be appended to the final output name if it is not\n//   * already present.\n//   */\n//  private boolean outputZipFile = false;\n\n  private final HeartBeater heartBeater;\n  private final BatchWriter batchWriter;\n  private final List<SolrInputDocument> batch;\n  private final int batchSize;\n  private long numDocsWritten = 0;\n  private long nextLogTime = System.currentTimeMillis();\n\n  private static HashMap<TaskID, Reducer<?,?,?,?>.Context> contextMap = new HashMap<TaskID, Reducer<?,?,?,?>.Context>();\n  \n  public SolrRecordWriter(TaskAttemptContext context, Path outputShardDir, int batchSize) {\n    this.batchSize = batchSize;\n    this.batch = new ArrayList(batchSize);\n    Configuration conf = context.getConfiguration();\n\n    // setLogLevel(\"org.apache.solr.core\", \"WARN\");\n    // setLogLevel(\"org.apache.solr.update\", \"WARN\");\n\n    heartBeater = new HeartBeater(context);\n    try {\n      heartBeater.needHeartBeat();\n\n      Path solrHomeDir = SolrRecordWriter.findSolrConfig(conf);\n      FileSystem fs = outputShardDir.getFileSystem(conf);\n      EmbeddedSolrServer solr = createEmbeddedSolrServer(solrHomeDir, fs, outputShardDir);\n      batchWriter = new BatchWriter(solr, batchSize,\n          context.getTaskAttemptID().getTaskID(),\n          SolrOutputFormat.getSolrWriterThreadCount(conf),\n          SolrOutputFormat.getSolrWriterQueueSize(conf));\n\n    } catch (Exception e) {\n      throw new IllegalStateException(String.format(Locale.ENGLISH,\n          \"Failed to initialize record writer for %s, %s\", context.getJobName(), conf\n              .get(\"mapred.task.id\")), e);\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n  }\n\n  public static EmbeddedSolrServer createEmbeddedSolrServer(Path solrHomeDir, FileSystem fs, Path outputShardDir)\n      throws IOException {\n\n    if (solrHomeDir == null) {\n      throw new IOException(\"Unable to find solr home setting\");\n    }\n    LOG.info(\"Creating embedded Solr server with solrHomeDir: \" + solrHomeDir + \", fs: \" + fs + \", outputShardDir: \" + outputShardDir);\n\n    Path solrDataDir = new Path(outputShardDir, \"data\");\n\n    String dataDirStr = solrDataDir.toUri().toString();\n\n    SolrResourceLoader loader = new SolrResourceLoader(solrHomeDir.toString(), null, null);\n\n    LOG.info(String\n        .format(Locale.ENGLISH, \n            \"Constructed instance information solr.home %s (%s), instance dir %s, conf dir %s, writing index to solr.data.dir %s, with permdir %s\",\n            solrHomeDir, solrHomeDir.toUri(), loader.getInstanceDir(),\n            loader.getConfigDir(), dataDirStr, outputShardDir));\n\n    // TODO: This is fragile and should be well documented\n    System.setProperty(\"solr.directoryFactory\", HdfsDirectoryFactory.class.getName()); \n    System.setProperty(\"solr.lock.type\", \"hdfs\"); \n    System.setProperty(\"solr.hdfs.nrtcachingdirectory\", \"false\");\n    System.setProperty(\"solr.hdfs.blockcache.enabled\", \"false\");\n    System.setProperty(\"solr.autoCommit.maxTime\", \"-1\");\n    System.setProperty(\"solr.autoSoftCommit.maxTime\", \"-1\");\n    \n    CoreContainer container = new CoreContainer(loader);\n    container.load();\n    \n    Properties props = new Properties();\n    props.setProperty(CoreDescriptor.CORE_DATADIR, dataDirStr);\n    \n    CoreDescriptor descr = new CoreDescriptor(container, \"core1\", solrHomeDir.toString(), props);\n    \n    SolrCore core = container.create(descr);\n    container.register(core, false);\n\n    EmbeddedSolrServer solr = new EmbeddedSolrServer(container, \"core1\");\n    return solr;\n  }\n\n  public static void incrementCounter(TaskID taskId, String groupName, String counterName, long incr) {\n    Reducer<?,?,?,?>.Context context = contextMap.get(taskId);\n    if (context != null) {\n      context.getCounter(groupName, counterName).increment(incr);\n    }\n  }\n\n  public static void incrementCounter(TaskID taskId, Enum counterName, long incr) {\n    Reducer<?,?,?,?>.Context context = contextMap.get(taskId);\n    if (context != null) {\n      context.getCounter(counterName).increment(incr);\n    }\n  }\n\n  public static void addReducerContext(Reducer<?,?,?,?>.Context context) {\n    TaskID taskID = context.getTaskAttemptID().getTaskID();\n    contextMap.put(taskID, context);\n  }\n\n  public static Path findSolrConfig(Configuration conf) throws IOException {\n    Path solrHome = null;\n    // FIXME when mrunit supports the new cache apis\n    //URI[] localArchives = context.getCacheArchives();\n    Path[] localArchives = DistributedCache.getLocalCacheArchives(conf);\n    if (localArchives.length == 0) {\n      throw new IOException(String.format(Locale.ENGLISH,\n          \"No local cache archives, where is %s:%s\", SolrOutputFormat\n              .getSetupOk(), SolrOutputFormat.getZipName(conf)));\n    }\n    for (Path unpackedDir : localArchives) {\n      // Only logged if debugging\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(String.format(Locale.ENGLISH, \"Examining unpack directory %s for %s\",\n            unpackedDir, SolrOutputFormat.getZipName(conf)));\n\n        ProcessBuilder lsCmd = new ProcessBuilder(new String[] { \"/bin/ls\",\n            \"-lR\", unpackedDir.toString() });\n        lsCmd.redirectErrorStream();\n        Process ls = lsCmd.start();\n        byte[] buf = new byte[16 * 1024];\n        InputStream all = ls.getInputStream();\n        try {\n          int count;\n          while ((count = all.read(buf)) >= 0) {\n            System.err.write(buf, 0, count);\n          }\n        } catch (IOException ignore) {\n        } finally {\n          all.close();\n        }\n        String exitValue;\n        try {\n          exitValue = String.valueOf(ls.waitFor());\n        } catch (InterruptedException e) {\n          exitValue = \"interrupted\";\n        }\n        System.err.format(\"Exit value of 'ls -lR' is %s%n\", exitValue);\n      }\n      if (unpackedDir.getName().equals(SolrOutputFormat.getZipName(conf))) {\n        LOG.info(\"Using this unpacked directory as solr home: {}\", unpackedDir);\n        solrHome = unpackedDir;\n        break;\n      }\n    }\n\n    return solrHome;\n  }\n\n  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.currentTimeMillis() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += 10000;\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n  @Override\n  public void close(TaskAttemptContext context) throws IOException, InterruptedException {\n    if (context != null) {\n      heartBeater.setProgress(context);\n    }\n    try {\n      heartBeater.needHeartBeat();\n      if (batch.size() > 0) {\n        batchWriter.queueBatch(batch);\n        numDocsWritten += batch.size();\n        batch.clear();\n      }\n      LOG.info(\"docsWritten: {}\", numDocsWritten);\n      batchWriter.close(context);\n//      if (outputZipFile) {\n//        context.setStatus(\"Writing Zip\");\n//        packZipFile(); // Written to the perm location\n//      } else {\n//        context.setStatus(\"Copying Index\");\n//        fs.completeLocalOutput(perm, temp); // copy to dfs\n//      }\n    } catch (Exception e) {\n      if (e instanceof IOException) {\n        throw (IOException) e;\n      }\n      throw new IOException(e);\n    } finally {\n      heartBeater.cancelHeartBeat();\n      heartBeater.close();\n//      File tempFile = new File(temp.toString());\n//      if (tempFile.exists()) {\n//        FileUtils.forceDelete(new File(temp.toString()));\n//      }\n    }\n\n    context.setStatus(\"Done\");\n  }\n\n//  private void packZipFile() throws IOException {\n//    FSDataOutputStream out = null;\n//    ZipOutputStream zos = null;\n//    int zipCount = 0;\n//    LOG.info(\"Packing zip file for \" + perm);\n//    try {\n//      out = fs.create(perm, false);\n//      zos = new ZipOutputStream(out);\n//\n//      String name = perm.getName().replaceAll(\".zip$\", \"\");\n//      LOG.info(\"adding index directory\" + temp);\n//      zipCount = zipDirectory(conf, zos, name, temp.toString(), temp);\n//      /**\n//      for (String configDir : allowedConfigDirectories) {\n//        if (!isRequiredConfigDirectory(configDir)) {\n//          continue;\n//        }\n//        final Path confPath = new Path(solrHome, configDir);\n//        LOG.info(\"adding configdirectory\" + confPath);\n//\n//        zipCount += zipDirectory(conf, zos, name, solrHome.toString(), confPath);\n//      }\n//      **/\n//    } catch (Throwable ohFoo) {\n//      LOG.error(\"packZipFile exception\", ohFoo);\n//      if (ohFoo instanceof RuntimeException) {\n//        throw (RuntimeException) ohFoo;\n//      }\n//      if (ohFoo instanceof IOException) {\n//        throw (IOException) ohFoo;\n//      }\n//      throw new IOException(ohFoo);\n//\n//    } finally {\n//      if (zos != null) {\n//        if (zipCount == 0) { // If no entries were written, only close out, as\n//                             // the zip will throw an error\n//          LOG.error(\"No entries written to zip file \" + perm);\n//          fs.delete(perm, false);\n//          // out.close();\n//        } else {\n//          LOG.info(String.format(\"Wrote %d items to %s for %s\", zipCount, perm,\n//              temp));\n//          zos.close();\n//        }\n//      }\n//    }\n//  }\n//\n//  /**\n//   * Write a file to a zip output stream, removing leading path name components\n//   * from the actual file name when creating the zip file entry.\n//   * \n//   * The entry placed in the zip file is <code>baseName</code>/\n//   * <code>relativePath</code>, where <code>relativePath</code> is constructed\n//   * by removing a leading <code>root</code> from the path for\n//   * <code>itemToZip</code>.\n//   * \n//   * If <code>itemToZip</code> is an empty directory, it is ignored. If\n//   * <code>itemToZip</code> is a directory, the contents of the directory are\n//   * added recursively.\n//   * \n//   * @param zos The zip output stream\n//   * @param baseName The base name to use for the file name entry in the zip\n//   *        file\n//   * @param root The path to remove from <code>itemToZip</code> to make a\n//   *        relative path name\n//   * @param itemToZip The path to the file to be added to the zip file\n//   * @return the number of entries added\n//   * @throws IOException\n//   */\n//  static public int zipDirectory(final Configuration conf,\n//      final ZipOutputStream zos, final String baseName, final String root,\n//      final Path itemToZip) throws IOException {\n//    LOG\n//        .info(String\n//            .format(\"zipDirectory: %s %s %s\", baseName, root, itemToZip));\n//    LocalFileSystem localFs = FileSystem.getLocal(conf);\n//    int count = 0;\n//\n//    final FileStatus itemStatus = localFs.getFileStatus(itemToZip);\n//    if (itemStatus.isDirectory()) {\n//      final FileStatus[] statai = localFs.listStatus(itemToZip);\n//\n//      // Add a directory entry to the zip file\n//      final String zipDirName = relativePathForZipEntry(itemToZip.toUri()\n//          .getPath(), baseName, root);\n//      final ZipEntry dirZipEntry = new ZipEntry(zipDirName\n//          + Path.SEPARATOR_CHAR);\n//      LOG.info(String.format(\"Adding directory %s to zip\", zipDirName));\n//      zos.putNextEntry(dirZipEntry);\n//      zos.closeEntry();\n//      count++;\n//\n//      if (statai == null || statai.length == 0) {\n//        LOG.info(String.format(\"Skipping empty directory %s\", itemToZip));\n//        return count;\n//      }\n//      for (FileStatus status : statai) {\n//        count += zipDirectory(conf, zos, baseName, root, status.getPath());\n//      }\n//      LOG.info(String.format(\"Wrote %d entries for directory %s\", count,\n//          itemToZip));\n//      return count;\n//    }\n//\n//    final String inZipPath = relativePathForZipEntry(itemToZip.toUri()\n//        .getPath(), baseName, root);\n//\n//    if (inZipPath.length() == 0) {\n//      LOG.warn(String.format(\"Skipping empty zip file path for %s (%s %s)\",\n//          itemToZip, root, baseName));\n//      return 0;\n//    }\n//\n//    // Take empty files in case the place holder is needed\n//    FSDataInputStream in = null;\n//    try {\n//      in = localFs.open(itemToZip);\n//      final ZipEntry ze = new ZipEntry(inZipPath);\n//      ze.setTime(itemStatus.getModificationTime());\n//      // Comments confuse looking at the zip file\n//      // ze.setComment(itemToZip.toString());\n//      zos.putNextEntry(ze);\n//\n//      IOUtils.copyBytes(in, zos, conf, false);\n//      zos.closeEntry();\n//      LOG.info(String.format(\"Wrote %d entries for file %s\", count, itemToZip));\n//      return 1;\n//    } finally {\n//      in.close();\n//    }\n//\n//  }\n//\n//  static String relativePathForZipEntry(final String rawPath,\n//      final String baseName, final String root) {\n//    String relativePath = rawPath.replaceFirst(Pattern.quote(root.toString()),\n//        \"\");\n//    LOG.info(String.format(\"RawPath %s, baseName %s, root %s, first %s\",\n//        rawPath, baseName, root, relativePath));\n//\n//    if (relativePath.startsWith(Path.SEPARATOR)) {\n//      relativePath = relativePath.substring(1);\n//    }\n//    LOG.info(String.format(\n//        \"RawPath %s, baseName %s, root %s, post leading slash %s\", rawPath,\n//        baseName, root, relativePath));\n//    if (relativePath.isEmpty()) {\n//      LOG.warn(String.format(\n//          \"No data after root (%s) removal from raw path %s\", root, rawPath));\n//      return baseName;\n//    }\n//    // Construct the path that will be written to the zip file, including\n//    // removing any leading '/' characters\n//    String inZipPath = baseName + Path.SEPARATOR_CHAR + relativePath;\n//\n//    LOG.info(String.format(\"RawPath %s, baseName %s, root %s, inZip 1 %s\",\n//        rawPath, baseName, root, inZipPath));\n//    if (inZipPath.startsWith(Path.SEPARATOR)) {\n//      inZipPath = inZipPath.substring(1);\n//    }\n//    LOG.info(String.format(\"RawPath %s, baseName %s, root %s, inZip 2 %s\",\n//        rawPath, baseName, root, inZipPath));\n//\n//    return inZipPath;\n//\n//  }\n//  \n  /*\n  static boolean setLogLevel(String packageName, String level) {\n    Log logger = LogFactory.getLog(packageName);\n    if (logger == null) {\n      return false;\n    }\n    // look for: org.apache.commons.logging.impl.SLF4JLocationAwareLog\n    LOG.warn(\"logger class:\"+logger.getClass().getName());\n    if (logger instanceof Log4JLogger) {\n      process(((Log4JLogger) logger).getLogger(), level);\n      return true;\n    }\n    if (logger instanceof Jdk14Logger) {\n      process(((Jdk14Logger) logger).getLogger(), level);\n      return true;\n    }\n    return false;\n  }\n\n  public static void process(org.apache.log4j.Logger log, String level) {\n    if (level != null) {\n      log.setLevel(org.apache.log4j.Level.toLevel(level));\n    }\n  }\n\n  public static void process(java.util.logging.Logger log, String level) {\n    if (level != null) {\n      log.setLevel(java.util.logging.Level.parse(level));\n    }\n  }\n  */\n}\n",
        "methodName": "close",
        "exampleID": 42,
        "dataset": "spotbugs",
        "filepath": "/solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter.java",
        "line": "279",
        "source": "context",
        "sourceLine": "312",
        "qualifier": "Possible null pointer dereference of $$context/$",
        "steps": [
            {
                "exampleID": 43
            }
        ],
        "line_number": "312"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/handler/SnapPuller.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.solr.handler;\n\nimport static org.apache.lucene.util.IOUtils.CHARSET_UTF_8;\nimport static org.apache.solr.handler.ReplicationHandler.ALIAS;\nimport static org.apache.solr.handler.ReplicationHandler.CHECKSUM;\nimport static org.apache.solr.handler.ReplicationHandler.CMD_DETAILS;\nimport static org.apache.solr.handler.ReplicationHandler.CMD_GET_FILE;\nimport static org.apache.solr.handler.ReplicationHandler.CMD_GET_FILE_LIST;\nimport static org.apache.solr.handler.ReplicationHandler.CMD_INDEX_VERSION;\nimport static org.apache.solr.handler.ReplicationHandler.COMMAND;\nimport static org.apache.solr.handler.ReplicationHandler.COMPRESSION;\nimport static org.apache.solr.handler.ReplicationHandler.CONF_FILES;\nimport static org.apache.solr.handler.ReplicationHandler.CONF_FILE_SHORT;\nimport static org.apache.solr.handler.ReplicationHandler.EXTERNAL;\nimport static org.apache.solr.handler.ReplicationHandler.FILE;\nimport static org.apache.solr.handler.ReplicationHandler.FILE_STREAM;\nimport static org.apache.solr.handler.ReplicationHandler.GENERATION;\nimport static org.apache.solr.handler.ReplicationHandler.INTERNAL;\nimport static org.apache.solr.handler.ReplicationHandler.MASTER_URL;\nimport static org.apache.solr.handler.ReplicationHandler.NAME;\nimport static org.apache.solr.handler.ReplicationHandler.OFFSET;\nimport static org.apache.solr.handler.ReplicationHandler.SIZE;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.io.OutputStreamWriter;\nimport java.io.Writer;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.FileChannel;\nimport java.text.SimpleDateFormat;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.Properties;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\nimport java.util.zip.Adler32;\nimport java.util.zip.Checksum;\nimport java.util.zip.InflaterInputStream;\n\nimport org.apache.commons.io.IOUtils;\nimport org.apache.http.client.HttpClient;\nimport org.apache.lucene.index.IndexCommit;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.IndexInput;\nimport org.apache.lucene.store.IndexOutput;\nimport org.apache.solr.client.solrj.SolrServerException;\nimport org.apache.solr.client.solrj.impl.HttpClientUtil;\nimport org.apache.solr.client.solrj.impl.HttpSolrServer;\nimport org.apache.solr.client.solrj.request.QueryRequest;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.SolrException.ErrorCode;\nimport org.apache.solr.common.params.CommonParams;\nimport org.apache.solr.common.params.ModifiableSolrParams;\nimport org.apache.solr.common.params.SolrParams;\nimport org.apache.solr.common.util.ExecutorUtil;\nimport org.apache.solr.common.util.FastInputStream;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.core.DirectoryFactory;\nimport org.apache.solr.core.DirectoryFactory.DirContext;\nimport org.apache.solr.core.IndexDeletionPolicyWrapper;\nimport org.apache.solr.core.SolrCore;\nimport org.apache.solr.handler.ReplicationHandler.FileInfo;\nimport org.apache.solr.request.LocalSolrQueryRequest;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.search.SolrIndexSearcher;\nimport org.apache.solr.update.CommitUpdateCommand;\nimport org.apache.solr.util.DefaultSolrThreadFactory;\nimport org.apache.solr.util.FileUtils;\nimport org.apache.solr.util.PropertiesInputStream;\nimport org.apache.solr.util.PropertiesOutputStream;\nimport org.apache.solr.util.RefCounted;\nimport org.eclipse.jetty.util.log.Log;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * <p/> Provides functionality of downloading changed index files as well as config files and a timer for scheduling fetches from the\n * master. </p>\n *\n *\n * @since solr 1.4\n */\npublic class SnapPuller {\n  public static final String INDEX_PROPERTIES = \"index.properties\";\n\n  private static final Logger LOG = LoggerFactory.getLogger(SnapPuller.class.getName());\n\n  private final String masterUrl;\n\n  private final ReplicationHandler replicationHandler;\n\n  private final Integer pollInterval;\n\n  private String pollIntervalStr;\n\n  private ScheduledExecutorService executorService;\n\n  private volatile long executorStartTime;\n\n  private volatile long replicationStartTime;\n\n  private final SolrCore solrCore;\n\n  private volatile List<Map<String, Object>> filesToDownload;\n\n  private volatile List<Map<String, Object>> confFilesToDownload;\n\n  private volatile List<Map<String, Object>> filesDownloaded;\n\n  private volatile List<Map<String, Object>> confFilesDownloaded;\n\n  private volatile Map<String, Object> currentFile;\n\n  private volatile DirectoryFileFetcher dirFileFetcher;\n  \n  private volatile LocalFsFileFetcher localFileFetcher;\n\n  private volatile ExecutorService fsyncService;\n\n  private volatile boolean stop = false;\n\n  private boolean useInternal = false;\n\n  private boolean useExternal = false;\n\n  /**\n   * Disable the timer task for polling\n   */\n  private AtomicBoolean pollDisabled = new AtomicBoolean(false);\n\n  private final HttpClient myHttpClient;\n\n  private static HttpClient createHttpClient(SolrCore core, String connTimeout, String readTimeout, String httpBasicAuthUser, String httpBasicAuthPassword, boolean useCompression) {\n    final ModifiableSolrParams httpClientParams = new ModifiableSolrParams();\n    httpClientParams.set(HttpClientUtil.PROP_CONNECTION_TIMEOUT, connTimeout != null ? connTimeout : \"5000\");\n    httpClientParams.set(HttpClientUtil.PROP_SO_TIMEOUT, readTimeout != null ? readTimeout : \"20000\");\n    httpClientParams.set(HttpClientUtil.PROP_BASIC_AUTH_USER, httpBasicAuthUser);\n    httpClientParams.set(HttpClientUtil.PROP_BASIC_AUTH_PASS, httpBasicAuthPassword);\n    httpClientParams.set(HttpClientUtil.PROP_ALLOW_COMPRESSION, useCompression);\n\n    HttpClient httpClient = HttpClientUtil.createClient(httpClientParams, core.getCoreDescriptor().getCoreContainer().getUpdateShardHandler().getConnectionManager());\n\n    return httpClient;\n  }\n\n  public SnapPuller(final NamedList initArgs, final ReplicationHandler handler, final SolrCore sc) {\n    solrCore = sc;\n    final SolrParams params = SolrParams.toSolrParams(initArgs);\n    String masterUrl = (String) initArgs.get(MASTER_URL);\n    if (masterUrl == null)\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n              \"'masterUrl' is required for a slave\");\n    if (masterUrl.endsWith(\"/replication\")) {\n      masterUrl = masterUrl.substring(0, masterUrl.length()-12);\n      LOG.warn(\"'masterUrl' must be specified without the /replication suffix\");\n    }\n    this.masterUrl = masterUrl;\n    \n    this.replicationHandler = handler;\n    pollIntervalStr = (String) initArgs.get(POLL_INTERVAL);\n    pollInterval = readInterval(pollIntervalStr);\n    String compress = (String) initArgs.get(COMPRESSION);\n    useInternal = INTERNAL.equals(compress);\n    useExternal = EXTERNAL.equals(compress);\n    String connTimeout = (String) initArgs.get(HttpClientUtil.PROP_CONNECTION_TIMEOUT);\n    String readTimeout = (String) initArgs.get(HttpClientUtil.PROP_SO_TIMEOUT);\n    String httpBasicAuthUser = (String) initArgs.get(HttpClientUtil.PROP_BASIC_AUTH_USER);\n    String httpBasicAuthPassword = (String) initArgs.get(HttpClientUtil.PROP_BASIC_AUTH_PASS);\n    myHttpClient = createHttpClient(solrCore, connTimeout, readTimeout, httpBasicAuthUser, httpBasicAuthPassword, useExternal);\n    if (pollInterval != null && pollInterval > 0) {\n      startExecutorService();\n    } else {\n      LOG.info(\" No value set for 'pollInterval'. Timer Task not started.\");\n    }\n  }\n\n  private void startExecutorService() {\n    Runnable task = new Runnable() {\n      @Override\n      public void run() {\n        if (pollDisabled.get()) {\n          LOG.info(\"Poll disabled\");\n          return;\n        }\n        try {\n          LOG.debug(\"Polling for index modifications\");\n          executorStartTime = System.currentTimeMillis();\n          replicationHandler.doFetch(null, false);\n        } catch (Exception e) {\n          LOG.error(\"Exception in fetching index\", e);\n        }\n      }\n    };\n    executorService = Executors.newSingleThreadScheduledExecutor(\n        new DefaultSolrThreadFactory(\"snapPuller\"));\n    long initialDelay = pollInterval - (System.currentTimeMillis() % pollInterval);\n    executorService.scheduleAtFixedRate(task, initialDelay, pollInterval, TimeUnit.MILLISECONDS);\n    LOG.info(\"Poll Scheduled at an interval of \" + pollInterval + \"ms\");\n  }\n\n  /**\n   * Gets the latest commit version and generation from the master\n   */\n  @SuppressWarnings(\"unchecked\")\n  NamedList getLatestVersion() throws IOException {\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(COMMAND, CMD_INDEX_VERSION);\n    params.set(CommonParams.WT, \"javabin\");\n    params.set(CommonParams.QT, \"/replication\");\n    QueryRequest req = new QueryRequest(params);\n    HttpSolrServer server = new HttpSolrServer(masterUrl, myHttpClient); //XXX modify to use shardhandler\n    NamedList rsp;\n    try {\n      server.setSoTimeout(60000);\n      server.setConnectionTimeout(15000);\n      \n      rsp = server.request(req);\n    } catch (SolrServerException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e.getMessage());\n    } finally {\n      server.shutdown();\n    }\n    return rsp;\n  }\n\n  /**\n   * Fetches the list of files in a given index commit point and updates internal list of files to download.\n   */\n  private void fetchFileList(long gen) throws IOException {\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(COMMAND,  CMD_GET_FILE_LIST);\n    params.set(GENERATION, String.valueOf(gen));\n    params.set(CommonParams.WT, \"javabin\");\n    params.set(CommonParams.QT, \"/replication\");\n    QueryRequest req = new QueryRequest(params);\n    HttpSolrServer server = new HttpSolrServer(masterUrl, myHttpClient);  //XXX modify to use shardhandler\n    try {\n      server.setSoTimeout(60000);\n      server.setConnectionTimeout(15000);\n      NamedList response = server.request(req);\n\n      List<Map<String, Object>> files = (List<Map<String,Object>>) response.get(CMD_GET_FILE_LIST);\n      if (files != null)\n        filesToDownload = Collections.synchronizedList(files);\n      else {\n        filesToDownload = Collections.emptyList();\n        LOG.error(\"No files to download for index generation: \"+ gen);\n      }\n\n      files = (List<Map<String,Object>>) response.get(CONF_FILES);\n      if (files != null)\n        confFilesToDownload = Collections.synchronizedList(files);\n\n    } catch (SolrServerException e) {\n      throw new IOException(e);\n    } finally {\n      server.shutdown();\n    }\n  }\n\n  private boolean successfulInstall = false;\n\n  /**\n   * This command downloads all the necessary files from master to install a index commit point. Only changed files are\n   * downloaded. It also downloads the conf files (if they are modified).\n   *\n   * @param core the SolrCore\n   * @param forceReplication force a replication in all cases \n   * @return true on success, false if slave is already in sync\n   * @throws IOException if an exception occurs\n   */\n  boolean fetchLatestIndex(final SolrCore core, boolean forceReplication) throws IOException, InterruptedException {\n    successfulInstall = false;\n    replicationStartTime = System.currentTimeMillis();\n    Directory tmpIndexDir = null;\n    String tmpIndex = null;\n    Directory indexDir = null;\n    String indexDirPath = null;\n    boolean deleteTmpIdxDir = true;\n    try {\n      //get the current 'replicateable' index version in the master\n      NamedList response = null;\n      try {\n        response = getLatestVersion();\n      } catch (Exception e) {\n        LOG.error(\"Master at: \" + masterUrl + \" is not available. Index fetch failed. Exception: \" + e.getMessage());\n        return false;\n      }\n      long latestVersion = (Long) response.get(CMD_INDEX_VERSION);\n      long latestGeneration = (Long) response.get(GENERATION);\n\n      // TODO: make sure that getLatestCommit only returns commit points for the main index (i.e. no side-car indexes)\n      IndexCommit commit = core.getDeletionPolicy().getLatestCommit();\n      if (commit == null) {\n        // Presumably the IndexWriter hasn't been opened yet, and hence the deletion policy hasn't been updated with commit points\n        RefCounted<SolrIndexSearcher> searcherRefCounted = null;\n        try {\n          searcherRefCounted = core.getNewestSearcher(false);\n          if (searcherRefCounted == null) {\n            LOG.warn(\"No open searcher found - fetch aborted\");\n            return false;\n          }\n          commit = searcherRefCounted.get().getIndexReader().getIndexCommit();\n        } finally {\n          if (searcherRefCounted != null)\n            searcherRefCounted.decref();\n        }\n      }\n\n\n      if (latestVersion == 0L) {\n        if (forceReplication && commit.getGeneration() != 0) {\n          // since we won't get the files for an empty index,\n          // we just clear ours and commit\n          RefCounted<IndexWriter> iw = core.getUpdateHandler().getSolrCoreState().getIndexWriter(core);\n          try {\n            iw.get().deleteAll();\n          } finally {\n            iw.decref();\n          }\n          SolrQueryRequest req = new LocalSolrQueryRequest(core,\n              new ModifiableSolrParams());\n          core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n        }\n        \n        //there is nothing to be replicated\n        successfulInstall = true;\n        return true;\n      }\n      \n      if (!forceReplication && IndexDeletionPolicyWrapper.getCommitTimestamp(commit) == latestVersion) {\n        //master and slave are already in sync just return\n        LOG.info(\"Slave in sync with master.\");\n        successfulInstall = true;\n        return true;\n      }\n      LOG.info(\"Master's generation: \" + latestGeneration);\n      LOG.info(\"Slave's generation: \" + commit.getGeneration());\n      LOG.info(\"Starting replication process\");\n      // get the list of files first\n      fetchFileList(latestGeneration);\n      // this can happen if the commit point is deleted before we fetch the file list.\n      if(filesToDownload.isEmpty()) return false;\n      LOG.info(\"Number of files in latest index in master: \" + filesToDownload.size());\n\n      // Create the sync service\n      fsyncService = Executors.newSingleThreadExecutor(new DefaultSolrThreadFactory(\"fsyncService\"));\n      // use a synchronized list because the list is read by other threads (to show details)\n      filesDownloaded = Collections.synchronizedList(new ArrayList<Map<String, Object>>());\n      // if the generation of master is older than that of the slave , it means they are not compatible to be copied\n      // then a new index directory to be created and all the files need to be copied\n      boolean isFullCopyNeeded = IndexDeletionPolicyWrapper\n          .getCommitTimestamp(commit) >= latestVersion\n          || commit.getGeneration() >= latestGeneration || forceReplication;\n\n      String tmpIdxDirName = \"index.\" + new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());\n      tmpIndex = createTempindexDir(core, tmpIdxDirName);\n\n      tmpIndexDir = core.getDirectoryFactory().get(tmpIndex, DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);\n      \n      // cindex dir...\n      indexDirPath = core.getIndexDir();\n      indexDir = core.getDirectoryFactory().get(indexDirPath, DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);\n\n      try {\n        \n        if (isIndexStale(indexDir)) {\n          isFullCopyNeeded = true;\n        }\n        \n        if (!isFullCopyNeeded) {\n          // rollback - and do it before we download any files\n          // so we don't remove files we thought we didn't need\n          // to download later\n          solrCore.getUpdateHandler().getSolrCoreState()\n          .closeIndexWriter(core, true);\n        }\n        \n        boolean reloadCore = false;\n        \n        try {\n          LOG.info(\"Starting download to \" + tmpIndexDir + \" fullCopy=\"\n              + isFullCopyNeeded);\n          successfulInstall = false;\n          \n          downloadIndexFiles(isFullCopyNeeded, indexDir, tmpIndexDir,\n              latestGeneration);\n          LOG.info(\"Total time taken for download : \"\n              + ((System.currentTimeMillis() - replicationStartTime) / 1000)\n              + \" secs\");\n          Collection<Map<String,Object>> modifiedConfFiles = getModifiedConfFiles(confFilesToDownload);\n          if (!modifiedConfFiles.isEmpty()) {\n            downloadConfFiles(confFilesToDownload, latestGeneration);\n            if (isFullCopyNeeded) {\n              successfulInstall = modifyIndexProps(tmpIdxDirName);\n              deleteTmpIdxDir = false;\n            } else {\n              successfulInstall = moveIndexFiles(tmpIndexDir, indexDir);\n            }\n            if (successfulInstall) {\n              if (isFullCopyNeeded) {\n                // let the system know we are changing dir's and the old one\n                // may be closed\n                if (indexDir != null) {\n                  LOG.info(\"removing old index directory \" + indexDir);\n                  core.getDirectoryFactory().doneWithDirectory(indexDir);\n                  core.getDirectoryFactory().remove(indexDir);\n                }\n              }\n              \n              LOG.info(\"Configuration files are modified, core will be reloaded\");\n              logReplicationTimeAndConfFiles(modifiedConfFiles,\n                  successfulInstall);// write to a file time of replication and\n                                     // conf files.\n              reloadCore = true;\n            }\n          } else {\n            terminateAndWaitFsyncService();\n            if (isFullCopyNeeded) {\n              successfulInstall = modifyIndexProps(tmpIdxDirName);\n              deleteTmpIdxDir = false;\n            } else {\n              successfulInstall = moveIndexFiles(tmpIndexDir, indexDir);\n            }\n            if (successfulInstall) {\n              logReplicationTimeAndConfFiles(modifiedConfFiles,\n                  successfulInstall);\n            }\n          }\n        } finally {\n          if (!isFullCopyNeeded) {\n            solrCore.getUpdateHandler().getSolrCoreState().openIndexWriter(core);\n          }\n        }\n        \n        // we must reload the core after we open the IW back up\n        if (reloadCore) {\n          reloadCore();\n        }\n\n        if (successfulInstall) {\n          if (isFullCopyNeeded) {\n            // let the system know we are changing dir's and the old one\n            // may be closed\n            if (indexDir != null) {\n              LOG.info(\"removing old index directory \" + indexDir);\n              core.getDirectoryFactory().doneWithDirectory(indexDir);\n              core.getDirectoryFactory().remove(indexDir);\n            }\n          }\n          if (isFullCopyNeeded) {\n            solrCore.getUpdateHandler().newIndexWriter(isFullCopyNeeded);\n          }\n          \n          openNewSearcherAndUpdateCommitPoint(isFullCopyNeeded);\n        }\n        \n        replicationStartTime = 0;\n        return successfulInstall;\n      } catch (ReplicationHandlerException e) {\n        LOG.error(\"User aborted Replication\");\n        return false;\n      } catch (SolrException e) {\n        throw e;\n      } catch (InterruptedException e) {\n        throw new InterruptedException(\"Index fetch interrupted\");\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Index fetch failed : \", e);\n      }\n    } finally {\n      try {\n        if (!successfulInstall) {\n          logReplicationTimeAndConfFiles(null, successfulInstall);\n        }\n        filesToDownload = filesDownloaded = confFilesDownloaded = confFilesToDownload = null;\n        replicationStartTime = 0;\n        dirFileFetcher = null;\n        localFileFetcher = null;\n        if (fsyncService != null && !fsyncService.isShutdown()) fsyncService\n            .shutdownNow();\n        fsyncService = null;\n        stop = false;\n        fsyncException = null;\n      } finally {\n        if (deleteTmpIdxDir && tmpIndexDir != null) {\n          try {\n            core.getDirectoryFactory().doneWithDirectory(tmpIndexDir);\n            core.getDirectoryFactory().remove(tmpIndexDir);\n          } catch (IOException e) {\n            SolrException.log(LOG, \"Error removing directory \" + tmpIndexDir, e);\n          }\n        }\n        \n        if (tmpIndexDir != null) {\n          core.getDirectoryFactory().release(tmpIndexDir);\n        }\n        \n        if (indexDir != null) {\n          core.getDirectoryFactory().release(indexDir);\n        }\n      }\n    }\n  }\n\n  private volatile Exception fsyncException;\n\n  /**\n   * terminate the fsync service and wait for all the tasks to complete. If it is already terminated\n   */\n  private void terminateAndWaitFsyncService() throws Exception {\n    if (fsyncService.isTerminated()) return;\n    fsyncService.shutdown();\n     // give a long wait say 1 hr\n    fsyncService.awaitTermination(3600, TimeUnit.SECONDS);\n    // if any fsync failed, throw that exception back\n    Exception fsyncExceptionCopy = fsyncException;\n    if (fsyncExceptionCopy != null) throw fsyncExceptionCopy;\n  }\n\n  /**\n   * Helper method to record the last replication's details so that we can show them on the statistics page across\n   * restarts.\n   * @throws IOException on IO error\n   */\n  private void logReplicationTimeAndConfFiles(Collection<Map<String, Object>> modifiedConfFiles, boolean successfulInstall) throws IOException {\n    List<String> confFiles = new ArrayList<String>();\n    if (modifiedConfFiles != null && !modifiedConfFiles.isEmpty())\n      for (Map<String, Object> map1 : modifiedConfFiles)\n        confFiles.add((String) map1.get(NAME));\n\n    Properties props = replicationHandler.loadReplicationProperties();\n    long replicationTime = System.currentTimeMillis();\n    long replicationTimeTaken = (replicationTime - getReplicationStartTime()) / 1000;\n    Directory dir = null;\n    try {\n      dir = solrCore.getDirectoryFactory().get(solrCore.getDataDir(), DirContext.META_DATA, solrCore.getSolrConfig().indexConfig.lockType);\n      \n      int indexCount = 1, confFilesCount = 1;\n      if (props.containsKey(TIMES_INDEX_REPLICATED)) {\n        indexCount = Integer.valueOf(props.getProperty(TIMES_INDEX_REPLICATED)) + 1;\n      }\n      StringBuffer sb = readToStringBuffer(replicationTime, props.getProperty(INDEX_REPLICATED_AT_LIST));\n      props.setProperty(INDEX_REPLICATED_AT_LIST, sb.toString());\n      props.setProperty(INDEX_REPLICATED_AT, String.valueOf(replicationTime));\n      props.setProperty(PREVIOUS_CYCLE_TIME_TAKEN, String.valueOf(replicationTimeTaken));\n      props.setProperty(TIMES_INDEX_REPLICATED, String.valueOf(indexCount));\n      if (modifiedConfFiles != null && !modifiedConfFiles.isEmpty()) {\n        props.setProperty(CONF_FILES_REPLICATED, confFiles.toString());\n        props.setProperty(CONF_FILES_REPLICATED_AT, String.valueOf(replicationTime));\n        if (props.containsKey(TIMES_CONFIG_REPLICATED)) {\n          confFilesCount = Integer.valueOf(props.getProperty(TIMES_CONFIG_REPLICATED)) + 1;\n        }\n        props.setProperty(TIMES_CONFIG_REPLICATED, String.valueOf(confFilesCount));\n      }\n\n      props.setProperty(LAST_CYCLE_BYTES_DOWNLOADED, String.valueOf(getTotalBytesDownloaded(this)));\n      if (!successfulInstall) {\n        int numFailures = 1;\n        if (props.containsKey(TIMES_FAILED)) {\n          numFailures = Integer.valueOf(props.getProperty(TIMES_FAILED)) + 1;\n        }\n        props.setProperty(TIMES_FAILED, String.valueOf(numFailures));\n        props.setProperty(REPLICATION_FAILED_AT, String.valueOf(replicationTime));\n        sb = readToStringBuffer(replicationTime, props.getProperty(REPLICATION_FAILED_AT_LIST));\n        props.setProperty(REPLICATION_FAILED_AT_LIST, sb.toString());\n      }\n\n      final IndexOutput out = dir.createOutput(REPLICATION_PROPERTIES, DirectoryFactory.IOCONTEXT_NO_CACHE);\n      Writer outFile = new OutputStreamWriter(new PropertiesOutputStream(out), CHARSET_UTF_8);\n      try {\n        props.store(outFile, \"Replication details\");\n        dir.sync(Collections.singleton(REPLICATION_PROPERTIES));\n      } finally {\n        IOUtils.closeQuietly(outFile);\n      }\n    } catch (Exception e) {\n      LOG.warn(\"Exception while updating statistics\", e);\n    } finally {\n      if (dir != null) {\n        solrCore.getDirectoryFactory().release(dir);\n      }\n    }\n  }\n\n  static long getTotalBytesDownloaded(SnapPuller snappuller) {\n    long bytesDownloaded = 0;\n    //get size from list of files to download\n    for (Map<String, Object> file : snappuller.getFilesDownloaded()) {\n      bytesDownloaded += (Long) file.get(SIZE);\n    }\n\n    //get size from list of conf files to download\n    for (Map<String, Object> file : snappuller.getConfFilesDownloaded()) {\n      bytesDownloaded += (Long) file.get(SIZE);\n    }\n\n    //get size from current file being downloaded\n    Map<String, Object> currentFile = snappuller.getCurrentFile();\n    if (currentFile != null) {\n      if (currentFile.containsKey(\"bytesDownloaded\")) {\n        bytesDownloaded += (Long) currentFile.get(\"bytesDownloaded\");\n      }\n    }\n    return bytesDownloaded;\n  }\n\n  private StringBuffer readToStringBuffer(long replicationTime, String str) {\n    StringBuffer sb = new StringBuffer();\n    List<String> l = new ArrayList<String>();\n    if (str != null && str.length() != 0) {\n      String[] ss = str.split(\",\");\n      for (int i = 0; i < ss.length; i++) {\n        l.add(ss[i]);\n      }\n    }\n    sb.append(replicationTime);\n    if (!l.isEmpty()) {\n      for (int i = 0; i < l.size() || i < 9; i++) {\n        if (i == l.size() || i == 9) break;\n        String s = l.get(i);\n        sb.append(\",\").append(s);\n      }\n    }\n    return sb;\n  }\n\n  private void openNewSearcherAndUpdateCommitPoint(boolean isFullCopyNeeded) throws IOException {\n    SolrQueryRequest req = new LocalSolrQueryRequest(solrCore,\n        new ModifiableSolrParams());\n    \n    RefCounted<SolrIndexSearcher> searcher = null;\n    IndexCommit commitPoint;\n    try {\n      Future[] waitSearcher = new Future[1];\n      searcher = solrCore.getSearcher(true, true, waitSearcher, true);\n      if (waitSearcher[0] != null) {\n        try {\n          waitSearcher[0].get();\n        } catch (InterruptedException e) {\n          SolrException.log(LOG, e);\n        } catch (ExecutionException e) {\n          SolrException.log(LOG, e);\n        }\n      }\n      commitPoint = searcher.get().getIndexReader().getIndexCommit();\n    } finally {\n      req.close();\n      if (searcher != null) {\n        searcher.decref();\n      }\n    }\n\n    // update the commit point in replication handler\n    replicationHandler.indexCommitPoint = commitPoint;\n    \n  }\n\n  /**\n   * All the files are copied to a temp dir first\n   */\n  private String createTempindexDir(SolrCore core, String tmpIdxDirName) {\n    // TODO: there should probably be a DirectoryFactory#concatPath(parent, name)\n    // or something\n    String tmpIdxDir = core.getDataDir() + tmpIdxDirName;\n    return tmpIdxDir;\n  }\n\n  private void reloadCore() {\n    final CountDownLatch latch = new CountDownLatch(1);\n    new Thread() {\n      @Override\n      public void run() {\n        try {\n          solrCore.getCoreDescriptor().getCoreContainer().reload(solrCore.getName());\n        } catch (Exception e) {\n          LOG.error(\"Could not reload core \", e);\n        } finally {\n          latch.countDown();\n        }\n      }\n    }.start();\n    try {\n      latch.await();\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      throw new RuntimeException(\"Interrupted while waiting for core reload to finish\", e);\n    }\n  }\n\n  private void downloadConfFiles(List<Map<String, Object>> confFilesToDownload, long latestGeneration) throws Exception {\n    LOG.info(\"Starting download of configuration files from master: \" + confFilesToDownload);\n    confFilesDownloaded = Collections.synchronizedList(new ArrayList<Map<String, Object>>());\n    File tmpconfDir = new File(solrCore.getResourceLoader().getConfigDir(), \"conf.\" + getDateAsStr(new Date()));\n    try {\n      boolean status = tmpconfDir.mkdirs();\n      if (!status) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                \"Failed to create temporary config folder: \" + tmpconfDir.getName());\n      }\n      for (Map<String, Object> file : confFilesToDownload) {\n        String saveAs = (String) (file.get(ALIAS) == null ? file.get(NAME) : file.get(ALIAS));\n        localFileFetcher = new LocalFsFileFetcher(tmpconfDir, file, saveAs, true, latestGeneration);\n        currentFile = file;\n        localFileFetcher.fetchFile();\n        confFilesDownloaded.add(new HashMap<String, Object>(file));\n      }\n      // this is called before copying the files to the original conf dir\n      // so that if there is an exception avoid corrupting the original files.\n      terminateAndWaitFsyncService();\n      copyTmpConfFiles2Conf(tmpconfDir);\n    } finally {\n      delTree(tmpconfDir);\n    }\n  }\n\n  /**\n   * Download the index files. If a new index is needed, download all the files.\n   *\n   * @param downloadCompleteIndex is it a fresh index copy\n   * @param tmpIndexDir              the directory to which files need to be downloadeed to\n   * @param indexDir                 the indexDir to be merged to\n   * @param latestGeneration         the version number\n   */\n  private void downloadIndexFiles(boolean downloadCompleteIndex,\n      Directory indexDir, Directory tmpIndexDir, long latestGeneration)\n      throws Exception {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Download files to dir: \" + Arrays.asList(indexDir.listAll()));\n    }\n    for (Map<String,Object> file : filesToDownload) {\n      if (!indexDir.fileExists((String) file.get(NAME))\n          || downloadCompleteIndex) {\n        dirFileFetcher = new DirectoryFileFetcher(tmpIndexDir, file,\n            (String) file.get(NAME), false, latestGeneration);\n        currentFile = file;\n        dirFileFetcher.fetchFile();\n        filesDownloaded.add(new HashMap<String,Object>(file));\n      } else {\n        LOG.info(\"Skipping download for \" + file.get(NAME)\n            + \" because it already exists\");\n      }\n    }\n  }\n\n  /**\n   * All the files which are common between master and slave must have same size else we assume they are\n   * not compatible (stale).\n   *\n   * @return true if the index stale and we need to download a fresh copy, false otherwise.\n   * @throws IOException  if low level io error\n   */\n  private boolean isIndexStale(Directory dir) throws IOException {\n    for (Map<String, Object> file : filesToDownload) {\n      if (dir.fileExists((String) file.get(NAME))\n              && dir.fileLength((String) file.get(NAME)) != (Long) file.get(SIZE)) {\n        LOG.warn(\"File \" + file.get(NAME) + \" expected to be \" + file.get(SIZE)\n            + \" while it is \" + dir.fileLength((String) file.get(NAME)));\n        // file exists and size is different, therefore we must assume\n        // corrupted index\n        return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Copy a file by the File#renameTo() method. If it fails, it is considered a failure\n   * <p/>\n   */\n  private boolean moveAFile(Directory tmpIdxDir, Directory indexDir, String fname, List<String> copiedfiles) {\n    LOG.debug(\"Moving file: {}\", fname);\n    boolean success = false;\n    try {\n      if (indexDir.fileExists(fname)) {\n        LOG.info(\"Skipping move file - it already exists:\" + fname);\n        return true;\n      }\n    } catch (IOException e) {\n      SolrException.log(LOG, \"could not check if a file exists\", e);\n      return false;\n    }\n    try {\n      solrCore.getDirectoryFactory().move(tmpIdxDir, indexDir, fname, DirectoryFactory.IOCONTEXT_NO_CACHE);\n      success = true;\n    } catch (IOException e) {\n      SolrException.log(LOG, \"Could not move file\", e);\n    }\n    return success;\n  }\n\n  /**\n   * Copy all index files from the temp index dir to the actual index. The segments_N file is copied last.\n   */\n  private boolean moveIndexFiles(Directory tmpIdxDir, Directory indexDir) {\n    if (LOG.isDebugEnabled()) {\n      try {\n        LOG.info(\"From dir files:\" + Arrays.asList(tmpIdxDir.listAll()));\n        LOG.info(\"To dir files:\" + Arrays.asList(indexDir.listAll()));\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    String segmentsFile = null;\n    List<String> movedfiles = new ArrayList<String>();\n    for (Map<String, Object> f : filesDownloaded) {\n      String fname = (String) f.get(NAME);\n      // the segments file must be copied last\n      // or else if there is a failure in between the\n      // index will be corrupted\n      if (fname.startsWith(\"segments_\")) {\n        //The segments file must be copied in the end\n        //Otherwise , if the copy fails index ends up corrupted\n        segmentsFile = fname;\n        continue;\n      }\n      if (!moveAFile(tmpIdxDir, indexDir, fname, movedfiles)) return false;\n      movedfiles.add(fname);\n    }\n    //copy the segments file last\n    if (segmentsFile != null) {\n      if (!moveAFile(tmpIdxDir, indexDir, segmentsFile, movedfiles)) return false;\n    }\n    return true;\n  }\n\n  /**\n   * Make file list \n   */\n  private List<File> makeTmpConfDirFileList(File dir, List<File> fileList) {\n    File[] files = dir.listFiles();\n    for (File file : files) {\n      if (file.isFile()) {\n        fileList.add(file);\n      } else if (file.isDirectory()) {\n        fileList = makeTmpConfDirFileList(file, fileList);\n      }\n    }\n    return fileList;\n  }\n  \n  /**\n   * The conf files are copied to the tmp dir to the conf dir. A backup of the old file is maintained\n   */\n  private void copyTmpConfFiles2Conf(File tmpconfDir) {\n    boolean status = false;\n    File confDir = new File(solrCore.getResourceLoader().getConfigDir());\n    for (File file : makeTmpConfDirFileList(tmpconfDir, new ArrayList<File>())) {\n      File oldFile = new File(confDir, file.getPath().substring(tmpconfDir.getPath().length(), file.getPath().length()));\n      if (!oldFile.getParentFile().exists()) {\n        status = oldFile.getParentFile().mkdirs();\n        if (status) {\n        } else {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                  \"Unable to mkdirs: \" + oldFile.getParentFile());\n        }\n      }\n      if (oldFile.exists()) {\n        File backupFile = new File(oldFile.getPath() + \".\" + getDateAsStr(new Date(oldFile.lastModified())));\n        if (!backupFile.getParentFile().exists()) {\n          status = backupFile.getParentFile().mkdirs();\n          if (status) {\n          } else {\n            throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                    \"Unable to mkdirs: \" + backupFile.getParentFile());\n          }\n        }\n        status = oldFile.renameTo(backupFile);\n        if (!status) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                  \"Unable to rename: \" + oldFile + \" to: \" + backupFile);\n        }\n      }\n      status = file.renameTo(oldFile);\n      if (status) {\n      } else {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                \"Unable to rename: \" + file + \" to: \" + oldFile);\n      }\n    }\n  }\n\n  private String getDateAsStr(Date d) {\n    return new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(d);\n  }\n\n  /**\n   * If the index is stale by any chance, load index from a different dir in the data dir.\n   */\n  private boolean modifyIndexProps(String tmpIdxDirName) {\n    LOG.info(\"New index installed. Updating index properties... index=\"+tmpIdxDirName);\n    Properties p = new Properties();\n    Directory dir = null;\n    try {\n      dir = solrCore.getDirectoryFactory().get(solrCore.getDataDir(), DirContext.META_DATA, solrCore.getSolrConfig().indexConfig.lockType);\n      if (dir.fileExists(SnapPuller.INDEX_PROPERTIES)){\n        final IndexInput input = dir.openInput(SnapPuller.INDEX_PROPERTIES, DirectoryFactory.IOCONTEXT_NO_CACHE);\n  \n        final InputStream is = new PropertiesInputStream(input);\n        try {\n          p.load(new InputStreamReader(is, CHARSET_UTF_8));\n        } catch (Exception e) {\n          LOG.error(\"Unable to load \" + SnapPuller.INDEX_PROPERTIES, e);\n        } finally {\n          IOUtils.closeQuietly(is);\n        }\n      }\n      try {\n        dir.deleteFile(SnapPuller.INDEX_PROPERTIES);\n      } catch (IOException e) {\n        // no problem\n      }\n      final IndexOutput out = dir.createOutput(SnapPuller.INDEX_PROPERTIES, DirectoryFactory.IOCONTEXT_NO_CACHE);\n      p.put(\"index\", tmpIdxDirName);\n      Writer os = null;\n      try {\n        os = new OutputStreamWriter(new PropertiesOutputStream(out), CHARSET_UTF_8);\n        p.store(os, SnapPuller.INDEX_PROPERTIES);\n        dir.sync(Collections.singleton(INDEX_PROPERTIES));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n            \"Unable to write \" + SnapPuller.INDEX_PROPERTIES, e);\n      } finally {\n        IOUtils.closeQuietly(os);\n      }\n      return true;\n\n    } catch (IOException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      if (dir != null) {\n        try {\n          solrCore.getDirectoryFactory().release(dir);\n        } catch (IOException e) {\n          SolrException.log(LOG, \"\", e);\n        }\n      }\n    }\n    \n  }\n\n  private final Map<String, FileInfo> confFileInfoCache = new HashMap<String, FileInfo>();\n\n  /**\n   * The local conf files are compared with the conf files in the master. If they are same (by checksum) do not copy.\n   *\n   * @param confFilesToDownload The list of files obtained from master\n   *\n   * @return a list of configuration files which have changed on the master and need to be downloaded.\n   */\n  private Collection<Map<String, Object>> getModifiedConfFiles(List<Map<String, Object>> confFilesToDownload) {\n    if (confFilesToDownload == null || confFilesToDownload.isEmpty())\n      return Collections.EMPTY_LIST;\n    //build a map with alias/name as the key\n    Map<String, Map<String, Object>> nameVsFile = new HashMap<String, Map<String, Object>>();\n    NamedList names = new NamedList();\n    for (Map<String, Object> map : confFilesToDownload) {\n      //if alias is present that is the name the file may have in the slave\n      String name = (String) (map.get(ALIAS) == null ? map.get(NAME) : map.get(ALIAS));\n      nameVsFile.put(name, map);\n      names.add(name, null);\n    }\n    //get the details of the local conf files with the same alias/name\n    List<Map<String, Object>> localFilesInfo = replicationHandler.getConfFileInfoFromCache(names, confFileInfoCache);\n    //compare their size/checksum to see if\n    for (Map<String, Object> fileInfo : localFilesInfo) {\n      String name = (String) fileInfo.get(NAME);\n      Map<String, Object> m = nameVsFile.get(name);\n      if (m == null) continue; // the file is not even present locally (so must be downloaded)\n      if (m.get(CHECKSUM).equals(fileInfo.get(CHECKSUM))) {\n        nameVsFile.remove(name); //checksums are same so the file need not be downloaded\n      }\n    }\n    return nameVsFile.isEmpty() ? Collections.EMPTY_LIST : nameVsFile.values();\n  }\n  \n  static boolean delTree(File dir) {\n    boolean isSuccess = true;\n    File contents[] = dir.listFiles();\n    if (contents != null) {\n      for (File file : contents) {\n        if (file.isDirectory()) {\n          boolean success = delTree(file);\n          if (!success) {\n            LOG.warn(\"Unable to delete directory : \" + file);\n            isSuccess = false;\n          }\n        } else {\n          boolean success = file.delete();\n          if (!success) {\n            LOG.warn(\"Unable to delete file : \" + file);\n            isSuccess = false;\n            return false;\n          }\n        }\n      }\n    }\n    return isSuccess && dir.delete();\n  }\n\n  /**\n   * Disable periodic polling\n   */\n  void disablePoll() {\n    pollDisabled.set(true);\n    LOG.info(\"inside disable poll, value of pollDisabled = \" + pollDisabled);\n  }\n\n  /**\n   * Enable periodic polling\n   */\n  void enablePoll() {\n    pollDisabled.set(false);\n    LOG.info(\"inside enable poll, value of pollDisabled = \" + pollDisabled);\n  }\n\n  /**\n   * Stops the ongoing pull\n   */\n  void abortPull() {\n    stop = true;\n  }\n\n  long getReplicationStartTime() {\n    return replicationStartTime;\n  }\n\n  List<Map<String, Object>> getConfFilesToDownload() {\n    //make a copy first because it can be null later\n    List<Map<String, Object>> tmp = confFilesToDownload;\n    //create a new instance. or else iterator may fail\n    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<Map<String, Object>>(tmp);\n  }\n\n  List<Map<String, Object>> getConfFilesDownloaded() {\n    //make a copy first because it can be null later\n    List<Map<String, Object>> tmp = confFilesDownloaded;\n    // NOTE: it's safe to make a copy of a SynchronizedCollection(ArrayList)\n    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<Map<String, Object>>(tmp);\n  }\n\n  List<Map<String, Object>> getFilesToDownload() {\n    //make a copy first because it can be null later\n    List<Map<String, Object>> tmp = filesToDownload;\n    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<Map<String, Object>>(tmp);\n  }\n\n  List<Map<String, Object>> getFilesDownloaded() {\n    List<Map<String, Object>> tmp = filesDownloaded;\n    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<Map<String, Object>>(tmp);\n  }\n\n  // TODO: currently does not reflect conf files\n  Map<String, Object> getCurrentFile() {\n    Map<String, Object> tmp = currentFile;\n    DirectoryFileFetcher tmpFileFetcher = dirFileFetcher;\n    if (tmp == null)\n      return null;\n    tmp = new HashMap<String, Object>(tmp);\n    if (tmpFileFetcher != null)\n      tmp.put(\"bytesDownloaded\", tmpFileFetcher.bytesDownloaded);\n    return tmp;\n  }\n\n  boolean isPollingDisabled() {\n    return pollDisabled.get();\n  }\n\n  Long getNextScheduledExecTime() {\n    Long nextTime = null;\n    if (executorStartTime > 0)\n      nextTime = executorStartTime + pollInterval;\n    return nextTime;\n  }\n\n  private static class ReplicationHandlerException extends InterruptedException {\n    public ReplicationHandlerException(String message) {\n      super(message);\n    }\n  }\n\n  /**\n   * The class acts as a client for ReplicationHandler.FileStream. It understands the protocol of wt=filestream\n   *\n   * @see org.apache.solr.handler.ReplicationHandler.DirectoryFileStream\n   */\n  private class DirectoryFileFetcher {\n    boolean includeChecksum = true;\n\n    Directory copy2Dir;\n\n    String fileName;\n\n    String saveAs;\n\n    long size;\n\n    long bytesDownloaded = 0;\n\n    byte[] buf = new byte[1024 * 1024];\n\n    Checksum checksum;\n\n    int errorCount = 0;\n\n    private boolean isConf;\n\n    private boolean aborted = false;\n\n    private Long indexGen;\n\n    private IndexOutput outStream;\n\n    DirectoryFileFetcher(Directory tmpIndexDir, Map<String, Object> fileDetails, String saveAs,\n                boolean isConf, long latestGen) throws IOException {\n      this.copy2Dir = tmpIndexDir;\n      this.fileName = (String) fileDetails.get(NAME);\n      this.size = (Long) fileDetails.get(SIZE);\n      this.isConf = isConf;\n      this.saveAs = saveAs;\n\n      indexGen = latestGen;\n      \n      outStream = copy2Dir.createOutput(saveAs, DirectoryFactory.IOCONTEXT_NO_CACHE);\n\n      if (includeChecksum)\n        checksum = new Adler32();\n    }\n\n    /**\n     * The main method which downloads file\n     */\n    void fetchFile() throws Exception {\n      try {\n        while (true) {\n          final FastInputStream is = getStream();\n          int result;\n          try {\n            //fetch packets one by one in a single request\n            result = fetchPackets(is);\n            if (result == 0 || result == NO_CONTENT) {\n\n              return;\n            }\n            //if there is an error continue. But continue from the point where it got broken\n          } finally {\n            IOUtils.closeQuietly(is);\n          }\n        }\n      } finally {\n        cleanup();\n        //if cleanup suceeds . The file is downloaded fully. do an fsync\n        fsyncService.submit(new Runnable(){\n          @Override\n          public void run() {\n            try {\n              copy2Dir.sync(Collections.singleton(saveAs));\n            } catch (IOException e) {\n              fsyncException = e;\n            }\n          }\n        });\n      }\n    }\n\n    private int fetchPackets(FastInputStream fis) throws Exception {\n      byte[] intbytes = new byte[4];\n      byte[] longbytes = new byte[8];\n      try {\n        while (true) {\n          if (stop) {\n            stop = false;\n            aborted = true;\n            throw new ReplicationHandlerException(\"User aborted replication\");\n          }\n          long checkSumServer = -1;\n          fis.readFully(intbytes);\n          //read the size of the packet\n          int packetSize = readInt(intbytes);\n          if (packetSize <= 0) {\n            LOG.warn(\"No content recieved for file: \" + currentFile);\n            return NO_CONTENT;\n          }\n          if (buf.length < packetSize)\n            buf = new byte[packetSize];\n          if (checksum != null) {\n            //read the checksum\n            fis.readFully(longbytes);\n            checkSumServer = readLong(longbytes);\n          }\n          //then read the packet of bytes\n          fis.readFully(buf, 0, packetSize);\n          //compare the checksum as sent from the master\n          if (includeChecksum) {\n            checksum.reset();\n            checksum.update(buf, 0, packetSize);\n            long checkSumClient = checksum.getValue();\n            if (checkSumClient != checkSumServer) {\n              LOG.error(\"Checksum not matched between client and server for: \" + currentFile);\n              //if checksum is wrong it is a problem return for retry\n              return 1;\n            }\n          }\n          //if everything is fine, write down the packet to the file\n          writeBytes(packetSize);\n          bytesDownloaded += packetSize;\n          if (bytesDownloaded >= size)\n            return 0;\n          //errorcount is always set to zero after a successful packet\n          errorCount = 0;\n        }\n      } catch (ReplicationHandlerException e) {\n        throw e;\n      } catch (Exception e) {\n        LOG.warn(\"Error in fetching packets \", e);\n        //for any failure , increment the error count\n        errorCount++;\n        //if it fails for the same pacaket for   MAX_RETRIES fail and come out\n        if (errorCount > MAX_RETRIES) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                  \"Fetch failed for file:\" + fileName, e);\n        }\n        return ERR;\n      }\n    }\n\n    protected void writeBytes(int packetSize) throws IOException {\n      outStream.writeBytes(buf, 0, packetSize);\n    }\n\n    /**\n     * The webcontainer flushes the data only after it fills the buffer size. So, all data has to be read as readFully()\n     * other wise it fails. So read everything as bytes and then extract an integer out of it\n     */\n    private int readInt(byte[] b) {\n      return (((b[0] & 0xff) << 24) | ((b[1] & 0xff) << 16)\n              | ((b[2] & 0xff) << 8) | (b[3] & 0xff));\n\n    }\n\n    /**\n     * Same as above but to read longs from a byte array\n     */\n    private long readLong(byte[] b) {\n      return (((long) (b[0] & 0xff)) << 56) | (((long) (b[1] & 0xff)) << 48)\n              | (((long) (b[2] & 0xff)) << 40) | (((long) (b[3] & 0xff)) << 32)\n              | (((long) (b[4] & 0xff)) << 24) | ((b[5] & 0xff) << 16)\n              | ((b[6] & 0xff) << 8) | ((b[7] & 0xff));\n\n    }\n\n    /**\n     * cleanup everything\n     */\n    private void cleanup() {\n      try {\n        outStream.close();\n      } catch (Exception e) {/* noop */\n          LOG.error(\"Error closing the file stream: \"+ this.saveAs ,e);\n      }\n      if (bytesDownloaded != size) {\n        //if the download is not complete then\n        //delete the file being downloaded\n        try {\n          copy2Dir.deleteFile(saveAs);\n        } catch (Exception e) {\n          LOG.error(\"Error deleting file in cleanup\" + e.getMessage());\n        }\n        //if the failure is due to a user abort it is returned nomally else an exception is thrown\n        if (!aborted)\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                  \"Unable to download \" + fileName + \" completely. Downloaded \"\n                          + bytesDownloaded + \"!=\" + size);\n      }\n    }\n\n    /**\n     * Open a new stream using HttpClient\n     */\n    FastInputStream getStream() throws IOException {\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n\n//    //the method is command=filecontent\n      params.set(COMMAND, CMD_GET_FILE);\n      params.set(GENERATION, Long.toString(indexGen));\n      params.set(CommonParams.QT, \"/replication\");\n      //add the version to download. This is used to reserve the download\n      if (isConf) {\n        //set cf instead of file for config file\n        params.set(CONF_FILE_SHORT, fileName);\n      } else {\n        params.set(FILE, fileName);\n      }\n      if (useInternal) {\n        params.set(COMPRESSION, \"true\"); \n      }\n      //use checksum\n      if (this.includeChecksum) {\n        params.set(CHECKSUM, true);\n      }\n      //wt=filestream this is a custom protocol\n      params.set(CommonParams.WT, FILE_STREAM);\n        // This happen if there is a failure there is a retry. the offset=<sizedownloaded> ensures that\n        // the server starts from the offset\n      if (bytesDownloaded > 0) {\n        params.set(OFFSET, Long.toString(bytesDownloaded));\n      }\n      \n\n      NamedList response;\n      InputStream is = null;\n      \n      HttpSolrServer s = new HttpSolrServer(masterUrl, myHttpClient, null);  //XXX use shardhandler\n      try {\n        s.setSoTimeout(60000);\n        s.setConnectionTimeout(15000);\n        QueryRequest req = new QueryRequest(params);\n        response = s.request(req);\n        is = (InputStream) response.get(\"stream\");\n        if(useInternal) {\n          is = new InflaterInputStream(is);\n        }\n        return new FastInputStream(is);\n      } catch (Throwable t) {\n        //close stream on error\n        IOUtils.closeQuietly(is);\n        throw new IOException(\"Could not download file '\" + fileName + \"'\", t);\n      } finally {\n        s.shutdown();\n      }\n    }\n  }\n  \n  /**\n   * The class acts as a client for ReplicationHandler.FileStream. It understands the protocol of wt=filestream\n   *\n   * @see org.apache.solr.handler.ReplicationHandler.LocalFsFileStream\n   */\n  private class LocalFsFileFetcher {\n    boolean includeChecksum = true;\n\n    private File copy2Dir;\n\n    String fileName;\n\n    String saveAs;\n\n    long size;\n\n    long bytesDownloaded = 0;\n\n    FileChannel fileChannel;\n    \n    private FileOutputStream fileOutputStream;\n\n    byte[] buf = new byte[1024 * 1024];\n\n    Checksum checksum;\n\n    File file;\n\n    int errorCount = 0;\n\n    private boolean isConf;\n\n    private boolean aborted = false;\n\n    private Long indexGen;\n\n    // TODO: could do more code sharing with DirectoryFileFetcher\n    LocalFsFileFetcher(File dir, Map<String, Object> fileDetails, String saveAs,\n                boolean isConf, long latestGen) throws IOException {\n      this.copy2Dir = dir;\n      this.fileName = (String) fileDetails.get(NAME);\n      this.size = (Long) fileDetails.get(SIZE);\n      this.isConf = isConf;\n      this.saveAs = saveAs;\n\n      indexGen = latestGen;\n\n      this.file = new File(copy2Dir, saveAs);\n      \n      File parentDir = this.file.getParentFile();\n      if( ! parentDir.exists() ){\n        if ( ! parentDir.mkdirs() ) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                                  \"Failed to create (sub)directory for file: \" + saveAs);\n        }\n      }\n      \n      this.fileOutputStream = new FileOutputStream(file);\n      this.fileChannel = this.fileOutputStream.getChannel();\n\n      if (includeChecksum)\n        checksum = new Adler32();\n    }\n\n    /**\n     * The main method which downloads file\n     */\n    void fetchFile() throws Exception {\n      try {\n        while (true) {\n          final FastInputStream is = getStream();\n          int result;\n          try {\n            //fetch packets one by one in a single request\n            result = fetchPackets(is);\n            if (result == 0 || result == NO_CONTENT) {\n              return;\n            }\n            //if there is an error continue. But continue from the point where it got broken\n          } finally {\n            IOUtils.closeQuietly(is);\n          }\n        }\n      } finally {\n        cleanup();\n        //if cleanup suceeds . The file is downloaded fully. do an fsync\n        fsyncService.submit(new Runnable(){\n          @Override\n          public void run() {\n            try {\n              FileUtils.sync(file);\n            } catch (IOException e) {\n              fsyncException = e;\n            }\n          }\n        });\n      }\n    }\n\n    private int fetchPackets(FastInputStream fis) throws Exception {\n      byte[] intbytes = new byte[4];\n      byte[] longbytes = new byte[8];\n      try {\n        while (true) {\n          if (stop) {\n            stop = false;\n            aborted = true;\n            throw new ReplicationHandlerException(\"User aborted replication\");\n          }\n          long checkSumServer = -1;\n          fis.readFully(intbytes);\n          //read the size of the packet\n          int packetSize = readInt(intbytes);\n          if (packetSize <= 0) {\n            LOG.warn(\"No content recieved for file: \" + currentFile);\n            return NO_CONTENT;\n          }\n          if (buf.length < packetSize)\n            buf = new byte[packetSize];\n          if (checksum != null) {\n            //read the checksum\n            fis.readFully(longbytes);\n            checkSumServer = readLong(longbytes);\n          }\n          //then read the packet of bytes\n          fis.readFully(buf, 0, packetSize);\n          //compare the checksum as sent from the master\n          if (includeChecksum) {\n            checksum.reset();\n            checksum.update(buf, 0, packetSize);\n            long checkSumClient = checksum.getValue();\n            if (checkSumClient != checkSumServer) {\n              LOG.error(\"Checksum not matched between client and server for: \" + currentFile);\n              //if checksum is wrong it is a problem return for retry\n              return 1;\n            }\n          }\n          //if everything is fine, write down the packet to the file\n          fileChannel.write(ByteBuffer.wrap(buf, 0, packetSize));\n          bytesDownloaded += packetSize;\n          if (bytesDownloaded >= size)\n            return 0;\n          //errorcount is always set to zero after a successful packet\n          errorCount = 0;\n        }\n      } catch (ReplicationHandlerException e) {\n        throw e;\n      } catch (Exception e) {\n        LOG.warn(\"Error in fetching packets \", e);\n        //for any failure , increment the error count\n        errorCount++;\n        //if it fails for the same pacaket for   MAX_RETRIES fail and come out\n        if (errorCount > MAX_RETRIES) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                  \"Fetch failed for file:\" + fileName, e);\n        }\n        return ERR;\n      }\n    }\n\n    /**\n     * The webcontainer flushes the data only after it fills the buffer size. So, all data has to be read as readFully()\n     * other wise it fails. So read everything as bytes and then extract an integer out of it\n     */\n    private int readInt(byte[] b) {\n      return (((b[0] & 0xff) << 24) | ((b[1] & 0xff) << 16)\n              | ((b[2] & 0xff) << 8) | (b[3] & 0xff));\n\n    }\n\n    /**\n     * Same as above but to read longs from a byte array\n     */\n    private long readLong(byte[] b) {\n      return (((long) (b[0] & 0xff)) << 56) | (((long) (b[1] & 0xff)) << 48)\n              | (((long) (b[2] & 0xff)) << 40) | (((long) (b[3] & 0xff)) << 32)\n              | (((long) (b[4] & 0xff)) << 24) | ((b[5] & 0xff) << 16)\n              | ((b[6] & 0xff) << 8) | ((b[7] & 0xff));\n\n    }\n\n    /**\n     * cleanup everything\n     */\n    private void cleanup() {\n      try {\n        //close the FileOutputStream (which also closes the Channel)\n        fileOutputStream.close();\n      } catch (Exception e) {/* noop */\n          LOG.error(\"Error closing the file stream: \"+ this.saveAs ,e);\n      }\n      if (bytesDownloaded != size) {\n        //if the download is not complete then\n        //delete the file being downloaded\n        try {\n          file.delete();\n        } catch (Exception e) {\n          LOG.error(\"Error deleting file in cleanup\" + e.getMessage());\n        }\n        //if the failure is due to a user abort it is returned nomally else an exception is thrown\n        if (!aborted)\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                  \"Unable to download \" + fileName + \" completely. Downloaded \"\n                          + bytesDownloaded + \"!=\" + size);\n      }\n    }\n\n    /**\n     * Open a new stream using HttpClient\n     */\n    FastInputStream getStream() throws IOException {\n\n      ModifiableSolrParams params = new ModifiableSolrParams();\n\n//    //the method is command=filecontent\n      params.set(COMMAND, CMD_GET_FILE);\n      params.set(GENERATION, Long.toString(indexGen));\n      params.set(CommonParams.QT, \"/replication\");\n      //add the version to download. This is used to reserve the download\n      if (isConf) {\n        //set cf instead of file for config file\n        params.set(CONF_FILE_SHORT, fileName);\n      } else {\n        params.set(FILE, fileName);\n      }\n      if (useInternal) {\n        params.set(COMPRESSION, \"true\"); \n      }\n      //use checksum\n      if (this.includeChecksum) {\n        params.set(CHECKSUM, true);\n      }\n      //wt=filestream this is a custom protocol\n      params.set(CommonParams.WT, FILE_STREAM);\n        // This happen if there is a failure there is a retry. the offset=<sizedownloaded> ensures that\n        // the server starts from the offset\n      if (bytesDownloaded > 0) {\n        params.set(OFFSET, Long.toString(bytesDownloaded));\n      }\n      \n\n      NamedList response;\n      InputStream is = null;\n      HttpSolrServer s = new HttpSolrServer(masterUrl, myHttpClient, null);  //XXX use shardhandler\n      try {\n        s.setSoTimeout(60000);\n        s.setConnectionTimeout(15000);\n        QueryRequest req = new QueryRequest(params);\n        response = s.request(req);\n        is = (InputStream) response.get(\"stream\");\n        if(useInternal) {\n          is = new InflaterInputStream(is);\n        }\n        return new FastInputStream(is);\n      } catch (Throwable t) {\n        //close stream on error\n        IOUtils.closeQuietly(is);\n        throw new IOException(\"Could not download file '\" + fileName + \"'\", t);\n      } finally {\n        s.shutdown();\n      }\n    }\n  }\n  \n  NamedList getDetails() throws IOException, SolrServerException {\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(COMMAND, CMD_DETAILS);\n    params.set(\"slave\", false);\n    params.set(CommonParams.QT, \"/replication\");\n    HttpSolrServer server = new HttpSolrServer(masterUrl, myHttpClient); //XXX use shardhandler\n    NamedList rsp;\n    try {\n      server.setSoTimeout(60000);\n      server.setConnectionTimeout(15000);\n      QueryRequest request = new QueryRequest(params);\n      rsp = server.request(request);\n    } finally {\n      server.shutdown();\n    }\n    return rsp;\n  }\n\n  static Integer readInterval(String interval) {\n    if (interval == null)\n      return null;\n    int result = 0;\n    if (interval != null) {\n      Matcher m = INTERVAL_PATTERN.matcher(interval.trim());\n      if (m.find()) {\n        String hr = m.group(1);\n        String min = m.group(2);\n        String sec = m.group(3);\n        result = 0;\n        try {\n          if (sec != null && sec.length() > 0)\n            result += Integer.parseInt(sec);\n          if (min != null && min.length() > 0)\n            result += (60 * Integer.parseInt(min));\n          if (hr != null && hr.length() > 0)\n            result += (60 * 60 * Integer.parseInt(hr));\n          result *= 1000;\n        } catch (NumberFormatException e) {\n          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                  INTERVAL_ERR_MSG);\n        }\n      } else {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n                INTERVAL_ERR_MSG);\n      }\n\n    }\n    return result;\n  }\n\n  public void destroy() {\n    try {\n      if (executorService != null) executorService.shutdown();\n    } catch (Throwable e) {\n      SolrException.log(LOG, e);\n    }\n    try {\n      abortPull();\n    } catch (Throwable e) {\n      SolrException.log(LOG, e);\n    }\n    try {\n      if (executorService != null) ExecutorUtil\n          .shutdownNowAndAwaitTermination(executorService);\n    } catch (Throwable e) {\n      SolrException.log(LOG, e);\n    }\n  }\n\n  String getMasterUrl() {\n    return masterUrl;\n  }\n\n  String getPollInterval() {\n    return pollIntervalStr;\n  }\n\n  private static final int MAX_RETRIES = 5;\n\n  private static final int NO_CONTENT = 1;\n\n  private static final int ERR = 2;\n\n  public static final String REPLICATION_PROPERTIES = \"replication.properties\";\n\n  public static final String POLL_INTERVAL = \"pollInterval\";\n\n  public static final String INTERVAL_ERR_MSG = \"The \" + POLL_INTERVAL + \" must be in this format 'HH:mm:ss'\";\n\n  private static final Pattern INTERVAL_PATTERN = Pattern.compile(\"(\\\\d*?):(\\\\d*?):(\\\\d*)\");\n\n  static final String INDEX_REPLICATED_AT = \"indexReplicatedAt\";\n\n  static final String TIMES_INDEX_REPLICATED = \"timesIndexReplicated\";\n\n  static final String CONF_FILES_REPLICATED = \"confFilesReplicated\";\n\n  static final String CONF_FILES_REPLICATED_AT = \"confFilesReplicatedAt\";\n\n  static final String TIMES_CONFIG_REPLICATED = \"timesConfigReplicated\";\n\n  static final String LAST_CYCLE_BYTES_DOWNLOADED = \"lastCycleBytesDownloaded\";\n\n  static final String TIMES_FAILED = \"timesFailed\";\n\n  static final String REPLICATION_FAILED_AT = \"replicationFailedAt\";\n\n  static final String PREVIOUS_CYCLE_TIME_TAKEN = \"previousCycleTimeInSeconds\";\n\n  static final String INDEX_REPLICATED_AT_LIST = \"indexReplicatedAtList\";\n\n  static final String REPLICATION_FAILED_AT_LIST = \"replicationFailedAtList\";\n}\n",
        "methodName": "makeTmpConfDirFileList",
        "exampleID": 44,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/handler/SnapPuller.java",
        "line": "865",
        "source": "files",
        "sourceLine": "866",
        "qualifier": "Possible null pointer dereference of $$files/$",
        "steps": [
            {
                "exampleID": 45
            }
        ],
        "line_number": "866"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/handler/SnapShooter.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.solr.handler;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.text.SimpleDateFormat;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport org.apache.lucene.index.IndexCommit;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.FSDirectory;\nimport org.apache.lucene.store.Lock;\nimport org.apache.lucene.store.SimpleFSLockFactory;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.core.DirectoryFactory;\nimport org.apache.solr.core.DirectoryFactory.DirContext;\nimport org.apache.solr.core.SolrCore;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * <p/> Provides functionality equivalent to the snapshooter script </p>\n * This is no longer used in standard replication.\n *\n *\n * @since solr 1.4\n */\npublic class SnapShooter {\n  private static final Logger LOG = LoggerFactory.getLogger(SnapShooter.class.getName());\n  private String snapDir = null;\n  private SolrCore solrCore;\n  private SimpleFSLockFactory lockFactory;\n  \n  public SnapShooter(SolrCore core, String location) {\n    solrCore = core;\n    if (location == null) snapDir = core.getDataDir();\n    else  {\n      File base = new File(core.getCoreDescriptor().getRawInstanceDir());\n      snapDir = org.apache.solr.util.FileUtils.resolvePath(base, location).getAbsolutePath();\n      File dir = new File(snapDir);\n      if (!dir.exists())  dir.mkdirs();\n    }\n    lockFactory = new SimpleFSLockFactory(snapDir);\n  }\n  \n  void createSnapAsync(final IndexCommit indexCommit, final ReplicationHandler replicationHandler) {\n    createSnapAsync(indexCommit, Integer.MAX_VALUE, replicationHandler);\n  }\n\n  void createSnapAsync(final IndexCommit indexCommit, final int numberToKeep, final ReplicationHandler replicationHandler) {\n    replicationHandler.core.getDeletionPolicy().saveCommitPoint(indexCommit.getGeneration());\n\n    new Thread() {\n      @Override\n      public void run() {\n        createSnapshot(indexCommit, numberToKeep, replicationHandler);\n      }\n    }.start();\n  }\n\n  void createSnapshot(final IndexCommit indexCommit, int numberToKeep, ReplicationHandler replicationHandler) {\n    LOG.info(\"Creating backup snapshot...\");\n    NamedList<Object> details = new NamedList<Object>();\n    details.add(\"startTime\", new Date().toString());\n    File snapShotDir = null;\n    String directoryName = null;\n    Lock lock = null;\n    try {\n      if(numberToKeep<Integer.MAX_VALUE) {\n        deleteOldBackups(numberToKeep);\n      }\n      SimpleDateFormat fmt = new SimpleDateFormat(DATE_FMT, Locale.ROOT);\n      directoryName = \"snapshot.\" + fmt.format(new Date());\n      lock = lockFactory.makeLock(directoryName + \".lock\");\n      if (lock.isLocked()) return;\n      snapShotDir = new File(snapDir, directoryName);\n      if (!snapShotDir.mkdir()) {\n        LOG.warn(\"Unable to create snapshot directory: \" + snapShotDir.getAbsolutePath());\n        return;\n      }\n      Collection<String> files = indexCommit.getFileNames();\n      FileCopier fileCopier = new FileCopier();\n      \n      Directory dir = solrCore.getDirectoryFactory().get(solrCore.getIndexDir(), DirContext.DEFAULT, solrCore.getSolrConfig().indexConfig.lockType);\n      try {\n        fileCopier.copyFiles(dir, files, snapShotDir);\n      } finally {\n        solrCore.getDirectoryFactory().release(dir);\n      }\n\n      details.add(\"fileCount\", files.size());\n      details.add(\"status\", \"success\");\n      details.add(\"snapshotCompletedAt\", new Date().toString());\n    } catch (Exception e) {\n      SnapPuller.delTree(snapShotDir);\n      LOG.error(\"Exception while creating snapshot\", e);\n      details.add(\"snapShootException\", e.getMessage());\n    } finally {\n      replicationHandler.core.getDeletionPolicy().releaseCommitPoint(indexCommit.getGeneration());\n      replicationHandler.snapShootDetails = details;\n      if (lock != null) {\n        try {\n          lock.release();\n        } catch (IOException e) {\n          LOG.error(\"Unable to release snapshoot lock: \" + directoryName + \".lock\");\n        }\n      }\n    }\n  }\n  private void deleteOldBackups(int numberToKeep) {\n    File[] files = new File(snapDir).listFiles();\n    List<OldBackupDirectory> dirs = new ArrayList<OldBackupDirectory>();\n    for(File f : files) {\n      OldBackupDirectory obd = new OldBackupDirectory(f);\n      if(obd.dir != null) {\n        dirs.add(obd);\n      }\n    }\n    Collections.sort(dirs);\n    int i=1;\n    for(OldBackupDirectory dir : dirs) {\n      if( i++ > numberToKeep-1 ) {\n        SnapPuller.delTree(dir.dir);\n      }\n    }   \n  }\n  private class OldBackupDirectory implements Comparable<OldBackupDirectory>{\n    File dir;\n    Date timestamp;\n    final Pattern dirNamePattern = Pattern.compile(\"^snapshot[.](.*)$\");\n    \n    OldBackupDirectory(File dir) {\n      if(dir.isDirectory()) {\n        Matcher m = dirNamePattern.matcher(dir.getName());\n        if(m.find()) {\n          try {\n            this.dir = dir;\n            this.timestamp = new SimpleDateFormat(DATE_FMT, Locale.ROOT).parse(m.group(1));\n          } catch(Exception e) {\n            this.dir = null;\n            this.timestamp = null;\n          }\n        }\n      }\n    }\n    @Override\n    public int compareTo(OldBackupDirectory that) {\n      return that.timestamp.compareTo(this.timestamp);\n    }\n  }\n\n  public static final String SNAP_DIR = \"snapDir\";\n  public static final String DATE_FMT = \"yyyyMMddHHmmssSSS\";\n  \n\n  private class FileCopier {\n    \n    public void copyFiles(Directory sourceDir, Collection<String> files,\n        File destDir) throws IOException {\n      // does destinations directory exist ?\n      if (destDir != null && !destDir.exists()) {\n        destDir.mkdirs();\n      }\n      \n      FSDirectory dir = FSDirectory.open(destDir);\n      try {\n        for (String indexFile : files) {\n          copyFile(sourceDir, indexFile, new File(destDir, indexFile), dir);\n        }\n      } finally {\n        dir.close();\n      }\n    }\n    \n    public void copyFile(Directory sourceDir, String indexFile, File destination, Directory destDir)\n      throws IOException {\n\n      // make sure we can write to destination\n      if (destination.exists() && !destination.canWrite()) {\n        String message = \"Unable to open file \" + destination + \" for writing.\";\n        throw new IOException(message);\n      }\n\n      sourceDir.copy(destDir, indexFile, indexFile, DirectoryFactory.IOCONTEXT_NO_CACHE);\n    }\n  }\n  \n\n}\n",
        "methodName": "deleteOldBackups",
        "exampleID": 46,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/handler/SnapShooter.java",
        "line": "133",
        "source": "files",
        "sourceLine": "135",
        "qualifier": "Possible null pointer dereference of $$files/$",
        "steps": [
            {
                "exampleID": 47
            }
        ],
        "line_number": "135"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.solr.handler.admin;\n\nimport org.apache.solr.cloud.ZkSolrResourceLoader;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.SolrException.ErrorCode;\nimport org.apache.solr.common.cloud.SolrZkClient;\nimport org.apache.solr.common.params.CommonParams;\nimport org.apache.solr.common.params.ModifiableSolrParams;\nimport org.apache.solr.common.params.SolrParams;\nimport org.apache.solr.common.util.ContentStreamBase;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.common.util.SimpleOrderedMap;\nimport org.apache.solr.core.CoreContainer;\nimport org.apache.solr.core.SolrCore;\nimport org.apache.solr.core.SolrResourceLoader;\nimport org.apache.solr.handler.RequestHandlerBase;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.response.RawResponseWriter;\nimport org.apache.solr.response.SolrQueryResponse;\nimport org.apache.solr.schema.IndexSchema;\nimport org.apache.solr.schema.ManagedIndexSchema;\nimport org.apache.zookeeper.KeeperException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\nimport java.net.URISyntaxException;\nimport java.util.Date;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Set;\n\n/**\n * This handler uses the RawResponseWriter to give client access to\n * files inside ${solr.home}/conf\n * <p>\n * If you want to selectively restrict access some configuration files, you can list\n * these files in the {@link #HIDDEN} invariants.  For example to hide \n * synonyms.txt and anotherfile.txt, you would register:\n * <p>\n * <pre>\n * &lt;requestHandler name=\"/admin/file\" class=\"org.apache.solr.handler.admin.ShowFileRequestHandler\" &gt;\n *   &lt;lst name=\"defaults\"&gt;\n *    &lt;str name=\"echoParams\"&gt;explicit&lt;/str&gt;\n *   &lt;/lst&gt;\n *   &lt;lst name=\"invariants\"&gt;\n *    &lt;str name=\"hidden\"&gt;synonyms.txt&lt;/str&gt; \n *    &lt;str name=\"hidden\"&gt;anotherfile.txt&lt;/str&gt;\n *    &lt;str name=\"hidden\"&gt;*&lt;/str&gt;\n *   &lt;/lst&gt;\n * &lt;/requestHandler&gt;\n * </pre>\n *\n * At present, there is only explicit file names (including path) or the glob '*' are supported. Variants like '*.xml'\n * are NOT supported.ere\n *\n * <p>\n * The ShowFileRequestHandler uses the {@link RawResponseWriter} (wt=raw) to return\n * file contents.  If you need to use a different writer, you will need to change \n * the registered invariant param for wt.\n * <p>\n * If you want to override the contentType header returned for a given file, you can\n * set it directly using: {@link #USE_CONTENT_TYPE}.  For example, to get a plain text\n * version of schema.xml, try:\n * <pre>\n *   http://localhost:8983/solr/admin/file?file=schema.xml&contentType=text/plain\n * </pre>\n *\n *\n * @since solr 1.3\n */\npublic class ShowFileRequestHandler extends RequestHandlerBase\n{\n  public static final String HIDDEN = \"hidden\";\n  public static final String USE_CONTENT_TYPE = \"contentType\";\n\n  protected Set<String> hiddenFiles;\n\n  protected static final Logger log = LoggerFactory\n      .getLogger(ShowFileRequestHandler.class);\n\n\n  public ShowFileRequestHandler()\n  {\n    super();\n  }\n\n  @Override\n  public void init(NamedList args) {\n    super.init( args );\n    hiddenFiles = initHidden(invariants);\n  }\n\n  public static Set<String> initHidden(SolrParams invariants) {\n\n    Set<String> hiddenRet = new HashSet<String>();\n    // Build a list of hidden files\n    if (invariants != null) {\n      String[] hidden = invariants.getParams(HIDDEN);\n      if (hidden != null) {\n        for (String s : hidden) {\n          hiddenRet.add(s.toUpperCase(Locale.ROOT));\n        }\n      }\n    }\n    return hiddenRet;\n  }\n\n  @Override\n  public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp)\n      throws InterruptedException, KeeperException, IOException {\n\n    CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n    if (coreContainer.isZooKeeperAware()) {\n      showFromZooKeeper(req, rsp, coreContainer);\n    } else {\n      showFromFileSystem(req, rsp);\n    }\n  }\n\n  // Get a list of files from ZooKeeper for from the path in the file= parameter.\n  private void showFromZooKeeper(SolrQueryRequest req, SolrQueryResponse rsp,\n      CoreContainer coreContainer) throws KeeperException,\n      InterruptedException, UnsupportedEncodingException {\n\n    SolrZkClient zkClient = coreContainer.getZkController().getZkClient();\n\n    String adminFile = getAdminFileFromZooKeeper(req, rsp, zkClient, hiddenFiles);\n\n    if (adminFile == null) {\n      return;\n    }\n\n    // Show a directory listing\n    List<String> children = zkClient.getChildren(adminFile, null, true);\n    if (children.size() > 0) {\n      \n      NamedList<SimpleOrderedMap<Object>> files = new SimpleOrderedMap<SimpleOrderedMap<Object>>();\n      for (String f : children) {\n        if (isHiddenFile(req, rsp, f, false, hiddenFiles)) {\n          continue;\n        }\n\n        SimpleOrderedMap<Object> fileInfo = new SimpleOrderedMap<Object>();\n        files.add(f, fileInfo);\n        List<String> fchildren = zkClient.getChildren(adminFile + \"/\" + f, null, true);\n        if (fchildren.size() > 0) {\n          fileInfo.add(\"directory\", true);\n        } else {\n          // TODO? content type\n          fileInfo.add(\"size\", f.length());\n        }\n        // TODO: ?\n        // fileInfo.add( \"modified\", new Date( f.lastModified() ) );\n      }\n      rsp.add(\"files\", files);\n    } else {\n      // Include the file contents\n      // The file logic depends on RawResponseWriter, so force its use.\n      ModifiableSolrParams params = new ModifiableSolrParams(req.getParams());\n      params.set(CommonParams.WT, \"raw\");\n      req.setParams(params);\n      ContentStreamBase content = new ContentStreamBase.ByteArrayStream(zkClient.getData(adminFile, null, null, true), adminFile);\n      content.setContentType(req.getParams().get(USE_CONTENT_TYPE));\n      \n      rsp.add(RawResponseWriter.CONTENT, content);\n    }\n    rsp.setHttpCaching(false);\n  }\n\n  // Return the file indicated (or the directory listing) from the local file system.\n  private void showFromFileSystem(SolrQueryRequest req, SolrQueryResponse rsp) {\n    File adminFile = getAdminFileFromFileSystem(req, rsp, hiddenFiles);\n\n    if (adminFile == null) { // exception already recorded\n      return;\n    }\n\n    // Make sure the file exists, is readable and is not a hidden file\n    if( !adminFile.exists() ) {\n      log.error(\"Can not find: \"+adminFile.getName() + \" [\"+adminFile.getAbsolutePath()+\"]\");\n      rsp.setException(new SolrException\n                       ( ErrorCode.NOT_FOUND, \"Can not find: \"+adminFile.getName() \n                         + \" [\"+adminFile.getAbsolutePath()+\"]\" ));\n      return;\n    }\n    if( !adminFile.canRead() || adminFile.isHidden() ) {\n      log.error(\"Can not show: \"+adminFile.getName() + \" [\"+adminFile.getAbsolutePath()+\"]\");\n      rsp.setException(new SolrException\n                       ( ErrorCode.NOT_FOUND, \"Can not show: \"+adminFile.getName() \n                         + \" [\"+adminFile.getAbsolutePath()+\"]\" ));\n      return;\n    }\n    \n    // Show a directory listing\n    if( adminFile.isDirectory() ) {\n      // it's really a directory, just go for it.\n      int basePath = adminFile.getAbsolutePath().length() + 1;\n      NamedList<SimpleOrderedMap<Object>> files = new SimpleOrderedMap<SimpleOrderedMap<Object>>();\n      for( File f : adminFile.listFiles() ) {\n        String path = f.getAbsolutePath().substring( basePath );\n        path = path.replace( '\\\\', '/' ); // normalize slashes\n\n        if (isHiddenFile(req, rsp, f.getName().replace('\\\\', '/'), false, hiddenFiles)) {\n          continue;\n        }\n\n        SimpleOrderedMap<Object> fileInfo = new SimpleOrderedMap<Object>();\n        files.add( path, fileInfo );\n        if( f.isDirectory() ) {\n          fileInfo.add( \"directory\", true ); \n        }\n        else {\n          // TODO? content type\n          fileInfo.add( \"size\", f.length() );\n        }\n        fileInfo.add( \"modified\", new Date( f.lastModified() ) );\n      }\n      rsp.add(\"files\", files);\n    }\n    else {\n      // Include the file contents\n      //The file logic depends on RawResponseWriter, so force its use.\n      ModifiableSolrParams params = new ModifiableSolrParams( req.getParams() );\n      params.set( CommonParams.WT, \"raw\" );\n      req.setParams(params);\n\n      ContentStreamBase content = new ContentStreamBase.FileStream( adminFile );\n      content.setContentType(req.getParams().get(USE_CONTENT_TYPE));\n\n      rsp.add(RawResponseWriter.CONTENT, content);\n    }\n    rsp.setHttpCaching(false);\n  }\n\n  //////////////////////// Static methods //////////////////////////////\n\n  public static boolean isHiddenFile(SolrQueryRequest req, SolrQueryResponse rsp, String fnameIn, boolean reportError,\n                                     Set<String> hiddenFiles) {\n    String fname = fnameIn.toUpperCase(Locale.ROOT);\n    if (hiddenFiles.contains(fname) || hiddenFiles.contains(\"*\")) {\n      if (reportError) {\n        log.error(\"Cannot access \" + fname);\n        rsp.setException(new SolrException(SolrException.ErrorCode.FORBIDDEN, \"Can not access: \" + fnameIn));\n      }\n      return true;\n    }\n\n    // This is slightly off, a valid path is something like ./schema.xml. I don't think it's worth the effort though\n    // to fix it to handle all possibilities though.\n    if (fname.indexOf(\"..\") >= 0 || fname.startsWith(\".\")) {\n      if (reportError) {\n        log.error(\"Invalid path: \" + fname);\n        rsp.setException(new SolrException(SolrException.ErrorCode.FORBIDDEN, \"Invalid path: \" + fnameIn));\n      }\n      return true;\n    }\n\n    // Make sure that if the schema is managed, we don't allow editing. Don't really want to put\n    // this in the init since we're not entirely sure when the managed schema will get initialized relative to this\n    // handler.\n    SolrCore core = req.getCore();\n    IndexSchema schema = core.getLatestSchema();\n    if (schema instanceof ManagedIndexSchema) {\n      String managed = schema.getResourceName();\n\n      if (fname.equalsIgnoreCase(managed)) {\n        return true;\n      }\n    }\n    return false;\n  }\n\n  // Refactored to be usable from multiple methods. Gets the path of the requested file from ZK.\n  // Returns null if the file is not found.\n  //\n  // Assumes that the file is in a parameter called \"file\".\n\n  public static String getAdminFileFromZooKeeper(SolrQueryRequest req, SolrQueryResponse rsp, SolrZkClient zkClient,\n                                                 Set<String> hiddenFiles)\n      throws KeeperException, InterruptedException {\n    String adminFile = null;\n    SolrCore core = req.getCore();\n\n    final ZkSolrResourceLoader loader = (ZkSolrResourceLoader) core\n        .getResourceLoader();\n    String confPath = loader.getCollectionZkPath();\n\n    String fname = req.getParams().get(\"file\", null);\n    if (fname == null) {\n      adminFile = confPath;\n    } else {\n      fname = fname.replace('\\\\', '/'); // normalize slashes\n      if (isHiddenFile(req, rsp, fname, true, hiddenFiles)) {\n        return null;\n      }\n      if (fname.startsWith(\"/\")) { // Only files relative to conf are valid\n        fname = fname.substring(1);\n      }\n      adminFile = confPath + \"/\" + fname;\n    }\n\n    // Make sure the file exists, is readable and is not a hidden file\n    if (!zkClient.exists(adminFile, true)) {\n      log.error(\"Can not find: \" + adminFile);\n      rsp.setException(new SolrException(SolrException.ErrorCode.NOT_FOUND, \"Can not find: \"\n          + adminFile));\n      return null;\n    }\n\n    return adminFile;\n  }\n\n\n  // Find the file indicated by the \"file=XXX\" parameter or the root of the conf directory on the local\n  // file system. Respects all the \"interesting\" stuff around what the resource loader does to find files.\n  public static File getAdminFileFromFileSystem(SolrQueryRequest req, SolrQueryResponse rsp,\n                                                Set<String> hiddenFiles) {\n    File adminFile = null;\n    final SolrResourceLoader loader = req.getCore().getResourceLoader();\n    File configdir = new File( loader.getConfigDir() );\n    if (!configdir.exists()) {\n      // TODO: maybe we should just open it this way to start with?\n      try {\n        configdir = new File( loader.getClassLoader().getResource(loader.getConfigDir()).toURI() );\n      } catch (URISyntaxException e) {\n        log.error(\"Can not access configuration directory!\");\n        rsp.setException(new SolrException( SolrException.ErrorCode.FORBIDDEN, \"Can not access configuration directory!\", e));\n        return null;\n      }\n    }\n    String fname = req.getParams().get(\"file\", null);\n    if( fname == null ) {\n      adminFile = configdir;\n    }\n    else {\n      fname = fname.replace( '\\\\', '/' ); // normalize slashes\n      if( hiddenFiles.contains( fname.toUpperCase(Locale.ROOT) ) ) {\n        log.error(\"Can not access: \"+ fname);\n        rsp.setException(new SolrException( SolrException.ErrorCode.FORBIDDEN, \"Can not access: \"+fname ));\n        return null;\n      }\n      if( fname.indexOf( \"..\" ) >= 0 ) {\n        log.error(\"Invalid path: \"+ fname);\n        rsp.setException(new SolrException( SolrException.ErrorCode.FORBIDDEN, \"Invalid path: \"+fname ));\n        return null;\n      }\n      adminFile = new File( configdir, fname );\n    }\n    return adminFile;\n  }\n\n  public final Set<String> getHiddenFiles() {\n    return hiddenFiles;\n  }\n\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n  @Override\n  public String getDescription() {\n    return \"Admin Config File -- view or update config files directly\";\n  }\n\n  @Override\n  public String getSource() {\n    return \"$URL$\";\n  }\n}\n",
        "methodName": "showFromFileSystem",
        "exampleID": 48,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java",
        "line": "220",
        "source": "?",
        "sourceLine": "220",
        "qualifier": "Possible null pointer dereference of the $$value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 49
            }
        ],
        "line_number": "220"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SqlEntityProcessor.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.solr.handler.dataimport;\n\nimport java.util.Iterator;\nimport java.util.Map;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n/**\n * <p>\n * An {@link EntityProcessor} instance which provides support for reading from\n * databases. It is used in conjunction with {@link JdbcDataSource}. This is the default\n * {@link EntityProcessor} if none is specified explicitly in data-config.xml\n * </p>\n * <p/>\n * <p>\n * Refer to <a\n * href=\"http://wiki.apache.org/solr/DataImportHandler\">http://wiki.apache.org/solr/DataImportHandler</a>\n * for more details.\n * </p>\n * <p/>\n * <b>This API is experimental and may change in the future.</b>\n *\n *\n * @since solr 1.3\n */\npublic class SqlEntityProcessor extends EntityProcessorBase {\n  private static final Logger LOG = LoggerFactory.getLogger(SqlEntityProcessor.class);\n\n  protected DataSource<Iterator<Map<String, Object>>> dataSource;\n\n  @Override\n  @SuppressWarnings(\"unchecked\")\n  public void init(Context context) {\n    super.init(context);\n    dataSource = context.getDataSource();\n  }\n\n  protected void initQuery(String q) {\n    try {\n      DataImporter.QUERY_COUNT.get().incrementAndGet();\n      rowIterator = dataSource.getData(q);\n      this.query = q;\n    } catch (DataImportHandlerException e) {\n      throw e;\n    } catch (Exception e) {\n      LOG.error( \"The query failed '\" + q + \"'\", e);\n      throw new DataImportHandlerException(DataImportHandlerException.SEVERE, e);\n    }\n  }\n\n  @Override\n  public Map<String, Object> nextRow() {    \n    if (rowIterator == null) {\n      String q = getQuery();\n      initQuery(context.replaceTokens(q));\n    }\n    return getNext();\n  }\n\n  @Override\n  public Map<String, Object> nextModifiedRowKey() {\n    if (rowIterator == null) {\n      String deltaQuery = context.getEntityAttribute(DELTA_QUERY);\n      if (deltaQuery == null)\n        return null;\n      initQuery(context.replaceTokens(deltaQuery));\n    }\n    return getNext();\n  }\n\n  @Override\n  public Map<String, Object> nextDeletedRowKey() {\n    if (rowIterator == null) {\n      String deletedPkQuery = context.getEntityAttribute(DEL_PK_QUERY);\n      if (deletedPkQuery == null)\n        return null;\n      initQuery(context.replaceTokens(deletedPkQuery));\n    }\n    return getNext();\n  }\n\n  @Override\n  public Map<String, Object> nextModifiedParentRowKey() {\n    if (rowIterator == null) {\n      String parentDeltaQuery = context.getEntityAttribute(PARENT_DELTA_QUERY);\n      if (parentDeltaQuery == null)\n        return null;\n      LOG.info(\"Running parentDeltaQuery for Entity: \"\n              + context.getEntityAttribute(\"name\"));\n      initQuery(context.replaceTokens(parentDeltaQuery));\n    }\n    return getNext();\n  }\n\n  public String getQuery() {\n    String queryString = context.getEntityAttribute(QUERY);\n    if (Context.FULL_DUMP.equals(context.currentProcess())) {\n      return queryString;\n    }\n    if (Context.DELTA_DUMP.equals(context.currentProcess())) {\n      String deltaImportQuery = context.getEntityAttribute(DELTA_IMPORT_QUERY);\n      if(deltaImportQuery != null) return deltaImportQuery;\n    }\n    LOG.warn(\"'deltaImportQuery' attribute is not specified for entity : \"+ entityName);\n    return getDeltaImportQuery(queryString);\n  }\n\n  public String getDeltaImportQuery(String queryString) {    \n    StringBuilder sb = new StringBuilder(queryString);\n    if (SELECT_WHERE_PATTERN.matcher(queryString).find()) {\n      sb.append(\" and \");\n    } else {\n      sb.append(\" where \");\n    }\n    boolean first = true;\n    String[] primaryKeys = context.getEntityAttribute(\"pk\").split(\",\");\n    for (String primaryKey : primaryKeys) {\n      if (!first) {\n        sb.append(\" and \");\n      }\n      first = false;\n      Object val = context.resolve(\"dataimporter.delta.\" + primaryKey);\n      if (val == null) {\n        Matcher m = DOT_PATTERN.matcher(primaryKey);\n        if (m.find()) {\n          val = context.resolve(\"dataimporter.delta.\" + m.group(1));\n        }\n      }\n      sb.append(primaryKey).append(\" = \");\n      if (val instanceof Number) {\n        sb.append(val.toString());\n      } else {\n        sb.append(\"'\").append(val.toString()).append(\"'\");\n      }\n    }\n    return sb.toString();\n  }\n\n  private static Pattern SELECT_WHERE_PATTERN = Pattern.compile(\n          \"^\\\\s*(select\\\\b.*?\\\\b)(where).*\", Pattern.CASE_INSENSITIVE);\n\n  public static final String QUERY = \"query\";\n\n  public static final String DELTA_QUERY = \"deltaQuery\";\n\n  public static final String DELTA_IMPORT_QUERY = \"deltaImportQuery\";\n\n  public static final String PARENT_DELTA_QUERY = \"parentDeltaQuery\";\n\n  public static final String DEL_PK_QUERY = \"deletedPkQuery\";\n\n  public static final Pattern DOT_PATTERN = Pattern.compile(\".*?\\\\.(.*)$\");\n}\n",
        "methodName": "getDeltaImportQuery",
        "exampleID": 50,
        "dataset": "spotbugs",
        "filepath": "/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SqlEntityProcessor.java",
        "line": "143",
        "source": "val",
        "sourceLine": "151",
        "qualifier": "Possible null pointer dereference of $$val/$",
        "steps": [
            {
                "exampleID": 51
            }
        ],
        "line_number": "151"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.solr.highlight;\n\nimport org.apache.lucene.analysis.CachingTokenFilter;\nimport org.apache.lucene.analysis.TokenFilter;\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.tokenattributes.OffsetAttribute;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.StorableField;\nimport org.apache.lucene.index.StoredDocument;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.highlight.*;\nimport org.apache.lucene.search.highlight.Formatter;\nimport org.apache.lucene.search.vectorhighlight.*;\nimport org.apache.lucene.util.AttributeSource.State;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.params.HighlightParams;\nimport org.apache.solr.common.params.SolrParams;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.common.util.SimpleOrderedMap;\nimport org.apache.solr.core.PluginInfo;\nimport org.apache.solr.core.SolrConfig;\nimport org.apache.solr.core.SolrCore;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.schema.IndexSchema;\nimport org.apache.solr.schema.SchemaField;\nimport org.apache.solr.search.DocIterator;\nimport org.apache.solr.search.DocList;\nimport org.apache.solr.search.SolrIndexSearcher;\nimport org.apache.solr.util.plugin.PluginInfoInitialized;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.IOException;\nimport java.util.*;\n\n/**\n * \n * @since solr 1.3\n */\npublic class DefaultSolrHighlighter extends SolrHighlighter implements PluginInfoInitialized\n{\n\n  public static Logger log = LoggerFactory.getLogger(DefaultSolrHighlighter.class);\n  \n  private SolrCore solrCore;\n\n  public DefaultSolrHighlighter() {\n  }\n\n  public DefaultSolrHighlighter(SolrCore solrCore) {\n    this.solrCore = solrCore;\n  }\n\n  // Thread safe registry\n  protected final Map<String,SolrFormatter> formatters =\n    new HashMap<String, SolrFormatter>();\n\n  // Thread safe registry\n  protected final Map<String,SolrEncoder> encoders =\n    new HashMap<String, SolrEncoder>();\n\n  // Thread safe registry\n  protected final Map<String,SolrFragmenter> fragmenters =\n    new HashMap<String, SolrFragmenter>() ;\n\n  // Thread safe registry\n  protected final Map<String, SolrFragListBuilder> fragListBuilders =\n    new HashMap<String, SolrFragListBuilder>() ;\n\n  // Thread safe registry\n  protected final Map<String, SolrFragmentsBuilder> fragmentsBuilders =\n    new HashMap<String, SolrFragmentsBuilder>() ;\n\n  // Thread safe registry\n  protected final Map<String, SolrBoundaryScanner> boundaryScanners =\n    new HashMap<String, SolrBoundaryScanner>() ;\n\n  @Override\n  public void init(PluginInfo info) {\n    formatters.clear();\n    encoders.clear();\n    fragmenters.clear();\n    fragListBuilders.clear();\n    fragmentsBuilders.clear();\n    boundaryScanners.clear();\n\n    // Load the fragmenters\n    SolrFragmenter frag = solrCore.initPlugins(info.getChildren(\"fragmenter\") , fragmenters,SolrFragmenter.class,null);\n    if (frag == null) frag = new GapFragmenter();\n    fragmenters.put(\"\", frag);\n    fragmenters.put(null, frag);\n\n    // Load the formatters\n    SolrFormatter fmt = solrCore.initPlugins(info.getChildren(\"formatter\"), formatters,SolrFormatter.class,null);\n    if (fmt == null) fmt = new HtmlFormatter();\n    formatters.put(\"\", fmt);\n    formatters.put(null, fmt);\n\n    // Load the encoders\n    SolrEncoder enc = solrCore.initPlugins(info.getChildren(\"encoder\"), encoders,SolrEncoder.class,null);\n    if (enc == null) enc = new DefaultEncoder();\n    encoders.put(\"\", enc);\n    encoders.put(null, enc);\n\n    // Load the FragListBuilders\n    SolrFragListBuilder fragListBuilder = solrCore.initPlugins(info.getChildren(\"fragListBuilder\"),\n        fragListBuilders, SolrFragListBuilder.class, null );\n    if( fragListBuilder == null ) fragListBuilder = new SimpleFragListBuilder();\n    fragListBuilders.put( \"\", fragListBuilder );\n    fragListBuilders.put( null, fragListBuilder );\n    \n    // Load the FragmentsBuilders\n    SolrFragmentsBuilder fragsBuilder = solrCore.initPlugins(info.getChildren(\"fragmentsBuilder\"),\n        fragmentsBuilders, SolrFragmentsBuilder.class, null);\n    if( fragsBuilder == null ) fragsBuilder = new ScoreOrderFragmentsBuilder();\n    fragmentsBuilders.put( \"\", fragsBuilder );\n    fragmentsBuilders.put( null, fragsBuilder );\n\n    // Load the BoundaryScanners\n    SolrBoundaryScanner boundaryScanner = solrCore.initPlugins(info.getChildren(\"boundaryScanner\"),\n        boundaryScanners, SolrBoundaryScanner.class, null);\n    if(boundaryScanner == null) boundaryScanner = new SimpleBoundaryScanner();\n    boundaryScanners.put(\"\", boundaryScanner);\n    boundaryScanners.put(null, boundaryScanner);\n    \n    initialized = true;\n  }\n  //just for back-compat with the deprecated method\n  private boolean initialized = false;\n  @Override\n  @Deprecated\n  public void initalize( SolrConfig config) {\n    if (initialized) return;\n    SolrFragmenter frag = new GapFragmenter();\n    fragmenters.put(\"\", frag);\n    fragmenters.put(null, frag);\n\n    SolrFormatter fmt = new HtmlFormatter();\n    formatters.put(\"\", fmt);\n    formatters.put(null, fmt);    \n\n    SolrEncoder enc = new DefaultEncoder();\n    encoders.put(\"\", enc);\n    encoders.put(null, enc);    \n\n    SolrFragListBuilder fragListBuilder = new SimpleFragListBuilder();\n    fragListBuilders.put( \"\", fragListBuilder );\n    fragListBuilders.put( null, fragListBuilder );\n    \n    SolrFragmentsBuilder fragsBuilder = new ScoreOrderFragmentsBuilder();\n    fragmentsBuilders.put( \"\", fragsBuilder );\n    fragmentsBuilders.put( null, fragsBuilder );\n    \n    SolrBoundaryScanner boundaryScanner = new SimpleBoundaryScanner();\n    boundaryScanners.put(\"\", boundaryScanner);\n    boundaryScanners.put(null, boundaryScanner);\n  }\n\n  /**\n   * Return a phrase {@link org.apache.lucene.search.highlight.Highlighter} appropriate for this field.\n   * @param query The current Query\n   * @param fieldName The name of the field\n   * @param request The current SolrQueryRequest\n   * @param tokenStream document text CachingTokenStream\n   * @throws IOException If there is a low-level I/O error.\n   */\n  protected Highlighter getPhraseHighlighter(Query query, String fieldName, SolrQueryRequest request, CachingTokenFilter tokenStream) throws IOException {\n    SolrParams params = request.getParams();\n    Highlighter highlighter = null;\n    \n    highlighter = new Highlighter(\n        getFormatter(fieldName, params),\n        getEncoder(fieldName, params),\n        getSpanQueryScorer(query, fieldName, tokenStream, request));\n    \n    highlighter.setTextFragmenter(getFragmenter(fieldName, params));\n\n    return highlighter;\n  }\n  \n  /**\n   * Return a {@link org.apache.lucene.search.highlight.Highlighter} appropriate for this field.\n   * @param query The current Query\n   * @param fieldName The name of the field\n   * @param request The current SolrQueryRequest\n   */\n  protected Highlighter getHighlighter(Query query, String fieldName, SolrQueryRequest request) {\n    SolrParams params = request.getParams(); \n    Highlighter highlighter = new Highlighter(\n           getFormatter(fieldName, params), \n           getEncoder(fieldName, params),\n           getQueryScorer(query, fieldName, request));\n     highlighter.setTextFragmenter(getFragmenter(fieldName, params));\n       return highlighter;\n  }\n  \n  /**\n   * Return a {@link org.apache.lucene.search.highlight.QueryScorer} suitable for this Query and field.\n   * @param query The current query\n   * @param tokenStream document text CachingTokenStream\n   * @param fieldName The name of the field\n   * @param request The SolrQueryRequest\n   */\n  private QueryScorer getSpanQueryScorer(Query query, String fieldName, TokenStream tokenStream, SolrQueryRequest request) {\n    boolean reqFieldMatch = request.getParams().getFieldBool(fieldName, HighlightParams.FIELD_MATCH, false);\n    Boolean highlightMultiTerm = request.getParams().getBool(HighlightParams.HIGHLIGHT_MULTI_TERM, true);\n    if(highlightMultiTerm == null) {\n      highlightMultiTerm = false;\n    }\n    QueryScorer scorer;\n    if (reqFieldMatch) {\n      scorer = new QueryScorer(query, fieldName);\n    }\n    else {\n      scorer = new QueryScorer(query, null);\n    }\n    scorer.setExpandMultiTermQuery(highlightMultiTerm);\n    return scorer;\n  }\n\n  /**\n   * Return a {@link org.apache.lucene.search.highlight.Scorer} suitable for this Query and field.\n   * @param query The current query\n   * @param fieldName The name of the field\n   * @param request The SolrQueryRequest\n   */\n  private Scorer getQueryScorer(Query query, String fieldName, SolrQueryRequest request) {\n     boolean reqFieldMatch = request.getParams().getFieldBool(fieldName, HighlightParams.FIELD_MATCH, false);\n     if (reqFieldMatch) {\n        return new QueryTermScorer(query, request.getSearcher().getIndexReader(), fieldName);\n     }\n     else {\n        return new QueryTermScorer(query);\n     }\n  }\n  \n  /**\n   * Return the max number of snippets for this field. If this has not\n   * been configured for this field, fall back to the configured default\n   * or the solr default.\n   * @param fieldName The name of the field\n   * @param params The params controlling Highlighting\n   */\n  protected int getMaxSnippets(String fieldName, SolrParams params) {\n     return params.getFieldInt(fieldName, HighlightParams.SNIPPETS,1);\n  }\n\n  /**\n   * Return whether adjacent fragments should be merged.\n   * @param fieldName The name of the field\n   * @param params The params controlling Highlighting\n   */\n  protected boolean isMergeContiguousFragments(String fieldName, SolrParams params){\n    return params.getFieldBool(fieldName, HighlightParams.MERGE_CONTIGUOUS_FRAGMENTS, false);\n  }\n  \n  /**\n   * Return a {@link org.apache.lucene.search.highlight.Formatter} appropriate for this field. If a formatter\n   * has not been configured for this field, fall back to the configured\n   * default or the solr default ({@link org.apache.lucene.search.highlight.SimpleHTMLFormatter}).\n   * \n   * @param fieldName The name of the field\n   * @param params The params controlling Highlighting\n   * @return An appropriate {@link org.apache.lucene.search.highlight.Formatter}.\n   */\n  protected Formatter getFormatter(String fieldName, SolrParams params ) \n  {\n    String str = params.getFieldParam( fieldName, HighlightParams.FORMATTER );\n    SolrFormatter formatter = formatters.get( str );\n    if( formatter == null ) {\n      throw new SolrException( SolrException.ErrorCode.BAD_REQUEST, \"Unknown formatter: \"+str );\n    }\n    return formatter.getFormatter( fieldName, params );\n  }\n\n  /**\n   * Return an {@link org.apache.lucene.search.highlight.Encoder} appropriate for this field. If an encoder\n   * has not been configured for this field, fall back to the configured\n   * default or the solr default ({@link org.apache.lucene.search.highlight.DefaultEncoder}).\n   * \n   * @param fieldName The name of the field\n   * @param params The params controlling Highlighting\n   * @return An appropriate {@link org.apache.lucene.search.highlight.Encoder}.\n   */\n  protected Encoder getEncoder(String fieldName, SolrParams params){\n    String str = params.getFieldParam( fieldName, HighlightParams.ENCODER );\n    SolrEncoder encoder = encoders.get( str );\n    if( encoder == null ) {\n      throw new SolrException( SolrException.ErrorCode.BAD_REQUEST, \"Unknown encoder: \"+str );\n    }\n    return encoder.getEncoder( fieldName, params );\n  }\n  \n  /**\n   * Return a {@link org.apache.lucene.search.highlight.Fragmenter} appropriate for this field. If a fragmenter\n   * has not been configured for this field, fall back to the configured\n   * default or the solr default ({@link GapFragmenter}).\n   * \n   * @param fieldName The name of the field\n   * @param params The params controlling Highlighting\n   * @return An appropriate {@link org.apache.lucene.search.highlight.Fragmenter}.\n   */\n  protected Fragmenter getFragmenter(String fieldName, SolrParams params) \n  {\n    String fmt = params.getFieldParam( fieldName, HighlightParams.FRAGMENTER );\n    SolrFragmenter frag = fragmenters.get( fmt );\n    if( frag == null ) {\n      throw new SolrException( SolrException.ErrorCode.BAD_REQUEST, \"Unknown fragmenter: \"+fmt );\n    }\n    return frag.getFragmenter( fieldName, params );\n  }\n  \n  protected FragListBuilder getFragListBuilder( String fieldName, SolrParams params ){\n    String flb = params.getFieldParam( fieldName, HighlightParams.FRAG_LIST_BUILDER );\n    SolrFragListBuilder solrFlb = fragListBuilders.get( flb );\n    if( solrFlb == null ){\n      throw new SolrException( SolrException.ErrorCode.BAD_REQUEST, \"Unknown fragListBuilder: \" + flb );\n    }\n    return solrFlb.getFragListBuilder( params );\n  }\n  \n  protected FragmentsBuilder getFragmentsBuilder( String fieldName, SolrParams params ){\n    BoundaryScanner bs = getBoundaryScanner(fieldName, params);\n    return getSolrFragmentsBuilder( fieldName, params ).getFragmentsBuilder( params, bs );\n  }\n  \n  private SolrFragmentsBuilder getSolrFragmentsBuilder( String fieldName, SolrParams params ){\n    String fb = params.getFieldParam( fieldName, HighlightParams.FRAGMENTS_BUILDER );\n    SolrFragmentsBuilder solrFb = fragmentsBuilders.get( fb );\n    if( solrFb == null ){\n      throw new SolrException( SolrException.ErrorCode.BAD_REQUEST, \"Unknown fragmentsBuilder: \" + fb );\n    }\n    return solrFb;\n  }\n  \n  private BoundaryScanner getBoundaryScanner(String fieldName, SolrParams params){\n    String bs = params.getFieldParam(fieldName, HighlightParams.BOUNDARY_SCANNER);\n    SolrBoundaryScanner solrBs = boundaryScanners.get(bs);\n    if(solrBs == null){\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown boundaryScanner: \" + bs);\n    }\n    return solrBs.getBoundaryScanner(fieldName, params);\n  }\n  \n  /**\n   * Generates a list of Highlighted query fragments for each item in a list\n   * of documents, or returns null if highlighting is disabled.\n   *\n   * @param docs query results\n   * @param query the query\n   * @param req the current request\n   * @param defaultFields default list of fields to summarize\n   *\n   * @return NamedList containing a NamedList for each document, which in \n   * turns contains sets (field, summary) pairs.\n   */\n  @Override\n  @SuppressWarnings(\"unchecked\")\n  public NamedList<Object> doHighlighting(DocList docs, Query query, SolrQueryRequest req, String[] defaultFields) throws IOException {\n    SolrParams params = req.getParams(); \n    if (!isHighlightingEnabled(params))\n        return null;\n     \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    NamedList fragments = new SimpleOrderedMap();\n    String[] fieldNames = getHighlightFields(query, req, defaultFields);\n    Set<String> fset = new HashSet<String>();\n     \n    {\n      // pre-fetch documents using the Searcher's doc cache\n      for(String f : fieldNames) { fset.add(f); }\n      // fetch unique key if one exists.\n      SchemaField keyField = schema.getUniqueKeyField();\n      if(null != keyField)\n        fset.add(keyField.getName());  \n    }\n\n    // get FastVectorHighlighter instance out of the processing loop\n    FastVectorHighlighter fvh = new FastVectorHighlighter(\n        // FVH cannot process hl.usePhraseHighlighter parameter per-field basis\n        params.getBool( HighlightParams.USE_PHRASE_HIGHLIGHTER, true ),\n        // FVH cannot process hl.requireFieldMatch parameter per-field basis\n        params.getBool( HighlightParams.FIELD_MATCH, false ) );\n    fvh.setPhraseLimit(params.getInt(HighlightParams.PHRASE_LIMIT, Integer.MAX_VALUE));\n    FieldQuery fieldQuery = fvh.getFieldQuery( query, searcher.getIndexReader() );\n\n    // Highlight each document\n    DocIterator iterator = docs.iterator();\n    for (int i = 0; i < docs.size(); i++) {\n      int docId = iterator.nextDoc();\n      StoredDocument doc = searcher.doc(docId, fset);\n      NamedList docSummaries = new SimpleOrderedMap();\n      for (String fieldName : fieldNames) {\n        fieldName = fieldName.trim();\n        if( useFastVectorHighlighter( params, schema, fieldName ) )\n          doHighlightingByFastVectorHighlighter( fvh, fieldQuery, req, docSummaries, docId, doc, fieldName );\n        else\n          doHighlightingByHighlighter( query, req, docSummaries, docId, doc, fieldName );\n      }\n      String printId = schema.printableUniqueKey(doc);\n      fragments.add(printId == null ? null : printId, docSummaries);\n    }\n    return fragments;\n  }\n  \n  /*\n   * If fieldName is undefined, this method returns false, then\n   * doHighlightingByHighlighter() will do nothing for the field.\n   */\n  private boolean useFastVectorHighlighter( SolrParams params, IndexSchema schema, String fieldName ){\n    SchemaField schemaField = schema.getFieldOrNull( fieldName );\n    if( schemaField == null ) return false;\n    boolean useFvhParam = params.getFieldBool( fieldName, HighlightParams.USE_FVH, false );\n    if( !useFvhParam ) return false;\n    boolean termPosOff = schemaField.storeTermPositions() && schemaField.storeTermOffsets();\n    if( !termPosOff ) {\n      log.warn( \"Solr will use Highlighter instead of FastVectorHighlighter because {} field does not store TermPositions and TermOffsets.\", fieldName );\n    }\n    return termPosOff;\n  }\n  \n  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, StoredDocument doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams();\n\n    // preserve order of values in a multiValued list\n    boolean preserveMulti = params.getFieldBool(fieldName, HighlightParams.PRESERVE_MULTI, false);\n\n    List<StorableField> allFields = doc.getFields();\n    if (allFields != null && allFields.size() == 0) return; // No explicit contract that getFields returns != null,\n                                                            // although currently it can't.\n\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);\n    if (tvStream != null) {\n      tots = new TermOffsetsTokenStream(tvStream);\n    }\n    int mvToExamine = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_EXAMINE,\n        Integer.toString(Integer.MAX_VALUE)));\n    int mvToMatch = Integer.parseInt(req.getParams().get(HighlightParams.MAX_MULTIVALUED_TO_MATCH,\n        Integer.toString(Integer.MAX_VALUE)));\n\n    for (StorableField thisField : allFields) {\n      if (mvToExamine <= 0 || mvToMatch <= 0) break;\n\n      if (! thisField.name().equals(fieldName)) continue; // Is there a better way to do this?\n\n      --mvToExamine;\n      String thisText = thisField.stringValue();\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( thisText.length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, thisText);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(thisText.length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, thisText, mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if (preserveMulti) {\n            if (bestTextFragments[k] != null) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          } else {\n            if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n              frags.add(bestTextFragments[k]);\n              --mvToMatch;\n            }\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n     if(!preserveMulti){\n        Collections.sort(frags, new Comparator<TextFragment>() {\n                @Override\n                public int compare(TextFragment arg0, TextFragment arg1) {\n                 return Math.round(arg1.getScore() - arg0.getScore());\n        }\n        });\n     }\n\n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if (preserveMulti) {\n          if (fragment != null) {\n            fragTexts.add(fragment.toString());\n          }\n        } else {\n          if ((fragment != null) && (fragment.getScore() > 0)) {\n            fragTexts.add(fragment.toString());\n          }\n        }\n\n        if (fragTexts.size() >= numFragments && !preserveMulti) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n  private void doHighlightingByFastVectorHighlighter( FastVectorHighlighter highlighter, FieldQuery fieldQuery,\n      SolrQueryRequest req, NamedList docSummaries, int docId, StoredDocument doc,\n      String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    SolrFragmentsBuilder solrFb = getSolrFragmentsBuilder( fieldName, params );\n    String[] snippets = highlighter.getBestFragments( fieldQuery, req.getSearcher().getIndexReader(), docId, fieldName,\n        params.getFieldInt( fieldName, HighlightParams.FRAGSIZE, 100 ),\n        params.getFieldInt( fieldName, HighlightParams.SNIPPETS, 1 ),\n        getFragListBuilder( fieldName, params ),\n        getFragmentsBuilder( fieldName, params ),\n        solrFb.getPreTags( params, fieldName ),\n        solrFb.getPostTags( params, fieldName ),\n        getEncoder( fieldName, params ) );\n    if( snippets != null && snippets.length > 0 )\n      docSummaries.add( fieldName, snippets );\n    else\n      alternateField( docSummaries, params, doc, fieldName );\n  }\n  \n  private void alternateField( NamedList docSummaries, SolrParams params, StoredDocument doc, String fieldName ){\n    String alternateField = params.getFieldParam(fieldName, HighlightParams.ALTERNATE_FIELD);\n    if (alternateField != null && alternateField.length() > 0) {\n      StorableField[] docFields = doc.getFields(alternateField);\n      List<String> listFields = new ArrayList<String>();\n      for (StorableField field : docFields) {\n        if (field.binaryValue() == null)\n          listFields.add(field.stringValue());\n      }\n\n      String[] altTexts = listFields.toArray(new String[listFields.size()]);\n\n      if (altTexts != null && altTexts.length > 0){\n        Encoder encoder = getEncoder(fieldName, params);\n        int alternateFieldLen = params.getFieldInt(fieldName, HighlightParams.ALTERNATE_FIELD_LENGTH,0);\n        List<String> altList = new ArrayList<String>();\n        int len = 0;\n        for( String altText: altTexts ){\n          if( alternateFieldLen <= 0 ){\n            altList.add(encoder.encodeText(altText));\n          }\n          else{\n            altList.add( len + altText.length() > alternateFieldLen ?\n                encoder.encodeText(new String(altText.substring( 0, alternateFieldLen - len ))) :\n                encoder.encodeText(altText) );\n            len += altText.length();\n            if( len >= alternateFieldLen ) break;\n          }\n        }\n        docSummaries.add(fieldName, altList);\n      }\n    }\n  }\n  \n  private TokenStream createAnalyzerTStream(IndexSchema schema, String fieldName, String docText) throws IOException {\n\n    TokenStream tstream;\n    TokenStream ts = schema.getAnalyzer().tokenStream(fieldName, docText);\n    ts.reset();\n    tstream = new TokenOrderingFilter(ts, 10);\n    return tstream;\n  }\n}\n\n/** Orders Tokens in a window first by their startOffset ascending.\n * endOffset is currently ignored.\n * This is meant to work around fickleness in the highlighter only.  It\n * can mess up token positions and should not be used for indexing or querying.\n */\nfinal class TokenOrderingFilter extends TokenFilter {\n  private final int windowSize;\n  private final LinkedList<OrderedToken> queue = new LinkedList<OrderedToken>();\n  private boolean done=false;\n  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n  \n  protected TokenOrderingFilter(TokenStream input, int windowSize) {\n    super(input);\n    this.windowSize = windowSize;\n  }\n\n  @Override\n  public boolean incrementToken() throws IOException {\n    while (!done && queue.size() < windowSize) {\n      if (!input.incrementToken()) {\n        done = true;\n        break;\n      }\n\n      // reverse iterating for better efficiency since we know the\n      // list is already sorted, and most token start offsets will be too.\n      ListIterator<OrderedToken> iter = queue.listIterator(queue.size());\n      while(iter.hasPrevious()) {\n        if (offsetAtt.startOffset() >= iter.previous().startOffset) {\n          // insertion will be before what next() would return (what\n          // we just compared against), so move back one so the insertion\n          // will be after.\n          iter.next();\n          break;\n        }\n      }\n      OrderedToken ot = new OrderedToken();\n      ot.state = captureState();\n      ot.startOffset = offsetAtt.startOffset();\n      iter.add(ot);\n    }\n\n    if (queue.isEmpty()) {\n      return false;\n    } else {\n      restoreState(queue.removeFirst().state);\n      return true;\n    }\n  }\n\n  @Override\n  public void reset() throws IOException {\n    // this looks wrong: but its correct.\n  }\n}\n\n// for TokenOrderingFilter, so it can easily sort by startOffset\nclass OrderedToken {\n  State state;\n  int startOffset;\n}\n\nclass TermOffsetsTokenStream {\n\n  TokenStream bufferedTokenStream = null;\n  OffsetAttribute bufferedOffsetAtt;\n  State bufferedToken;\n  int bufferedStartOffset;\n  int bufferedEndOffset;\n  int startOffset;\n  int endOffset;\n\n  public TermOffsetsTokenStream( TokenStream tstream ){\n    bufferedTokenStream = tstream;\n    bufferedOffsetAtt = bufferedTokenStream.addAttribute(OffsetAttribute.class);\n    startOffset = 0;\n    bufferedToken = null;\n  }\n\n  public TokenStream getMultiValuedTokenStream( final int length ){\n    endOffset = startOffset + length;\n    return new MultiValuedStream(length);\n  }\n  \n  final class MultiValuedStream extends TokenStream {\n    private final int length;\n    OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n\n      MultiValuedStream(int length) { \n        super(bufferedTokenStream.cloneAttributes());\n        this.length = length;\n      }\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        while( true ){\n          if( bufferedToken == null ) {\n            if (!bufferedTokenStream.incrementToken())\n              return false;\n            bufferedToken = bufferedTokenStream.captureState();\n            bufferedStartOffset = bufferedOffsetAtt.startOffset();\n            bufferedEndOffset = bufferedOffsetAtt.endOffset();\n          }\n          \n          if( startOffset <= bufferedStartOffset &&\n              bufferedEndOffset <= endOffset ){\n            restoreState(bufferedToken);\n            bufferedToken = null;\n            offsetAtt.setOffset( offsetAtt.startOffset() - startOffset, offsetAtt.endOffset() - startOffset );\n            return true;\n          }\n          else if( bufferedEndOffset > endOffset ){\n            startOffset += length + 1;\n            return false;\n          }\n          bufferedToken = null;\n        }\n      }\n\n  };\n};\n",
        "methodName": "doHighlightingByHighlighter",
        "exampleID": 52,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java",
        "line": "461",
        "source": "allFields",
        "sourceLine": "481",
        "qualifier": "Possible null pointer dereference of $$allFields/$",
        "steps": [
            {
                "exampleID": 53
            }
        ],
        "line_number": "481"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/response/XMLWriter.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.solr.response;\n\nimport java.io.IOException;\nimport java.io.Writer;\nimport java.util.Arrays;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Set;\n\nimport org.apache.solr.common.SolrDocument;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.params.CommonParams;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.common.util.XML;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.search.ReturnFields;\n\n\n/**\n * @lucene.internal\n */\npublic class XMLWriter extends TextResponseWriter {\n\n  public static float CURRENT_VERSION=2.2f;\n\n  private static final char[] XML_START1=\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\".toCharArray();\n\n  private static final char[] XML_STYLESHEET=\"<?xml-stylesheet type=\\\"text/xsl\\\" href=\\\"\".toCharArray();\n  private static final char[] XML_STYLESHEET_END=\"\\\"?>\\n\".toCharArray();\n\n  /*\n  private static final char[] XML_START2_SCHEMA=(\n  \"<response xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\"\\n\"\n  +\" xsi:noNamespaceSchemaLocation=\\\"http://pi.cnet.com/cnet-search/response.xsd\\\">\\n\"\n          ).toCharArray();\n  ***/\n  \n  private static final char[] XML_START2_NOSCHEMA=(\"<response>\\n\").toCharArray();\n\n  final int version;\n\n  public static void writeResponse(Writer writer, SolrQueryRequest req, SolrQueryResponse rsp) throws IOException {\n    XMLWriter xmlWriter = null;\n    try {\n      xmlWriter = new XMLWriter(writer, req, rsp);\n      xmlWriter.writeResponse();\n    } finally {\n      xmlWriter.close();\n    }\n  }\n\n  public XMLWriter(Writer writer, SolrQueryRequest req, SolrQueryResponse rsp) {\n    super(writer, req, rsp);\n\n    String version = req.getParams().get(CommonParams.VERSION);\n    float ver = version==null? CURRENT_VERSION : Float.parseFloat(version);\n    this.version = (int)(ver*1000);\n    if( this.version < 2200 ) {\n      throw new SolrException( SolrException.ErrorCode.BAD_REQUEST,\n          \"XMLWriter does not support version: \"+version );\n    }\n  }\n\n\n\n  public void writeResponse() throws IOException {\n    writer.write(XML_START1);\n\n    String stylesheet = req.getParams().get(\"stylesheet\");\n    if (stylesheet != null && stylesheet.length() > 0) {\n      writer.write(XML_STYLESHEET);\n      XML.escapeAttributeValue(stylesheet, writer);\n      writer.write(XML_STYLESHEET_END);\n    }\n\n    /*\n    String noSchema = req.getParams().get(\"noSchema\");\n    // todo - change when schema becomes available?\n    if (false && noSchema == null)\n      writer.write(XML_START2_SCHEMA);\n    else\n      writer.write(XML_START2_NOSCHEMA);\n     ***/\n    writer.write(XML_START2_NOSCHEMA);\n\n    // dump response values\n    NamedList<?> lst = rsp.getValues();\n    Boolean omitHeader = req.getParams().getBool(CommonParams.OMIT_HEADER);\n    if(omitHeader != null && omitHeader) lst.remove(\"responseHeader\");\n    int sz = lst.size();\n    int start=0;\n\n    for (int i=start; i<sz; i++) {\n      writeVal(lst.getName(i),lst.getVal(i));\n    }\n\n    writer.write(\"\\n</response>\\n\");\n  }\n\n\n\n\n\n  /** Writes the XML attribute name/val. A null val means that the attribute is missing. */\n  private void writeAttr(String name, String val) throws IOException {\n    writeAttr(name, val, true);\n  }\n\n  public void writeAttr(String name, String val, boolean escape) throws IOException{\n    if (val != null) {\n      writer.write(' ');\n      writer.write(name);\n      writer.write(\"=\\\"\");\n      if(escape){\n        XML.escapeAttributeValue(val, writer);\n      } else {\n        writer.write(val);\n      }\n      writer.write('\"');\n    }\n  }\n\n  void startTag(String tag, String name, boolean closeTag) throws IOException {\n    if (doIndent) indent();\n\n    writer.write('<');\n    writer.write(tag);\n    if (name!=null) {\n      writeAttr(\"name\", name);\n      if (closeTag) {\n        writer.write(\"/>\");\n      } else {\n        writer.write(\">\");\n      }\n    } else {\n      if (closeTag) {\n        writer.write(\"/>\");\n      } else {\n        writer.write('>');\n      }\n    }\n  }\n\n\n  @Override\n  public void writeStartDocumentList(String name, \n      long start, int size, long numFound, Float maxScore) throws IOException\n  {\n    if (doIndent) indent();\n\n    writer.write(\"<result\");\n    writeAttr(\"name\",name);\n    writeAttr(\"numFound\",Long.toString(numFound));\n    writeAttr(\"start\",Long.toString(start));\n    if(maxScore!=null) {\n      writeAttr(\"maxScore\",Float.toString(maxScore));\n    }\n    writer.write(\">\");\n    \n    incLevel();\n  }\n\n\n  /**\n   * The SolrDocument should already have multivalued fields implemented as\n   * Collections -- this will not rewrite to &lt;arr&gt;\n   */ \n  @Override\n  public void writeSolrDocument(String name, SolrDocument doc, ReturnFields returnFields, int idx ) throws IOException {\n    startTag(\"doc\", name, false);\n    incLevel();\n\n    for (String fname : doc.getFieldNames()) {\n      if (!returnFields.wantsField(fname)) {\n        continue;\n      }\n      \n      Object val = doc.getFieldValue(fname);\n      if( \"_explain_\".equals( fname ) ) {\n        System.out.println( val );\n      }\n      writeVal(fname, val);\n    }\n    \n    decLevel();\n    writer.write(\"</doc>\");\n  }\n  \n  @Override\n  public void writeEndDocumentList() throws IOException\n  {\n    decLevel();\n    if (doIndent) indent();\n    writer.write(\"</result>\");\n  }\n\n\n\n  //\n  // Generic compound types\n  //\n\n  @Override\n  public void writeNamedList(String name, NamedList val) throws IOException {\n    int sz = val.size();\n    startTag(\"lst\", name, sz<=0);\n\n    incLevel();\n    for (int i=0; i<sz; i++) {\n      writeVal(val.getName(i),val.getVal(i));\n    }\n    decLevel();\n\n    if (sz > 0) {\n      if (doIndent) indent();\n      writer.write(\"</lst>\");\n    }\n  }\n\n  @Override\n  public void writeMap(String name, Map map, boolean excludeOuter, boolean isFirstVal) throws IOException {\n    int sz = map.size();\n\n    if (!excludeOuter) {\n      startTag(\"lst\", name, sz<=0);\n      incLevel();\n    }\n\n    for (Map.Entry entry : (Set<Map.Entry>)map.entrySet()) {\n      Object k = entry.getKey();\n      Object v = entry.getValue();\n      // if (sz<indentThreshold) indent();\n      writeVal( null == k ? null : k.toString(), v);\n    }\n\n    if (!excludeOuter) {\n      decLevel();\n      if (sz > 0) {\n        if (doIndent) indent();\n        writer.write(\"</lst>\");\n      }\n    }\n  }\n\n  @Override\n  public void writeArray(String name, Object[] val) throws IOException {\n    writeArray(name, Arrays.asList(val).iterator());\n  }\n\n  @Override\n  public void writeArray(String name, Iterator iter) throws IOException {\n    if( iter.hasNext() ) {\n      startTag(\"arr\", name, false );\n      incLevel();\n      while( iter.hasNext() ) {\n        writeVal(null, iter.next());\n      }\n      decLevel();\n      if (doIndent) indent();\n      writer.write(\"</arr>\");\n    }\n    else {\n      startTag(\"arr\", name, true );\n    }\n  }\n\n  //\n  // Primitive types\n  //\n\n  @Override\n  public void writeNull(String name) throws IOException {\n    writePrim(\"null\",name,\"\",false);\n  }\n\n  @Override\n  public void writeStr(String name, String val, boolean escape) throws IOException {\n    writePrim(\"str\",name,val,escape);\n  }\n\n  @Override\n  public void writeInt(String name, String val) throws IOException {\n    writePrim(\"int\",name,val,false);\n  }\n\n  @Override\n  public void writeLong(String name, String val) throws IOException {\n    writePrim(\"long\",name,val,false);\n  }\n\n  @Override\n  public void writeBool(String name, String val) throws IOException {\n    writePrim(\"bool\",name,val,false);\n  }\n\n  @Override\n  public void writeFloat(String name, String val) throws IOException {\n    writePrim(\"float\",name,val,false);\n  }\n\n  @Override\n  public void writeFloat(String name, float val) throws IOException {\n    writeFloat(name,Float.toString(val));\n  }\n\n  @Override\n  public void writeDouble(String name, String val) throws IOException {\n    writePrim(\"double\",name,val,false);\n  }\n\n  @Override\n  public void writeDouble(String name, double val) throws IOException {\n    writeDouble(name,Double.toString(val));\n  }\n\n\n  @Override\n  public void writeDate(String name, String val) throws IOException {\n    writePrim(\"date\",name,val,false);\n  }\n\n\n  //\n  // OPT - specific writeInt, writeFloat, methods might be faster since\n  // there would be less write calls (write(\"<int name=\\\"\" + name + ... + </int>)\n  //\n  private void writePrim(String tag, String name, String val, boolean escape) throws IOException {\n    int contentLen = val==null ? 0 : val.length();\n\n    startTag(tag, name, contentLen==0);\n    if (contentLen==0) return;\n\n    if (escape) {\n      XML.escapeCharData(val,writer);\n    } else {\n      writer.write(val,0,contentLen);\n    }\n\n    writer.write('<');\n    writer.write('/');\n    writer.write(tag);\n    writer.write('>');\n  }\n\n}\n",
        "methodName": "writeResponse",
        "exampleID": 54,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/response/XMLWriter.java",
        "line": "62",
        "source": "xmlWriter",
        "sourceLine": "65",
        "qualifier": "Possible null pointer dereference of $$xmlWriter/$",
        "steps": [
            {
                "exampleID": 55
            }
        ],
        "line_number": "65"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.solr.servlet;\n\nimport org.apache.commons.io.IOUtils;\nimport org.apache.commons.lang.StringUtils;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.SolrException.ErrorCode;\nimport org.apache.solr.common.cloud.Aliases;\nimport org.apache.solr.common.cloud.ClusterState;\nimport org.apache.solr.common.cloud.Replica;\nimport org.apache.solr.common.cloud.Slice;\nimport org.apache.solr.common.cloud.SolrZkClient;\nimport org.apache.solr.common.cloud.ZkCoreNodeProps;\nimport org.apache.solr.common.cloud.ZkNodeProps;\nimport org.apache.solr.common.cloud.ZkStateReader;\nimport org.apache.solr.common.params.CommonParams;\nimport org.apache.solr.common.params.MapSolrParams;\nimport org.apache.solr.common.params.ModifiableSolrParams;\nimport org.apache.solr.common.params.SolrParams;\nimport org.apache.solr.common.util.ContentStreamBase;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.common.util.SimpleOrderedMap;\nimport org.apache.solr.common.util.StrUtils;\nimport org.apache.solr.core.ConfigSolr;\nimport org.apache.solr.core.CoreContainer;\nimport org.apache.solr.core.SolrConfig;\nimport org.apache.solr.core.SolrCore;\nimport org.apache.solr.core.SolrResourceLoader;\nimport org.apache.solr.handler.ContentStreamHandlerBase;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.request.SolrQueryRequestBase;\nimport org.apache.solr.request.SolrRequestHandler;\nimport org.apache.solr.request.SolrRequestInfo;\nimport org.apache.solr.response.BinaryQueryResponseWriter;\nimport org.apache.solr.response.QueryResponseWriter;\nimport org.apache.solr.response.SolrQueryResponse;\nimport org.apache.solr.servlet.cache.HttpCacheHeaderUtil;\nimport org.apache.solr.servlet.cache.Method;\nimport org.apache.solr.update.processor.DistributingUpdateProcessorFactory;\nimport org.apache.solr.util.FastWriter;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport javax.servlet.Filter;\nimport javax.servlet.FilterChain;\nimport javax.servlet.FilterConfig;\nimport javax.servlet.ServletException;\nimport javax.servlet.ServletRequest;\nimport javax.servlet.ServletResponse;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.io.OutputStreamWriter;\nimport java.io.Writer;\nimport java.net.HttpURLConnection;\nimport java.net.URL;\nimport java.nio.charset.Charset;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Enumeration;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport java.util.Set;\nimport java.util.WeakHashMap;\n\n/**\n * This filter looks at the incoming URL maps them to handlers defined in solrconfig.xml\n *\n * @since solr 1.2\n */\npublic class SolrDispatchFilter implements Filter\n{\n  private static final String CONNECTION_HEADER = \"Connection\";\n  private static final String TRANSFER_ENCODING_HEADER = \"Transfer-Encoding\";\n\n  final Logger log;\n\n  protected volatile CoreContainer cores;\n\n  protected String pathPrefix = null; // strip this from the beginning of a path\n  protected String abortErrorMessage = null;\n  protected final Map<SolrConfig, SolrRequestParsers> parsers = new WeakHashMap<SolrConfig, SolrRequestParsers>();\n  \n  private static final Charset UTF8 = Charset.forName(\"UTF-8\");\n\n  public SolrDispatchFilter() {\n    try {\n      log = LoggerFactory.getLogger(SolrDispatchFilter.class);\n    } catch (NoClassDefFoundError e) {\n      throw new SolrException(\n          ErrorCode.SERVER_ERROR,\n          \"Could not find necessary SLF4j logging jars. If using Jetty, the SLF4j logging jars need to go in \"\n          +\"the jetty lib/ext directory. For other containers, the corresponding directory should be used. \"\n          +\"For more information, see: http://wiki.apache.org/solr/SolrLogging\",\n          e);\n    }\n  }\n  \n  @Override\n  public void init(FilterConfig config) throws ServletException\n  {\n    log.info(\"SolrDispatchFilter.init()\");\n\n    try {\n      // web.xml configuration\n      this.pathPrefix = config.getInitParameter( \"path-prefix\" );\n\n      this.cores = createCoreContainer();\n      log.info(\"user.dir=\" + System.getProperty(\"user.dir\"));\n    }\n    catch( Throwable t ) {\n      // catch this so our filter still works\n      log.error( \"Could not start Solr. Check solr/home property and the logs\");\n      SolrCore.log( t );\n    }\n\n    log.info(\"SolrDispatchFilter.init() done\");\n  }\n\n  private ConfigSolr loadConfigSolr(SolrResourceLoader loader) {\n\n    String solrxmlLocation = System.getProperty(\"solr.solrxml.location\", \"solrhome\");\n\n    if (solrxmlLocation == null || \"solrhome\".equalsIgnoreCase(solrxmlLocation))\n      return ConfigSolr.fromSolrHome(loader, loader.getInstanceDir());\n\n    if (\"zookeeper\".equalsIgnoreCase(solrxmlLocation)) {\n      String zkHost = System.getProperty(\"zkHost\");\n      log.info(\"Trying to read solr.xml from \" + zkHost);\n      if (StringUtils.isEmpty(zkHost))\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Could not load solr.xml from zookeeper: zkHost system property not set\");\n      SolrZkClient zkClient = new SolrZkClient(zkHost, 30000);\n      try {\n        if (!zkClient.exists(\"/solr.xml\", true))\n          throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not load solr.xml from zookeeper: node not found\");\n        byte[] data = zkClient.getData(\"/solr.xml\", null, null, true);\n        return ConfigSolr.fromInputStream(loader, new ByteArrayInputStream(data));\n      } catch (Exception e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not load solr.xml from zookeeper\", e);\n      } finally {\n        zkClient.close();\n      }\n    }\n\n    throw new SolrException(ErrorCode.SERVER_ERROR,\n        \"Bad solr.solrxml.location set: \" + solrxmlLocation + \" - should be 'solrhome' or 'zookeeper'\");\n  }\n\n  /**\n   * Override this to change CoreContainer initialization\n   * @return a CoreContainer to hold this server's cores\n   */\n  protected CoreContainer createCoreContainer() {\n    SolrResourceLoader loader = new SolrResourceLoader(SolrResourceLoader.locateSolrHome());\n    ConfigSolr config = loadConfigSolr(loader);\n    CoreContainer cores = new CoreContainer(loader, config);\n    cores.load();\n    return cores;\n  }\n  \n  public CoreContainer getCores() {\n    return cores;\n  }\n  \n  @Override\n  public void destroy() {\n    if (cores != null) {\n      cores.shutdown();\n      cores = null;\n    }    \n  }\n  \n  @Override\n  public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n    doFilter(request, response, chain, false);\n  }\n  \n  public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain, boolean retry) throws IOException, ServletException {\n    if( abortErrorMessage != null ) {\n      ((HttpServletResponse)response).sendError( 500, abortErrorMessage );\n      return;\n    }\n    \n    if (this.cores == null) {\n      ((HttpServletResponse)response).sendError( 503, \"Server is shutting down or failed to initialize\" );\n      return;\n    }\n    CoreContainer cores = this.cores;\n    SolrCore core = null;\n    SolrQueryRequest solrReq = null;\n    Aliases aliases = null;\n    \n    if( request instanceof HttpServletRequest) {\n      HttpServletRequest req = (HttpServletRequest)request;\n      HttpServletResponse resp = (HttpServletResponse)response;\n      SolrRequestHandler handler = null;\n      String corename = \"\";\n      String origCorename = null;\n      try {\n        // put the core container in request attribute\n        req.setAttribute(\"org.apache.solr.CoreContainer\", cores);\n        String path = req.getServletPath();\n        if( req.getPathInfo() != null ) {\n          // this lets you handle /update/commit when /update is a servlet\n          path += req.getPathInfo();\n        }\n        if( pathPrefix != null && path.startsWith( pathPrefix ) ) {\n          path = path.substring( pathPrefix.length() );\n        }\n        // check for management path\n        String alternate = cores.getManagementPath();\n        if (alternate != null && path.startsWith(alternate)) {\n          path = path.substring(0, alternate.length());\n        }\n        // unused feature ?\n        int idx = path.indexOf( ':' );\n        if( idx > 0 ) {\n          // save the portion after the ':' for a 'handler' path parameter\n          path = path.substring( 0, idx );\n        }\n\n        // Check for the core admin page\n        if( path.equals( cores.getAdminPath() ) ) {\n          handler = cores.getMultiCoreHandler();\n          solrReq =  SolrRequestParsers.DEFAULT.parse(null,path, req);\n          handleAdminRequest(req, response, handler, solrReq);\n          return;\n        }\n        boolean usingAliases = false;\n        List<String> collectionsList = null;\n        // Check for the core admin collections url\n        if( path.equals( \"/admin/collections\" ) ) {\n          handler = cores.getCollectionsHandler();\n          solrReq =  SolrRequestParsers.DEFAULT.parse(null,path, req);\n          handleAdminRequest(req, response, handler, solrReq);\n          return;\n        }\n        // Check for the core admin info url\n        if( path.startsWith( \"/admin/info\" ) ) {\n          handler = cores.getInfoHandler();\n          solrReq =  SolrRequestParsers.DEFAULT.parse(null,path, req);\n          handleAdminRequest(req, response, handler, solrReq);\n          return;\n        }\n        else {\n          //otherwise, we should find a core from the path\n          idx = path.indexOf( \"/\", 1 );\n          if( idx > 1 ) {\n            // try to get the corename as a request parameter first\n            corename = path.substring( 1, idx );\n            \n            // look at aliases\n            if (cores.isZooKeeperAware()) {\n              origCorename = corename;\n              ZkStateReader reader = cores.getZkController().getZkStateReader();\n              aliases = reader.getAliases();\n              if (aliases != null && aliases.collectionAliasSize() > 0) {\n                usingAliases = true;\n                String alias = aliases.getCollectionAlias(corename);\n                if (alias != null) {\n                  collectionsList = StrUtils.splitSmart(alias, \",\", true);\n                  corename = collectionsList.get(0);\n                }\n              }\n            }\n            \n            core = cores.getCore(corename);\n\n            if (core != null) {\n              path = path.substring( idx );\n            }\n          }\n          if (core == null) {\n            if (!cores.isZooKeeperAware() ) {\n              core = cores.getCore(\"\");\n            }\n          }\n        }\n        \n        if (core == null && cores.isZooKeeperAware()) {\n          // we couldn't find the core - lets make sure a collection was not specified instead\n          core = getCoreByCollection(cores, corename, path);\n          \n          if (core != null) {\n            // we found a core, update the path\n            path = path.substring( idx );\n          }\n          \n          // if we couldn't find it locally, look on other nodes\n          if (core == null && idx > 0) {\n            String coreUrl = getRemotCoreUrl(cores, corename, origCorename);\n            // don't proxy for internal update requests\n            SolrParams queryParams = SolrRequestParsers.parseQueryString(req.getQueryString());\n            if (coreUrl != null\n                && queryParams\n                    .get(DistributingUpdateProcessorFactory.DISTRIB_UPDATE_PARAM) == null) {\n              path = path.substring(idx);\n              remoteQuery(coreUrl + path, req, solrReq, resp);\n              return;\n            } else {\n              if (!retry) {\n                // we couldn't find a core to work with, try reloading aliases\n                // TODO: it would be nice if admin ui elements skipped this...\n                ZkStateReader reader = cores.getZkController()\n                    .getZkStateReader();\n                reader.updateAliases();\n                doFilter(request, response, chain, true);\n                return;\n              }\n            }\n          }\n          \n          // try the default core\n          if (core == null) {\n            core = cores.getCore(\"\");\n          }\n        }\n\n        // With a valid core...\n        if( core != null ) {\n          final SolrConfig config = core.getSolrConfig();\n          // get or create/cache the parser for the core\n          SolrRequestParsers parser = null;\n          parser = parsers.get(config);\n          if( parser == null ) {\n            parser = new SolrRequestParsers(config);\n            parsers.put(config, parser );\n          }\n\n          // Handle /schema/* paths via Restlet\n          if( path.startsWith(\"/schema\") ) {\n            solrReq = parser.parse(core, path, req);\n            SolrRequestInfo.setRequestInfo(new SolrRequestInfo(solrReq, new SolrQueryResponse()));\n            if( path.equals(req.getServletPath()) ) {\n              // avoid endless loop - pass through to Restlet via webapp\n              chain.doFilter(request, response);\n            } else {\n              // forward rewritten URI (without path prefix and core/collection name) to Restlet\n              req.getRequestDispatcher(path).forward(request, response);\n            }\n            return;\n          }\n\n          // Determine the handler from the url path if not set\n          // (we might already have selected the cores handler)\n          if( handler == null && path.length() > 1 ) { // don't match \"\" or \"/\" as valid path\n            handler = core.getRequestHandler( path );\n            // no handler yet but allowed to handle select; let's check\n            if( handler == null && parser.isHandleSelect() ) {\n              if( \"/select\".equals( path ) || \"/select/\".equals( path ) ) {\n                solrReq = parser.parse( core, path, req );\n                String qt = solrReq.getParams().get( CommonParams.QT );\n                handler = core.getRequestHandler( qt );\n                if( handler == null ) {\n                  throw new SolrException( SolrException.ErrorCode.BAD_REQUEST, \"unknown handler: \"+qt);\n                }\n                if( qt != null && qt.startsWith(\"/\") && (handler instanceof ContentStreamHandlerBase)) {\n                  //For security reasons it's a bad idea to allow a leading '/', ex: /select?qt=/update see SOLR-3161\n                  //There was no restriction from Solr 1.4 thru 3.5 and it's not supported for update handlers.\n                  throw new SolrException( SolrException.ErrorCode.BAD_REQUEST, \"Invalid Request Handler ('qt').  Do not use /select to access: \"+qt);\n                }\n              }\n            }\n          }\n\n          // With a valid handler and a valid core...\n          if( handler != null ) {\n            // if not a /select, create the request\n            if( solrReq == null ) {\n              solrReq = parser.parse( core, path, req );\n            }\n\n            if (usingAliases) {\n              processAliases(solrReq, aliases, collectionsList);\n            }\n            \n            final Method reqMethod = Method.getMethod(req.getMethod());\n            HttpCacheHeaderUtil.setCacheControlHeader(config, resp, reqMethod);\n            // unless we have been explicitly told not to, do cache validation\n            // if we fail cache validation, execute the query\n            if (config.getHttpCachingConfig().isNever304() ||\n                !HttpCacheHeaderUtil.doCacheHeaderValidation(solrReq, req, reqMethod, resp)) {\n                SolrQueryResponse solrRsp = new SolrQueryResponse();\n                /* even for HEAD requests, we need to execute the handler to\n                 * ensure we don't get an error (and to make sure the correct\n                 * QueryResponseWriter is selected and we get the correct\n                 * Content-Type)\n                 */\n                SolrRequestInfo.setRequestInfo(new SolrRequestInfo(solrReq, solrRsp));\n                this.execute( req, handler, solrReq, solrRsp );\n                HttpCacheHeaderUtil.checkHttpCachingVeto(solrRsp, resp, reqMethod);\n              // add info to http headers\n              //TODO: See SOLR-232 and SOLR-267.  \n                /*try {\n                  NamedList solrRspHeader = solrRsp.getResponseHeader();\n                 for (int i=0; i<solrRspHeader.size(); i++) {\n                   ((javax.servlet.http.HttpServletResponse) response).addHeader((\"Solr-\" + solrRspHeader.getName(i)), String.valueOf(solrRspHeader.getVal(i)));\n                 }\n                } catch (ClassCastException cce) {\n                  log.log(Level.WARNING, \"exception adding response header log information\", cce);\n                }*/\n               QueryResponseWriter responseWriter = core.getQueryResponseWriter(solrReq);\n               writeResponse(solrRsp, response, responseWriter, solrReq, reqMethod);\n            }\n            return; // we are done with a valid handler\n          }\n        }\n        log.debug(\"no handler or core retrieved for \" + path + \", follow through...\");\n      } \n      catch (Throwable ex) {\n        sendError( core, solrReq, request, (HttpServletResponse)response, ex );\n        return;\n      } \n      finally {\n        if( solrReq != null ) {\n          log.debug(\"Closing out SolrRequest: {}\", solrReq);\n          solrReq.close();\n        }\n        if (core != null) {\n          core.close();\n        }\n        SolrRequestInfo.clearRequestInfo();        \n      }\n    }\n\n    // Otherwise let the webapp handle the request\n    chain.doFilter(request, response);\n  }\n  \n  private void processAliases(SolrQueryRequest solrReq, Aliases aliases,\n      List<String> collectionsList) {\n    String collection = solrReq.getParams().get(\"collection\");\n    if (collection != null) {\n      collectionsList = StrUtils.splitSmart(collection, \",\", true);\n    }\n    if (collectionsList != null) {\n      Set<String> newCollectionsList = new HashSet<String>(\n          collectionsList.size());\n      for (String col : collectionsList) {\n        String al = aliases.getCollectionAlias(col);\n        if (al != null) {\n          List<String> aliasList = StrUtils.splitSmart(al, \",\", true);\n          newCollectionsList.addAll(aliasList);\n        } else {\n          newCollectionsList.add(col);\n        }\n      }\n      if (newCollectionsList.size() > 0) {\n        StringBuilder collectionString = new StringBuilder();\n        Iterator<String> it = newCollectionsList.iterator();\n        int sz = newCollectionsList.size();\n        for (int i = 0; i < sz; i++) {\n          collectionString.append(it.next());\n          if (i < newCollectionsList.size() - 1) {\n            collectionString.append(\",\");\n          }\n        }\n        ModifiableSolrParams params = new ModifiableSolrParams(\n            solrReq.getParams());\n        params.set(\"collection\", collectionString.toString());\n        solrReq.setParams(params);\n      }\n    }\n  }\n  \n  private void remoteQuery(String coreUrl, HttpServletRequest req,\n      SolrQueryRequest solrReq, HttpServletResponse resp) throws IOException {\n    try {\n      String urlstr = coreUrl;\n      \n      String queryString = req.getQueryString();\n      \n      urlstr += queryString == null ? \"\" : \"?\" + queryString;\n      \n      URL url = new URL(urlstr);\n      HttpURLConnection con = (HttpURLConnection) url.openConnection();\n      con.setRequestMethod(req.getMethod());\n      con.setUseCaches(false);\n      \n      boolean isPostOrPutRequest = \"POST\".equals(req.getMethod()) || \"PUT\".equals(req.getMethod());\n      \n      if (isPostOrPutRequest) {\n        con.setDoOutput(true);\n      }\n      con.setDoInput(true);\n      for (Enumeration<String> e = req.getHeaderNames(); e.hasMoreElements();) {\n        String headerName = e.nextElement();\n        con.setRequestProperty(headerName, req.getHeader(headerName));\n      }\n      try {\n        con.connect();\n\n        InputStream is;\n        OutputStream os;\n        if (isPostOrPutRequest) {\n          is = req.getInputStream();\n          os = con.getOutputStream(); // side effect: method is switched to POST\n          try {\n            IOUtils.copyLarge(is, os);\n            os.flush();\n          } finally {\n            IOUtils.closeQuietly(os);\n            IOUtils.closeQuietly(is);  // TODO: I thought we weren't supposed to explicitly close servlet streams\n          }\n        }\n        \n        resp.setStatus(con.getResponseCode());\n        \n        for (Iterator<Entry<String,List<String>>> i = con.getHeaderFields().entrySet().iterator(); i.hasNext();) {\n          Map.Entry<String,List<String>> mapEntry = i.next();\n          String header = mapEntry.getKey();\n          \n          // We pull out these two headers below because they can cause chunked\n          // encoding issues with Tomcat and certain clients\n          if (header != null && !header.equals(TRANSFER_ENCODING_HEADER)\n              && !header.equals(CONNECTION_HEADER)) {\n            for (String value : mapEntry.getValue()) {\n              resp.addHeader(mapEntry.getKey(), value);\n            }\n          }\n        }\n        \n        resp.setCharacterEncoding(con.getContentEncoding());\n        resp.setContentType(con.getContentType());\n        \n        is = con.getInputStream();\n        os = resp.getOutputStream();\n        try {\n          IOUtils.copyLarge(is, os);\n          os.flush();\n        } finally {\n          IOUtils.closeQuietly(os);   // TODO: I thought we weren't supposed to explicitly close servlet streams\n          IOUtils.closeQuietly(is);\n        }\n      } finally {\n        con.disconnect();\n      }\n    } catch (IOException e) {\n      sendError(null, solrReq, req, resp, new SolrException(\n          SolrException.ErrorCode.SERVER_ERROR,\n          \"Error trying to proxy request for url: \" + coreUrl, e));\n    }\n    \n  }\n  \n  private String getRemotCoreUrl(CoreContainer cores, String collectionName, String origCorename) {\n    ClusterState clusterState = cores.getZkController().getClusterState();\n    Collection<Slice> slices = clusterState.getActiveSlices(collectionName);\n    boolean byCoreName = false;\n    \n    if (slices == null) {\n      slices = new ArrayList<Slice>();\n      // look by core name\n      byCoreName = true;\n      slices = getSlicesForCollections(clusterState, slices, true);\n      if (slices == null || slices.size() == 0) {\n        slices = getSlicesForCollections(clusterState, slices, false);\n      }\n    }\n    \n    if (slices == null || slices.size() == 0) {\n      return null;\n    }\n    \n    String coreUrl = getCoreUrl(cores, collectionName, origCorename, clusterState,\n        slices, byCoreName, true);\n    \n    if (coreUrl == null) {\n      coreUrl = getCoreUrl(cores, collectionName, origCorename, clusterState,\n          slices, byCoreName, false);\n    }\n    \n    return coreUrl;\n  }\n\n  private String getCoreUrl(CoreContainer cores, String collectionName,\n      String origCorename, ClusterState clusterState, Collection<Slice> slices,\n      boolean byCoreName, boolean activeReplicas) {\n    String coreUrl;\n    Set<String> liveNodes = clusterState.getLiveNodes();\n    Iterator<Slice> it = slices.iterator();\n    while (it.hasNext()) {\n      Slice slice = it.next();\n      Map<String,Replica> sliceShards = slice.getReplicasMap();\n      for (ZkNodeProps nodeProps : sliceShards.values()) {\n        ZkCoreNodeProps coreNodeProps = new ZkCoreNodeProps(nodeProps);\n        if (!activeReplicas || (liveNodes.contains(coreNodeProps.getNodeName())\n            && coreNodeProps.getState().equals(ZkStateReader.ACTIVE))) {\n\n          if (byCoreName && !collectionName.equals(coreNodeProps.getCoreName())) {\n            // if it's by core name, make sure they match\n            continue;\n          }\n          if (coreNodeProps.getBaseUrl().equals(cores.getZkController().getBaseUrl())) {\n            // don't count a local core\n            continue;\n          }\n\n          if (origCorename != null) {\n            coreUrl = coreNodeProps.getBaseUrl() + \"/\" + origCorename;\n          } else {\n            coreUrl = coreNodeProps.getCoreUrl();\n            if (coreUrl.endsWith(\"/\")) {\n              coreUrl = coreUrl.substring(0, coreUrl.length() - 1);\n            }\n          }\n\n          return coreUrl;\n        }\n      }\n    }\n    return null;\n  }\n\n  private Collection<Slice> getSlicesForCollections(ClusterState clusterState,\n      Collection<Slice> slices, boolean activeSlices) {\n    Set<String> collections = clusterState.getCollections();\n    for (String collection : collections) {\n      if (activeSlices) {\n        slices.addAll(clusterState.getActiveSlices(collection));\n      } else {\n        slices.addAll(clusterState.getSlices(collection));\n      }\n    }\n    return slices;\n  }\n  \n  private SolrCore getCoreByCollection(CoreContainer cores, String corename, String path) {\n    String collection = corename;\n    ZkStateReader zkStateReader = cores.getZkController().getZkStateReader();\n    \n    ClusterState clusterState = zkStateReader.getClusterState();\n    Map<String,Slice> slices = clusterState.getActiveSlicesMap(collection);\n    if (slices == null) {\n      return null;\n    }\n    // look for a core on this node\n    Set<Entry<String,Slice>> entries = slices.entrySet();\n    SolrCore core = null;\n    done:\n    for (Entry<String,Slice> entry : entries) {\n      // first see if we have the leader\n      ZkNodeProps leaderProps = clusterState.getLeader(collection, entry.getKey());\n      if (leaderProps != null) {\n        core = checkProps(cores, path, leaderProps);\n      }\n      if (core != null) {\n        break done;\n      }\n      \n      // check everyone then\n      Map<String,Replica> shards = entry.getValue().getReplicasMap();\n      Set<Entry<String,Replica>> shardEntries = shards.entrySet();\n      for (Entry<String,Replica> shardEntry : shardEntries) {\n        Replica zkProps = shardEntry.getValue();\n        core = checkProps(cores, path, zkProps);\n        if (core != null) {\n          break done;\n        }\n      }\n    }\n    return core;\n  }\n\n  private SolrCore checkProps(CoreContainer cores, String path,\n      ZkNodeProps zkProps) {\n    String corename;\n    SolrCore core = null;\n    if (cores.getZkController().getNodeName().equals(zkProps.getStr(ZkStateReader.NODE_NAME_PROP))) {\n      corename = zkProps.getStr(ZkStateReader.CORE_NAME_PROP);\n      core = cores.getCore(corename);\n    }\n    return core;\n  }\n\n  private void handleAdminRequest(HttpServletRequest req, ServletResponse response, SolrRequestHandler handler,\n                                  SolrQueryRequest solrReq) throws IOException {\n    SolrQueryResponse solrResp = new SolrQueryResponse();\n    SolrCore.preDecorateResponse(solrReq, solrResp);\n    handler.handleRequest(solrReq, solrResp);\n    SolrCore.postDecorateResponse(handler, solrReq, solrResp);\n    if (log.isInfoEnabled() && solrResp.getToLog().size() > 0) {\n      log.info(solrResp.getToLogAsString(\"[admin] \"));\n    }\n    QueryResponseWriter respWriter = SolrCore.DEFAULT_RESPONSE_WRITERS.get(solrReq.getParams().get(CommonParams.WT));\n    if (respWriter == null) respWriter = SolrCore.DEFAULT_RESPONSE_WRITERS.get(\"standard\");\n    writeResponse(solrResp, response, respWriter, solrReq, Method.getMethod(req.getMethod()));\n  }\n\n  private void writeResponse(SolrQueryResponse solrRsp, ServletResponse response,\n                             QueryResponseWriter responseWriter, SolrQueryRequest solrReq, Method reqMethod)\n          throws IOException {\n\n    // Now write it out\n    final String ct = responseWriter.getContentType(solrReq, solrRsp);\n    // don't call setContentType on null\n    if (null != ct) response.setContentType(ct); \n\n    if (solrRsp.getException() != null) {\n      NamedList info = new SimpleOrderedMap();\n      int code = ResponseUtils.getErrorInfo(solrRsp.getException(), info, log);\n      solrRsp.add(\"error\", info);\n      ((HttpServletResponse) response).setStatus(code);\n    }\n    \n    if (Method.HEAD != reqMethod) {\n      if (responseWriter instanceof BinaryQueryResponseWriter) {\n        BinaryQueryResponseWriter binWriter = (BinaryQueryResponseWriter) responseWriter;\n        binWriter.write(response.getOutputStream(), solrReq, solrRsp);\n      } else {\n        String charset = ContentStreamBase.getCharsetFromContentType(ct);\n        Writer out = (charset == null || charset.equalsIgnoreCase(\"UTF-8\"))\n          ? new OutputStreamWriter(response.getOutputStream(), UTF8)\n          : new OutputStreamWriter(response.getOutputStream(), charset);\n        out = new FastWriter(out);\n        responseWriter.write(out, solrReq, solrRsp);\n        out.flush();\n      }\n    }\n    //else http HEAD request, nothing to write out, waited this long just to get ContentType\n  }\n  \n  protected void execute( HttpServletRequest req, SolrRequestHandler handler, SolrQueryRequest sreq, SolrQueryResponse rsp) {\n    // a custom filter could add more stuff to the request before passing it on.\n    // for example: sreq.getContext().put( \"HttpServletRequest\", req );\n    // used for logging query stats in SolrCore.execute()\n    sreq.getContext().put( \"webapp\", req.getContextPath() );\n    sreq.getCore().execute( handler, sreq, rsp );\n  }\n\n  protected void sendError(SolrCore core, \n      SolrQueryRequest req, \n      ServletRequest request, \n      HttpServletResponse response, \n      Throwable ex) throws IOException {\n    SolrCore localCore = null;\n    try {\n      SolrQueryResponse solrResp = new SolrQueryResponse();\n      if(ex instanceof Exception) {\n        solrResp.setException((Exception)ex);\n      }\n      else {\n        solrResp.setException(new RuntimeException(ex));\n      }\n      if(core==null) {\n        localCore = cores.getCore(\"\"); // default core\n      } else {\n        localCore = core;\n      }\n      if(req==null) {\n        final SolrParams solrParams;\n        if (request instanceof HttpServletRequest) {\n          // use GET parameters if available:\n          solrParams = SolrRequestParsers.parseQueryString(((HttpServletRequest) request).getQueryString());\n        } else {\n          // we have no params at all, use empty ones:\n          solrParams = new MapSolrParams(Collections.<String,String>emptyMap());\n        }\n        req = new SolrQueryRequestBase(core, solrParams) {};\n      }\n      QueryResponseWriter writer = core.getQueryResponseWriter(req);\n      writeResponse(solrResp, response, writer, req, Method.GET);\n    }\n    catch( Throwable t ) { // This error really does not matter\n      SimpleOrderedMap info = new SimpleOrderedMap();\n      int code = ResponseUtils.getErrorInfo(ex, info, log);\n      response.sendError( code, info.toString() );\n    } finally {\n      if (core == null && localCore != null) {\n        localCore.close();\n      }\n    }\n  }\n\n  //---------------------------------------------------------------------\n  //---------------------------------------------------------------------\n\n  /**\n   * Set the prefix for all paths.  This is useful if you want to apply the\n   * filter to something other then /*, perhaps because you are merging this\n   * filter into a larger web application.\n   *\n   * For example, if web.xml specifies:\n   * <pre class=\"prettyprint\">\n   * {@code\n   * <filter-mapping>\n   *  <filter-name>SolrRequestFilter</filter-name>\n   *  <url-pattern>/xxx/*</url-pattern>\n   * </filter-mapping>}\n   * </pre>\n   *\n   * Make sure to set the PathPrefix to \"/xxx\" either with this function\n   * or in web.xml.\n   *\n   * <pre class=\"prettyprint\">\n   * {@code\n   * <init-param>\n   *  <param-name>path-prefix</param-name>\n   *  <param-value>/xxx</param-value>\n   * </init-param>}\n   * </pre>\n   */\n  public void setPathPrefix(String pathPrefix) {\n    this.pathPrefix = pathPrefix;\n  }\n\n  public String getPathPrefix() {\n    return pathPrefix;\n  }\n}\n",
        "methodName": "sendError",
        "exampleID": 56,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java",
        "line": "771",
        "source": "core",
        "sourceLine": "786",
        "qualifier": "Possible null pointer dereference of $$core/$",
        "steps": [
            {
                "exampleID": 57
            }
        ],
        "line_number": "786"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/update/processor/AbstractDefaultValueUpdateProcessorFactory.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.solr.update.processor;\n\nimport java.io.IOException;\n\nimport org.apache.solr.common.SolrException;\nimport static org.apache.solr.common.SolrException.ErrorCode.*;\nimport org.apache.solr.common.SolrInputDocument;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.response.SolrQueryResponse;\nimport org.apache.solr.update.AddUpdateCommand;\n\n/**\n * <p>\n * Base class that can be extended by any\n * <code>UpdateRequestProcessorFactory</code> designed to add a default value \n * to the document in an <code>AddUpdateCommand</code> when that field is not \n * already specified.\n * </p>\n * <p>\n * This base class handles initialization of the <code>fieldName</code> init \n * param, and provides an {@link DefaultValueUpdateProcessor} that Factory \n * subclasses may choose to return from their <code>getInstance</code> \n * implementation.\n * </p>\n */\npublic abstract class AbstractDefaultValueUpdateProcessorFactory\n  extends UpdateRequestProcessorFactory {\n\n  protected String fieldName = null;\n\n  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void init(NamedList args) {\n\n    Object obj = args.remove(\"fieldName\");\n    if (null == obj && null == fieldName) {\n      throw new SolrException\n        (SERVER_ERROR, \"'fieldName' init param must be specified and non-null\"); \n    } else {\n      fieldName = obj.toString();\n    }\n\n    if (0 < args.size()) {\n      throw new SolrException(SERVER_ERROR, \n                              \"Unexpected init param(s): '\" + \n                              args.getName(0) + \"'\");\n    }\n    \n    super.init(args);\n  }\n\n  /**\n   * A simple processor that adds the results of {@link #getDefaultValue} \n   * to any document which does not already have a value in \n   * <code>fieldName</code>\n   */\n  protected static abstract class DefaultValueUpdateProcessor \n    extends UpdateRequestProcessor {\n\n    final String fieldName;\n\n    public DefaultValueUpdateProcessor(final String fieldName,\n                                       final UpdateRequestProcessor next) {\n      super(next);\n      this.fieldName = fieldName;\n    }\n\n    @Override\n    public void processAdd(AddUpdateCommand cmd) throws IOException {\n      final SolrInputDocument doc = cmd.getSolrInputDocument();\n\n      if (! doc.containsKey(fieldName)) {\n        doc.addField(fieldName, getDefaultValue());\n      }\n\n      super.processAdd(cmd);\n    }\n    \n    public abstract Object getDefaultValue();\n  }\n}\n\n\n\n",
        "methodName": "init",
        "exampleID": 58,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/update/processor/AbstractDefaultValueUpdateProcessorFactory.java",
        "line": "54",
        "source": "obj",
        "sourceLine": "58",
        "qualifier": "Possible null pointer dereference of $$obj/$",
        "steps": [
            {
                "exampleID": 59
            }
        ],
        "line_number": "58"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java",
        "rawCode": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n\npackage org.apache.solr.util;\n\n\nimport java.io.File;\nimport java.util.HashSet;\n\nimport org.apache.lucene.util.QuickPatchThreadsFilter;\nimport org.apache.solr.SolrIgnoredThreadsFilter;\nimport org.apache.solr.SolrTestCaseJ4;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.core.SolrConfig;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;\n\n/**\n * An Abstract base class that makes writing Solr JUnit tests \"easier\"\n *\n * <p>\n * Test classes that subclass this need only specify the path to the\n * schema.xml file (:TODO: the solrconfig.xml as well) and write some\n * testMethods.  This class takes care of creating/destroying the index,\n * and provides several assert methods to assist you.\n * </p>\n *\n * @see #setUp\n * @see #tearDown\n */\n@ThreadLeakFilters(defaultFilters = true, filters = {\n    SolrIgnoredThreadsFilter.class,\n    QuickPatchThreadsFilter.class\n})\npublic abstract class AbstractSolrTestCase extends SolrTestCaseJ4 {\n  protected SolrConfig solrConfig;\n\n\n  /**\n   * Subclasses can override this to change a test's solr home\n   * (default is in test-files)\n   */\n  public String getSolrHome() {\n    return SolrTestCaseJ4.TEST_HOME();\n  }\n\n  public static Logger log = LoggerFactory.getLogger(AbstractSolrTestCase.class);\n\n\n    /** Causes an exception matching the regex pattern to not be logged. */\n  public static void ignoreException(String pattern) {\n    if (SolrException.ignorePatterns == null)\n      SolrException.ignorePatterns = new HashSet<String>();\n    SolrException.ignorePatterns.add(pattern);\n  }\n\n  public static void resetExceptionIgnores() {\n    SolrException.ignorePatterns = null;\n    ignoreException(\"ignore_exception\");  // always ignore \"ignore_exception\"\n  }\n\n  /** Subclasses that override setUp can optionally call this method\n   * to log the fact that their setUp process has ended.\n   */\n  @Override\n  public void postSetUp() {\n    log.info(\"####POSTSETUP \" + getTestName());\n  }\n\n\n  /** Subclasses that override tearDown can optionally call this method\n   * to log the fact that the tearDown process has started.  This is necessary\n   * since subclasses will want to call super.tearDown() at the *end* of their\n   * tearDown method.\n   */\n  @Override\n  public void preTearDown() {\n    log.info(\"####PRETEARDOWN \" + getTestName());      \n  }\n\n\n  /**\n   * Generates a simple &lt;add&gt;&lt;doc&gt;... XML String with the\n   * commitWithin attribute.\n   *\n   * @param commitWithin the value of the commitWithin attribute \n   * @param fieldsAndValues 0th and Even numbered args are fields names odds are field values.\n   * @see #add\n   * @see #doc\n   */\n  public String adoc(int commitWithin, String... fieldsAndValues) {\n    XmlDoc d = doc(fieldsAndValues);\n    return add(d, \"commitWithin\", String.valueOf(commitWithin));\n  }\n\n\n  /**\n   * Generates a &lt;delete&gt;... XML string for an ID\n   *\n   * @see TestHarness#deleteById\n   */\n  public String delI(String id, String... args) {\n    return TestHarness.deleteById(id, args);\n  }\n  \n  /**\n   * Generates a &lt;delete&gt;... XML string for an query\n   *\n   * @see TestHarness#deleteByQuery\n   */\n  public String delQ(String q, String... args) {\n    return TestHarness.deleteByQuery(q, args);\n  }\n\n\n  public static boolean recurseDelete(File f) {\n    if (f.isDirectory()) {\n      for (File sub : f.listFiles()) {\n        if (!recurseDelete(sub)) {\n          System.err.println(\"!!!! WARNING: best effort to remove \" + sub.getAbsolutePath() + \" FAILED !!!!!\");\n          return false;\n        }\n      }\n    }\n    return f.delete();\n  }\n\n  /** @see SolrTestCaseJ4#getFile */\n  public static File getFile(String name) {\n    return SolrTestCaseJ4.getFile(name);\n  }\n}\n",
        "methodName": "recurseDelete",
        "exampleID": 60,
        "dataset": "spotbugs",
        "filepath": "/solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java",
        "line": "135",
        "source": "?",
        "sourceLine": "135",
        "qualifier": "Possible null pointer dereference of the $$value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 61
            }
        ],
        "line_number": "135"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/util/SimplePostTool.java",
        "rawCode": "package org.apache.solr.util;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.BufferedReader;\nimport java.io.ByteArrayOutputStream;\nimport java.io.File;\nimport java.io.FileFilter;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.ByteArrayInputStream;\nimport java.io.InputStreamReader;\nimport java.io.OutputStream;\nimport java.io.UnsupportedEncodingException;\nimport java.text.SimpleDateFormat;\nimport java.util.ArrayList;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.LinkedHashSet;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.HashSet;\nimport java.util.TimeZone;\nimport java.util.regex.Pattern;\nimport java.util.regex.PatternSyntaxException;\nimport java.util.zip.GZIPInputStream;\nimport java.util.zip.Inflater;\nimport java.util.zip.InflaterInputStream;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.ProtocolException;\nimport java.net.URL;\nimport java.net.URLEncoder;\n\nimport javax.xml.parsers.DocumentBuilderFactory;\nimport javax.xml.parsers.ParserConfigurationException;\nimport javax.xml.xpath.XPath;\nimport javax.xml.xpath.XPathConstants;\nimport javax.xml.xpath.XPathExpression;\nimport javax.xml.xpath.XPathExpressionException;\nimport javax.xml.xpath.XPathFactory;\n\nimport org.w3c.dom.Document;\nimport org.w3c.dom.Node;\nimport org.w3c.dom.NodeList;\nimport org.xml.sax.SAXException;\n\n/**\n * A simple utility class for posting raw updates to a Solr server, \n * has a main method so it can be run on the command line.\n * View this not as a best-practice code example, but as a standalone \n * example built with an explicit purpose of not having external\n * jar dependencies.\n */\npublic class SimplePostTool {\n  private static final String DEFAULT_POST_URL = \"http://localhost:8983/solr/update\";\n  private static final String VERSION_OF_THIS_TOOL = \"1.5\";\n\n  private static final String DEFAULT_COMMIT = \"yes\";\n  private static final String DEFAULT_OPTIMIZE = \"no\";\n  private static final String DEFAULT_OUT = \"no\";\n  private static final String DEFAULT_AUTO = \"no\";\n  private static final String DEFAULT_RECURSIVE = \"0\";\n  private static final int DEFAULT_WEB_DELAY = 10;\n  private static final int MAX_WEB_DEPTH = 10;\n  private static final String DEFAULT_CONTENT_TYPE = \"application/xml\";\n  private static final String DEFAULT_FILE_TYPES = \"xml,json,csv,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,ott,otp,ots,rtf,htm,html,txt,log\"; \n\n  static final String DATA_MODE_FILES = \"files\";\n  static final String DATA_MODE_ARGS = \"args\";\n  static final String DATA_MODE_STDIN = \"stdin\";\n  static final String DATA_MODE_WEB = \"web\";\n  static final String DEFAULT_DATA_MODE = DATA_MODE_FILES;\n\n  // Input args\n  boolean auto = false;\n  int recursive = 0;\n  int delay = 0;\n  String fileTypes;\n  URL solrUrl;\n  OutputStream out = null;\n  String type;\n  String mode;\n  boolean commit;\n  boolean optimize;\n  String[] args;\n\n  private int currentDepth;\n\n  static HashMap<String,String> mimeMap;\n  GlobFileFilter globFileFilter;\n  // Backlog for crawling\n  List<LinkedHashSet<URL>> backlog = new ArrayList<LinkedHashSet<URL>>();\n  Set<URL> visited = new HashSet<URL>();\n  \n  static final Set<String> DATA_MODES = new HashSet<String>();\n  static final String USAGE_STRING_SHORT =\n      \"Usage: java [SystemProperties] -jar post.jar [-h|-] [<file|folder|url|arg> [<file|folder|url|arg>...]]\";\n\n  // Used in tests to avoid doing actual network traffic\n  static boolean mockMode = false;\n  static PageFetcher pageFetcher;\n\n  static {\n    DATA_MODES.add(DATA_MODE_FILES);\n    DATA_MODES.add(DATA_MODE_ARGS);\n    DATA_MODES.add(DATA_MODE_STDIN);\n    DATA_MODES.add(DATA_MODE_WEB);\n    \n    mimeMap = new HashMap<String,String>();\n    mimeMap.put(\"xml\", \"text/xml\");\n    mimeMap.put(\"csv\", \"text/csv\");\n    mimeMap.put(\"json\", \"application/json\");\n    mimeMap.put(\"pdf\", \"application/pdf\");\n    mimeMap.put(\"rtf\", \"text/rtf\");\n    mimeMap.put(\"html\", \"text/html\");\n    mimeMap.put(\"htm\", \"text/html\");\n    mimeMap.put(\"doc\", \"application/msword\");\n    mimeMap.put(\"docx\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\");\n    mimeMap.put(\"ppt\", \"application/vnd.ms-powerpoint\");\n    mimeMap.put(\"pptx\", \"application/vnd.openxmlformats-officedocument.presentationml.presentation\");\n    mimeMap.put(\"xls\", \"application/vnd.ms-excel\");\n    mimeMap.put(\"xlsx\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\");\n    mimeMap.put(\"odt\", \"application/vnd.oasis.opendocument.text\");\n    mimeMap.put(\"ott\", \"application/vnd.oasis.opendocument.text\");\n    mimeMap.put(\"odp\", \"application/vnd.oasis.opendocument.presentation\");\n    mimeMap.put(\"otp\", \"application/vnd.oasis.opendocument.presentation\");\n    mimeMap.put(\"ods\", \"application/vnd.oasis.opendocument.spreadsheet\");\n    mimeMap.put(\"ots\", \"application/vnd.oasis.opendocument.spreadsheet\");\n    mimeMap.put(\"txt\", \"text/plain\");\n    mimeMap.put(\"log\", \"text/plain\");\n  }\n  \n  /**\n   * See usage() for valid command line usage\n   * @param args the params on the command line\n   */\n  public static void main(String[] args) {\n    info(\"SimplePostTool version \" + VERSION_OF_THIS_TOOL);\n    if (0 < args.length && (\"-help\".equals(args[0]) || \"--help\".equals(args[0]) || \"-h\".equals(args[0]))) {\n      usage();\n    } else {\n      final SimplePostTool t = parseArgsAndInit(args);\n      t.execute();\n    }\n  }\n\n  /**\n   * After initialization, call execute to start the post job.\n   * This method delegates to the correct mode method.\n   */\n  public void execute() {\n    final long startTime = System.currentTimeMillis();\n    if (DATA_MODE_FILES.equals(mode) && args.length > 0) {\n      doFilesMode();\n    } else if(DATA_MODE_ARGS.equals(mode) && args.length > 0) {\n      doArgsMode();\n    } else if(DATA_MODE_WEB.equals(mode) && args.length > 0) {\n      doWebMode();\n    } else if(DATA_MODE_STDIN.equals(mode)) {\n      doStdinMode();\n    } else {\n      usageShort();\n      return;\n    }\n    \n    if (commit)   commit();\n    if (optimize) optimize();\n    final long endTime = System.currentTimeMillis();\n    displayTiming(endTime - startTime);\n  }\n  \n  /**\n   * Pretty prints the number of milliseconds taken to post the content to Solr\n   * @param millis the time in milliseconds\n   */\n  private void displayTiming(long millis) {\n    SimpleDateFormat df = new SimpleDateFormat(\"H:mm:ss.SSS\", Locale.getDefault());\n    df.setTimeZone(TimeZone.getTimeZone(\"UTC\"));\n    System.out.println(\"Time spent: \"+df.format(new Date(millis)));\n  }\n\n  /**\n   * Parses incoming arguments and system params and initializes the tool\n   * @param args the incoming cmd line args\n   * @return an instance of SimplePostTool\n   */\n  protected static SimplePostTool parseArgsAndInit(String[] args) {\n    String urlStr = null;\n    try {\n      // Parse args\n      final String mode = System.getProperty(\"data\", DEFAULT_DATA_MODE);\n      if (! DATA_MODES.contains(mode)) {\n        fatal(\"System Property 'data' is not valid for this tool: \" + mode);\n      }\n      String params = System.getProperty(\"params\", \"\");\n      urlStr = System.getProperty(\"url\", DEFAULT_POST_URL);\n      urlStr = SimplePostTool.appendParam(urlStr, params);\n      URL url = new URL(urlStr);\n      boolean auto = isOn(System.getProperty(\"auto\", DEFAULT_AUTO));\n      String type = System.getProperty(\"type\");\n      // Recursive\n      int recursive = 0;\n      String r = System.getProperty(\"recursive\", DEFAULT_RECURSIVE);\n      try {\n        recursive = Integer.parseInt(r);\n      } catch(Exception e) {\n        if (isOn(r))\n          recursive = DATA_MODE_WEB.equals(mode)?1:999;\n      }\n      // Delay\n      int delay = DATA_MODE_WEB.equals(mode) ? DEFAULT_WEB_DELAY : 0;\n      try {\n        delay = Integer.parseInt(System.getProperty(\"delay\", \"\"+delay));\n      } catch(Exception e) { }\n      OutputStream out = isOn(System.getProperty(\"out\", DEFAULT_OUT)) ? System.out : null;\n      String fileTypes = System.getProperty(\"filetypes\", DEFAULT_FILE_TYPES);\n      boolean commit = isOn(System.getProperty(\"commit\",DEFAULT_COMMIT));\n      boolean optimize = isOn(System.getProperty(\"optimize\",DEFAULT_OPTIMIZE));\n      \n      return new SimplePostTool(mode, url, auto, type, recursive, delay, fileTypes, out, commit, optimize, args);\n    } catch (MalformedURLException e) {\n      fatal(\"System Property 'url' is not a valid URL: \" + urlStr);\n      return null;\n    }\n  }\n\n  /**\n   * Constructor which takes in all mandatory input for the tool to work.\n   * Also see usage() for further explanation of the params.\n   * @param mode whether to post files, web pages, params or stdin\n   * @param url the Solr base Url to post to, should end with /update\n   * @param auto if true, we'll guess type and add resourcename/url\n   * @param type content-type of the data you are posting\n   * @param recursive number of levels for file/web mode, or 0 if one file only\n   * @param delay if recursive then delay will be the wait time between posts\n   * @param fileTypes a comma separated list of file-name endings to accept for file/web\n   * @param out an OutputStream to write output to, e.g. stdout to print to console\n   * @param commit if true, will commit at end of posting\n   * @param optimize if true, will optimize at end of posting\n   * @param args a String[] of arguments, varies between modes\n   */\n  public SimplePostTool(String mode, URL url, boolean auto, String type,\n      int recursive, int delay, String fileTypes, OutputStream out, \n      boolean commit, boolean optimize, String[] args) {\n    this.mode = mode;\n    this.solrUrl = url;\n    this.auto = auto;\n    this.type = type;\n    this.recursive = recursive;\n    this.delay = delay;\n    this.fileTypes = fileTypes;\n    this.globFileFilter = getFileFilterFromFileTypes(fileTypes);\n    this.out = out;\n    this.commit = commit;\n    this.optimize = optimize;\n    this.args = args;\n    pageFetcher = new PageFetcher();\n  }\n\n  public SimplePostTool() {}\n  \n  //\n  // Do some action depending on which mode we have\n  //\n  private void doFilesMode() {\n    currentDepth = 0;\n    // Skip posting files if special param \"-\" given  \n    if (!args[0].equals(\"-\")) {\n      info(\"Posting files to base url \" + solrUrl + (!auto?\" using content-type \"+(type==null?DEFAULT_CONTENT_TYPE:type):\"\")+\"..\");\n      if(auto)\n        info(\"Entering auto mode. File endings considered are \"+fileTypes);\n      if(recursive > 0)\n        info(\"Entering recursive mode, max depth=\"+recursive+\", delay=\"+delay+\"s\"); \n      int numFilesPosted = postFiles(args, 0, out, type);\n      info(numFilesPosted + \" files indexed.\");\n    }\n  }\n\n  private void doArgsMode() {\n    info(\"POSTing args to \" + solrUrl + \"..\");\n    for (String a : args) {\n      postData(stringToStream(a), null, out, type, solrUrl);\n    }\n  }\n\n  private int doWebMode() {\n    reset();\n    int numPagesPosted = 0;\n    try {\n      if(type != null) {\n        fatal(\"Specifying content-type with \\\"-Ddata=web\\\" is not supported\");\n      }\n      if (args[0].equals(\"-\")) {\n        // Skip posting url if special param \"-\" given  \n        return 0;\n      }\n      // Set Extracting handler as default\n      solrUrl = appendUrlPath(solrUrl, \"/extract\");\n      \n      info(\"Posting web pages to Solr url \"+solrUrl);\n      auto=true;\n      info(\"Entering auto mode. Indexing pages with content-types corresponding to file endings \"+fileTypes);\n      if(recursive > 0) {\n        if(recursive > MAX_WEB_DEPTH) {\n          recursive = MAX_WEB_DEPTH;\n          warn(\"Too large recursion depth for web mode, limiting to \"+MAX_WEB_DEPTH+\"...\");\n        }\n        if(delay < DEFAULT_WEB_DELAY)\n          warn(\"Never crawl an external web site faster than every 10 seconds, your IP will probably be blocked\");\n        info(\"Entering recursive mode, depth=\"+recursive+\", delay=\"+delay+\"s\");\n      }\n      numPagesPosted = postWebPages(args, 0, out);\n      info(numPagesPosted + \" web pages indexed.\");\n    } catch(MalformedURLException e) {\n      fatal(\"Wrong URL trying to append /extract to \"+solrUrl);\n    }\n    return numPagesPosted;\n  }\n\n  private void doStdinMode() {\n    info(\"POSTing stdin to \" + solrUrl + \"..\");\n    postData(System.in, null, out, type, solrUrl);    \n  }\n\n  private void reset() {\n    fileTypes = DEFAULT_FILE_TYPES;\n    globFileFilter = this.getFileFilterFromFileTypes(fileTypes);\n    backlog = new ArrayList<LinkedHashSet<URL>>();\n    visited = new HashSet<URL>();\n  }\n\n\n  //\n  // USAGE\n  //\n  private static void usageShort() {\n    System.out.println(USAGE_STRING_SHORT+\"\\n\"+\n        \"       Please invoke with -h option for extended usage help.\");\n  }\n\n  private static void usage() {\n    System.out.println\n    (USAGE_STRING_SHORT+\"\\n\\n\" +\n     \"Supported System Properties and their defaults:\\n\"+\n     \"  -Ddata=files|web|args|stdin (default=\" + DEFAULT_DATA_MODE + \")\\n\"+\n     \"  -Dtype=<content-type> (default=\" + DEFAULT_CONTENT_TYPE + \")\\n\"+\n     \"  -Durl=<solr-update-url> (default=\" + DEFAULT_POST_URL + \")\\n\"+\n     \"  -Dauto=yes|no (default=\" + DEFAULT_AUTO + \")\\n\"+\n     \"  -Drecursive=yes|no|<depth> (default=\" + DEFAULT_RECURSIVE + \")\\n\"+\n     \"  -Ddelay=<seconds> (default=0 for files, 10 for web)\\n\"+\n     \"  -Dfiletypes=<type>[,<type>,...] (default=\" + DEFAULT_FILE_TYPES + \")\\n\"+\n     \"  -Dparams=\\\"<key>=<value>[&<key>=<value>...]\\\" (values must be URL-encoded)\\n\"+\n     \"  -Dcommit=yes|no (default=\" + DEFAULT_COMMIT + \")\\n\"+\n     \"  -Doptimize=yes|no (default=\" + DEFAULT_OPTIMIZE + \")\\n\"+\n     \"  -Dout=yes|no (default=\" + DEFAULT_OUT + \")\\n\\n\"+\n     \"This is a simple command line tool for POSTing raw data to a Solr\\n\"+\n     \"port.  Data can be read from files specified as commandline args,\\n\"+\n     \"URLs specified as args, as raw commandline arg strings or via STDIN.\\n\"+\n     \"Examples:\\n\"+\n     \"  java -jar post.jar *.xml\\n\"+\n     \"  java -Ddata=args  -jar post.jar '<delete><id>42</id></delete>'\\n\"+\n     \"  java -Ddata=stdin -jar post.jar < hd.xml\\n\"+\n     \"  java -Ddata=web -jar post.jar http://example.com/\\n\"+\n     \"  java -Dtype=text/csv -jar post.jar *.csv\\n\"+\n     \"  java -Dtype=application/json -jar post.jar *.json\\n\"+\n     \"  java -Durl=http://localhost:8983/solr/update/extract -Dparams=literal.id=a -Dtype=application/pdf -jar post.jar a.pdf\\n\"+\n     \"  java -Dauto -jar post.jar *\\n\"+\n     \"  java -Dauto -Drecursive -jar post.jar afolder\\n\"+\n     \"  java -Dauto -Dfiletypes=ppt,html -jar post.jar afolder\\n\"+\n     \"The options controlled by System Properties include the Solr\\n\"+\n     \"URL to POST to, the Content-Type of the data, whether a commit\\n\"+\n     \"or optimize should be executed, and whether the response should\\n\"+\n     \"be written to STDOUT. If auto=yes the tool will try to set type\\n\"+\n     \"and url automatically from file name. When posting rich documents\\n\"+\n     \"the file name will be propagated as \\\"resource.name\\\" and also used\\n\"+\n     \"as \\\"literal.id\\\". You may override these or any other request parameter\\n\"+\n     \"through the -Dparams property. To do a commit only, use \\\"-\\\" as argument.\\n\"+\n     \"The web mode is a simple crawler following links within domain, default delay=10s.\");\n  }\n\n  /** Post all filenames provided in args\n   * @param args array of file names\n   * @param startIndexInArgs offset to start\n   * @param out output stream to post data to\n   * @param type default content-type to use when posting (may be overridden in auto mode)\n   * @return number of files posted\n   * */\n  public int postFiles(String [] args,int startIndexInArgs, OutputStream out, String type) {\n    reset();\n    int filesPosted = 0;\n    for (int j = startIndexInArgs; j < args.length; j++) {\n      File srcFile = new File(args[j]);\n      if(srcFile.isDirectory() && srcFile.canRead()) {\n        filesPosted += postDirectory(srcFile, out, type);\n      } else if (srcFile.isFile() && srcFile.canRead()) {\n        filesPosted += postFiles(new File[] {srcFile}, out, type);\n      } else {\n        File parent = srcFile.getParentFile();\n        if(parent == null) parent = new File(\".\");\n        String fileGlob = srcFile.getName();\n        GlobFileFilter ff = new GlobFileFilter(fileGlob, false);\n        File[] files = parent.listFiles(ff);\n        if(files == null || files.length == 0) {\n          warn(\"No files or directories matching \"+srcFile);\n          continue;          \n        }\n        filesPosted += postFiles(parent.listFiles(ff), out, type);\n      }\n    }\n    return filesPosted;\n  }\n  \n  /** Post all filenames provided in args\n   * @param files array of Files\n   * @param startIndexInArgs offset to start\n   * @param out output stream to post data to\n   * @param type default content-type to use when posting (may be overridden in auto mode)\n   * @return number of files posted\n   * */\n  public int postFiles(File[] files, int startIndexInArgs, OutputStream out, String type) {\n    reset();\n    int filesPosted = 0;\n    for (File srcFile : files) {\n      if(srcFile.isDirectory() && srcFile.canRead()) {\n        filesPosted += postDirectory(srcFile, out, type);\n      } else if (srcFile.isFile() && srcFile.canRead()) {\n        filesPosted += postFiles(new File[] {srcFile}, out, type);\n      } else {\n        File parent = srcFile.getParentFile();\n        if(parent == null) parent = new File(\".\");\n        String fileGlob = srcFile.getName();\n        GlobFileFilter ff = new GlobFileFilter(fileGlob, false);\n        File[] fileList = parent.listFiles(ff);\n        if(fileList == null || fileList.length == 0) {\n          warn(\"No files or directories matching \"+srcFile);\n          continue;          \n        }\n        filesPosted += postFiles(fileList, out, type);\n      }\n    }\n    return filesPosted;\n  }\n  \n  /**\n   * Posts a whole directory\n   * @return number of files posted total\n   */\n  private int postDirectory(File dir, OutputStream out, String type) {\n    if(dir.isHidden() && !dir.getName().equals(\".\"))\n      return(0);\n    info(\"Indexing directory \"+dir.getPath()+\" (\"+dir.listFiles(globFileFilter).length+\" files, depth=\"+currentDepth+\")\");\n    int posted = 0;\n    posted += postFiles(dir.listFiles(globFileFilter), out, type);\n    if(recursive > currentDepth) {\n      for(File d : dir.listFiles()) {\n        if(d.isDirectory()) {\n          currentDepth++;\n          posted += postDirectory(d, out, type);\n          currentDepth--;\n        }\n      }\n    }\n    return posted;\n  }\n\n  /**\n   * Posts a list of file names\n   * @return number of files posted\n   */\n  int postFiles(File[] files, OutputStream out, String type) {\n    int filesPosted = 0;\n    for(File srcFile : files) {\n      try {\n        if(!srcFile.isFile() || srcFile.isHidden())\n          continue;\n        postFile(srcFile, out, type);\n        Thread.sleep(delay * 1000);\n        filesPosted++;\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    return filesPosted;\n  }\n\n  /**\n   * This method takes as input a list of start URL strings for crawling,\n   * adds each one to a backlog and then starts crawling\n   * @param args the raw input args from main()\n   * @param startIndexInArgs offset for where to start\n   * @param out outputStream to write results to\n   * @return the number of web pages posted\n   */\n  public int postWebPages(String[] args, int startIndexInArgs, OutputStream out) {\n    reset();\n    LinkedHashSet<URL> s = new LinkedHashSet<URL>();\n    for (int j = startIndexInArgs; j < args.length; j++) {\n      try {\n        URL u = new URL(normalizeUrlEnding(args[j]));\n        s.add(u);\n      } catch(MalformedURLException e) {\n        warn(\"Skipping malformed input URL: \"+args[j]);\n      }\n    }\n    // Add URLs to level 0 of the backlog and start recursive crawling\n    backlog.add(s);\n    return webCrawl(0, out);\n  }\n\n  /**\n   * Normalizes a URL string by removing anchor part and trailing slash\n   * @return the normalized URL string\n   */\n  protected static String normalizeUrlEnding(String link) {\n    if(link.indexOf(\"#\") > -1)\n      link = link.substring(0,link.indexOf(\"#\"));\n    if(link.endsWith(\"?\"))\n      link = link.substring(0,link.length()-1);\n    if(link.endsWith(\"/\"))\n      link = link.substring(0,link.length()-1);\n    return link;\n  }\n\n  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<URL>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n  /**\n   * Reads an input stream into a byte array\n   * @param is the input stream\n   * @return the byte array\n   * @throws IOException If there is a low-level I/O error.\n   */\n  protected byte[] inputStreamToByteArray(InputStream is) throws IOException {\n    ByteArrayOutputStream bos = new ByteArrayOutputStream();\n    int next = is.read();\n    while (next > -1) {\n        bos.write(next);\n        next = is.read();\n    }\n    bos.flush();\n    is.close();\n    return bos.toByteArray();\n  }\n\n  /**\n   * Computes the full URL based on a base url and a possibly relative link found\n   * in the href param of an HTML anchor.\n   * @param baseUrl the base url from where the link was found\n   * @param link the absolute or relative link\n   * @return the string version of the full URL\n   */\n  protected String computeFullUrl(URL baseUrl, String link) {\n    if(link == null || link.length() == 0) {\n      return null;\n    }\n    if(!link.startsWith(\"http\")) {\n      if(link.startsWith(\"/\")) {\n        link = baseUrl.getProtocol() + \"://\" + baseUrl.getAuthority() + link;\n      } else {\n        if(link.contains(\":\")) {\n          return null; // Skip non-relative URLs\n        }\n        String path = baseUrl.getPath();\n        if(!path.endsWith(\"/\")) {\n          int sep = path.lastIndexOf(\"/\");\n          String file = path.substring(sep+1);\n          if(file.contains(\".\") || file.contains(\"?\"))\n            path = path.substring(0,sep);\n        }\n        link = baseUrl.getProtocol() + \"://\" + baseUrl.getAuthority() + path + \"/\" + link;\n      }\n    }\n    link = normalizeUrlEnding(link);\n    String l = link.toLowerCase(Locale.ROOT);\n    // Simple brute force skip images\n    if(l.endsWith(\".jpg\") || l.endsWith(\".jpeg\") || l.endsWith(\".png\") || l.endsWith(\".gif\")) {\n      return null; // Skip images\n    }\n    return link;\n  }\n\n  /**\n   * Uses the mime-type map to reverse lookup whether the file ending for our type\n   * is supported by the fileTypes option\n   * @param type what content-type to lookup\n   * @return true if this is a supported content type\n   */\n  protected boolean typeSupported(String type) {\n    for(String key : mimeMap.keySet()) {\n      if(mimeMap.get(key).equals(type)) {\n        if(fileTypes.contains(key))\n          return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Tests if a string is either \"true\", \"on\", \"yes\" or \"1\"\n   * @param property the string to test\n   * @return true if \"on\"\n   */\n  protected static boolean isOn(String property) {\n    return(\"true,on,yes,1\".indexOf(property) > -1);\n  }\n  \n  static void warn(String msg) {\n    System.err.println(\"SimplePostTool: WARNING: \" + msg);\n  }\n\n  static void info(String msg) {\n    System.out.println(msg);\n  }\n\n  static void fatal(String msg) {\n    System.err.println(\"SimplePostTool: FATAL: \" + msg);\n    System.exit(2);\n  }\n\n  /**\n   * Does a simple commit operation \n   */\n  public void commit() {\n    info(\"COMMITting Solr index changes to \" + solrUrl + \"..\");\n    doGet(appendParam(solrUrl.toString(), \"commit=true\"));\n  }\n\n  /**\n   * Does a simple optimize operation \n   */\n  public void optimize() {\n    info(\"Performing an OPTIMIZE to \" + solrUrl + \"..\");\n    doGet(appendParam(solrUrl.toString(), \"optimize=true\"));\n  }\n\n  /**\n   * Appends a URL query parameter to a URL \n   * @param url the original URL\n   * @param param the parameter(s) to append, separated by \"&\"\n   * @return the string version of the resulting URL\n   */\n  public static String appendParam(String url, String param) {\n    String[] pa = param.split(\"&\");\n    for(String p : pa) {\n      if(p.trim().length() == 0) continue;\n      String[] kv = p.split(\"=\");\n      if(kv.length == 2) {\n        url = url + (url.indexOf('?')>0 ? \"&\" : \"?\") + kv[0] +\"=\"+ kv[1];\n      } else {\n        warn(\"Skipping param \"+p+\" which is not on form key=value\");\n      }\n    }\n    return url;\n  }\n\n  /**\n   * Opens the file and posts it's contents to the solrUrl,\n   * writes to response to output. \n   */\n  public void postFile(File file, OutputStream output, String type) {\n    InputStream is = null;\n    try {\n      URL url = solrUrl;\n      if(auto) {\n        if(type == null) {\n          type = guessType(file);\n        }\n        if(type != null) {\n          if(type.equals(\"text/xml\") || type.equals(\"text/csv\") || type.equals(\"application/json\")) {\n            // Default handler\n          } else {\n            // SolrCell\n            String urlStr = appendUrlPath(solrUrl, \"/extract\").toString();\n            if(urlStr.indexOf(\"resource.name\")==-1)\n              urlStr = appendParam(urlStr, \"resource.name=\" + URLEncoder.encode(file.getAbsolutePath(), \"UTF-8\"));\n            if(urlStr.indexOf(\"literal.id\")==-1)\n              urlStr = appendParam(urlStr, \"literal.id=\" + URLEncoder.encode(file.getAbsolutePath(), \"UTF-8\"));\n            url = new URL(urlStr);\n          }\n        } else {\n          warn(\"Skipping \"+file.getName()+\". Unsupported file type for auto mode.\");\n          return;\n        }\n      } else {\n        if(type == null) type = DEFAULT_CONTENT_TYPE;\n      }\n      info(\"POSTing file \" + file.getName() + (auto?\" (\"+type+\")\":\"\"));\n      is = new FileInputStream(file);\n      postData(is, (int)file.length(), output, type, url);\n    } catch (IOException e) {\n      e.printStackTrace();\n      warn(\"Can't open/read file: \" + file);\n    } finally {\n      try {\n        if(is!=null) is.close();\n      } catch (IOException e) {\n        fatal(\"IOException while closing file: \"+ e);\n      }\n    }\n  }\n\n  /**\n   * Appends to the path of the URL\n   * @param url the URL\n   * @param append the path to append\n   * @return the final URL version \n   */\n  protected static URL appendUrlPath(URL url, String append) throws MalformedURLException {\n    return new URL(url.getProtocol() + \"://\" + url.getAuthority() + url.getPath() + append + (url.getQuery() != null ? \"?\"+url.getQuery() : \"\"));\n  }\n\n  /**\n   * Guesses the type of a file, based on file name suffix\n   * @param file the file\n   * @return the content-type guessed\n   */\n  protected static String guessType(File file) {\n    String name = file.getName();\n    String suffix = name.substring(name.lastIndexOf(\".\")+1);\n    return mimeMap.get(suffix.toLowerCase(Locale.ROOT));\n  }\n\n  /**\n   * Performs a simple get on the given URL\n   */\n  public static void doGet(String url) {\n    try {\n      doGet(new URL(url));\n    } catch (MalformedURLException e) {\n      warn(\"The specified URL \"+url+\" is not a valid URL. Please check\");\n    }\n  }\n  \n  /**\n   * Performs a simple get on the given URL\n   */\n  public static void doGet(URL url) {\n    try {\n      if(mockMode) return;\n      HttpURLConnection urlc = (HttpURLConnection) url.openConnection();\n      if (HttpURLConnection.HTTP_OK != urlc.getResponseCode()) {\n        warn(\"Solr returned an error #\" + urlc.getResponseCode() + \n            \" \" + urlc.getResponseMessage() + \" for url \"+url);\n      }\n    } catch (IOException e) {\n      warn(\"An error occurred posting data to \"+url+\". Please check that Solr is running.\");\n    }\n  }\n\n  /**\n   * Reads data from the data stream and posts it to solr,\n   * writes to the response to output\n   * @return true if success\n   */\n  public boolean postData(InputStream data, Integer length, OutputStream output, String type, URL url) {\n    if(mockMode) return true;\n    boolean success = true;\n    if(type == null)\n      type = DEFAULT_CONTENT_TYPE;\n    HttpURLConnection urlc = null;\n    try {\n      try {\n        urlc = (HttpURLConnection) url.openConnection();\n        try {\n          urlc.setRequestMethod(\"POST\");\n        } catch (ProtocolException e) {\n          fatal(\"Shouldn't happen: HttpURLConnection doesn't support POST??\"+e);\n        }\n        urlc.setDoOutput(true);\n        urlc.setDoInput(true);\n        urlc.setUseCaches(false);\n        urlc.setAllowUserInteraction(false);\n        urlc.setRequestProperty(\"Content-type\", type);\n\n        if (null != length) urlc.setFixedLengthStreamingMode(length);\n\n      } catch (IOException e) {\n        fatal(\"Connection error (is Solr running at \" + solrUrl + \" ?): \" + e);\n        success = false;\n      }\n      \n      OutputStream out = null;\n      try {\n        out = urlc.getOutputStream();\n        pipe(data, out);\n      } catch (IOException e) {\n        fatal(\"IOException while posting data: \" + e);\n        success = false;\n      } finally {\n        try { if(out!=null) out.close(); } catch (IOException x) { /*NOOP*/ }\n      }\n      \n      InputStream in = null;\n      try {\n        if (HttpURLConnection.HTTP_OK != urlc.getResponseCode()) {\n          warn(\"Solr returned an error #\" + urlc.getResponseCode() + \n                \" \" + urlc.getResponseMessage());\n          success = false;\n        }\n\n        in = urlc.getInputStream();\n        pipe(in, output);\n      } catch (IOException e) {\n        warn(\"IOException while reading response: \" + e);\n        success = false;\n      } finally {\n        try { if(in!=null) in.close(); } catch (IOException x) { /*NOOP*/ }\n      }\n      \n    } finally {\n      if(urlc!=null) urlc.disconnect();\n    }\n    return success;\n  }\n\n  /**\n   * Converts a string to an input stream \n   * @param s the string\n   * @return the input stream\n   */\n  public static InputStream stringToStream(String s) {\n    InputStream is = null;\n    try {\n      is = new ByteArrayInputStream(s.getBytes(\"UTF-8\"));\n    } catch (UnsupportedEncodingException e) {\n      fatal(\"Shouldn't happen: UTF-8 not supported?!?!?!\");\n    }\n    return is;\n  }\n\n  /**\n   * Pipes everything from the source to the dest.  If dest is null, \n   * then everything is read from source and thrown away.\n   */\n  private static void pipe(InputStream source, OutputStream dest) throws IOException {\n    byte[] buf = new byte[1024];\n    int read = 0;\n    while ( (read = source.read(buf) ) >= 0) {\n      if (null != dest) dest.write(buf, 0, read);\n    }\n    if (null != dest) dest.flush();\n  }\n\n  public GlobFileFilter getFileFilterFromFileTypes(String fileTypes) {\n    String glob;\n    if(fileTypes.equals(\"*\"))\n      glob = \".*\";\n    else\n      glob = \"^.*\\\\.(\" + fileTypes.replace(\",\", \"|\") + \")$\";\n    return new GlobFileFilter(glob, true);\n  }\n\n  //\n  // Utility methods for XPath handing\n  //\n  \n  /**\n   * Gets all nodes matching an XPath\n   */\n  public static NodeList getNodesFromXP(Node n, String xpath) throws XPathExpressionException {\n    XPathFactory factory = XPathFactory.newInstance();\n    XPath xp = factory.newXPath();\n    XPathExpression expr = xp.compile(xpath);\n    return (NodeList) expr.evaluate(n, XPathConstants.NODESET);\n  }\n  \n  /**\n   * Gets the string content of the matching an XPath\n   * @param n the node (or doc)\n   * @param xpath the xpath string\n   * @param concatAll if true, text from all matching nodes will be concatenated, else only the first returned\n   */\n  public static String getXP(Node n, String xpath, boolean concatAll)\n      throws XPathExpressionException {\n    NodeList nodes = getNodesFromXP(n, xpath);\n    StringBuffer sb = new StringBuffer();\n    if (nodes.getLength() > 0) {\n      for(int i = 0; i < nodes.getLength() ; i++) {\n        sb.append(nodes.item(i).getNodeValue() + \" \");\n        if(!concatAll) break;\n      }\n      return sb.toString().trim();\n    } else\n      return \"\";\n  }\n  \n  /**\n   * Takes a string as input and returns a DOM \n   */\n  public static Document makeDom(String in, String inputEncoding) throws SAXException, IOException,\n  ParserConfigurationException {\n    InputStream is = new ByteArrayInputStream(in\n        .getBytes(inputEncoding));\n    Document dom = DocumentBuilderFactory.newInstance()\n        .newDocumentBuilder().parse(is);\n    return dom;\n  }\n\n  /**\n   * Inner class to filter files based on glob wildcards\n   */\n  class GlobFileFilter implements FileFilter\n  {\n    private String _pattern;\n    private Pattern p;\n    \n    public GlobFileFilter(String pattern, boolean isRegex)\n    {\n      _pattern = pattern;\n      if(!isRegex) {\n        _pattern = _pattern\n            .replace(\"^\", \"\\\\^\")\n            .replace(\"$\", \"\\\\$\")\n            .replace(\".\", \"\\\\.\")\n            .replace(\"(\", \"\\\\(\")\n            .replace(\")\", \"\\\\)\")\n            .replace(\"+\", \"\\\\+\")\n            .replace(\"*\", \".*\")\n            .replace(\"?\", \".\");\n        _pattern = \"^\" + _pattern + \"$\";\n      }\n      \n      try {\n        p = Pattern.compile(_pattern,Pattern.CASE_INSENSITIVE);\n      } catch(PatternSyntaxException e) {\n        fatal(\"Invalid type list \"+pattern+\". \"+e.getDescription());\n      }\n    }\n    \n    @Override\n    public boolean accept(File file)\n    {\n      return p.matcher(file.getName()).find();\n    }\n  }\n  \n  //\n  // Simple crawler class which can fetch a page and check for robots.txt\n  //\n  class PageFetcher {\n    Map<String, List<String>> robotsCache;\n    final String DISALLOW = \"Disallow:\";\n    \n    public PageFetcher() {\n      robotsCache = new HashMap<String,List<String>>();\n    }\n    \n    public PageFetcherResult readPageFromUrl(URL u) {\n      PageFetcherResult res = new PageFetcherResult();\n      try {\n        if (isDisallowedByRobots(u)) {\n          warn(\"The URL \"+u+\" is disallowed by robots.txt and will not be crawled.\");\n          res.httpStatus = 403;\n          visited.add(u);\n          return res;\n        }\n        res.httpStatus = 404;\n        HttpURLConnection conn = (HttpURLConnection) u.openConnection();\n        conn.setRequestProperty(\"User-Agent\", \"SimplePostTool-crawler/\"+VERSION_OF_THIS_TOOL+\" (http://lucene.apache.org/solr/)\");\n        conn.setRequestProperty(\"Accept-Encoding\", \"gzip, deflate\");\n        conn.connect();\n        res.httpStatus = conn.getResponseCode();\n        if(!normalizeUrlEnding(conn.getURL().toString()).equals(normalizeUrlEnding(u.toString()))) {\n          info(\"The URL \"+u+\" caused a redirect to \"+conn.getURL());\n          u = conn.getURL();\n          res.redirectUrl = u;\n          visited.add(u);\n        }\n        if(res.httpStatus == 200) {\n          // Raw content type of form \"text/html; encoding=utf-8\"\n          String rawContentType = conn.getContentType();\n          String type = rawContentType.split(\";\")[0];\n          if(typeSupported(type)) {\n            String encoding = conn.getContentEncoding();\n            InputStream is;\n            if (encoding != null && encoding.equalsIgnoreCase(\"gzip\")) {\n              is = new GZIPInputStream(conn.getInputStream());\n            } else if (encoding != null && encoding.equalsIgnoreCase(\"deflate\")) {\n              is = new InflaterInputStream(conn.getInputStream(), new Inflater(true));\n            } else {\n              is = conn.getInputStream();\n            }\n            \n            // Read into memory, so that we later can pull links from the page without re-fetching \n            res.content = inputStreamToByteArray(is);\n            is.close();\n          } else {\n            warn(\"Skipping URL with unsupported type \"+type);\n            res.httpStatus = 415;\n          }\n        }\n      } catch(IOException e) {\n        warn(\"IOException when reading page from url \"+u+\": \"+e.getMessage());\n      }\n      return res;\n    }\n    \n    public boolean isDisallowedByRobots(URL url) {\n      String host = url.getHost();\n      String strRobot = url.getProtocol() + \"://\" + host + \"/robots.txt\";\n      List<String> disallows = robotsCache.get(host);\n      if(disallows == null) {\n        disallows = new ArrayList<String>();\n        URL urlRobot;\n        try { \n          urlRobot = new URL(strRobot);\n          disallows = parseRobotsTxt(urlRobot.openStream());\n        } catch (MalformedURLException e) {\n          return true; // We cannot trust this robots URL, should not happen\n        } catch (IOException e) {\n          // There is no robots.txt, will cache an empty disallow list\n        }\n      }\n      \n      robotsCache.put(host, disallows);\n\n      String strURL = url.getFile();\n      for (String path : disallows) {\n        if (path.equals(\"/\") || strURL.indexOf(path) == 0)\n          return true;\n      }\n      return false;\n    }\n\n    /**\n     * Very simple robots.txt parser which obeys all Disallow lines regardless\n     * of user agent or whether there are valid Allow: lines.\n     * @param is Input stream of the robots.txt file\n     * @return a list of disallow paths\n     * @throws IOException if problems reading the stream\n     */\n    protected List<String> parseRobotsTxt(InputStream is) throws IOException {\n      List<String> disallows = new ArrayList<String>();\n      BufferedReader r = new BufferedReader(new InputStreamReader(is, \"UTF-8\"));\n      String l;\n      while((l = r.readLine()) != null) {\n        String[] arr = l.split(\"#\");\n        if(arr.length == 0) continue;\n        l = arr[0].trim();\n        if(l.startsWith(DISALLOW)) {\n          l = l.substring(DISALLOW.length()).trim();\n          if(l.length() == 0) continue;\n          disallows.add(l);\n        }\n      }\n      is.close();\n      return disallows;\n    }\n\n    /**\n     * Finds links on a web page, using /extract?extractOnly=true\n     * @param u the URL of the web page\n     * @param is the input stream of the page\n     * @param type the content-type\n     * @param postUrl the URL (typically /solr/extract) in order to pull out links\n     * @return a set of URLs parsed from the page\n     */\n    protected Set<URL> getLinksFromWebPage(URL u, InputStream is, String type, URL postUrl) {\n      Set<URL> l = new HashSet<URL>();\n      URL url = null;\n      try {\n        ByteArrayOutputStream os = new ByteArrayOutputStream();\n        URL extractUrl = new URL(appendParam(postUrl.toString(), \"extractOnly=true\"));\n        boolean success = postData(is, null, os, type, extractUrl);\n        if(success) {\n          String rawXml = os.toString(\"UTF-8\");\n          Document d = makeDom(rawXml, \"UTF-8\");\n          String innerXml = getXP(d, \"/response/str/text()[1]\", false);\n          d = makeDom(innerXml, \"UTF-8\");\n          NodeList links = getNodesFromXP(d, \"/html/body//a/@href\");\n          for(int i = 0; i < links.getLength(); i++) {\n            String link = links.item(i).getTextContent();\n            link = computeFullUrl(u, link);\n            if(link == null)\n              continue;\n            url = new URL(link);\n            if(url.getAuthority() == null || !url.getAuthority().equals(u.getAuthority()))\n              continue;\n            l.add(url);\n          }\n        }\n      } catch (MalformedURLException e) {\n        warn(\"Malformed URL \"+url);\n      } catch (IOException e) {\n        warn(\"IOException opening URL \"+url+\": \"+e.getMessage());\n      } catch (Exception e) {\n        throw new RuntimeException();\n      }\n      return l;\n    }\n  }\n    \n  /**\n   * Utility class to hold the result form a page fetch\n   */\n  public class PageFetcherResult {\n    int httpStatus = 200;\n    String contentType = \"text/html\";\n    URL redirectUrl = null;\n    byte[] content;\n  }\n}\n",
        "methodName": "postDirectory",
        "exampleID": 62,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/util/SimplePostTool.java",
        "line": "470",
        "source": "?",
        "sourceLine": "470",
        "qualifier": "Possible null pointer dereference of the $$value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 63
            }
        ],
        "line_number": "470"
    },
    {
        "url": "https://github.com/apache/lucene-solr/blob/43535fecb8455b3f9364f447e129ae05f79697e2//solr/core/src/java/org/apache/solr/util/SimplePostTool.java",
        "rawCode": "package org.apache.solr.util;\n\n/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.BufferedReader;\nimport java.io.ByteArrayOutputStream;\nimport java.io.File;\nimport java.io.FileFilter;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.ByteArrayInputStream;\nimport java.io.InputStreamReader;\nimport java.io.OutputStream;\nimport java.io.UnsupportedEncodingException;\nimport java.text.SimpleDateFormat;\nimport java.util.ArrayList;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.LinkedHashSet;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.HashSet;\nimport java.util.TimeZone;\nimport java.util.regex.Pattern;\nimport java.util.regex.PatternSyntaxException;\nimport java.util.zip.GZIPInputStream;\nimport java.util.zip.Inflater;\nimport java.util.zip.InflaterInputStream;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.ProtocolException;\nimport java.net.URL;\nimport java.net.URLEncoder;\n\nimport javax.xml.parsers.DocumentBuilderFactory;\nimport javax.xml.parsers.ParserConfigurationException;\nimport javax.xml.xpath.XPath;\nimport javax.xml.xpath.XPathConstants;\nimport javax.xml.xpath.XPathExpression;\nimport javax.xml.xpath.XPathExpressionException;\nimport javax.xml.xpath.XPathFactory;\n\nimport org.w3c.dom.Document;\nimport org.w3c.dom.Node;\nimport org.w3c.dom.NodeList;\nimport org.xml.sax.SAXException;\n\n/**\n * A simple utility class for posting raw updates to a Solr server, \n * has a main method so it can be run on the command line.\n * View this not as a best-practice code example, but as a standalone \n * example built with an explicit purpose of not having external\n * jar dependencies.\n */\npublic class SimplePostTool {\n  private static final String DEFAULT_POST_URL = \"http://localhost:8983/solr/update\";\n  private static final String VERSION_OF_THIS_TOOL = \"1.5\";\n\n  private static final String DEFAULT_COMMIT = \"yes\";\n  private static final String DEFAULT_OPTIMIZE = \"no\";\n  private static final String DEFAULT_OUT = \"no\";\n  private static final String DEFAULT_AUTO = \"no\";\n  private static final String DEFAULT_RECURSIVE = \"0\";\n  private static final int DEFAULT_WEB_DELAY = 10;\n  private static final int MAX_WEB_DEPTH = 10;\n  private static final String DEFAULT_CONTENT_TYPE = \"application/xml\";\n  private static final String DEFAULT_FILE_TYPES = \"xml,json,csv,pdf,doc,docx,ppt,pptx,xls,xlsx,odt,odp,ods,ott,otp,ots,rtf,htm,html,txt,log\"; \n\n  static final String DATA_MODE_FILES = \"files\";\n  static final String DATA_MODE_ARGS = \"args\";\n  static final String DATA_MODE_STDIN = \"stdin\";\n  static final String DATA_MODE_WEB = \"web\";\n  static final String DEFAULT_DATA_MODE = DATA_MODE_FILES;\n\n  // Input args\n  boolean auto = false;\n  int recursive = 0;\n  int delay = 0;\n  String fileTypes;\n  URL solrUrl;\n  OutputStream out = null;\n  String type;\n  String mode;\n  boolean commit;\n  boolean optimize;\n  String[] args;\n\n  private int currentDepth;\n\n  static HashMap<String,String> mimeMap;\n  GlobFileFilter globFileFilter;\n  // Backlog for crawling\n  List<LinkedHashSet<URL>> backlog = new ArrayList<LinkedHashSet<URL>>();\n  Set<URL> visited = new HashSet<URL>();\n  \n  static final Set<String> DATA_MODES = new HashSet<String>();\n  static final String USAGE_STRING_SHORT =\n      \"Usage: java [SystemProperties] -jar post.jar [-h|-] [<file|folder|url|arg> [<file|folder|url|arg>...]]\";\n\n  // Used in tests to avoid doing actual network traffic\n  static boolean mockMode = false;\n  static PageFetcher pageFetcher;\n\n  static {\n    DATA_MODES.add(DATA_MODE_FILES);\n    DATA_MODES.add(DATA_MODE_ARGS);\n    DATA_MODES.add(DATA_MODE_STDIN);\n    DATA_MODES.add(DATA_MODE_WEB);\n    \n    mimeMap = new HashMap<String,String>();\n    mimeMap.put(\"xml\", \"text/xml\");\n    mimeMap.put(\"csv\", \"text/csv\");\n    mimeMap.put(\"json\", \"application/json\");\n    mimeMap.put(\"pdf\", \"application/pdf\");\n    mimeMap.put(\"rtf\", \"text/rtf\");\n    mimeMap.put(\"html\", \"text/html\");\n    mimeMap.put(\"htm\", \"text/html\");\n    mimeMap.put(\"doc\", \"application/msword\");\n    mimeMap.put(\"docx\", \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\");\n    mimeMap.put(\"ppt\", \"application/vnd.ms-powerpoint\");\n    mimeMap.put(\"pptx\", \"application/vnd.openxmlformats-officedocument.presentationml.presentation\");\n    mimeMap.put(\"xls\", \"application/vnd.ms-excel\");\n    mimeMap.put(\"xlsx\", \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\");\n    mimeMap.put(\"odt\", \"application/vnd.oasis.opendocument.text\");\n    mimeMap.put(\"ott\", \"application/vnd.oasis.opendocument.text\");\n    mimeMap.put(\"odp\", \"application/vnd.oasis.opendocument.presentation\");\n    mimeMap.put(\"otp\", \"application/vnd.oasis.opendocument.presentation\");\n    mimeMap.put(\"ods\", \"application/vnd.oasis.opendocument.spreadsheet\");\n    mimeMap.put(\"ots\", \"application/vnd.oasis.opendocument.spreadsheet\");\n    mimeMap.put(\"txt\", \"text/plain\");\n    mimeMap.put(\"log\", \"text/plain\");\n  }\n  \n  /**\n   * See usage() for valid command line usage\n   * @param args the params on the command line\n   */\n  public static void main(String[] args) {\n    info(\"SimplePostTool version \" + VERSION_OF_THIS_TOOL);\n    if (0 < args.length && (\"-help\".equals(args[0]) || \"--help\".equals(args[0]) || \"-h\".equals(args[0]))) {\n      usage();\n    } else {\n      final SimplePostTool t = parseArgsAndInit(args);\n      t.execute();\n    }\n  }\n\n  /**\n   * After initialization, call execute to start the post job.\n   * This method delegates to the correct mode method.\n   */\n  public void execute() {\n    final long startTime = System.currentTimeMillis();\n    if (DATA_MODE_FILES.equals(mode) && args.length > 0) {\n      doFilesMode();\n    } else if(DATA_MODE_ARGS.equals(mode) && args.length > 0) {\n      doArgsMode();\n    } else if(DATA_MODE_WEB.equals(mode) && args.length > 0) {\n      doWebMode();\n    } else if(DATA_MODE_STDIN.equals(mode)) {\n      doStdinMode();\n    } else {\n      usageShort();\n      return;\n    }\n    \n    if (commit)   commit();\n    if (optimize) optimize();\n    final long endTime = System.currentTimeMillis();\n    displayTiming(endTime - startTime);\n  }\n  \n  /**\n   * Pretty prints the number of milliseconds taken to post the content to Solr\n   * @param millis the time in milliseconds\n   */\n  private void displayTiming(long millis) {\n    SimpleDateFormat df = new SimpleDateFormat(\"H:mm:ss.SSS\", Locale.getDefault());\n    df.setTimeZone(TimeZone.getTimeZone(\"UTC\"));\n    System.out.println(\"Time spent: \"+df.format(new Date(millis)));\n  }\n\n  /**\n   * Parses incoming arguments and system params and initializes the tool\n   * @param args the incoming cmd line args\n   * @return an instance of SimplePostTool\n   */\n  protected static SimplePostTool parseArgsAndInit(String[] args) {\n    String urlStr = null;\n    try {\n      // Parse args\n      final String mode = System.getProperty(\"data\", DEFAULT_DATA_MODE);\n      if (! DATA_MODES.contains(mode)) {\n        fatal(\"System Property 'data' is not valid for this tool: \" + mode);\n      }\n      String params = System.getProperty(\"params\", \"\");\n      urlStr = System.getProperty(\"url\", DEFAULT_POST_URL);\n      urlStr = SimplePostTool.appendParam(urlStr, params);\n      URL url = new URL(urlStr);\n      boolean auto = isOn(System.getProperty(\"auto\", DEFAULT_AUTO));\n      String type = System.getProperty(\"type\");\n      // Recursive\n      int recursive = 0;\n      String r = System.getProperty(\"recursive\", DEFAULT_RECURSIVE);\n      try {\n        recursive = Integer.parseInt(r);\n      } catch(Exception e) {\n        if (isOn(r))\n          recursive = DATA_MODE_WEB.equals(mode)?1:999;\n      }\n      // Delay\n      int delay = DATA_MODE_WEB.equals(mode) ? DEFAULT_WEB_DELAY : 0;\n      try {\n        delay = Integer.parseInt(System.getProperty(\"delay\", \"\"+delay));\n      } catch(Exception e) { }\n      OutputStream out = isOn(System.getProperty(\"out\", DEFAULT_OUT)) ? System.out : null;\n      String fileTypes = System.getProperty(\"filetypes\", DEFAULT_FILE_TYPES);\n      boolean commit = isOn(System.getProperty(\"commit\",DEFAULT_COMMIT));\n      boolean optimize = isOn(System.getProperty(\"optimize\",DEFAULT_OPTIMIZE));\n      \n      return new SimplePostTool(mode, url, auto, type, recursive, delay, fileTypes, out, commit, optimize, args);\n    } catch (MalformedURLException e) {\n      fatal(\"System Property 'url' is not a valid URL: \" + urlStr);\n      return null;\n    }\n  }\n\n  /**\n   * Constructor which takes in all mandatory input for the tool to work.\n   * Also see usage() for further explanation of the params.\n   * @param mode whether to post files, web pages, params or stdin\n   * @param url the Solr base Url to post to, should end with /update\n   * @param auto if true, we'll guess type and add resourcename/url\n   * @param type content-type of the data you are posting\n   * @param recursive number of levels for file/web mode, or 0 if one file only\n   * @param delay if recursive then delay will be the wait time between posts\n   * @param fileTypes a comma separated list of file-name endings to accept for file/web\n   * @param out an OutputStream to write output to, e.g. stdout to print to console\n   * @param commit if true, will commit at end of posting\n   * @param optimize if true, will optimize at end of posting\n   * @param args a String[] of arguments, varies between modes\n   */\n  public SimplePostTool(String mode, URL url, boolean auto, String type,\n      int recursive, int delay, String fileTypes, OutputStream out, \n      boolean commit, boolean optimize, String[] args) {\n    this.mode = mode;\n    this.solrUrl = url;\n    this.auto = auto;\n    this.type = type;\n    this.recursive = recursive;\n    this.delay = delay;\n    this.fileTypes = fileTypes;\n    this.globFileFilter = getFileFilterFromFileTypes(fileTypes);\n    this.out = out;\n    this.commit = commit;\n    this.optimize = optimize;\n    this.args = args;\n    pageFetcher = new PageFetcher();\n  }\n\n  public SimplePostTool() {}\n  \n  //\n  // Do some action depending on which mode we have\n  //\n  private void doFilesMode() {\n    currentDepth = 0;\n    // Skip posting files if special param \"-\" given  \n    if (!args[0].equals(\"-\")) {\n      info(\"Posting files to base url \" + solrUrl + (!auto?\" using content-type \"+(type==null?DEFAULT_CONTENT_TYPE:type):\"\")+\"..\");\n      if(auto)\n        info(\"Entering auto mode. File endings considered are \"+fileTypes);\n      if(recursive > 0)\n        info(\"Entering recursive mode, max depth=\"+recursive+\", delay=\"+delay+\"s\"); \n      int numFilesPosted = postFiles(args, 0, out, type);\n      info(numFilesPosted + \" files indexed.\");\n    }\n  }\n\n  private void doArgsMode() {\n    info(\"POSTing args to \" + solrUrl + \"..\");\n    for (String a : args) {\n      postData(stringToStream(a), null, out, type, solrUrl);\n    }\n  }\n\n  private int doWebMode() {\n    reset();\n    int numPagesPosted = 0;\n    try {\n      if(type != null) {\n        fatal(\"Specifying content-type with \\\"-Ddata=web\\\" is not supported\");\n      }\n      if (args[0].equals(\"-\")) {\n        // Skip posting url if special param \"-\" given  \n        return 0;\n      }\n      // Set Extracting handler as default\n      solrUrl = appendUrlPath(solrUrl, \"/extract\");\n      \n      info(\"Posting web pages to Solr url \"+solrUrl);\n      auto=true;\n      info(\"Entering auto mode. Indexing pages with content-types corresponding to file endings \"+fileTypes);\n      if(recursive > 0) {\n        if(recursive > MAX_WEB_DEPTH) {\n          recursive = MAX_WEB_DEPTH;\n          warn(\"Too large recursion depth for web mode, limiting to \"+MAX_WEB_DEPTH+\"...\");\n        }\n        if(delay < DEFAULT_WEB_DELAY)\n          warn(\"Never crawl an external web site faster than every 10 seconds, your IP will probably be blocked\");\n        info(\"Entering recursive mode, depth=\"+recursive+\", delay=\"+delay+\"s\");\n      }\n      numPagesPosted = postWebPages(args, 0, out);\n      info(numPagesPosted + \" web pages indexed.\");\n    } catch(MalformedURLException e) {\n      fatal(\"Wrong URL trying to append /extract to \"+solrUrl);\n    }\n    return numPagesPosted;\n  }\n\n  private void doStdinMode() {\n    info(\"POSTing stdin to \" + solrUrl + \"..\");\n    postData(System.in, null, out, type, solrUrl);    \n  }\n\n  private void reset() {\n    fileTypes = DEFAULT_FILE_TYPES;\n    globFileFilter = this.getFileFilterFromFileTypes(fileTypes);\n    backlog = new ArrayList<LinkedHashSet<URL>>();\n    visited = new HashSet<URL>();\n  }\n\n\n  //\n  // USAGE\n  //\n  private static void usageShort() {\n    System.out.println(USAGE_STRING_SHORT+\"\\n\"+\n        \"       Please invoke with -h option for extended usage help.\");\n  }\n\n  private static void usage() {\n    System.out.println\n    (USAGE_STRING_SHORT+\"\\n\\n\" +\n     \"Supported System Properties and their defaults:\\n\"+\n     \"  -Ddata=files|web|args|stdin (default=\" + DEFAULT_DATA_MODE + \")\\n\"+\n     \"  -Dtype=<content-type> (default=\" + DEFAULT_CONTENT_TYPE + \")\\n\"+\n     \"  -Durl=<solr-update-url> (default=\" + DEFAULT_POST_URL + \")\\n\"+\n     \"  -Dauto=yes|no (default=\" + DEFAULT_AUTO + \")\\n\"+\n     \"  -Drecursive=yes|no|<depth> (default=\" + DEFAULT_RECURSIVE + \")\\n\"+\n     \"  -Ddelay=<seconds> (default=0 for files, 10 for web)\\n\"+\n     \"  -Dfiletypes=<type>[,<type>,...] (default=\" + DEFAULT_FILE_TYPES + \")\\n\"+\n     \"  -Dparams=\\\"<key>=<value>[&<key>=<value>...]\\\" (values must be URL-encoded)\\n\"+\n     \"  -Dcommit=yes|no (default=\" + DEFAULT_COMMIT + \")\\n\"+\n     \"  -Doptimize=yes|no (default=\" + DEFAULT_OPTIMIZE + \")\\n\"+\n     \"  -Dout=yes|no (default=\" + DEFAULT_OUT + \")\\n\\n\"+\n     \"This is a simple command line tool for POSTing raw data to a Solr\\n\"+\n     \"port.  Data can be read from files specified as commandline args,\\n\"+\n     \"URLs specified as args, as raw commandline arg strings or via STDIN.\\n\"+\n     \"Examples:\\n\"+\n     \"  java -jar post.jar *.xml\\n\"+\n     \"  java -Ddata=args  -jar post.jar '<delete><id>42</id></delete>'\\n\"+\n     \"  java -Ddata=stdin -jar post.jar < hd.xml\\n\"+\n     \"  java -Ddata=web -jar post.jar http://example.com/\\n\"+\n     \"  java -Dtype=text/csv -jar post.jar *.csv\\n\"+\n     \"  java -Dtype=application/json -jar post.jar *.json\\n\"+\n     \"  java -Durl=http://localhost:8983/solr/update/extract -Dparams=literal.id=a -Dtype=application/pdf -jar post.jar a.pdf\\n\"+\n     \"  java -Dauto -jar post.jar *\\n\"+\n     \"  java -Dauto -Drecursive -jar post.jar afolder\\n\"+\n     \"  java -Dauto -Dfiletypes=ppt,html -jar post.jar afolder\\n\"+\n     \"The options controlled by System Properties include the Solr\\n\"+\n     \"URL to POST to, the Content-Type of the data, whether a commit\\n\"+\n     \"or optimize should be executed, and whether the response should\\n\"+\n     \"be written to STDOUT. If auto=yes the tool will try to set type\\n\"+\n     \"and url automatically from file name. When posting rich documents\\n\"+\n     \"the file name will be propagated as \\\"resource.name\\\" and also used\\n\"+\n     \"as \\\"literal.id\\\". You may override these or any other request parameter\\n\"+\n     \"through the -Dparams property. To do a commit only, use \\\"-\\\" as argument.\\n\"+\n     \"The web mode is a simple crawler following links within domain, default delay=10s.\");\n  }\n\n  /** Post all filenames provided in args\n   * @param args array of file names\n   * @param startIndexInArgs offset to start\n   * @param out output stream to post data to\n   * @param type default content-type to use when posting (may be overridden in auto mode)\n   * @return number of files posted\n   * */\n  public int postFiles(String [] args,int startIndexInArgs, OutputStream out, String type) {\n    reset();\n    int filesPosted = 0;\n    for (int j = startIndexInArgs; j < args.length; j++) {\n      File srcFile = new File(args[j]);\n      if(srcFile.isDirectory() && srcFile.canRead()) {\n        filesPosted += postDirectory(srcFile, out, type);\n      } else if (srcFile.isFile() && srcFile.canRead()) {\n        filesPosted += postFiles(new File[] {srcFile}, out, type);\n      } else {\n        File parent = srcFile.getParentFile();\n        if(parent == null) parent = new File(\".\");\n        String fileGlob = srcFile.getName();\n        GlobFileFilter ff = new GlobFileFilter(fileGlob, false);\n        File[] files = parent.listFiles(ff);\n        if(files == null || files.length == 0) {\n          warn(\"No files or directories matching \"+srcFile);\n          continue;          \n        }\n        filesPosted += postFiles(parent.listFiles(ff), out, type);\n      }\n    }\n    return filesPosted;\n  }\n  \n  /** Post all filenames provided in args\n   * @param files array of Files\n   * @param startIndexInArgs offset to start\n   * @param out output stream to post data to\n   * @param type default content-type to use when posting (may be overridden in auto mode)\n   * @return number of files posted\n   * */\n  public int postFiles(File[] files, int startIndexInArgs, OutputStream out, String type) {\n    reset();\n    int filesPosted = 0;\n    for (File srcFile : files) {\n      if(srcFile.isDirectory() && srcFile.canRead()) {\n        filesPosted += postDirectory(srcFile, out, type);\n      } else if (srcFile.isFile() && srcFile.canRead()) {\n        filesPosted += postFiles(new File[] {srcFile}, out, type);\n      } else {\n        File parent = srcFile.getParentFile();\n        if(parent == null) parent = new File(\".\");\n        String fileGlob = srcFile.getName();\n        GlobFileFilter ff = new GlobFileFilter(fileGlob, false);\n        File[] fileList = parent.listFiles(ff);\n        if(fileList == null || fileList.length == 0) {\n          warn(\"No files or directories matching \"+srcFile);\n          continue;          \n        }\n        filesPosted += postFiles(fileList, out, type);\n      }\n    }\n    return filesPosted;\n  }\n  \n  /**\n   * Posts a whole directory\n   * @return number of files posted total\n   */\n  private int postDirectory(File dir, OutputStream out, String type) {\n    if(dir.isHidden() && !dir.getName().equals(\".\"))\n      return(0);\n    info(\"Indexing directory \"+dir.getPath()+\" (\"+dir.listFiles(globFileFilter).length+\" files, depth=\"+currentDepth+\")\");\n    int posted = 0;\n    posted += postFiles(dir.listFiles(globFileFilter), out, type);\n    if(recursive > currentDepth) {\n      for(File d : dir.listFiles()) {\n        if(d.isDirectory()) {\n          currentDepth++;\n          posted += postDirectory(d, out, type);\n          currentDepth--;\n        }\n      }\n    }\n    return posted;\n  }\n\n  /**\n   * Posts a list of file names\n   * @return number of files posted\n   */\n  int postFiles(File[] files, OutputStream out, String type) {\n    int filesPosted = 0;\n    for(File srcFile : files) {\n      try {\n        if(!srcFile.isFile() || srcFile.isHidden())\n          continue;\n        postFile(srcFile, out, type);\n        Thread.sleep(delay * 1000);\n        filesPosted++;\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    return filesPosted;\n  }\n\n  /**\n   * This method takes as input a list of start URL strings for crawling,\n   * adds each one to a backlog and then starts crawling\n   * @param args the raw input args from main()\n   * @param startIndexInArgs offset for where to start\n   * @param out outputStream to write results to\n   * @return the number of web pages posted\n   */\n  public int postWebPages(String[] args, int startIndexInArgs, OutputStream out) {\n    reset();\n    LinkedHashSet<URL> s = new LinkedHashSet<URL>();\n    for (int j = startIndexInArgs; j < args.length; j++) {\n      try {\n        URL u = new URL(normalizeUrlEnding(args[j]));\n        s.add(u);\n      } catch(MalformedURLException e) {\n        warn(\"Skipping malformed input URL: \"+args[j]);\n      }\n    }\n    // Add URLs to level 0 of the backlog and start recursive crawling\n    backlog.add(s);\n    return webCrawl(0, out);\n  }\n\n  /**\n   * Normalizes a URL string by removing anchor part and trailing slash\n   * @return the normalized URL string\n   */\n  protected static String normalizeUrlEnding(String link) {\n    if(link.indexOf(\"#\") > -1)\n      link = link.substring(0,link.indexOf(\"#\"));\n    if(link.endsWith(\"?\"))\n      link = link.substring(0,link.length()-1);\n    if(link.endsWith(\"/\"))\n      link = link.substring(0,link.length()-1);\n    return link;\n  }\n\n  /**\n   * A very simple crawler, pulling URLs to fetch from a backlog and then\n   * recurses N levels deep if recursive>0. Links are parsed from HTML\n   * through first getting an XHTML version using SolrCell with extractOnly,\n   * and followed if they are local. The crawler pauses for a default delay\n   * of 10 seconds between each fetch, this can be configured in the delay\n   * variable. This is only meant for test purposes, as it does not respect\n   * robots or anything else fancy :)\n   * @param level which level to crawl\n   * @param out output stream to write to\n   * @return number of pages crawled on this level and below\n   */\n  protected int webCrawl(int level, OutputStream out) {\n    int numPages = 0;\n    LinkedHashSet<URL> stack = backlog.get(level);\n    int rawStackSize = stack.size();\n    stack.removeAll(visited);\n    int stackSize = stack.size();\n    LinkedHashSet<URL> subStack = new LinkedHashSet<URL>();\n    info(\"Entering crawl at level \"+level+\" (\"+rawStackSize+\" links total, \"+stackSize+\" new)\");\n    for(URL u : stack) {\n      try {\n        visited.add(u);\n        PageFetcherResult result = pageFetcher.readPageFromUrl(u);\n        if(result.httpStatus == 200) {\n          u = (result.redirectUrl != null) ? result.redirectUrl : u;\n          URL postUrl = new URL(appendParam(solrUrl.toString(), \n              \"literal.id=\"+URLEncoder.encode(u.toString(),\"UTF-8\") +\n              \"&literal.url=\"+URLEncoder.encode(u.toString(),\"UTF-8\")));\n          boolean success = postData(new ByteArrayInputStream(result.content), null, out, result.contentType, postUrl);\n          if (success) {\n            info(\"POSTed web resource \"+u+\" (depth: \"+level+\")\");\n            Thread.sleep(delay * 1000);\n            numPages++;\n            // Pull links from HTML pages only\n            if(recursive > level && result.contentType.equals(\"text/html\")) {\n              Set<URL> children = pageFetcher.getLinksFromWebPage(u, new ByteArrayInputStream(result.content), result.contentType, postUrl);\n              subStack.addAll(children);\n            }\n          } else {\n            warn(\"An error occurred while posting \"+u);\n          }\n        } else {\n          warn(\"The URL \"+u+\" returned a HTTP result status of \"+result.httpStatus);\n        }\n      } catch (IOException e) {\n        warn(\"Caught exception when trying to open connection to \"+u+\": \"+e.getMessage());\n      } catch (InterruptedException e) {\n        throw new RuntimeException();\n      }\n    }\n    if(!subStack.isEmpty()) {\n      backlog.add(subStack);\n      numPages += webCrawl(level+1, out);\n    }\n    return numPages;    \n  }\n\n  /**\n   * Reads an input stream into a byte array\n   * @param is the input stream\n   * @return the byte array\n   * @throws IOException If there is a low-level I/O error.\n   */\n  protected byte[] inputStreamToByteArray(InputStream is) throws IOException {\n    ByteArrayOutputStream bos = new ByteArrayOutputStream();\n    int next = is.read();\n    while (next > -1) {\n        bos.write(next);\n        next = is.read();\n    }\n    bos.flush();\n    is.close();\n    return bos.toByteArray();\n  }\n\n  /**\n   * Computes the full URL based on a base url and a possibly relative link found\n   * in the href param of an HTML anchor.\n   * @param baseUrl the base url from where the link was found\n   * @param link the absolute or relative link\n   * @return the string version of the full URL\n   */\n  protected String computeFullUrl(URL baseUrl, String link) {\n    if(link == null || link.length() == 0) {\n      return null;\n    }\n    if(!link.startsWith(\"http\")) {\n      if(link.startsWith(\"/\")) {\n        link = baseUrl.getProtocol() + \"://\" + baseUrl.getAuthority() + link;\n      } else {\n        if(link.contains(\":\")) {\n          return null; // Skip non-relative URLs\n        }\n        String path = baseUrl.getPath();\n        if(!path.endsWith(\"/\")) {\n          int sep = path.lastIndexOf(\"/\");\n          String file = path.substring(sep+1);\n          if(file.contains(\".\") || file.contains(\"?\"))\n            path = path.substring(0,sep);\n        }\n        link = baseUrl.getProtocol() + \"://\" + baseUrl.getAuthority() + path + \"/\" + link;\n      }\n    }\n    link = normalizeUrlEnding(link);\n    String l = link.toLowerCase(Locale.ROOT);\n    // Simple brute force skip images\n    if(l.endsWith(\".jpg\") || l.endsWith(\".jpeg\") || l.endsWith(\".png\") || l.endsWith(\".gif\")) {\n      return null; // Skip images\n    }\n    return link;\n  }\n\n  /**\n   * Uses the mime-type map to reverse lookup whether the file ending for our type\n   * is supported by the fileTypes option\n   * @param type what content-type to lookup\n   * @return true if this is a supported content type\n   */\n  protected boolean typeSupported(String type) {\n    for(String key : mimeMap.keySet()) {\n      if(mimeMap.get(key).equals(type)) {\n        if(fileTypes.contains(key))\n          return true;\n      }\n    }\n    return false;\n  }\n\n  /**\n   * Tests if a string is either \"true\", \"on\", \"yes\" or \"1\"\n   * @param property the string to test\n   * @return true if \"on\"\n   */\n  protected static boolean isOn(String property) {\n    return(\"true,on,yes,1\".indexOf(property) > -1);\n  }\n  \n  static void warn(String msg) {\n    System.err.println(\"SimplePostTool: WARNING: \" + msg);\n  }\n\n  static void info(String msg) {\n    System.out.println(msg);\n  }\n\n  static void fatal(String msg) {\n    System.err.println(\"SimplePostTool: FATAL: \" + msg);\n    System.exit(2);\n  }\n\n  /**\n   * Does a simple commit operation \n   */\n  public void commit() {\n    info(\"COMMITting Solr index changes to \" + solrUrl + \"..\");\n    doGet(appendParam(solrUrl.toString(), \"commit=true\"));\n  }\n\n  /**\n   * Does a simple optimize operation \n   */\n  public void optimize() {\n    info(\"Performing an OPTIMIZE to \" + solrUrl + \"..\");\n    doGet(appendParam(solrUrl.toString(), \"optimize=true\"));\n  }\n\n  /**\n   * Appends a URL query parameter to a URL \n   * @param url the original URL\n   * @param param the parameter(s) to append, separated by \"&\"\n   * @return the string version of the resulting URL\n   */\n  public static String appendParam(String url, String param) {\n    String[] pa = param.split(\"&\");\n    for(String p : pa) {\n      if(p.trim().length() == 0) continue;\n      String[] kv = p.split(\"=\");\n      if(kv.length == 2) {\n        url = url + (url.indexOf('?')>0 ? \"&\" : \"?\") + kv[0] +\"=\"+ kv[1];\n      } else {\n        warn(\"Skipping param \"+p+\" which is not on form key=value\");\n      }\n    }\n    return url;\n  }\n\n  /**\n   * Opens the file and posts it's contents to the solrUrl,\n   * writes to response to output. \n   */\n  public void postFile(File file, OutputStream output, String type) {\n    InputStream is = null;\n    try {\n      URL url = solrUrl;\n      if(auto) {\n        if(type == null) {\n          type = guessType(file);\n        }\n        if(type != null) {\n          if(type.equals(\"text/xml\") || type.equals(\"text/csv\") || type.equals(\"application/json\")) {\n            // Default handler\n          } else {\n            // SolrCell\n            String urlStr = appendUrlPath(solrUrl, \"/extract\").toString();\n            if(urlStr.indexOf(\"resource.name\")==-1)\n              urlStr = appendParam(urlStr, \"resource.name=\" + URLEncoder.encode(file.getAbsolutePath(), \"UTF-8\"));\n            if(urlStr.indexOf(\"literal.id\")==-1)\n              urlStr = appendParam(urlStr, \"literal.id=\" + URLEncoder.encode(file.getAbsolutePath(), \"UTF-8\"));\n            url = new URL(urlStr);\n          }\n        } else {\n          warn(\"Skipping \"+file.getName()+\". Unsupported file type for auto mode.\");\n          return;\n        }\n      } else {\n        if(type == null) type = DEFAULT_CONTENT_TYPE;\n      }\n      info(\"POSTing file \" + file.getName() + (auto?\" (\"+type+\")\":\"\"));\n      is = new FileInputStream(file);\n      postData(is, (int)file.length(), output, type, url);\n    } catch (IOException e) {\n      e.printStackTrace();\n      warn(\"Can't open/read file: \" + file);\n    } finally {\n      try {\n        if(is!=null) is.close();\n      } catch (IOException e) {\n        fatal(\"IOException while closing file: \"+ e);\n      }\n    }\n  }\n\n  /**\n   * Appends to the path of the URL\n   * @param url the URL\n   * @param append the path to append\n   * @return the final URL version \n   */\n  protected static URL appendUrlPath(URL url, String append) throws MalformedURLException {\n    return new URL(url.getProtocol() + \"://\" + url.getAuthority() + url.getPath() + append + (url.getQuery() != null ? \"?\"+url.getQuery() : \"\"));\n  }\n\n  /**\n   * Guesses the type of a file, based on file name suffix\n   * @param file the file\n   * @return the content-type guessed\n   */\n  protected static String guessType(File file) {\n    String name = file.getName();\n    String suffix = name.substring(name.lastIndexOf(\".\")+1);\n    return mimeMap.get(suffix.toLowerCase(Locale.ROOT));\n  }\n\n  /**\n   * Performs a simple get on the given URL\n   */\n  public static void doGet(String url) {\n    try {\n      doGet(new URL(url));\n    } catch (MalformedURLException e) {\n      warn(\"The specified URL \"+url+\" is not a valid URL. Please check\");\n    }\n  }\n  \n  /**\n   * Performs a simple get on the given URL\n   */\n  public static void doGet(URL url) {\n    try {\n      if(mockMode) return;\n      HttpURLConnection urlc = (HttpURLConnection) url.openConnection();\n      if (HttpURLConnection.HTTP_OK != urlc.getResponseCode()) {\n        warn(\"Solr returned an error #\" + urlc.getResponseCode() + \n            \" \" + urlc.getResponseMessage() + \" for url \"+url);\n      }\n    } catch (IOException e) {\n      warn(\"An error occurred posting data to \"+url+\". Please check that Solr is running.\");\n    }\n  }\n\n  /**\n   * Reads data from the data stream and posts it to solr,\n   * writes to the response to output\n   * @return true if success\n   */\n  public boolean postData(InputStream data, Integer length, OutputStream output, String type, URL url) {\n    if(mockMode) return true;\n    boolean success = true;\n    if(type == null)\n      type = DEFAULT_CONTENT_TYPE;\n    HttpURLConnection urlc = null;\n    try {\n      try {\n        urlc = (HttpURLConnection) url.openConnection();\n        try {\n          urlc.setRequestMethod(\"POST\");\n        } catch (ProtocolException e) {\n          fatal(\"Shouldn't happen: HttpURLConnection doesn't support POST??\"+e);\n        }\n        urlc.setDoOutput(true);\n        urlc.setDoInput(true);\n        urlc.setUseCaches(false);\n        urlc.setAllowUserInteraction(false);\n        urlc.setRequestProperty(\"Content-type\", type);\n\n        if (null != length) urlc.setFixedLengthStreamingMode(length);\n\n      } catch (IOException e) {\n        fatal(\"Connection error (is Solr running at \" + solrUrl + \" ?): \" + e);\n        success = false;\n      }\n      \n      OutputStream out = null;\n      try {\n        out = urlc.getOutputStream();\n        pipe(data, out);\n      } catch (IOException e) {\n        fatal(\"IOException while posting data: \" + e);\n        success = false;\n      } finally {\n        try { if(out!=null) out.close(); } catch (IOException x) { /*NOOP*/ }\n      }\n      \n      InputStream in = null;\n      try {\n        if (HttpURLConnection.HTTP_OK != urlc.getResponseCode()) {\n          warn(\"Solr returned an error #\" + urlc.getResponseCode() + \n                \" \" + urlc.getResponseMessage());\n          success = false;\n        }\n\n        in = urlc.getInputStream();\n        pipe(in, output);\n      } catch (IOException e) {\n        warn(\"IOException while reading response: \" + e);\n        success = false;\n      } finally {\n        try { if(in!=null) in.close(); } catch (IOException x) { /*NOOP*/ }\n      }\n      \n    } finally {\n      if(urlc!=null) urlc.disconnect();\n    }\n    return success;\n  }\n\n  /**\n   * Converts a string to an input stream \n   * @param s the string\n   * @return the input stream\n   */\n  public static InputStream stringToStream(String s) {\n    InputStream is = null;\n    try {\n      is = new ByteArrayInputStream(s.getBytes(\"UTF-8\"));\n    } catch (UnsupportedEncodingException e) {\n      fatal(\"Shouldn't happen: UTF-8 not supported?!?!?!\");\n    }\n    return is;\n  }\n\n  /**\n   * Pipes everything from the source to the dest.  If dest is null, \n   * then everything is read from source and thrown away.\n   */\n  private static void pipe(InputStream source, OutputStream dest) throws IOException {\n    byte[] buf = new byte[1024];\n    int read = 0;\n    while ( (read = source.read(buf) ) >= 0) {\n      if (null != dest) dest.write(buf, 0, read);\n    }\n    if (null != dest) dest.flush();\n  }\n\n  public GlobFileFilter getFileFilterFromFileTypes(String fileTypes) {\n    String glob;\n    if(fileTypes.equals(\"*\"))\n      glob = \".*\";\n    else\n      glob = \"^.*\\\\.(\" + fileTypes.replace(\",\", \"|\") + \")$\";\n    return new GlobFileFilter(glob, true);\n  }\n\n  //\n  // Utility methods for XPath handing\n  //\n  \n  /**\n   * Gets all nodes matching an XPath\n   */\n  public static NodeList getNodesFromXP(Node n, String xpath) throws XPathExpressionException {\n    XPathFactory factory = XPathFactory.newInstance();\n    XPath xp = factory.newXPath();\n    XPathExpression expr = xp.compile(xpath);\n    return (NodeList) expr.evaluate(n, XPathConstants.NODESET);\n  }\n  \n  /**\n   * Gets the string content of the matching an XPath\n   * @param n the node (or doc)\n   * @param xpath the xpath string\n   * @param concatAll if true, text from all matching nodes will be concatenated, else only the first returned\n   */\n  public static String getXP(Node n, String xpath, boolean concatAll)\n      throws XPathExpressionException {\n    NodeList nodes = getNodesFromXP(n, xpath);\n    StringBuffer sb = new StringBuffer();\n    if (nodes.getLength() > 0) {\n      for(int i = 0; i < nodes.getLength() ; i++) {\n        sb.append(nodes.item(i).getNodeValue() + \" \");\n        if(!concatAll) break;\n      }\n      return sb.toString().trim();\n    } else\n      return \"\";\n  }\n  \n  /**\n   * Takes a string as input and returns a DOM \n   */\n  public static Document makeDom(String in, String inputEncoding) throws SAXException, IOException,\n  ParserConfigurationException {\n    InputStream is = new ByteArrayInputStream(in\n        .getBytes(inputEncoding));\n    Document dom = DocumentBuilderFactory.newInstance()\n        .newDocumentBuilder().parse(is);\n    return dom;\n  }\n\n  /**\n   * Inner class to filter files based on glob wildcards\n   */\n  class GlobFileFilter implements FileFilter\n  {\n    private String _pattern;\n    private Pattern p;\n    \n    public GlobFileFilter(String pattern, boolean isRegex)\n    {\n      _pattern = pattern;\n      if(!isRegex) {\n        _pattern = _pattern\n            .replace(\"^\", \"\\\\^\")\n            .replace(\"$\", \"\\\\$\")\n            .replace(\".\", \"\\\\.\")\n            .replace(\"(\", \"\\\\(\")\n            .replace(\")\", \"\\\\)\")\n            .replace(\"+\", \"\\\\+\")\n            .replace(\"*\", \".*\")\n            .replace(\"?\", \".\");\n        _pattern = \"^\" + _pattern + \"$\";\n      }\n      \n      try {\n        p = Pattern.compile(_pattern,Pattern.CASE_INSENSITIVE);\n      } catch(PatternSyntaxException e) {\n        fatal(\"Invalid type list \"+pattern+\". \"+e.getDescription());\n      }\n    }\n    \n    @Override\n    public boolean accept(File file)\n    {\n      return p.matcher(file.getName()).find();\n    }\n  }\n  \n  //\n  // Simple crawler class which can fetch a page and check for robots.txt\n  //\n  class PageFetcher {\n    Map<String, List<String>> robotsCache;\n    final String DISALLOW = \"Disallow:\";\n    \n    public PageFetcher() {\n      robotsCache = new HashMap<String,List<String>>();\n    }\n    \n    public PageFetcherResult readPageFromUrl(URL u) {\n      PageFetcherResult res = new PageFetcherResult();\n      try {\n        if (isDisallowedByRobots(u)) {\n          warn(\"The URL \"+u+\" is disallowed by robots.txt and will not be crawled.\");\n          res.httpStatus = 403;\n          visited.add(u);\n          return res;\n        }\n        res.httpStatus = 404;\n        HttpURLConnection conn = (HttpURLConnection) u.openConnection();\n        conn.setRequestProperty(\"User-Agent\", \"SimplePostTool-crawler/\"+VERSION_OF_THIS_TOOL+\" (http://lucene.apache.org/solr/)\");\n        conn.setRequestProperty(\"Accept-Encoding\", \"gzip, deflate\");\n        conn.connect();\n        res.httpStatus = conn.getResponseCode();\n        if(!normalizeUrlEnding(conn.getURL().toString()).equals(normalizeUrlEnding(u.toString()))) {\n          info(\"The URL \"+u+\" caused a redirect to \"+conn.getURL());\n          u = conn.getURL();\n          res.redirectUrl = u;\n          visited.add(u);\n        }\n        if(res.httpStatus == 200) {\n          // Raw content type of form \"text/html; encoding=utf-8\"\n          String rawContentType = conn.getContentType();\n          String type = rawContentType.split(\";\")[0];\n          if(typeSupported(type)) {\n            String encoding = conn.getContentEncoding();\n            InputStream is;\n            if (encoding != null && encoding.equalsIgnoreCase(\"gzip\")) {\n              is = new GZIPInputStream(conn.getInputStream());\n            } else if (encoding != null && encoding.equalsIgnoreCase(\"deflate\")) {\n              is = new InflaterInputStream(conn.getInputStream(), new Inflater(true));\n            } else {\n              is = conn.getInputStream();\n            }\n            \n            // Read into memory, so that we later can pull links from the page without re-fetching \n            res.content = inputStreamToByteArray(is);\n            is.close();\n          } else {\n            warn(\"Skipping URL with unsupported type \"+type);\n            res.httpStatus = 415;\n          }\n        }\n      } catch(IOException e) {\n        warn(\"IOException when reading page from url \"+u+\": \"+e.getMessage());\n      }\n      return res;\n    }\n    \n    public boolean isDisallowedByRobots(URL url) {\n      String host = url.getHost();\n      String strRobot = url.getProtocol() + \"://\" + host + \"/robots.txt\";\n      List<String> disallows = robotsCache.get(host);\n      if(disallows == null) {\n        disallows = new ArrayList<String>();\n        URL urlRobot;\n        try { \n          urlRobot = new URL(strRobot);\n          disallows = parseRobotsTxt(urlRobot.openStream());\n        } catch (MalformedURLException e) {\n          return true; // We cannot trust this robots URL, should not happen\n        } catch (IOException e) {\n          // There is no robots.txt, will cache an empty disallow list\n        }\n      }\n      \n      robotsCache.put(host, disallows);\n\n      String strURL = url.getFile();\n      for (String path : disallows) {\n        if (path.equals(\"/\") || strURL.indexOf(path) == 0)\n          return true;\n      }\n      return false;\n    }\n\n    /**\n     * Very simple robots.txt parser which obeys all Disallow lines regardless\n     * of user agent or whether there are valid Allow: lines.\n     * @param is Input stream of the robots.txt file\n     * @return a list of disallow paths\n     * @throws IOException if problems reading the stream\n     */\n    protected List<String> parseRobotsTxt(InputStream is) throws IOException {\n      List<String> disallows = new ArrayList<String>();\n      BufferedReader r = new BufferedReader(new InputStreamReader(is, \"UTF-8\"));\n      String l;\n      while((l = r.readLine()) != null) {\n        String[] arr = l.split(\"#\");\n        if(arr.length == 0) continue;\n        l = arr[0].trim();\n        if(l.startsWith(DISALLOW)) {\n          l = l.substring(DISALLOW.length()).trim();\n          if(l.length() == 0) continue;\n          disallows.add(l);\n        }\n      }\n      is.close();\n      return disallows;\n    }\n\n    /**\n     * Finds links on a web page, using /extract?extractOnly=true\n     * @param u the URL of the web page\n     * @param is the input stream of the page\n     * @param type the content-type\n     * @param postUrl the URL (typically /solr/extract) in order to pull out links\n     * @return a set of URLs parsed from the page\n     */\n    protected Set<URL> getLinksFromWebPage(URL u, InputStream is, String type, URL postUrl) {\n      Set<URL> l = new HashSet<URL>();\n      URL url = null;\n      try {\n        ByteArrayOutputStream os = new ByteArrayOutputStream();\n        URL extractUrl = new URL(appendParam(postUrl.toString(), \"extractOnly=true\"));\n        boolean success = postData(is, null, os, type, extractUrl);\n        if(success) {\n          String rawXml = os.toString(\"UTF-8\");\n          Document d = makeDom(rawXml, \"UTF-8\");\n          String innerXml = getXP(d, \"/response/str/text()[1]\", false);\n          d = makeDom(innerXml, \"UTF-8\");\n          NodeList links = getNodesFromXP(d, \"/html/body//a/@href\");\n          for(int i = 0; i < links.getLength(); i++) {\n            String link = links.item(i).getTextContent();\n            link = computeFullUrl(u, link);\n            if(link == null)\n              continue;\n            url = new URL(link);\n            if(url.getAuthority() == null || !url.getAuthority().equals(u.getAuthority()))\n              continue;\n            l.add(url);\n          }\n        }\n      } catch (MalformedURLException e) {\n        warn(\"Malformed URL \"+url);\n      } catch (IOException e) {\n        warn(\"IOException opening URL \"+url+\": \"+e.getMessage());\n      } catch (Exception e) {\n        throw new RuntimeException();\n      }\n      return l;\n    }\n  }\n    \n  /**\n   * Utility class to hold the result form a page fetch\n   */\n  public class PageFetcherResult {\n    int httpStatus = 200;\n    String contentType = \"text/html\";\n    URL redirectUrl = null;\n    byte[] content;\n  }\n}\n",
        "methodName": "postDirectory",
        "exampleID": 64,
        "dataset": "spotbugs",
        "filepath": "/solr/core/src/java/org/apache/solr/util/SimplePostTool.java",
        "line": "474",
        "source": "?",
        "sourceLine": "474",
        "qualifier": "Possible null pointer dereference of the $$value returned by listFiles()/$",
        "steps": [
            {
                "exampleID": 65
            }
        ],
        "line_number": "474"
    }
]